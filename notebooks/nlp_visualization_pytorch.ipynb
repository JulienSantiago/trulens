{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP Visualization Tests\n",
    "\n",
    "This notebook tests the visualization utilities for NLP as provided by `trulens.vis.nlp.NLP`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "# ! pip uninstall -y trulens\n",
    "\n",
    "# Use this if running this notebook from within its place in the truera repository.\n",
    "sys.path.insert(0, \"..\")\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "import torch\n",
    "from torch import Tensor\n",
    "from torch import nn\n",
    "import re\n",
    "from typing import Iterable, List\n",
    "\n",
    "from trulens.nn.models import get_model_wrapper\n",
    "from trulens.vis.nlp import NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_tokens = 32\n",
    "\n",
    "class ToyTokenizer:\n",
    "    def __init__(self, max_tokens: int = 8):\n",
    "        self.max_tokens = max_tokens\n",
    "\n",
    "        self.pad_token = '[PAD]'\n",
    "        self.mask_token = '[MASK]'\n",
    "        self.sep_token = '[SEP]'\n",
    "        self.cls_token = '[SEP]'\n",
    "        self.unk_token = '[UNK]'\n",
    "\n",
    "        self.vocab = {\n",
    "            self.unk_token: 0,\n",
    "            self.pad_token: 1,\n",
    "            self.mask_token: 2,\n",
    "            self.sep_token: 3,\n",
    "            \"neutral\": 4,\n",
    "            \"good\": 5,\n",
    "            \"bad\": 6\n",
    "        }\n",
    "        \n",
    "        self.ids = {i: k for k, i in self.vocab.items()}\n",
    "\n",
    "        self.pad_token_id = self.vocab[self.pad_token]\n",
    "        self.cls_token_id = self.vocab[self.cls_token]\n",
    "        self.sep_token_id = self.vocab[self.sep_token]\n",
    "        self.unk_token_id = self.vocab[self.unk_token]\n",
    "\n",
    "        self.pat = re.compile(r\"\\w+\")\n",
    "\n",
    "    def decode(self, token_id: int) -> str:\n",
    "        return self.ids[token_id]\n",
    "\n",
    "    def id_of_token(self, token: str) -> str:\n",
    "        if token in self.vocab:\n",
    "            return self.vocab[token]\n",
    "        else:\n",
    "            return self.vocab[self.unk_token]\n",
    "\n",
    "    def split_sentence_into_words(self, sentence: str) -> List[str]:\n",
    "        tokens = []\n",
    "        for token in re.findall(self.pat, sentence):\n",
    "            tokens.append(token)\n",
    "        return tokens\n",
    "\n",
    "    def _ids_of_tokens(self, tokens: Iterable[str]) -> Iterable[int]:\n",
    "        return list(map(self.id_of_token, tokens))\n",
    "\n",
    "    def tokenize(self, texts: Iterable[str]):\n",
    "        tokenization = []\n",
    "        masks = []\n",
    "\n",
    "        # track longest sentence length in number of tokens\n",
    "        n_tokens = 0\n",
    "\n",
    "        for sentence in texts:\n",
    "            tokens = [self.sep_token_id] + self._ids_of_tokens(self.split_sentence_into_words(sentence)) + [self.cls_token_id]\n",
    "\n",
    "            # crop to max_tokens\n",
    "            tokens = tokens[0:self.max_tokens]\n",
    "\n",
    "            tokenization.append(tokens)\n",
    "            \n",
    "            if len(tokens) > n_tokens:\n",
    "                n_tokens = len(tokens)\n",
    "\n",
    "        # pad to max length input\n",
    "        for tokens in tokenization:\n",
    "            mask = [1] * len(tokens)\n",
    "            while len(tokens) < n_tokens:\n",
    "                tokens.append(self.pad_token_id)\n",
    "                mask.append(0)\n",
    "            masks.append(mask)\n",
    "\n",
    "        tokenization = torch.tensor(tokenization)\n",
    "        masks = torch.tensor(masks)\n",
    "\n",
    "        return dict(input_ids=tokenization, attention_mask=masks)\n",
    "\n",
    "    \n",
    "tokenizer = ToyTokenizer(max_tokens=max_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Test the model to make sure it is 100% accurate on its task.\n",
    "def generate_dataset(n, l):\n",
    "    \"\"\"Generate random sentiment sentences and their labels.\"\"\"\n",
    "\n",
    "    ret = []\n",
    "    cls = []\n",
    "    for i in range(n):\n",
    "        sent = []\n",
    "        while len(sent) < l:\n",
    "            r = random.random()\n",
    "\n",
    "            word = \"neutral\"\n",
    "\n",
    "            if r > 0.9 and len(sent) > 0:\n",
    "                continue\n",
    "            elif r > 0.8:\n",
    "                word = \"good\"\n",
    "            elif r > 0.7:\n",
    "                word = \"bad\"\n",
    "\n",
    "            sent.append(word)\n",
    "\n",
    "        ret.append(\" \".join(sent))\n",
    "\n",
    "        gt = 0 # neutral\n",
    "        if \"good\" in sent and \"bad\" in sent:\n",
    "            gt = 3 # confused\n",
    "        elif \"good\" in sent:\n",
    "            gt = 1 # positive\n",
    "        elif \"bad\" in sent:\n",
    "            gt = 2 # negative\n",
    "\n",
    "        cls.append(gt)\n",
    "\n",
    "    return pd.DataFrame(dict(sentence=ret, sentiment=cls))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = generate_dataset(1000, max_tokens-1)\n",
    "sentences = dataset['sentence'].to_numpy()\n",
    "sentiments = dataset['sentiment'].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Outputs:\n",
    "    logits: Tensor = None\n",
    "    probits: Tensor = None\n",
    "\n",
    "class Sentiment(torch.nn.Module):\n",
    "    def set_parameters(self) -> None:\n",
    "        \"\"\"Set model parameters as per fixed specification.\"\"\"\n",
    "\n",
    "        Wi = torch.zeros_like(self.lstm.weight_ih_l0)\n",
    "        bi = torch.zeros_like(self.lstm.bias_ih_l0)\n",
    "        Wh = torch.zeros_like(self.lstm.weight_hh_l0)\n",
    "        bh = torch.zeros_like(self.lstm.bias_hh_l0)\n",
    "\n",
    "        big = 8.0 # Multipliers to help dealing with LSTM sigmoids.\n",
    "        half = 4.0 # Intention here is that sigmoid((x*big) - half) is ~0 if x is ~0; and\n",
    "                    # ~1 when x is >~ 1.\n",
    "\n",
    "        S_NEUTRAL = 0\n",
    "        S_GOOD = 1\n",
    "        S_BAD = 2\n",
    "        S_GOOD_TO_BAD = 3\n",
    "        S_BAD_TO_GOOD = 4\n",
    "        W_UNKNOWN = 0\n",
    "        W_NEUTRAL = tokenizer.vocab['neutral']\n",
    "        W_GOOD = tokenizer.vocab['good']\n",
    "        W_BAD = tokenizer.vocab['bad']\n",
    "\n",
    "        hs = self.hidden_size\n",
    "\n",
    "        # make sure c gate is always big\n",
    "        bi[0:hs*3] = big*10.0\n",
    "        bh[0:hs*3] = big*10.0\n",
    "\n",
    "        # o gate weights:\n",
    "        Wi[3*hs+S_GOOD,    W_GOOD]    = big # read good word\n",
    "        Wi[3*hs+S_BAD,     W_BAD]     = big # read bad word\n",
    "        Wh[3*hs+S_NEUTRAL, S_NEUTRAL] = big * 10.0 # keep prior neutral, good, bad states\n",
    "        Wh[3*hs+S_GOOD,    S_GOOD]    = big * 10.0 # \n",
    "        Wh[3*hs+S_BAD,     S_BAD]     = big * 10.0 #\n",
    "        bi[3*hs:4*hs] = -half * 1.5 # sigmoid will be 0 unless one of good/bad words was read\n",
    "        # bh[3*hs:4*hs] = -big * 10.0\n",
    "\n",
    "        # set \"good to bad\" confused if prior was good, and input was bad\n",
    "        Wh[3*hs+S_GOOD_TO_BAD, S_GOOD]        = big      # (prior state was good\n",
    "        Wi[3*hs+S_GOOD_TO_BAD, W_BAD]         = big      #  and input was bad)\n",
    "        Wh[3*hs+S_GOOD_TO_BAD, S_GOOD_TO_BAD] = 10.0*big # or (was already in this confused state)\n",
    "        bh[3*hs+S_GOOD_TO_BAD] = -(half*1.85) # Want at least 2 of first two to fire, or just the last one to fire.\n",
    "\n",
    "        # set \"bad to good\" confused if prior was bad, and input was good\n",
    "        Wh[3*hs+S_BAD_TO_GOOD, S_BAD]         = big      # (prior state was bad\n",
    "        Wi[3*hs+S_BAD_TO_GOOD, W_GOOD]        = big      #  and input was good)\n",
    "        Wh[3*hs+S_BAD_TO_GOOD, S_BAD_TO_GOOD] = 10.0*big # or (was already confused)\n",
    "        bh[3*hs+S_BAD_TO_GOOD] = -(half*1.85) #\n",
    "\n",
    "        self.lstm.weight_hh_l0 = nn.Parameter(Wh)\n",
    "        self.lstm.bias_hh_l0 = nn.Parameter(bh)\n",
    "        self.lstm.weight_ih_l0 = nn.Parameter(Wi)\n",
    "        self.lstm.bias_ih_l0 = nn.Parameter(bi)\n",
    "\n",
    "        self.embedding.weight = nn.Parameter(torch.eye(self.emb_size))\n",
    "\n",
    "        self.logits.weight = nn.Parameter(torch.tensor(\n",
    "            [\n",
    "                [10.0, 0.0,  0.0,  0.0,  0.0],\n",
    "                [0.0, 20.0,  0.0,  0.0,  0.0],\n",
    "                [0.0,  0.0, 20.0,  0.0,  0.0],\n",
    "                [0.0,  0.0,  0.0, 30.0, 30.0]\n",
    "            ]\n",
    "        ))\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.labels = ['0', '+', '-', '?']\n",
    "\n",
    "        self.emb_size = 7 # len(tokanizer.vocab)\n",
    "\n",
    "        self.hidden_size = 5\n",
    "        # 5 states, one for neutral, one for positive, one for negative, and two for confused. Requiring two\n",
    "        # confused states for simplicity of the model; it is easier to encode semantics of confusion based on \n",
    "        # which initial positive/negative state was set first.\n",
    "\n",
    "        # Identity embedding, each vocab word has its own dimension where its presence is encoded.\n",
    "        self.embedding = nn.Embedding(\n",
    "            padding_idx=0, \n",
    "            embedding_dim=self.emb_size, \n",
    "            num_embeddings=self.emb_size\n",
    "        )\n",
    "\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=self.emb_size,\n",
    "            hidden_size=self.hidden_size,\n",
    "            num_layers=1,\n",
    "            batch_first=True\n",
    "        )\n",
    "\n",
    "        # Linear layer to combine the two types of confused state and weight things so that\n",
    "        # confused outweighs positive and negative, while positive and negative outweigh neutral \n",
    "        # if more than one of these states is set.\n",
    "        self.logits = torch.nn.Linear(\n",
    "            in_features=self.hidden_size,\n",
    "            out_features=4,\n",
    "            bias=False\n",
    "        )\n",
    "\n",
    "        # Finally add a softmax for classification.\n",
    "        self.softmax = torch.nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self,\n",
    "        inputs_embeds: torch.Tensor = None,\n",
    "        attention_mask: torch.Tensor = None, \n",
    "        input_ids: torch.Tensor = None, \n",
    "        token_type_ids: torch.Tensor = None\n",
    "    ) -> torch.Tensor:\n",
    "        # Signature and some functionality imitiates huggingface models.\n",
    "\n",
    "        if input_ids is not None:\n",
    "            batch_size = input_ids.shape[0]\n",
    "            device = input_ids.device\n",
    "        else:\n",
    "            batch_size = inputs_embeds.shape[0]\n",
    "            device = inputs_embeds.device\n",
    "\n",
    "        h0 = torch.zeros(1, batch_size, self.hidden_size).to(device)\n",
    "        h0[:,:,0] = 10.0 # initial state is neutral\n",
    "        c0 = 10.0 * torch.ones(1, batch_size, self.hidden_size).to(device)\n",
    "\n",
    "        if inputs_embeds is None:\n",
    "            embeds = self.embedding(input_ids)\n",
    "        else:\n",
    "            embeds = inputs_embeds\n",
    "\n",
    "        _, (hn, _) = self.lstm(embeds, (h0, c0))\n",
    "\n",
    "        logits = self.logits(hn)[0]\n",
    "\n",
    "        probits = self.softmax(logits)\n",
    "\n",
    "        # outputs also imitate hugging face\n",
    "        return Outputs(logits=logits, probits=probits)\n",
    "\n",
    "model = Sentiment()\n",
    "model.set_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = model(**tokenizer.tokenize(sentences)).logits.detach().numpy()\n",
    "preds = np.argmax(scores, axis=1)\n",
    "acc = (preds == sentiments).mean()\n",
    "print(f\"accuracy={100.0*acc:0.3f}%\")\n",
    "\n",
    "wrongs = np.argwhere(preds != sentiments)\n",
    "for t, p, gt in zip(sentences[wrongs], preds[wrongs], sentiments[wrongs]):\n",
    "    print (model.labels[p[0]], model.labels[gt[0]], t, tokenizer.tokenize(t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wrapper = get_model_wrapper(model, input_shape=(None, tokenizer.max_tokens), device=\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wrapper.print_layer_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts=['good', \"bad\", \"nothing\", \"good and bad\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Minimal usage provides only tokenize and input_accessor if neccessary.\n",
    "\n",
    "V = NLP(\n",
    "    tokenize=lambda sentences: tokenizer.tokenize(sentences),\n",
    "    input_accessor=lambda x: x['input_ids'],\n",
    ")\n",
    "V.tokens(texts=texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test hidden tokens\n",
    "\n",
    "V = NLP(\n",
    "    tokenize=lambda sentences: tokenizer.tokenize(sentences),\n",
    "    input_accessor=lambda x: x['input_ids'],\n",
    "    hidden_tokens=set([tokenizer.pad_token_id])\n",
    "    # do not display these tokens\n",
    ")\n",
    "V.tokens(texts=texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test decode to show readable representations of token id's.\n",
    "\n",
    "V = NLP(\n",
    "    decode=lambda x: tokenizer.decode(x),\n",
    "    tokenize=lambda sentences: tokenizer.tokenize(sentences),\n",
    "    input_accessor=lambda x: x['input_ids'],\n",
    "    hidden_tokens=set([tokenizer.pad_token_id])\n",
    ")\n",
    "V.tokens(texts=texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show token id's alongside readable forms.\n",
    "\n",
    "V.tokens(texts=texts, show_id=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Also show pre-tokenized text.\n",
    "\n",
    "V.tokens(texts=texts, show_id=True, show_text=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test model outputs.\n",
    "\n",
    "V = NLP(\n",
    "    wrapper=wrapper,\n",
    "    # labels=model.labels,\n",
    "    decode=lambda x: tokenizer.decode(x),\n",
    "    tokenize=lambda sentences: tokenizer.tokenize(sentences),\n",
    "    # huggingface models can take as input the keyword args as per produced by their tokenizers.\n",
    "\n",
    "    input_accessor=lambda x: x['input_ids'],\n",
    "    # for huggingface models, input/token ids are under input_ids key in the input dictionary\n",
    "\n",
    "    output_accessor=lambda x: x.logits,\n",
    "    # and logits under 'logits' key in the output dictionary\n",
    "\n",
    "    hidden_tokens=set([tokenizer.pad_token_id])\n",
    "    # do not display these tokens\n",
    ")\n",
    "V.tokens(texts=texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test model outputs with labels.\n",
    "\n",
    "V = NLP(\n",
    "    wrapper=wrapper,\n",
    "    labels=model.labels,\n",
    "    decode=lambda x: tokenizer.decode(x),\n",
    "    tokenize=lambda sentences: tokenizer.tokenize(sentences),\n",
    "    # huggingface models can take as input the keyword args as per produced by their tokenizers.\n",
    "\n",
    "    input_accessor=lambda x: x['input_ids'],\n",
    "    # for huggingface models, input/token ids are under input_ids key in the input dictionary\n",
    "\n",
    "    output_accessor=lambda x: x.logits,\n",
    "    # and logits under 'logits' key in the output dictionary\n",
    "\n",
    "    hidden_tokens=set([tokenizer.pad_token_id])\n",
    "    # do not display these tokens\n",
    ")\n",
    "V.tokens(texts=texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test attributions; various QoI, point DoI.\n",
    "\n",
    "from trulens.nn.distributions import PointDoi, GaussianDoi, LinearDoi\n",
    "from trulens.nn.attribution import InternalInfluence, IntegratedGradients, Cut, OutputCut\n",
    "from trulens.nn.quantities import MaxClassQoI, ClassQoI, ComparativeQoI\n",
    "\n",
    "common_args = dict(doi=PointDoi(Cut('embedding')), model=wrapper, cuts=(Cut('embedding'), OutputCut(accessor=lambda o: o.logits)))\n",
    "\n",
    "attributors = [\n",
    "    InternalInfluence(\n",
    "        qoi=MaxClassQoI(),\n",
    "        **common_args\n",
    "    ),\n",
    "    InternalInfluence(\n",
    "        qoi=ClassQoI(1),\n",
    "        **common_args\n",
    "    ),\n",
    "    InternalInfluence(\n",
    "        qoi=ClassQoI(3),\n",
    "        **common_args\n",
    "    ),\n",
    "    InternalInfluence(\n",
    "        qoi=ComparativeQoI(1,2),\n",
    "        **common_args\n",
    "    )  \n",
    "]\n",
    "\n",
    "for infl in attributors:\n",
    "\n",
    "    V = NLP(\n",
    "        wrapper=wrapper,\n",
    "        labels=model.labels,\n",
    "        decode=lambda x: tokenizer.decode(x),\n",
    "        tokenize=lambda sentences: tokenizer.tokenize(sentences),\n",
    "        # huggingface models can take as input the keyword args as per produced by their tokenizers.\n",
    "\n",
    "        input_accessor=lambda x: x['input_ids'],\n",
    "        # for huggingface models, input/token ids are under input_ids key in the input dictionary\n",
    "\n",
    "        output_accessor=lambda x: x.logits,\n",
    "        # and logits under 'logits' key in the output dictionary\n",
    "\n",
    "        hidden_tokens=set([tokenizer.pad_token_id])\n",
    "        # do not display these tokens\n",
    "    )\n",
    "\n",
    "    display(V.tokens(texts=texts, attributor=infl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_args = dict(qoi=MaxClassQoI(), model=wrapper, cuts=(Cut('embedding'), OutputCut(accessor=lambda o: o.logits)))\n",
    "\n",
    "attributors = [\n",
    "    InternalInfluence(\n",
    "        doi=PointDoi(Cut('embedding')),\n",
    "        **common_args\n",
    "    ),\n",
    "    InternalInfluence(\n",
    "        doi=GaussianDoi(var=0.1, resolution=10, cut=Cut('embedding')),\n",
    "        **common_args\n",
    "    ),\n",
    "    InternalInfluence(\n",
    "        doi=LinearDoi(resolution=10, cut=Cut('embedding')),\n",
    "        **common_args\n",
    "    ),\n",
    "]\n",
    "\n",
    "for infl in attributors:\n",
    "\n",
    "    V = NLP(\n",
    "        wrapper=wrapper,\n",
    "        labels=model.labels,\n",
    "        decode=lambda x: tokenizer.decode(x),\n",
    "        tokenize=lambda sentences: tokenizer.tokenize(sentences),\n",
    "        # huggingface models can take as input the keyword args as per produced by their tokenizers.\n",
    "\n",
    "        input_accessor=lambda x: x['input_ids'],\n",
    "        # for huggingface models, input/token ids are under input_ids key in the input dictionary\n",
    "\n",
    "        output_accessor=lambda x: x.logits,\n",
    "        # and logits under 'logits' key in the output dictionary\n",
    "\n",
    "        hidden_tokens=set([tokenizer.pad_token_id])\n",
    "        # do not display these tokens\n",
    "    )\n",
    "\n",
    "    display(V.tokens(texts=texts, attributor=infl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test stability pairs.\n",
    "\n",
    "V = NLP(\n",
    "    wrapper=wrapper,\n",
    "    labels=model.labels,\n",
    "    decode=lambda x: tokenizer.decode(x),\n",
    "    tokenize=lambda sentences: tokenizer.tokenize(sentences),\n",
    "    # huggingface models can take as input the keyword args as per produced by their tokenizers.\n",
    "\n",
    "    input_accessor=lambda x: x['input_ids'],\n",
    "    # for huggingface models, input/token ids are under input_ids key in the input dictionary\n",
    "\n",
    "    output_accessor=lambda x: x.logits,\n",
    "    # and logits under 'logits' key in the output dictionary\n",
    "\n",
    "    hidden_tokens=set([tokenizer.pad_token_id])\n",
    "    # do not display these tokens\n",
    ")\n",
    "\n",
    "texts1 = ['this is good', \"good good bad good\"]\n",
    "texts2 = ['this is bad', \"good bad bad bad\"]\n",
    "\n",
    "V.tokens_stability(texts1=texts1, texts2=texts2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test doi enumeration.\n",
    "\n",
    "common_args = dict(return_grads=True, return_doi=True, qoi=MaxClassQoI(), model=wrapper, cuts=(Cut('embedding'), OutputCut(accessor=lambda o: o.logits)))\n",
    "\n",
    "attributors = [\n",
    "    InternalInfluence(\n",
    "        doi=PointDoi(Cut('embedding')),\n",
    "        **common_args\n",
    "    ),\n",
    "    InternalInfluence(\n",
    "        doi=GaussianDoi(var=0.4, resolution=20, cut=Cut('embedding')),\n",
    "        **common_args\n",
    "    ),\n",
    "    InternalInfluence(\n",
    "        doi=LinearDoi(resolution=20, cut=Cut('embedding')),\n",
    "        **common_args\n",
    "    ),\n",
    "]\n",
    "\n",
    "for infl in attributors:\n",
    "\n",
    "    V = NLP(\n",
    "        wrapper=wrapper,\n",
    "        labels=model.labels,\n",
    "        decode=lambda x: tokenizer.decode(x),\n",
    "        tokenize=lambda sentences: tokenizer.tokenize(sentences),\n",
    "        embedder=model.embedding,\n",
    "        embeddings=list(model.embedding.parameters())[0].detach().numpy(),\n",
    "        # huggingface models can take as input the keyword args as per produced by their tokenizers.\n",
    "\n",
    "        input_accessor=lambda x: x['input_ids'],\n",
    "        # for huggingface models, input/token ids are under input_ids key in the input dictionary\n",
    "\n",
    "        output_accessor=lambda x: x.logits,\n",
    "        # and logits under 'logits' key in the output dictionary\n",
    "\n",
    "        hidden_tokens=set([tokenizer.pad_token_id])\n",
    "        # do not display these tokens\n",
    "    )\n",
    "\n",
    "    display(f\"doi={infl.doi}\")\n",
    "    display(V.tokens(texts=texts, attributor=infl, show_doi=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test doi enumeration with token stability pairs.\n",
    "\n",
    "common_args = dict(\n",
    "    return_grads=True, \n",
    "    return_doi=True, \n",
    "    qoi=MaxClassQoI(), \n",
    "    model=wrapper, \n",
    "    cuts=(Cut('embedding'), OutputCut(accessor=lambda o: o.logits))\n",
    ")\n",
    "\n",
    "attributors = [\n",
    "    InternalInfluence(\n",
    "        doi=PointDoi(Cut('embedding')),\n",
    "        **common_args\n",
    "    ),\n",
    "    InternalInfluence(\n",
    "        doi=GaussianDoi(var=0.1, resolution=10, cut=Cut('embedding')),\n",
    "        **common_args\n",
    "    ),\n",
    "    InternalInfluence(\n",
    "        doi=LinearDoi(resolution=10, cut=Cut('embedding')),\n",
    "        **common_args\n",
    "    ),\n",
    "]\n",
    "\n",
    "for infl in attributors:\n",
    "\n",
    "    V = NLP(\n",
    "        wrapper=wrapper,\n",
    "        labels=model.labels,\n",
    "        decode=lambda x: tokenizer.decode(x),\n",
    "        tokenize=lambda sentences: tokenizer.tokenize(sentences),\n",
    "        embedder=model.embedding,\n",
    "        embeddings=list(model.embedding.parameters())[0].detach().numpy(),\n",
    "        # huggingface models can take as input the keyword args as per produced by their tokenizers.\n",
    "\n",
    "        input_accessor=lambda x: x['input_ids'],\n",
    "        # for huggingface models, input/token ids are under input_ids key in the input dictionary\n",
    "\n",
    "        output_accessor=lambda x: x.logits,\n",
    "        # and logits under 'logits' key in the output dictionary\n",
    "\n",
    "        hidden_tokens=set([tokenizer.pad_token_id])\n",
    "        # do not display these tokens\n",
    "    )\n",
    "\n",
    "    display(V.tokens_stability(texts1=texts1, texts2=texts2, attributor=infl, show_doi=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test IG baselines.\n",
    "\n",
    "from trulens.utils.nlp import token_baseline_swap\n",
    "from trulens.utils.typing import ModelInputs\n",
    "\n",
    "inputs_swap_baseline_ids, inputs_swap_baseline_embeddings = token_baseline_swap(\n",
    "    \n",
    "    token_pairs = [(\n",
    "        tokenizer.vocab['good'],\n",
    "        tokenizer.vocab['bad']\n",
    "    )],\n",
    "\n",
    "    input_accessor=lambda x: x.kwargs['input_ids'],\n",
    "\n",
    "    ids_to_embeddings=model.embedding\n",
    "    # Callable to produce embeddings from token ids.\n",
    ")\n",
    "\n",
    "from trulens.utils.nlp import token_baseline\n",
    "\n",
    "inputs_baseline_ids, inputs_baseline_embeddings = token_baseline(\n",
    "    keep_tokens=set([tokenizer.cls_token_id, tokenizer.sep_token_id]),\n",
    "    # Which tokens to preserve.\n",
    "\n",
    "    replacement_token=tokenizer.pad_token_id,\n",
    "    # What to replace tokens with.\n",
    "\n",
    "    input_accessor=lambda x: x.kwargs['input_ids'],\n",
    "    # input_accessor = lambda x: x, \n",
    "\n",
    "    ids_to_embeddings=model.embedding\n",
    "    # Callable to produce embeddings from token ids.\n",
    ")\n",
    "\n",
    "common_args = dict(\n",
    "    return_grads=True, \n",
    "    return_doi=True, \n",
    "    qoi=ClassQoI(1), \n",
    "    model=wrapper, \n",
    "    cuts=(Cut('embedding'), OutputCut(accessor=lambda o: o.logits))\n",
    ")\n",
    "\n",
    "attributors = [InternalInfluence(\n",
    "        doi=LinearDoi(resolution=20, cut=Cut('embedding'), baseline=baseline),\n",
    "        **common_args\n",
    "    ) for baseline in [\n",
    "        None,\n",
    "        inputs_baseline_embeddings,\n",
    "        inputs_swap_baseline_embeddings\n",
    "    ]]\n",
    "\n",
    "for infl in attributors:\n",
    "\n",
    "    V = NLP(\n",
    "        wrapper=wrapper,\n",
    "        labels=model.labels,\n",
    "        decode=lambda x: tokenizer.decode(x),\n",
    "        tokenize=lambda sentences: tokenizer.tokenize(sentences),\n",
    "        embedder=model.embedding,\n",
    "        embeddings=list(model.embedding.parameters())[0].detach().numpy(),\n",
    "        # huggingface models can take as input the keyword args as per produced by their tokenizers.\n",
    "\n",
    "        input_accessor=lambda x: x['input_ids'],\n",
    "        # for huggingface models, input/token ids are under input_ids key in the input dictionary\n",
    "\n",
    "        output_accessor=lambda x: x.logits,\n",
    "        # and logits under 'logits' key in the output dictionary\n",
    "\n",
    "        hidden_tokens=set([tokenizer.pad_token_id])\n",
    "        # do not display these tokens\n",
    "    )\n",
    "    display(f\"baseline={infl.doi.baseline}\")\n",
    "\n",
    "    display(V.tokens(texts=texts, attributor=infl, show_doi=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check different embedding distance methods for finding closest tokens for interventions\n",
    "\n",
    "common_args = dict(\n",
    "    return_grads=True, \n",
    "    return_doi=True, \n",
    "    qoi=ClassQoI(1), \n",
    "    model=wrapper, \n",
    "    cuts=(Cut('embedding'), OutputCut(accessor=lambda o: o.logits))\n",
    ")\n",
    "\n",
    "attributor = InternalInfluence(\n",
    "        doi=LinearDoi(resolution=20, cut=Cut('embedding'), baseline=inputs_baseline_embeddings),\n",
    "        **common_args\n",
    "    )\n",
    "\n",
    "for dist in ['l1', 'l2', 'cosine']:\n",
    "\n",
    "    V = NLP(\n",
    "        wrapper=wrapper,\n",
    "        labels=model.labels,\n",
    "        decode=lambda x: tokenizer.decode(x),\n",
    "        tokenize=lambda sentences: tokenizer.tokenize(sentences),\n",
    "        embedder=model.embedding,\n",
    "        embeddings=list(model.embedding.parameters())[0].detach().numpy(),\n",
    "        embedding_distance=dist,\n",
    "        # huggingface models can take as input the keyword args as per produced by their tokenizers.\n",
    "\n",
    "        input_accessor=lambda x: x['input_ids'],\n",
    "        # for huggingface models, input/token ids are under input_ids key in the input dictionary\n",
    "\n",
    "        output_accessor=lambda x: x.logits,\n",
    "        # and logits under 'logits' key in the output dictionary\n",
    "\n",
    "        hidden_tokens=set([tokenizer.pad_token_id])\n",
    "        # do not display these tokens\n",
    "    )\n",
    "    display(f\"distance={dist}\")\n",
    "\n",
    "    display(V.tokens(texts=[\"good and bad\"], attributor=attributor, show_doi=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "5c887fbf053ca26987bbc53439e0f6b5aed35740853e2f251f93c3825002a8ec"
  },
  "kernelspec": {
   "display_name": "Python 3.7.13 ('python37_pytorch_cuda')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
