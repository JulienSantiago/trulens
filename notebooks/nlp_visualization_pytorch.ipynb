{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP Visualization Tests\n",
    "\n",
    "This notebook tests the visualization utilities for NLP as provided by `trulens.vis.nlp.NLP`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "# ! pip uninstall -y trulens\n",
    "\n",
    "# Use this if running this notebook from within its place in the truera repository.\n",
    "sys.path.insert(0, \"..\")\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "import torch\n",
    "from torch import Tensor\n",
    "from torch import nn\n",
    "import re\n",
    "from typing import Iterable, List\n",
    "\n",
    "from trulens.nn.models import get_model_wrapper\n",
    "from trulens.vis.nlp import NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Generic, TypeVar\n",
    "\n",
    "Token = str\n",
    "Word = str\n",
    "Part = TypeVar(\"Part\", Token, Word)\n",
    "\n",
    "max_tokens = 8\n",
    "\n",
    "# Collections of synonyms for test data generation for toy sentiment models.\n",
    "GOOD_SYNONYMS = [\"good\", \"alright\", \"buena\", \"well\", \"nice\", \"decent\", \"best\"]\n",
    "BAD_SYNONYMS = [\n",
    "    \"bad\", \"mala\", \"naughty\", \"rotten\", \"amiss\", \"wicked\", \"negative\",\n",
    "    \"unfavourable\", \"horrid\"\n",
    "]\n",
    "NEUTRAL_SYNONYMS = [\n",
    "    \"neutral\"\n",
    "]  #, \"neutrality\"] # not including neutrality as the greedy tokenizer cannot handle it due to it having a prefix that is another token.\n",
    "SYNONYMS = dict(good=GOOD_SYNONYMS, bad=BAD_SYNONYMS, neutral=NEUTRAL_SYNONYMS)\n",
    "\n",
    "@dataclass\n",
    "class Span(Generic[Part]):\n",
    "    \"\"\"\n",
    "    Tokens or words along with indices into the string from which they were\n",
    "    derived.\n",
    "    \"\"\"\n",
    "\n",
    "    item: Part\n",
    "    begin: int\n",
    "    end: int\n",
    "\n",
    "class GreedyTokenizer:\n",
    "    # The following special tokens mimic use of huggingface tokenizers like Bert.\n",
    "\n",
    "    # The following tokens must be unique.\n",
    "\n",
    "    # Whitespace/separator.\n",
    "    space_token = \"[SPC]\"\n",
    "\n",
    "    # Input separator, used at end of an token input sequence\n",
    "    sep_token = \"[SEP]\"\n",
    "\n",
    "    # Unknown tokens, everything that is not a vocabulary word mapped to unknown.\n",
    "    unk_token = \"[UNK]\"\n",
    "\n",
    "    # Padding to input token sequences up to a fixed length.\n",
    "    pad_token = \"[PAD]\"\n",
    "\n",
    "    # Begining of input token. Token sequences start with this.\n",
    "    cls_token = \"[CLS]\"\n",
    "\n",
    "    # Regex for various separator characters including whitespace and punctuation.\n",
    "    r_whitespace = r\"(?P<whitespace>[\\s\\.\\!\\;\\:])\"\n",
    "\n",
    "    # Regex for non-separator characters.\n",
    "    r_blackspace = r\"(?P<blackspace>\\S)\"\n",
    "\n",
    "    def __init__(self, max_tokens: int = 8):\n",
    "        self.mask_token = '[MASK]'\n",
    "        self.sep_token = GreedyTokenizer.sep_token\n",
    "        self.cls_token = GreedyTokenizer.cls_token\n",
    "\n",
    "        # parent defines accessors:\n",
    "        self.pad_token = GreedyTokenizer.pad_token\n",
    "        self.unk_token = GreedyTokenizer.unk_token\n",
    "\n",
    "        self.vocab = {\n",
    "            self.unk_token: 0,\n",
    "            self.pad_token: 1,\n",
    "            self.mask_token: 2,\n",
    "            self.sep_token: 3,\n",
    "            self.cls_token: 4,\n",
    "            \"neutral\": 5,\n",
    "            \"good\": 6,\n",
    "            \"bad\": 7\n",
    "        }\n",
    "\n",
    "        # Also add synonyms that tokenize to the same tokens as the original words.\n",
    "        for token, synonyms in SYNONYMS.items():\n",
    "            for syn in synonyms:\n",
    "                self.vocab[syn] = self.vocab[token]\n",
    "\n",
    "        self.ids = {i: k for k, i in self.vocab.items()}\n",
    "\n",
    "        self.pad_token_id = self.vocab[self.pad_token]\n",
    "        self.cls_token_id = self.vocab[self.cls_token]\n",
    "        self.sep_token_id = self.vocab[self.sep_token]\n",
    "        self.unk_token_id = self.vocab[self.unk_token]\n",
    "\n",
    "        self.normal_tokens = [tok for tok in self.vocab.keys() if tok[0] != \"[\"]\n",
    "\n",
    "        # Regex for all normal (non-special) tokens.\n",
    "        self.r_toks = \"(?P<token>\" + (\n",
    "            \"|\".join(re.escape(tok) for tok in self.normal_tokens)\n",
    "        ) + \")\"\n",
    "\n",
    "        self.max_tokens = max_tokens\n",
    "\n",
    "        # Regex pattern for finding a token, a whitespace, or a non-whitespace.\n",
    "        # It is important that non-whitespace comes after tokens as they are\n",
    "        # potentially made of the same characters, with token having precedence.\n",
    "        self.pattern = re.compile(\n",
    "            (\n",
    "                \"|\".join(\n",
    "                    [\n",
    "                        self.r_toks, GreedyTokenizer.r_whitespace,\n",
    "                        GreedyTokenizer.r_blackspace\n",
    "                    ]\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "\n",
    "    def decode(self, token_id: int) -> str:\n",
    "        return self.ids[token_id]\n",
    "\n",
    "    def _greedy_tokenize(self, text: str) -> List[Span]:\n",
    "        \"\"\"\n",
    "        Tokenize a text string into token spans in a greedy manner.\n",
    "        \"\"\"\n",
    "        spans = []\n",
    "\n",
    "        # Accumulators for constructing separator and unknown tokens. Regexp\n",
    "        # reads whitespace and \"blackspace\" characters, one at a time, if no\n",
    "        # token is found. These need to be accumulated into one large spacing or\n",
    "        # unknown token.\n",
    "        current_type = None\n",
    "\n",
    "        # For every match of main pattern.\n",
    "        for m in self.pattern.finditer(text.lower()):\n",
    "            # Type of match (token/whitespace/blackspace)\n",
    "            type = m.lastgroup\n",
    "            # Matching string.\n",
    "            tok = m.groupdict()[type]\n",
    "            # Its span in input text.\n",
    "            span = m.span(type)\n",
    "\n",
    "            # Replace whitespace and blackspace matches with temporary special\n",
    "            # token indicators.\n",
    "            if type == \"token\":\n",
    "                pass\n",
    "            elif type == \"whitespace\":\n",
    "                tok = self.space_token\n",
    "            elif type == \"blackspace\":\n",
    "                tok = self.unk_token\n",
    "\n",
    "            # Accumulte separators and unknown tokens.\n",
    "            if type != \"token\":\n",
    "                if current_type == type:\n",
    "                    spans[-1].end = span[1]\n",
    "                else:\n",
    "                    current_type = type\n",
    "                    spans.append(\n",
    "                        Span(tok, begin=span[0], end=span[1])\n",
    "                    )\n",
    "\n",
    "            # Otherwise append a non-special token.\n",
    "            else:\n",
    "                spans.append(Span(tok, begin=span[0], end=span[1]))\n",
    "                current_type = None\n",
    "\n",
    "        return spans\n",
    "\n",
    "    def _tokenize(self, text: str) -> dict:\n",
    "        all_spans = self._greedy_tokenize(text)\n",
    "\n",
    "        spans = []\n",
    "        input_ids = []\n",
    "\n",
    "        def add_special(tok):\n",
    "            spans.append(Span(item=tok, begin=0, end=0))\n",
    "            input_ids.append(self.vocab[tok])\n",
    "\n",
    "        add_special(self.sep_token)\n",
    "\n",
    "        for span in all_spans:\n",
    "            if span.item in [self.space_token]:\n",
    "                pass\n",
    "            else:\n",
    "                spans.append(span)\n",
    "                input_ids.append(self.vocab[span.item])\n",
    "\n",
    "            if len(spans) + 1 >= self.max_tokens:\n",
    "                break\n",
    "\n",
    "        add_special(self.cls_token)\n",
    "\n",
    "        while len(spans) < self.max_tokens:\n",
    "            add_special(self.pad_token)\n",
    "\n",
    "        return dict(\n",
    "            input_ids=np.array(input_ids)\n",
    "        )\n",
    "\n",
    "    def tokenize(self, texts: Iterable[str]) -> dict:\n",
    "        toks = [self._tokenize(t) for t in texts]\n",
    "        ins = []\n",
    "        for tok in toks:\n",
    "            ins.append(tok['input_ids'])\n",
    "\n",
    "        return dict(input_ids=torch.tensor(np.array(ins)))\n",
    "    \n",
    "tokenizer = GreedyTokenizer(max_tokens=max_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.tokenize(np.array([\"hello there\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_dataset(num_rows: int, row_length: int, seed=0xdeadbeef):\n",
    "    \"\"\"\n",
    "    Generate random sentiment sentences and their labels. Uses only the tokens\n",
    "    \"good\", \"bad\", \"neutral\", and their synonyms but also creates words that\n",
    "    combine these tokens. Output class is determined by which of the tokens\n",
    "    appears most frequently. Can be tokenized by GreedyTokenizer. \n",
    "    \"\"\"\n",
    "\n",
    "    random.seed(a=seed)\n",
    "\n",
    "    ret = []\n",
    "    cls = []\n",
    "    for i in range(num_rows):\n",
    "        sent = []\n",
    "\n",
    "        goods = 0\n",
    "        bads = 0\n",
    "\n",
    "        while len(sent) < row_length * 2:\n",
    "            r = random.random()\n",
    "\n",
    "            word = \"neutral\"\n",
    "\n",
    "            if r > 0.9:\n",
    "                word = \"good\"\n",
    "                goods += 1\n",
    "            elif r > 0.8:\n",
    "                word = \"bad\"\n",
    "                bads += 1\n",
    "            elif r > 0.2:\n",
    "                word = \" \"\n",
    "\n",
    "            if word != \" \":\n",
    "                word = random.choice(SYNONYMS[word])\n",
    "\n",
    "            sent.append(word)\n",
    "\n",
    "        ret.append(\"\".join(sent))\n",
    "\n",
    "        if goods > bads:\n",
    "            gt = 0\n",
    "        elif bads > goods:\n",
    "            gt = 1\n",
    "        else:\n",
    "            gt = random.randint(0, 1)\n",
    "\n",
    "        cls.append(gt)\n",
    "\n",
    "    return pd.DataFrame(dict(sentence=ret, sentiment=cls))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = generate_dataset(1000, row_length=max_tokens-1)\n",
    "sentences = dataset['sentence'].to_numpy()\n",
    "sentiments = dataset['sentiment'].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Outputs:\n",
    "    logits: Tensor = None\n",
    "    probits: Tensor = None\n",
    "\n",
    "\n",
    "class SentimentSoft(torch.nn.Module):\n",
    "\n",
    "    def set_parameters(self) -> None:\n",
    "        \"\"\"Set model parameters as per fixed specification.\"\"\"\n",
    "\n",
    "        Wi = torch.zeros_like(\n",
    "            self.lstm.weight_ih_l0\n",
    "        )  # order: W_ii|W_if|W_ig|W_io\n",
    "        bi = torch.zeros_like(self.lstm.bias_ih_l0)  # order b_ii|b_if|b_ig|b_io\n",
    "        Wh = torch.zeros_like(\n",
    "            self.lstm.weight_hh_l0\n",
    "        )  # order W_hi|W_hf|W_hg|W_ho\n",
    "        bh = torch.zeros_like(self.lstm.bias_hh_l0)  # b_hi|b_hf|b_hg|b_ho\n",
    "\n",
    "        big = 2.0  # Multipliers to help dealing with LSTM sigmoids.\n",
    "        # half = 4.0  # Intention here is that sigmoid((x*big) - half) is ~0 if x is ~0; and\n",
    "        # ~1 when x is >~ 1.\n",
    "\n",
    "        # internal states\n",
    "        S_POSITIVITY = 0\n",
    "\n",
    "        # words/tokens\n",
    "        W_UNKNOWN = tokenizer.unk_token_id\n",
    "        W_NEUTRAL = tokenizer.vocab['neutral']\n",
    "        W_GOOD = tokenizer.vocab['good']\n",
    "        W_BAD = tokenizer.vocab['bad']\n",
    "\n",
    "        hs = self.hidden_size\n",
    "\n",
    "        # make sure c gate is always big, so tanh(c) is always ~1 .\n",
    "        bi[0:hs * 3] = big * 10.0\n",
    "        bh[0:hs * 3] = big * 10.0\n",
    "\n",
    "        # o gate weights:\n",
    "        Wi[3 * hs + S_POSITIVITY, W_NEUTRAL] = 0  # ignore neutral word\n",
    "        Wi[3 * hs + S_POSITIVITY, W_GOOD] = big  # read good word\n",
    "        Wi[3 * hs + S_POSITIVITY, W_BAD] = -big  # read bad word\n",
    "        Wh[3 * hs + S_POSITIVITY, S_POSITIVITY] = big  #\n",
    "        bh[3 * hs + S_POSITIVITY] = -0.5 * big\n",
    "\n",
    "        self.lstm.weight_hh_l0 = nn.Parameter(Wh)\n",
    "        self.lstm.bias_hh_l0 = nn.Parameter(bh)\n",
    "        self.lstm.weight_ih_l0 = nn.Parameter(Wi)\n",
    "        self.lstm.bias_ih_l0 = nn.Parameter(bi)\n",
    "\n",
    "        self.embedding.weight = nn.Parameter(torch.eye(self.emb_size))\n",
    "\n",
    "        self.logits.weight = nn.Parameter(\n",
    "            torch.tensor([\n",
    "                [1.0],  # positive\n",
    "                [-1.0],  # negative\n",
    "            ])\n",
    "        )\n",
    "        self.logits.bias = nn.Parameter(\n",
    "            torch.tensor([\n",
    "                0.0,  # positive\n",
    "                1.0,  # negative\n",
    "            ])\n",
    "        )\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        # self.model_path = model_path\n",
    "\n",
    "        # self.tokenizer = CustomTokenizerWrapper(model_path=self.model_path)\n",
    "\n",
    "        self.labels = ['+', '-']\n",
    "\n",
    "        self.emb_size = 8\n",
    "\n",
    "        self.hidden_size = 1\n",
    "\n",
    "        # Identity embedding, each vocab word has its own dimension where its presence is encoded.\n",
    "        self.embedding = nn.Embedding(\n",
    "            padding_idx=1,\n",
    "            embedding_dim=self.emb_size,\n",
    "            num_embeddings=self.emb_size\n",
    "        )\n",
    "\n",
    "        # here only to get us a separate layer for the lstm's first input (the embeddings)\n",
    "        self.lstm_embedding_input = nn.Identity()\n",
    "\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=self.emb_size,\n",
    "            hidden_size=self.hidden_size,\n",
    "            num_layers=1,\n",
    "            batch_first=True\n",
    "        )\n",
    "\n",
    "        # Linear layer to combine the two types of confused state and weight things so that\n",
    "        # confused outweighs positive and negative, while positive and negative outweigh neutral\n",
    "        # if more than one of these states is set.\n",
    "        self.logits = torch.nn.Linear(\n",
    "            in_features=self.hidden_size, out_features=2, bias=True\n",
    "        )\n",
    "\n",
    "        self.logits_squeezed = nn.Identity()\n",
    "\n",
    "        # Finally add a softmax for classification.\n",
    "        self.softmax = torch.nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: torch.Tensor = None,\n",
    "        *,\n",
    "        inputs_embeds: torch.Tensor = None,\n",
    "        attention_mask: torch.Tensor = None,\n",
    "        token_type_ids: torch.Tensor = None\n",
    "    ) -> torch.Tensor:\n",
    "        # Signature and some functionality imitiates huggingface models.\n",
    "        # TODO: Use `attention_mask`.\n",
    "\n",
    "        if input_ids is not None:\n",
    "            batch_size = input_ids.shape[0]\n",
    "            device = input_ids.device\n",
    "        else:\n",
    "            batch_size = inputs_embeds.shape[0]\n",
    "            device = inputs_embeds.device\n",
    "\n",
    "        S_POSITIVITY = 0\n",
    "\n",
    "        h0 = torch.zeros(1, batch_size, self.hidden_size).to(device)\n",
    "        h0[:, :, S_POSITIVITY] = 0.5  # initial state is neutral\n",
    "        c0 = 2.0 * torch.ones(1, batch_size, self.hidden_size).to(device)\n",
    "\n",
    "        if inputs_embeds is None:\n",
    "            embeds = self.embedding(input_ids)\n",
    "        else:\n",
    "            embeds = inputs_embeds\n",
    "\n",
    "        # here only to get us a separate layer for the lstm's first input (the embeddings)\n",
    "        embeds = self.lstm_embedding_input(embeds)\n",
    "\n",
    "        _, (hn, _) = self.lstm(embeds, (h0, c0))\n",
    "\n",
    "        logits = self.logits(hn)[0]\n",
    "\n",
    "        logits_squeezed = self.logits_squeezed(logits)\n",
    "\n",
    "        probits = self.softmax(logits_squeezed)\n",
    "\n",
    "        # outputs also imitates hugging face\n",
    "        return Outputs(logits=logits, probits=probits)\n",
    "\n",
    "\n",
    "model = SentimentSoft()\n",
    "model.set_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = model(**tokenizer.tokenize(sentences)).logits.detach().numpy()\n",
    "preds = np.argmax(scores, axis=1)\n",
    "acc = (preds == sentiments).mean()\n",
    "print(f\"accuracy={100.0*acc:0.3f}%\")\n",
    "\n",
    "wrongs = np.argwhere(preds != sentiments)\n",
    "for t, s, p, gt in zip(sentences[wrongs], scores[wrongs], preds[wrongs], sentiments[wrongs]):\n",
    "    print (model.labels[p[0]], s, model.labels[gt[0]], t, tokenizer.tokenize(t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wrapper = get_model_wrapper(model, input_shape=(None, tokenizer.max_tokens), device=\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wrapper.print_layer_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts=['good', \"bad\", \"nothing\", \"good and bad\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Minimal usage provides only tokenize and input_accessor if neccessary.\n",
    "\n",
    "V = NLP(\n",
    "    tokenize=tokenizer.tokenize,\n",
    "    input_accessor=lambda x: x['input_ids'],\n",
    ")\n",
    "V.tokens(texts=texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test hidden tokens\n",
    "\n",
    "V = NLP(\n",
    "    tokenize=tokenizer.tokenize,\n",
    "    input_accessor=lambda x: x['input_ids'],\n",
    "    hidden_tokens=set([tokenizer.pad_token_id])\n",
    "    # do not display these tokens\n",
    ")\n",
    "V.tokens(texts=texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test decode to show readable representations of token id's.\n",
    "\n",
    "V = NLP(\n",
    "    decode=tokenizer.decode,\n",
    "    tokenize=tokenizer.tokenize,\n",
    "    input_accessor=lambda x: x['input_ids'],\n",
    "    hidden_tokens=set([tokenizer.pad_token_id])\n",
    ")\n",
    "V.tokens(texts=texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show token id's alongside readable forms.\n",
    "\n",
    "V.tokens(texts=texts, show_id=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Also show pre-tokenized text.\n",
    "\n",
    "V.tokens(texts=texts, show_id=True, show_text=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test model outputs.\n",
    "\n",
    "V = NLP(\n",
    "    wrapper=wrapper,\n",
    "    # labels=model.labels,\n",
    "    decode=tokenizer.decode,\n",
    "    tokenize=tokenizer.tokenize,\n",
    "    # huggingface models can take as input the keyword args as per produced by their tokenizers.\n",
    "\n",
    "    input_accessor=lambda x: x['input_ids'],\n",
    "    # for huggingface models, input/token ids are under input_ids key in the input dictionary\n",
    "\n",
    "    output_accessor=lambda x: x.logits,\n",
    "    # and logits under 'logits' key in the output dictionary\n",
    "\n",
    "    hidden_tokens=set([tokenizer.pad_token_id])\n",
    "    # do not display these tokens\n",
    ")\n",
    "V.tokens(texts=texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test model outputs with labels.\n",
    "\n",
    "V = NLP(\n",
    "    wrapper=wrapper,\n",
    "    labels=model.labels,\n",
    "    decode=lambda x: tokenizer.decode(x),\n",
    "    tokenize=lambda sentences: tokenizer.tokenize(sentences),\n",
    "    # huggingface models can take as input the keyword args as per produced by their tokenizers.\n",
    "\n",
    "    input_accessor=lambda x: x['input_ids'],\n",
    "    # for huggingface models, input/token ids are under input_ids key in the input dictionary\n",
    "\n",
    "    output_accessor=lambda x: x.logits,\n",
    "    # and logits under 'logits' key in the output dictionary\n",
    "\n",
    "    hidden_tokens=set([tokenizer.pad_token_id])\n",
    "    # do not display these tokens\n",
    ")\n",
    "V.tokens(texts=texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test attributions; various QoI, point DoI.\n",
    "\n",
    "from trulens.nn.distributions import PointDoi, GaussianDoi, LinearDoi\n",
    "from trulens.nn.attribution import InternalInfluence, IntegratedGradients, Cut, OutputCut\n",
    "from trulens.nn.quantities import MaxClassQoI, ClassQoI, ComparativeQoI\n",
    "\n",
    "common_args = dict(\n",
    "    doi=PointDoi(Cut('embedding')),\n",
    "    model=wrapper,\n",
    "    cuts=(Cut('embedding'), OutputCut(accessor=lambda o: o.logits))\n",
    ")\n",
    "\n",
    "attributors = [\n",
    "    InternalInfluence(qoi=MaxClassQoI(), **common_args),\n",
    "    InternalInfluence(qoi=ClassQoI(1), **common_args),\n",
    "    InternalInfluence(qoi=ClassQoI(0), **common_args),\n",
    "    InternalInfluence(qoi=ComparativeQoI(0, 1), **common_args)\n",
    "]\n",
    "\n",
    "for infl in attributors:\n",
    "\n",
    "    V = NLP(\n",
    "        wrapper=wrapper,\n",
    "        labels=model.labels,\n",
    "        decode=lambda x: tokenizer.decode(x),\n",
    "        tokenize=lambda sentences: tokenizer.tokenize(sentences),\n",
    "        # huggingface models can take as input the keyword args as per produced by their tokenizers.\n",
    "        input_accessor=lambda x: x['input_ids'],\n",
    "        # for huggingface models, input/token ids are under input_ids key in the input dictionary\n",
    "        output_accessor=lambda x: x.logits,\n",
    "        # and logits under 'logits' key in the output dictionary\n",
    "        hidden_tokens=set([tokenizer.pad_token_id])\n",
    "        # do not display these tokens\n",
    "    )\n",
    "\n",
    "    display(V.tokens(texts=texts, attributor=infl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_args = dict(qoi=MaxClassQoI(), model=wrapper, cuts=(Cut('embedding'), OutputCut(accessor=lambda o: o.logits)))\n",
    "\n",
    "attributors = [\n",
    "    InternalInfluence(\n",
    "        doi=PointDoi(Cut('embedding')),\n",
    "        **common_args\n",
    "    ),\n",
    "    InternalInfluence(\n",
    "        doi=GaussianDoi(var=0.1, resolution=10, cut=Cut('embedding')),\n",
    "        **common_args\n",
    "    ),\n",
    "    InternalInfluence(\n",
    "        doi=LinearDoi(resolution=10, cut=Cut('embedding')),\n",
    "        **common_args\n",
    "    ),\n",
    "]\n",
    "\n",
    "for infl in attributors:\n",
    "\n",
    "    V = NLP(\n",
    "        wrapper=wrapper,\n",
    "        labels=model.labels,\n",
    "        decode=lambda x: tokenizer.decode(x),\n",
    "        tokenize=lambda sentences: tokenizer.tokenize(sentences),\n",
    "        # huggingface models can take as input the keyword args as per produced by their tokenizers.\n",
    "\n",
    "        input_accessor=lambda x: x['input_ids'],\n",
    "        # for huggingface models, input/token ids are under input_ids key in the input dictionary\n",
    "\n",
    "        output_accessor=lambda x: x.logits,\n",
    "        # and logits under 'logits' key in the output dictionary\n",
    "\n",
    "        hidden_tokens=set([tokenizer.pad_token_id])\n",
    "        # do not display these tokens\n",
    "    )\n",
    "\n",
    "    display(V.tokens(texts=texts, attributor=infl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test stability pairs.\n",
    "\n",
    "V = NLP(\n",
    "    wrapper=wrapper,\n",
    "    labels=model.labels,\n",
    "    decode=lambda x: tokenizer.decode(x),\n",
    "    tokenize=lambda sentences: tokenizer.tokenize(sentences),\n",
    "    # huggingface models can take as input the keyword args as per produced by their tokenizers.\n",
    "\n",
    "    input_accessor=lambda x: x['input_ids'],\n",
    "    # for huggingface models, input/token ids are under input_ids key in the input dictionary\n",
    "\n",
    "    output_accessor=lambda x: x.logits,\n",
    "    # and logits under 'logits' key in the output dictionary\n",
    "\n",
    "    hidden_tokens=set([tokenizer.pad_token_id])\n",
    "    # do not display these tokens\n",
    ")\n",
    "\n",
    "texts1 = ['this is good', \"good good bad good\"]\n",
    "texts2 = ['this is bad', \"good bad bad bad\"]\n",
    "\n",
    "V.tokens_stability(texts1=texts1, texts2=texts2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test doi enumeration.\n",
    "\n",
    "common_args = dict(return_grads=True, return_doi=True, qoi=MaxClassQoI(), model=wrapper, cuts=(Cut('embedding'), OutputCut(accessor=lambda o: o.logits)))\n",
    "\n",
    "attributors = [\n",
    "    InternalInfluence(\n",
    "        doi=PointDoi(Cut('embedding')),\n",
    "        **common_args\n",
    "    ),\n",
    "    InternalInfluence(\n",
    "        doi=GaussianDoi(var=0.4, resolution=20, cut=Cut('embedding')),\n",
    "        **common_args\n",
    "    ),\n",
    "    InternalInfluence(\n",
    "        doi=LinearDoi(resolution=20, cut=Cut('embedding')),\n",
    "        **common_args\n",
    "    ),\n",
    "]\n",
    "\n",
    "for infl in attributors:\n",
    "\n",
    "    V = NLP(\n",
    "        wrapper=wrapper,\n",
    "        labels=model.labels,\n",
    "        decode=lambda x: tokenizer.decode(x),\n",
    "        tokenize=lambda sentences: tokenizer.tokenize(sentences),\n",
    "        embedder=model.embedding,\n",
    "        embeddings=list(model.embedding.parameters())[0].detach().numpy(),\n",
    "        # huggingface models can take as input the keyword args as per produced by their tokenizers.\n",
    "\n",
    "        input_accessor=lambda x: x['input_ids'],\n",
    "        # for huggingface models, input/token ids are under input_ids key in the input dictionary\n",
    "\n",
    "        output_accessor=lambda x: x.logits,\n",
    "        # and logits under 'logits' key in the output dictionary\n",
    "\n",
    "        hidden_tokens=set([tokenizer.pad_token_id])\n",
    "        # do not display these tokens\n",
    "    )\n",
    "\n",
    "    display(f\"doi={infl.doi}\")\n",
    "    display(V.tokens(texts=texts, attributor=infl, show_doi=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test doi enumeration with token stability pairs.\n",
    "\n",
    "common_args = dict(\n",
    "    return_grads=True, \n",
    "    return_doi=True, \n",
    "    qoi=MaxClassQoI(), \n",
    "    model=wrapper, \n",
    "    cuts=(Cut('embedding'), OutputCut(accessor=lambda o: o.logits))\n",
    ")\n",
    "\n",
    "attributors = [\n",
    "    InternalInfluence(\n",
    "        doi=PointDoi(Cut('embedding')),\n",
    "        **common_args\n",
    "    ),\n",
    "    InternalInfluence(\n",
    "        doi=GaussianDoi(var=0.1, resolution=10, cut=Cut('embedding')),\n",
    "        **common_args\n",
    "    ),\n",
    "    InternalInfluence(\n",
    "        doi=LinearDoi(resolution=10, cut=Cut('embedding')),\n",
    "        **common_args\n",
    "    ),\n",
    "]\n",
    "\n",
    "for infl in attributors:\n",
    "\n",
    "    V = NLP(\n",
    "        wrapper=wrapper,\n",
    "        labels=model.labels,\n",
    "        decode=lambda x: tokenizer.decode(x),\n",
    "        tokenize=lambda sentences: tokenizer.tokenize(sentences),\n",
    "        embedder=model.embedding,\n",
    "        embeddings=list(model.embedding.parameters())[0].detach().numpy(),\n",
    "        # huggingface models can take as input the keyword args as per produced by their tokenizers.\n",
    "\n",
    "        input_accessor=lambda x: x['input_ids'],\n",
    "        # for huggingface models, input/token ids are under input_ids key in the input dictionary\n",
    "\n",
    "        output_accessor=lambda x: x.logits,\n",
    "        # and logits under 'logits' key in the output dictionary\n",
    "\n",
    "        hidden_tokens=set([tokenizer.pad_token_id])\n",
    "        # do not display these tokens\n",
    "    )\n",
    "\n",
    "    display(V.tokens_stability(texts1=texts1, texts2=texts2, attributor=infl, show_doi=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test IG baselines.\n",
    "\n",
    "from trulens.utils.nlp import token_baseline_swap\n",
    "from trulens.utils.typing import ModelInputs\n",
    "\n",
    "inputs_swap_baseline_ids, inputs_swap_baseline_embeddings = token_baseline_swap(\n",
    "    \n",
    "    token_pairs = [(\n",
    "        tokenizer.vocab['good'],\n",
    "        tokenizer.vocab['bad']\n",
    "    )],\n",
    "\n",
    "    input_accessor=lambda x: x.kwargs['input_ids'],\n",
    "\n",
    "    ids_to_embeddings=model.embedding\n",
    "    # Callable to produce embeddings from token ids.\n",
    ")\n",
    "\n",
    "from trulens.utils.nlp import token_baseline\n",
    "\n",
    "inputs_baseline_ids, inputs_baseline_embeddings = token_baseline(\n",
    "    keep_tokens=set([tokenizer.cls_token_id, tokenizer.sep_token_id]),\n",
    "    # Which tokens to preserve.\n",
    "\n",
    "    replacement_token=tokenizer.pad_token_id,\n",
    "    # What to replace tokens with.\n",
    "\n",
    "    input_accessor=lambda x: x.kwargs['input_ids'],\n",
    "    # input_accessor = lambda x: x, \n",
    "\n",
    "    ids_to_embeddings=model.embedding\n",
    "    # Callable to produce embeddings from token ids.\n",
    ")\n",
    "\n",
    "common_args = dict(\n",
    "    return_grads=True, \n",
    "    return_doi=True, \n",
    "    qoi=ClassQoI(1), \n",
    "    model=wrapper, \n",
    "    cuts=(Cut('embedding'), OutputCut(accessor=lambda o: o.logits))\n",
    ")\n",
    "\n",
    "attributors = [InternalInfluence(\n",
    "        doi=LinearDoi(resolution=20, cut=Cut('embedding'), baseline=baseline),\n",
    "        **common_args\n",
    "    ) for baseline in [\n",
    "        None,\n",
    "        inputs_baseline_embeddings,\n",
    "        inputs_swap_baseline_embeddings\n",
    "    ]]\n",
    "\n",
    "for infl in attributors:\n",
    "\n",
    "    V = NLP(\n",
    "        wrapper=wrapper,\n",
    "        labels=model.labels,\n",
    "        decode=lambda x: tokenizer.decode(x),\n",
    "        tokenize=lambda sentences: tokenizer.tokenize(sentences),\n",
    "        embedder=model.embedding,\n",
    "        embeddings=list(model.embedding.parameters())[0].detach().numpy(),\n",
    "        # huggingface models can take as input the keyword args as per produced by their tokenizers.\n",
    "\n",
    "        input_accessor=lambda x: x['input_ids'],\n",
    "        # for huggingface models, input/token ids are under input_ids key in the input dictionary\n",
    "\n",
    "        output_accessor=lambda x: x.logits,\n",
    "        # and logits under 'logits' key in the output dictionary\n",
    "\n",
    "        hidden_tokens=set([tokenizer.pad_token_id])\n",
    "        # do not display these tokens\n",
    "    )\n",
    "    display(f\"baseline={infl.doi.baseline}\")\n",
    "\n",
    "    display(V.tokens(texts=texts, attributor=infl, show_doi=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check different embedding distance methods for finding closest tokens for interventions\n",
    "\n",
    "common_args = dict(\n",
    "    return_grads=True, \n",
    "    return_doi=True, \n",
    "    qoi=ClassQoI(1), \n",
    "    model=wrapper, \n",
    "    cuts=(Cut('embedding'), OutputCut(accessor=lambda o: o.logits))\n",
    ")\n",
    "\n",
    "attributor = InternalInfluence(\n",
    "        doi=LinearDoi(resolution=20, cut=Cut('embedding'), baseline=inputs_baseline_embeddings),\n",
    "        **common_args\n",
    "    )\n",
    "\n",
    "for dist in ['l1', 'l2', 'cosine']:\n",
    "\n",
    "    V = NLP(\n",
    "        wrapper=wrapper,\n",
    "        labels=model.labels,\n",
    "        decode=lambda x: tokenizer.decode(x),\n",
    "        tokenize=lambda sentences: tokenizer.tokenize(sentences),\n",
    "        embedder=model.embedding,\n",
    "        embeddings=list(model.embedding.parameters())[0].detach().numpy(),\n",
    "        embedding_distance=dist,\n",
    "        # huggingface models can take as input the keyword args as per produced by their tokenizers.\n",
    "\n",
    "        input_accessor=lambda x: x['input_ids'],\n",
    "        # for huggingface models, input/token ids are under input_ids key in the input dictionary\n",
    "\n",
    "        output_accessor=lambda x: x.logits,\n",
    "        # and logits under 'logits' key in the output dictionary\n",
    "\n",
    "        hidden_tokens=set([tokenizer.pad_token_id])\n",
    "        # do not display these tokens\n",
    "    )\n",
    "    display(f\"distance={dist}\")\n",
    "\n",
    "    display(V.tokens(texts=[\"good and bad\"], attributor=attributor, show_doi=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.0 ('py39_pytorch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "173ccfcf31b501cc6ac5670daa745d8c9842120d30348cd6db330e34f050f55c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
