{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "# Use this if running this notebook from within its place in the truera repository.\n",
    "sys.path.insert(0, \"../..\")\n",
    "\n",
    "# Install transformers / huggingface.\n",
    "# ! {sys.executable} -m pip install torch\n",
    "# ! {sys.executable} -m pip install transformers\n",
    "# ! {sys.executable} -m pip install mkl\n",
    "# ! {sys.executable} -m pip install vision\n",
    "\n",
    "# Or otherwise install trulens.\n",
    "# TODO: UPDATE TO GET THE CORRECT TRULENS:\n",
    "# ! {sys.executable} -m pip install git+https://github.com/truera/trulens.git\n",
    "# ! {sys.executable} -m pip uninstall trulens -y\n",
    "\n",
    "from IPython.display import display\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from types import SimpleNamespace\n",
    "import base64\n",
    "from tqdm.auto import tqdm\n",
    "import re\n",
    "from typing import List, Tuple\n",
    "import functools\n",
    "from ipywidgets import widgets, interactive, interact\n",
    "import os\n",
    "import multiprocessing as mp\n",
    "\n",
    "os.environ['TOKENIZERS_PARALLELISM'] = '0'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Twitter Sentiment Model\n",
    "\n",
    "[Huggingface](https://huggingface.co/models) offers a variety of pre-trained NLP models to explore. We exemplify in this notebook a [transformer-based twitter sentiment classification model](https://huggingface.co/cardiffnlp/twitter-roberta-base-sentiment). Before getting started, familiarize yourself with the general Truera API as demonstrated in the [intro notebook using pytorch](intro_demo_pytorch.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "\n",
    "# Wrap all of the necessary components.\n",
    "class TwitterSentiment:\n",
    "    #device = 'cpu'\n",
    "    # Can also use cuda if available:\n",
    "    device = 'cuda:0'\n",
    "\n",
    "    MODEL = f\"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(MODEL).to(device)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL)\n",
    "    embeddings = model.distilbert.embeddings.word_embeddings.weight.detach(\n",
    "    ).cpu().numpy()\n",
    "    embeddings_layer = 'distilbert_embeddings_word_embeddings'\n",
    "\n",
    "    max_length = 256\n",
    "\n",
    "    # MODEL = f\"cardiffnlp/twitter-roberta-base-sentiment\"\n",
    "    #embeddings = model.roberta.embeddings.word_embeddings.weight.detach().cpu().numpy()\n",
    "    #embeddings_layer = 'roberta_embeddings_word_embeddings'\n",
    "\n",
    "    def tokenize(texts: List[str]):\n",
    "        return TwitterSentiment.tokenizer(\n",
    "            texts,\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=TwitterSentiment.max_length,\n",
    "            return_tensors='pt'\n",
    "        ).to(TwitterSentiment.device)\n",
    "\n",
    "    def token_str(token_id: int):\n",
    "        tok = task.tokenizer.decode(token_id)\n",
    "        if tok.startswith(\"##\"\n",
    "                         ):  # token starts with \"##\" to denote a word postfix\n",
    "            return tok[2:]\n",
    "        else:\n",
    "            return \" \" + tok  # if not a postfix, add space better indicate a complete word separation\n",
    "\n",
    "    id_of_token = tokenizer.get_vocab()\n",
    "    token_of_id = {v: k for k, v in id_of_token.items()}\n",
    "\n",
    "    labels = [\n",
    "        'negative',\n",
    "        #'neutral', # for roberta\n",
    "        'positive'\n",
    "    ]\n",
    "\n",
    "    NEGATIVE = labels.index('negative')\n",
    "    #NEUTRAL = labels.index('neutral') # for roberta\n",
    "    POSITIVE = labels.index('positive')\n",
    "\n",
    "task = TwitterSentiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model quantifies tweets (or really any text you give it) according to its sentiment: positive, negative, or neutral. Lets try it out on some examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [\"I'm so happy!\", \"I'm so sad!\", \"I cannot tell whether I should be happy or sad!\", \"meh\"]\n",
    "\n",
    "# Input sentences need to be tokenized first.\n",
    "\n",
    "inputs = task.tokenize(sentences)\n",
    "\n",
    "# The tokenizer gives us vocabulary indexes for each input token (in this case,\n",
    "# words and some word parts like the \"'m\" part of \"I'm\" are tokens).\n",
    "\n",
    "print(inputs)\n",
    "\n",
    "# Decode helps inspecting the tokenization produced:\n",
    "\n",
    "print(task.tokenizer.batch_decode(torch.flatten(inputs['input_ids'])))\n",
    "# Normally decode would give us a single string for each sentence but we would\n",
    "# not be able to see some of the non-word tokens there. Flattening first gives\n",
    "# us a string for each input_id."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AIQ\n",
    "\n",
    "textbox = lambda t=\"The last part of this sentence is not a real wordle.\", c=True: widgets.Text(\n",
    "    value=t,\n",
    "    continuous_update=c,\n",
    "    layout=dict(border=\"10px solid teal\", padding=\"5px\", width=\"100%\", margin=\"0px\")\n",
    ")\n",
    "\n",
    "@interact(text=textbox())\n",
    "def show_parse(text: str):\n",
    "    inputs = task.tokenize([text])\n",
    "\n",
    "    print(inputs)\n",
    "\n",
    "    print(task.tokenizer.batch_decode(torch.flatten(inputs['input_ids'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluating huggingface models is straight-forward if we use the structure produced by the tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = task.model(**inputs)\n",
    "\n",
    "print(outputs)\n",
    "\n",
    "# From logits we can extract the most likely class for each sentence and its readable label.\n",
    "\n",
    "predictions = [task.labels[i] for i in outputs.logits.argmax(axis=1)]\n",
    "\n",
    "for sentence, logits, prediction in zip(sentences, outputs.logits, predictions):\n",
    "    print(logits.to('cpu').detach().numpy(), prediction, sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AIQ\n",
    "\n",
    "results = []\n",
    "\n",
    "@interact(text=textbox())\n",
    "def show_output(text):\n",
    "    global results\n",
    "    inputs = task.tokenize([text])\n",
    "\n",
    "    outputs = task.model(**inputs)\n",
    "\n",
    "    predictions = [task.labels[i] for i in outputs.logits.argmax(axis=1)]\n",
    "\n",
    "    results.insert(0, (predictions[0], outputs.logits.detach().cpu().numpy()[0], text))\n",
    "    results = results[0:10]\n",
    "\n",
    "    for result in results:\n",
    "        print(*result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_to_probits(texts: List[str]) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Evaluate a collection of `texts` into their probits scores.\n",
    "    \"\"\"\n",
    "\n",
    "    inputs = task.tokenize(texts)\n",
    "\n",
    "    logits = task.model(**inputs).logits\n",
    "\n",
    "    probits = torch.nn.functional.softmax(logits, dim=1)\n",
    "\n",
    "    return probits\n",
    "\n",
    "evaluate_to_probits([\"This is great.\", \"This is not great.\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Wrapper\n",
    "\n",
    "As in the prior notebooks, we need to wrap the pytorch model with the appropriate Trulens functionality. Here we specify the maximum input size (in terms of tokens) each tweet may have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trulens.nn.models import get_model_wrapper\n",
    "from trulens.nn.quantities import ClassQoI\n",
    "from trulens.nn.attribution import IntegratedGradients\n",
    "from trulens.nn.attribution import Cut, OutputCut\n",
    "from trulens.utils.typing import ModelInputs\n",
    "\n",
    "task.wrapper = get_model_wrapper(task.model, device=task.device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "4ed9c783-b745-4c6a-b674-d8b6935dd62d"
    }
   },
   "source": [
    "# Attributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_attributor_arguments = dict(\n",
    "    model=task.wrapper,\n",
    "    resolution=128,\n",
    "    rebatch_size=32,\n",
    "    doi_cut=Cut(task.embeddings_layer),\n",
    "    qoi=ClassQoI(task.POSITIVE),\n",
    "    qoi_cut=OutputCut(accessor=lambda o: o['logits'])\n",
    ")\n",
    "\n",
    "infl = IntegratedGradients(\n",
    "    **common_attributor_arguments\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A listing as above is not very readable so Trulens comes with some utilities to present token influences a bit more concisely. First we need to set up a few parameters to make use of it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trulens.visualizations import NLP\n",
    "\n",
    "V = NLP(\n",
    "    wrapper=task.wrapper,\n",
    "    labels=task.labels,\n",
    "    decode=task.token_str,\n",
    "    tokenize=lambda sentences: ModelInputs(kwargs=task.tokenize(sentences,)).\n",
    "    map(lambda t: t.to(task.device)),\n",
    "    # huggingface models can take as input the keyword args as per produced by their tokenizers.\n",
    "    input_accessor=lambda x: x.kwargs['input_ids'],\n",
    "    # for huggingface models, input/token ids are under input_ids key in the input dictionary\n",
    "    output_accessor=lambda x: x['logits'],\n",
    "    # and logits under 'logits' key in the output dictionary\n",
    "    hidden_tokens=set([task.tokenizer.pad_token_id])\n",
    "    # do not display these tokens\n",
    ")\n",
    "\n",
    "print(\"QOI = POSITIVE\")\n",
    "display(V.tokens(sentences, infl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AIQ\n",
    "\n",
    "results = []\n",
    "\n",
    "@interact(text=textbox())\n",
    "def show_attribution(text):\n",
    "    global results\n",
    "\n",
    "    results.insert(0, (V.tokens([text], infl)))\n",
    "    results = results[:10]\n",
    "\n",
    "    for result in results:\n",
    "        display(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baselines\n",
    "\n",
    "We see in the above results that special tokens such as the sentence end **&lt;/s&gt;** contributes are found to contribute a lot to the model outputs. While this may be useful in some contexts, we are more interested in the contributions of the actual words in these sentences. To focus on the words more, we need to adjust the **baseline** used in the integrated gradients computation. By default in the instantiation so far, the baseline for each token is a zero vector of the same shape as its embedding. By making the basaeline be identicaly to the explained instances on special tokens, we can rid their impact from our measurement. Trulens provides a utility for this purpose in terms of `token_baseline` which constructs for you the methods to compute the appropriate baseline. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trulens.utils.nlp import token_baseline\n",
    "\n",
    "inputs_baseline_ids, inputs_baseline_embeddings = token_baseline(\n",
    "    keep_tokens=set([task.tokenizer.cls_token_id, task.tokenizer.sep_token_id]),\n",
    "    # Which tokens to preserve.\n",
    "\n",
    "    replacement_token=task.tokenizer.pad_token_id,\n",
    "    # AIQ: Try changing `replacement_token` parameter to other special or non special tokens.\n",
    "    # replacement_token=task.tokenizer.mask_token_id,\n",
    "    # replacement_token=task.tokenizer.vocab[\"happy\"],\n",
    "\n",
    "    # What to replace tokens with.\n",
    "\n",
    "    input_accessor=lambda x: x.kwargs['input_ids'],\n",
    "\n",
    "    ids_to_embeddings=task.model.get_input_embeddings()\n",
    "    # Callable to produce embeddings from token ids.\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now inspect the baselines on some example sentences. The first method returned by `token_baseline` gives us token ids to inspect while the second gives us the embeddings of the baseline which we will pass to the attributions method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"originals=\", task.tokenizer.batch_decode(inputs['input_ids']))\n",
    "\n",
    "baseline_word_ids = inputs_baseline_ids(model_inputs=ModelInputs(args=[], kwargs=inputs))\n",
    "\n",
    "print(\"baselines=\", task.tokenizer.batch_decode(baseline_word_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "infl_positive_baseline = IntegratedGradients(\n",
    "    baseline=inputs_baseline_embeddings, **common_attributor_arguments\n",
    ")\n",
    "\n",
    "print(\"QOI = POSITIVE WITH BASELINE\")\n",
    "display(V.tokens(sentences, infl_positive_baseline))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AIQ\n",
    "\n",
    "results = []\n",
    "\n",
    "@interact(text=textbox(c=False))\n",
    "def show_attribution(text):\n",
    "    global results\n",
    "\n",
    "    default_result = widgets.HTML(V.tokens([text], infl).data)\n",
    "    baseline_result = widgets.HTML(V.tokens([text], infl_positive_baseline).data)\n",
    "\n",
    "    results.insert(0, (default_result, baseline_result))\n",
    "    results = results[:3]\n",
    "\n",
    "    for result in results:\n",
    "        display(widgets.HBox(result))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "rotten_train = load_dataset(\"rotten_tomatoes\", split=\"train\")\n",
    "rotten_test = load_dataset(\"rotten_tomatoes\", split=\"test\")\n",
    "rotten_texts = rotten_train['text'] + rotten_test['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_pattern(word):\n",
    "    \"\"\"\n",
    "    Create a pattern that matches the given `word` as long as it is not\n",
    "    immediately next to an alpha-numeric character.\n",
    "    \"\"\"\n",
    "    return \"(?<!\\w)\" + re.escape(word) + \"(?!\\w)\"\n",
    "\n",
    "def swap(thing1: str, thing2: str):\n",
    "    \"\"\"\n",
    "    Create a method to swap occurances of `thing1` and `thing2`.\n",
    "    \"\"\"\n",
    "\n",
    "    pat_swapper = re.compile(r\":swapper:\")\n",
    "    pat1 = re.compile(word_pattern(thing1), re.IGNORECASE)\n",
    "    pat2 = re.compile(word_pattern(thing2), re.IGNORECASE)\n",
    "\n",
    "    def f(sentence: str):        \n",
    "        \"\"\"\n",
    "        Swap instances of thing1 and thing2 in sentence.\n",
    "        \"\"\"\n",
    "        \n",
    "        temp1 = pat1.sub(\":swapper:\", sentence)\n",
    "        temp2 = pat2.sub(thing1, temp1)\n",
    "        temp3 = pat_swapper.sub(thing2, temp2)\n",
    "        return temp3\n",
    "\n",
    "    return f\n",
    "\n",
    "def contains(s: str, pat: re.Pattern):\n",
    "    \"\"\"\n",
    "    Determine whether the given string `s` satisfies regular expression `pat`.\n",
    "    \"\"\"\n",
    "    return pat.search(s) is not None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_pairs = [\n",
    "    (\"good\", \"great\"),\n",
    "    (\"great\", \"amazing\"),\n",
    "    (\"good\", \"amazing\"),\n",
    "]\n",
    "\n",
    "def get_sentence_pairs(token_pairs: List[Tuple[str, str]],\n",
    "                       texts: List[str]) -> List[Tuple[str, str]]:\n",
    "    \"\"\"\n",
    "    Create sentence pairs from examples in `texts` that swap words from the\n",
    "    pairs list `token_pairs`.\n",
    "    \"\"\"\n",
    "\n",
    "    patterns = [\n",
    "        re.compile(\n",
    "            \"|\".join([word_pattern(tok) for tok in pair]), re.IGNORECASE\n",
    "        ) for pair in token_pairs\n",
    "    ]\n",
    "    swappers = [swap(*pair) for pair in token_pairs]\n",
    "\n",
    "    sentence_pairs = [\n",
    "        (sentence, swap(sentence))\n",
    "        for pattern, swap in\n",
    "        tqdm(zip(patterns, swappers), desc=\"finding swap pairs\", unit=\"pair\")\n",
    "        for sentence in texts\n",
    "        if contains(sentence, pattern)\n",
    "    ]\n",
    "\n",
    "    print(f\"found {len(sentence_pairs)} pair(s)\")\n",
    "\n",
    "    return sentence_pairs\n",
    "\n",
    "\n",
    "sentence_pairs_quality = get_sentence_pairs(token_pairs, rotten_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_pair_disparities(\n",
    "    sentence_pairs: List[Tuple[str, str]]\n",
    ") -> List[Tuple[Tuple[str, str], float]]:\n",
    "    \"\"\"\n",
    "    Given a collection of `sentence_pairs`, produce a list of tuples containing\n",
    "    the pairs as the first element and the disparity in model scores as the second.\n",
    "    \"\"\"\n",
    "\n",
    "    diffs = []\n",
    "\n",
    "    for pair in tqdm(sentence_pairs,\n",
    "                     desc=\"evaluating sentence pair score differences\"):\n",
    "        a_probits, b_probits = evaluate_to_probits(list(pair))\n",
    "\n",
    "        diffs.append(\n",
    "            torch.nn.functional.cross_entropy(\n",
    "                torch.unsqueeze(a_probits, dim=0),\n",
    "                torch.unsqueeze(b_probits, dim=0)\n",
    "            ).detach().cpu().numpy()\n",
    "        )\n",
    "\n",
    "    diffs = np.array(diffs)\n",
    "    diffs_pairs = list(\n",
    "        reversed(sorted(zip(sentence_pairs, diffs), key=lambda pair: pair[1]))\n",
    "    )\n",
    "\n",
    "    return diffs_pairs\n",
    "\n",
    "diffs_pairs_quality = compute_pair_disparities(sentence_pairs_quality)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_biggest_disparities(\n",
    "    diffs: List[Tuple[Tuple[str, str], float]],\n",
    "    attributor=infl_positive_baseline,\n",
    "    n=3\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Display the top disparate pairs along with their attributions.\n",
    "    \"\"\"\n",
    "\n",
    "    display(\n",
    "        V.tokens_stability(\n",
    "            texts1=[p[0][0] for p in diffs][0:n],\n",
    "            texts2=[p[0][1] for p in diffs][0:n],\n",
    "            attributor=attributor\n",
    "        )\n",
    "    )\n",
    "\n",
    "show_biggest_disparities(diffs_pairs_quality)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AIQ\n",
    "\n",
    "tokenbox = lambda t, c=False: widgets.Text(\n",
    "    value=t,\n",
    "    continuous_update=c,\n",
    "    layout=dict(border=\"10px solid teal\", padding=\"5px\", width=\"300px\", margin=\"0px\")\n",
    ")\n",
    "\n",
    "@interact(token1=tokenbox(\"good\"), token2=tokenbox(\"bad\"))\n",
    "def show_disparities(token1, token2):\n",
    "    \n",
    "    sentence_pairs = get_sentence_pairs([(token1, token2)], rotten_texts)\n",
    "    \n",
    "    if len(sentence_pairs) == 0: return\n",
    "\n",
    "    diffs_pairs = compute_pair_disparities(sentence_pairs)\n",
    "    show_biggest_disparities(diffs_pairs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fairness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gender_pairs = [\n",
    "    ('he', 'she'),\n",
    "    ('guy', 'gal'),\n",
    "    ('himself', 'herself'),\n",
    "    ('boy', 'girl'),\n",
    "    ('husband', 'wife'),\n",
    "    ('man', 'woman'),\n",
    "    ('men', 'women'),\n",
    "    ('brother', 'sister'),\n",
    "    ('uncle', 'aunt'),\n",
    "    ('nephew', 'niece'),\n",
    "    ('dad', 'mom'),\n",
    "    ('father', 'mother'),\n",
    "    ('son', 'daughter'),\n",
    "    ('actor', 'actress'),\n",
    "    ('male', 'female'),\n",
    "    ('hero', 'heroine'),\n",
    "]\n",
    "\n",
    "sentence_pairs_gender = get_sentence_pairs(gender_pairs, rotten_texts)\n",
    "diffs_pairs_gender = compute_pair_disparities(sentence_pairs_gender)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_biggest_disparities(diffs_pairs_gender)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = task.embeddings\n",
    "\n",
    "# A vector approximating the difference between embeddings of pairs of words\n",
    "# of the opposite gender. This one is for the token embedding used in\n",
    "# distilbert.\n",
    "gender_vector = np.frombuffer(\n",
    "    base64.b85decode(\n",
    "        b'?4-;r+n_BbxT&BWWUIlV^Qc9YM<?$ffFvBOyeVWPZ6QZ1ttD(IWT3Vs@g>z1(IHALq#<!8`6&IFovN59FQQ<rjVfRxf2awiYAAOf0xHHLzn)JeL8GLalpxcndmiqk3oS&V=c3uGKP<ea=_dCgN-Vq~^C<SDBB=f;{-m#?lBazp6`Fi20ii4{HmQFaeJNI`eJG!zN2p7nT&MsSPpc;*^d-=!XDFVgZ7N3{HKO`0uqm#msif;Ew5=Wq#ib{lJ)>zVdn~jjwWg4$E-Hm9mL8ELf+nV@>7XN}EFh#J6Du&8d89uqq^KIJJfWAH<s(?A83iFILnMlrgei)r<*S^fJgHDAWT^b8{i%wqmaLPhbS9T6Jt#99Sf6L8+bydo;3dAJS}Uz4E1CW%<)@0Q!Ypv2O(|R<c%D?4xR(l{UL)hBP^^)syPV#omZOCyB&3O+Iv%o~8KW|tw5a_oU?h^MQ!4KuK_zc2qAUp_g(MRpzbQy4vZ$V-e<o?1A1Fqg-lZKX51h559ip|Y3@9lngeeCdV4_MV1tujcN-QR*NT-IaRil6?RICoHU?Y?#zN8MQpeo*^uBGuJ$tk-V*C>ytF{UOeM4I9z{VUa<Vx*-d-l$9`fGFf76r}JNuPLCYkTBb<^`&<w5-3!r4Wiwe2&9!9nI?EFcBnfjS1ipX@gcS+XD6R2W3AF1h>!iL@|kC$dla6YHl}MM<0kVa@~U*KA0pVH3?n6;I4Dsj{2DZ*bRJnH^CRFT$f!e}dniDvR;=tH3Mn0=KqeCw)1Lk)mZ)|kEUBrWAE-g9fhH>_i>J6EXBbkVJ|R9RW2H-=R-}(7FDB6^EvphMpez}nRGA2_{iNV0kf_8hG_1QR;-b?fwIDDpnI}W4-5<av4k0q42p)f_%Ay~t-K6*-@+Mjz%$YHy51(}(H7Uv>(<l=m@*IUHzoEq}BB-CJ^(USw-Yae=oTara=C19iiYMJ3?yV>$*qY=iC!+l-R;E;>I4FCe5iVFIf2tZC5UZpoNG!mnxS~QM38iYMm?*WNN1_!Zh^a27Eu`Qoc$}IfR+ikN7@WSNXQ}EcY!wqH2PuOl#HIx#l_0O7ogf&f@u_Slmm`>_AgeQ^RwvCWk}20BY9RBYb|sFgpeAH0Dx#e#tt9TASt-Dq@1$NOs3duoK`EM`FQsOske;`wT_Z0g0w<>+xh5K%eJIJG%cHs?)}7U(Vx_Yrcpk?fF(C#ZaHY4W)TXW`zn~3}NvZsnOQu36Q6W#L)T+R%yeKOyr=sMY+ou^QsHsz@^&6EdH6?hZOqr#q&8vec0VOpo7b`0ylcE<TLM8XAv7>h-WRQC!;i)z*1gXcS@g$NXP%J#EOdu8^39HJf-!4$5t1F|X>nB1bOs2G`_@1C8;w3n$h$<i`f+X;tZX^b#;Ucys8?I0(!zlPFE1h{F1FV`R&#Ig!^d}%BvmpE}lBB68#i@S|a3H`YHy+|3qoeSwl&35tv!2JIffi^f_$lxpN~Hd%My0>0I32|%;S;zZ38EJ*&ZM^~8z+q-g_sARQK)mI6e2np%BPPeTP$EE%&YOJg(wxF_94wE;wph611IgKq9+HaTdc1oA14&4ysA2+%p*K3yQG9HBPA3nuBm+}Bc<mgCL^*Z>ztA#$1O7^A1jL_&?9yoLLzG;4W}ldVyJc}@u_hmODydxda9wBL7j9dl?n!@W-8FCn5dm7l`5;HN2rgX&7LBtm8K*teWQw>D=p^^x~S!>GaR6#Mkv3kktndAR2qe)DyAc)iKy+Q&ZhH}K&En`@+qt+lOfouwj+$EbEZh08mM%hgeI${1Sv!*_$5}TS09xr!ljccx+a^awVhe0JfIpJSSmoK&7Kw@hbW;WB_D1h{;R}`OQ|d^%d6>`KqUt#-K4;xLadW0q^d!nF(UV!=_;$CJ1X3%@2E4Yk*i)Mz>y9k<fnnC5Gh|R<|O+jB#_&s2B9h)'\n",
    "    ),\n",
    "    dtype='float16'\n",
    ")\n",
    "\n",
    "\n",
    "def normalize(v):\n",
    "    \"\"\"\n",
    "    Normalize a single vector.\n",
    "    \"\"\"\n",
    "    return v / np.linalg.norm(v, ord=2)\n",
    "\n",
    "\n",
    "def normalize_many(v):\n",
    "    \"\"\"\n",
    "    Normalize an array of vectors.\n",
    "    \"\"\"\n",
    "    return v / np.linalg.norm(v, axis=1, ord=2)[:, np.newaxis]\n",
    "\n",
    "\n",
    "all_embs_norm = normalize_many(embeddings)\n",
    "baseline_penalties = np.abs(np.dot(all_embs_norm, gender_vector))\n",
    "\n",
    "direction_vector: np.ndarray = gender_vector\n",
    "\n",
    "\n",
    "def embedding_opposite_id(emb: np.ndarray) -> Tuple[int, float]:\n",
    "    \"\"\"\n",
    "    Get the token id of the token closest to the gender-opposite of the given\n",
    "    `emb`.\n",
    "    \"\"\"\n",
    "\n",
    "    emb = normalize(emb)\n",
    "    scores = np.abs(\n",
    "        np.dot(\n",
    "            normalize_many(emb - all_embs_norm + 0.000000001), direction_vector\n",
    "        )\n",
    "    ) - 0.55 * baseline_penalties\n",
    "\n",
    "    best = np.argmax(scores)\n",
    "\n",
    "    return best, scores[best]\n",
    "\n",
    "\n",
    "def embedding_opposite(emb: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Try to find the embedding close to the opposite gender relative to the given\n",
    "    `emb`. \n",
    "    \"\"\"\n",
    "\n",
    "    best_id, best_score = embedding_opposite_id(emb)\n",
    "\n",
    "    if best_score > 0.25:\n",
    "        return embeddings[best_id]\n",
    "    else:\n",
    "        return emb\n",
    "\n",
    "\n",
    "def embedding_neutralize(emb: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Remove the component of the given embedding that points in the gender\n",
    "    direction.\n",
    "    \"\"\"\n",
    "    return emb - np.dot(emb, direction_vector) * direction_vector\n",
    "\n",
    "\n",
    "@functools.lru_cache(maxsize=len(embeddings))\n",
    "def token_id_opposite(token_id: int):\n",
    "    \"\"\"\n",
    "    Try to find the opposite of `token_id` according to the direction of\n",
    "    `direction_vector`. If a good candidate is not found, returns the given\n",
    "    `token_id` instead.\n",
    "    \"\"\"\n",
    "    best_id, best_score = embedding_opposite_id(all_embs_norm[token_id])\n",
    "\n",
    "    if best_score > 0.20:\n",
    "        return best_id\n",
    "    else:\n",
    "        return token_id\n",
    "\n",
    "\n",
    "def swap_token(token: str) -> str:\n",
    "    a_id = task.id_of_token[token]\n",
    "    b_id = token_id_opposite(a_id)\n",
    "    return task.token_of_id[b_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for (s1, _), _ in diffs_pairs_gender[0:4]:\n",
    "    print(\"original sentence:\", s1)\n",
    "    toks = task.tokenize([s1])['input_ids'][0].detach().cpu().numpy()\n",
    "    toks_opposites = [token_id_opposite(t) for t in toks]\n",
    "    print(\"swapped sentence:\", task.tokenizer.decode(toks_opposites))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def baseline_swap(z: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Given input tensor of embeddings, produce a baseline that swaps their gender\n",
    "    component.\n",
    "    \"\"\"\n",
    "\n",
    "    if isinstance(z, torch.Tensor):\n",
    "        z = z.detach().cpu().numpy()\n",
    "\n",
    "    return torch.tensor(\n",
    "        [[embedding_opposite(emb) for emb in instance] for instance in z]\n",
    "    ).to(task.device)\n",
    "\n",
    "infl_swap_gender = IntegratedGradients(\n",
    "    baseline=baseline_swap,\n",
    "    **common_attributor_arguments\n",
    ")\n",
    "\n",
    "show_biggest_disparities(diffs_pairs_gender, attributor=infl_positive_baseline)\n",
    "show_biggest_disparities(diffs_pairs_gender, attributor=infl_swap_gender)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AIQ\n",
    "\n",
    "results = []\n",
    "\n",
    "\n",
    "@interact(\n",
    "    text=textbox(\n",
    "        t=\n",
    "        \"Johnson has, in his first film, set himself a task he is not early up to.\",\n",
    "        c=False\n",
    "    )\n",
    ")\n",
    "def show_attribution(text):\n",
    "    global results\n",
    "\n",
    "    default_result = widgets.HTML(V.tokens([text], infl_positive_baseline).data)\n",
    "    baseline_result = widgets.HTML(V.tokens([text], infl_swap_gender).data)\n",
    "\n",
    "    results.insert(0, (default_result, baseline_result))\n",
    "    results = results[:3]\n",
    "\n",
    "    for result in results:\n",
    "        display(widgets.HBox(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def baseline_neutralize(z: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Given input tensor of embeddings, produce a baseline that removes their\n",
    "    gender component.\n",
    "    \"\"\"\n",
    "\n",
    "    if isinstance(z, torch.Tensor):\n",
    "        z = z.detach().cpu().numpy()\n",
    "\n",
    "    return torch.tensor(\n",
    "        [[embedding_neutralize(emb) for emb in instance] for instance in z]\n",
    "    ).to(task.device)\n",
    "\n",
    "\n",
    "infl_neutralize_gender = IntegratedGradients(\n",
    "    baseline=baseline_neutralize, **common_attributor_arguments\n",
    ")\n",
    "\n",
    "show_biggest_disparities(diffs_pairs_gender, attributor=infl_positive_baseline)\n",
    "show_biggest_disparities(diffs_pairs_gender, attributor=infl_neutralize_gender)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AIQ\n",
    "\n",
    "results = []\n",
    "\n",
    "\n",
    "@interact(\n",
    "    text=textbox(\n",
    "        t=\n",
    "        \"Johnson has, in his first film, set himself a task he is not early up to.\",\n",
    "        c=False\n",
    "    )\n",
    ")\n",
    "def show_attribution(text):\n",
    "    global results\n",
    "\n",
    "    default_result = widgets.HTML(V.tokens([text], infl_positive_baseline).data)\n",
    "    baseline_result = widgets.HTML(\n",
    "        V.tokens([text], infl_neutralize_gender).data\n",
    "    )\n",
    "\n",
    "    results.insert(0, (default_result, baseline_result))\n",
    "    results = results[:3]\n",
    "\n",
    "    for result in results:\n",
    "        display(widgets.HBox(result))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Drift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get another dataset to compare to.\n",
    "\n",
    "imdb_train = load_dataset(\"imdb\", \"plain_text\", split=\"train\")\n",
    "imdb_test = load_dataset(\"imdb\", \"plain_text\", split=\"test\")\n",
    "imdb_texts = imdb_train['text'] + imdb_test['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(portion):\n",
    "    return task.tokenizer.batch_encode_plus(\n",
    "        portion,\n",
    "        add_special_tokens=True,\n",
    "        return_attention_mask=False,\n",
    "        max_length=512,\n",
    "        truncation=True\n",
    "    )['input_ids']\n",
    "\n",
    "p = mp.Pool(24)\n",
    "\n",
    "def toks_of_texts(texts):\n",
    "    toks = p.map(tokenize, [texts[1000*i: 1000*(i+1)] for i in range(len(texts)//1000)])\n",
    "    all = np.array([i for tok in toks for t in tok for i in t ])\n",
    "\n",
    "    return all\n",
    "\n",
    "def dists_of_texts(texts):\n",
    "    all = toks_of_texts(texts)   \n",
    "\n",
    "    counts = np.zeros(task.tokenizer.vocab_size)\n",
    "    total = len(all)\n",
    "    for i in all:\n",
    "        counts[i] += 1\n",
    "\n",
    "    dist = counts / total\n",
    "\n",
    "    return counts, dist\n",
    "\n",
    "def tops_of_texts(texts, n = 10):\n",
    "    counts, dist = dists_of_texts(texts)\n",
    "\n",
    "    return tops_of_dists(counts, dist, n=n)\n",
    "\n",
    "def tops_of_dists(c, d, n=10):\n",
    "    sortindex = np.argsort(d)\n",
    "    top = []\n",
    "\n",
    "    for idx in sortindex[0:n]:\n",
    "        top.append((idx, c[idx], d[idx], task.tokenizer.decode(idx)))\n",
    "\n",
    "    crest_pos = 0\n",
    "    crest_neg = 0\n",
    "    drest_pos = 0\n",
    "    drest_neg = 0\n",
    "\n",
    "    for idx in sortindex[n:-n]:\n",
    "        if c[idx] >= 0:\n",
    "            crest_pos += c[idx]\n",
    "            drest_pos += d[idx]\n",
    "        else:\n",
    "            crest_neg += c[idx]\n",
    "            drest_neg += d[idx]\n",
    "\n",
    "    top.append((-1, crest_neg, drest_neg, \"*\"))\n",
    "    top.append((-1, crest_pos, drest_pos, \"*\"))\n",
    "\n",
    "    for idx in sortindex[-n:]:\n",
    "        top.append((idx, c[idx], d[idx], task.tokenizer.decode(idx)))\n",
    "\n",
    "    return top"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c1, d1 = dists_of_texts(imdb_train['text'])\n",
    "c2, d2 = dists_of_texts(rotten_train['text'])\n",
    "top = tops_of_dists(c1 - c2, d1 - d2, n = 20)\n",
    "sum([t[2] for t in top])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import transforms as trans\n",
    "\n",
    "def plotdist(top, l1, l2):\n",
    "    fig, ax = plt.subplots(2,1, figsize=(8,6))\n",
    "\n",
    "    # ax[0].grid()\n",
    "    ax[1].bar(x=range(len(top)), width=0.8, height=[t[2] for t in top], alpha=1.0, color=\"black\", label=f\"{l1} - {l2}\")\n",
    "    ax[0].bar(x=np.arange(len(top)) + 0.2, width=0.4, height=[d1[t[0]] if t[0]>=0 else 0 for t in top], alpha=1.0, color=\"green\", label=l1)    \n",
    "    ax[0].bar(x=np.arange(len(top)) - 0.2, width=0.4, height=[d2[t[0]] if t[0]>=0 else 0 for t in top], alpha=1.0, color=\"blue\", label=l2)\n",
    "    ax[0].set_xticks(ticks=range(len(top)), labels=[t[3] for t in top], rotation=90)\n",
    "    ax[1].set_xticks(ticks=range(len(top)), labels=[str(t[0]) if t[0]>=0 else \"*\" for t in top], rotation=90)\n",
    "\n",
    "    # ax[0].grid()\n",
    "    # ax[1].grid()\n",
    "    ax[1].plot([0,len(top)-0.6], [0,0], lw=1, color='black')\n",
    "\n",
    "    ax[0].set_ylabel(\"prob.\")\n",
    "    ax[1].set_ylabel(\"prob. diff\")\n",
    "\n",
    "    fig.legend()\n",
    "    fig.tight_layout()\n",
    "\n",
    "plotdist(top, l1='imdb', l2='rotten tomatoes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c1, d1 = dists_of_texts(imdb_train['text'])\n",
    "c2, d2 = dists_of_texts(imdb_test['text'])\n",
    "top = tops_of_dists(c1 - c2, d1 - d2, n = 20)\n",
    "# top"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotdist(d1, d2, top, l1, l2):\n",
    "    fig, ax = plt.subplots(2,1, figsize=(8,6))\n",
    "\n",
    "    # ax[0].grid()\n",
    "    ax[1].bar(x=range(len(top)), width=0.8, height=[t[2] for t in top], alpha=1.0, color=\"black\", label=f\"{l1} - {l2}\")\n",
    "    ax[0].bar(x=np.arange(len(top)) + 0.2, width=0.4, height=[d1[t[0]] if t[0]>=0 else 0 for t in top], alpha=1.0, color=\"green\", label=l1)    \n",
    "    ax[0].bar(x=np.arange(len(top)) - 0.2, width=0.4, height=[d2[t[0]] if t[0]>=0 else 0 for t in top], alpha=1.0, color=\"blue\", label=l2)\n",
    "    ax[0].set_xticks(ticks=range(len(top)), labels=[t[3] for t in top], rotation=90)\n",
    "    ax[1].set_xticks(ticks=range(len(top)), labels=[str(t[0]) if t[0]>=0 else \"*\" for t in top], rotation=90)\n",
    "\n",
    "    #ax[0].grid()\n",
    "    #ax[1].grid()\n",
    "\n",
    "    ax[1].plot([0,len(top) - 0.6], [0,0], lw=1, color='black')\n",
    "\n",
    "    ax[0].set_ylabel(\"prob.\")\n",
    "    ax[1].set_ylabel(\"prob. diff\")\n",
    "\n",
    "    fig.legend()\n",
    "    fig.tight_layout()\n",
    "\n",
    "plotdist(d1=d1, d2=d2, top=top, l1='imdb train', l2='imdb test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# toks = torch.tensor(toks_of_texts(imdb_train['text'])).to(\"cpu\")\n",
    "#task.model.roberta.embeddings.word_embeddings(toks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embs = task.model.roberta.embeddings.word_embeddings.to(\"cpu\")(toks.to(\"cpu\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embs = embs.detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.hist2d(embs[:,0], embs[:,1])"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3.7.13 ('python37_pytorch_cuda')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "5c887fbf053ca26987bbc53439e0f6b5aed35740853e2f251f93c3825002a8ec"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
