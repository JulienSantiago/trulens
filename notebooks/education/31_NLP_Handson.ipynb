{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S2--1HVkQVtC"
      },
      "source": [
        "TODO: some description here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q_wpBSP5QRwm"
      },
      "source": [
        "# Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gyqRPOXI8cmt"
      },
      "outputs": [],
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "\n",
        "import sys\n",
        "\n",
        "# Use this if running this notebook from within its place in the trulens repository.\n",
        "# sys.path.insert(0, \"../..\")\n",
        "# Or otherwise install trulens.\n",
        "try:\n",
        "  import trulens\n",
        "except Exception:\n",
        "  ! {sys.executable} -m pip install git+https://github.com/truera/trulens.git@piotrm/aiq-nlp\n",
        "\n",
        "# Install other requirements.\n",
        "try:\n",
        "  import datasets\n",
        "  import domonic\n",
        "  import gdown\n",
        "  import openTSNE\n",
        "  import torch\n",
        "  import transformers\n",
        "except Exception:\n",
        "  ! {sys.executable} -m pip install transformers datasets openTSNE domonic==0.9.8 gdown torch\n",
        "\n",
        "import base64\n",
        "import functools\n",
        "import multiprocessing as mp\n",
        "import os\n",
        "from pathlib import Path\n",
        "import re\n",
        "from typing import Callable, Dict, List, Tuple\n",
        "\n",
        "from datasets import load_dataset\n",
        "from IPython.display import clear_output\n",
        "from IPython.display import display\n",
        "from ipywidgets import interact\n",
        "from ipywidgets import interactive\n",
        "from ipywidgets import widgets\n",
        "import numpy as np\n",
        "import numpy.typing as npt\n",
        "import pandas as pd\n",
        "import plotly.express as px\n",
        "import plotly.graph_objs as go\n",
        "import torch\n",
        "from tqdm.auto import tqdm\n",
        "from transformers import AutoModelForSequenceClassification\n",
        "from transformers import AutoTokenizer\n",
        "import transformers as hugs\n",
        "\n",
        "from trulens.nn.attribution import Cut\n",
        "from trulens.nn.attribution import IntegratedGradients\n",
        "from trulens.nn.attribution import OutputCut\n",
        "from trulens.nn.models import get_model_wrapper\n",
        "from trulens.nn.quantities import ClassQoI\n",
        "from trulens.utils.nlp import token_baseline\n",
        "from trulens.utils.typing import ModelInputs\n",
        "\n",
        "\n",
        "Figure = go.FigureWidget # use this if running in vscode\n",
        "# Figure = go.Figure # use this if running in google colab or jupyter\n",
        "\n",
        "# Download some pre-computed data.\n",
        "if not Path(\"tsne_embedding.lzma\").exists():\n",
        "  import gdown\n",
        "  gdown.download(\n",
        "    \"https://drive.google.com/file/d/1ZA8jyv026Q7T1RCJFtxxfCUl1JHXNFVP/view?usp=sharing\",\n",
        "    fuzzy=True, resume=True\n",
        ")\n",
        "\n",
        "# os.environ['TOKENIZERS_PARALLELISM'] = '0'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RMf8BfYp8cmu"
      },
      "source": [
        "# Twitter Sentiment Model\n",
        "\n",
        "[Huggingface](https://huggingface.co/models) offers a variety of pre-trained NLP models to explore. We exemplify in this notebook a [transformer-based twitter sentiment classification model](https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english).\n",
        "\n",
        "In the below cell, we point out, with `HUGS`, elements that you would need to update to replace the given model with another hugging face model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g4UOObDW8cmu"
      },
      "outputs": [],
      "source": [
        "# AIQ: Talk about how this is required to update. Homework. Try out your own model.\n",
        "\n",
        "\n",
        "# Wrap all of the components needed to run a model.\n",
        "class TwitterSentiment:\n",
        "    # device = torch.device(\"cpu\", 0)\n",
        "    # Can also use cuda if available:\n",
        "    device = torch.device(\"cuda\", 0)\n",
        "\n",
        "    # HUGS: model name, see https://huggingface.co/models for others\n",
        "    # https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english\n",
        "    MODEL = f\"distilbert-base-uncased-finetuned-sst-2-english\"\n",
        "\n",
        "    model: hugs.PreTrainedModel = AutoModelForSequenceClassification.from_pretrained(\n",
        "        MODEL\n",
        "    ).to(device)\n",
        "\n",
        "    tokenizer: hugs.PreTrainedTokenizer = AutoTokenizer.from_pretrained(MODEL)\n",
        "\n",
        "    # HUGS: the embeddings vectors, one for each token\n",
        "    embeddings: npt.NDArray[np.float32] = \\\n",
        "        model.distilbert.embeddings.word_embeddings.weight.detach().cpu().numpy()\n",
        "\n",
        "    # HUGS: name of the layer that produces token embeddings. The trulens\n",
        "    # wrapping cell later in this notebook can be helpful in figuring out this\n",
        "    # parameter.\n",
        "    embeddings_layer: str = 'distilbert_embeddings_word_embeddings'\n",
        "\n",
        "    # number of dimensions in token embedding\n",
        "    embedding_size: int = embeddings.shape[1]\n",
        "\n",
        "    # maximum number of tokens to send to model\n",
        "    max_length: int = 128\n",
        "\n",
        "    # HUGS: Maximum number of instances we can evaluate the model on at once. This is\n",
        "    # necessary when using a GPU with a limited amount of memory.\n",
        "    rebatch_size: int = 16\n",
        "\n",
        "    id_of_token: Dict[str, int] = tokenizer.get_vocab()\n",
        "    token_of_id: Dict[int, str] = {v: k for k, v in id_of_token.items()}\n",
        "\n",
        "    # number of tokens in vocabulary\n",
        "    vocab_size: int = len(id_of_token)\n",
        "\n",
        "    def _vocab(token_of_id, vocab_size):\n",
        "        # Python list comprehension scoping workaround\n",
        "        return np.array([token_of_id[i] for i in range(vocab_size)])\n",
        "\n",
        "    # tokens in order\n",
        "    vocab: npt.NDArray[str] = _vocab(token_of_id, vocab_size)\n",
        "\n",
        "    labels = ['negative', 'positive']\n",
        "\n",
        "    NEGATIVE: int = labels.index('negative')\n",
        "    POSITIVE: int = labels.index('positive')\n",
        "\n",
        "    def tokenize(texts: List[str]) -> Dict[str, torch.Tensor]:\n",
        "        \"\"\"\n",
        "        Tokenize a list of `texts` into a form appropriate for `TwitterSentiment.model` .\n",
        "        \"\"\"\n",
        "        return TwitterSentiment.tokenizer(\n",
        "            texts,\n",
        "            padding=True,\n",
        "            truncation=True,\n",
        "            max_length=TwitterSentiment.max_length,\n",
        "            return_tensors='pt'\n",
        "        ).to(TwitterSentiment.device)\n",
        "\n",
        "    def evaluate_to_logits(\n",
        "        texts: List[str], batch_size=rebatch_size\n",
        "    ) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Evaluate a collection of `texts` into their logits scores.\n",
        "        \"\"\"\n",
        "\n",
        "        logits = []\n",
        "\n",
        "        inputs = TwitterSentiment.tokenize(texts)\n",
        "\n",
        "        for idx in tqdm(range(0, len(texts), batch_size),\n",
        "                        desc=\"evaluating model\"):\n",
        "\n",
        "            batch_logits = TwitterSentiment.model(\n",
        "                input_ids=inputs['input_ids'][idx:idx + batch_size],\n",
        "                attention_mask=inputs['attention_mask'][idx:idx + batch_size]\n",
        "            ).logits\n",
        "\n",
        "            logits.append(batch_logits.detach())\n",
        "\n",
        "        return torch.concat(logits).detach().cpu()\n",
        "\n",
        "    def evaluate_to_probits(\n",
        "        texts: List[str], batch_size=rebatch_size\n",
        "    ) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Evaluate a collection of `texts` into their probits scores.\n",
        "        \"\"\"\n",
        "\n",
        "        logits = TwitterSentiment.evaluate_to_logits(\n",
        "            texts, batch_size=batch_size\n",
        "        )\n",
        "        return torch.nn.functional.softmax(logits, dim=1).detach().cpu()\n",
        "\n",
        "    # HUGS\n",
        "    def token_str(token_id: int) -> str:\n",
        "        \"\"\"\n",
        "        Given a `token_id`, produce a string of how it should be drawn.\n",
        "        \"\"\"\n",
        "        tok = TwitterSentiment.tokenizer.decode(token_id)\n",
        "        if tok.startswith(\"##\"):\n",
        "            # token starts with \"##\" to denote a word postfix\n",
        "            return tok[2:]\n",
        "        else:\n",
        "            # if not a postfix, add space better indicate a complete word\n",
        "            # separation\n",
        "            return \" \" + tok\n",
        "\n",
        "\n",
        "task = TwitterSentiment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GkxHugWm8cmv"
      },
      "source": [
        "## Data pre-processing\n",
        "\n",
        "This section demonstrates the initial steps of an NLP model evaluation, the tokenization and conversion to embeddings."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QvFb7aUt8cmv"
      },
      "outputs": [],
      "source": [
        "# AIQ\n",
        "# TODO: add comment on how the output is to be interpreted\n",
        "\n",
        "# Utilities we will use for interactive parts of this notebook. Please ignore.\n",
        "aiq_layout = dict(\n",
        "    border=\"10px solid teal\", padding=\"5px\", width=\"100%\", margin=\"0px\"\n",
        ")\n",
        "\n",
        "# Interaction utilities.\n",
        "textbox = (\n",
        "    lambda t=\n",
        "    \"I'm a sentence. The last part of this sentence is not a real wordle.\", c=\n",
        "    True, d=\"input\": widgets.Text(\n",
        "        value=t,\n",
        "        continuous_update=c,\n",
        "        layout=aiq_layout,\n",
        "        description=d + (\" (enter to update)\" if not c else \"\"),\n",
        "        style={'description_width': 'initial'}\n",
        "    )\n",
        ")\n",
        "\n",
        "\n",
        "@interact(text=textbox())\n",
        "def show_parse(text: str):\n",
        "\n",
        "    print(\"INPUT TEXT\\n\", text, \"\\n\")\n",
        "\n",
        "    # Input sentences need to be tokenized first.\n",
        "    inputs = task.tokenize([text])\n",
        "\n",
        "    # The tokenizer gives us vocabulary indexes for each input token (in this case,\n",
        "    # words and some word parts like the \"'m\" part of \"I'm\" are tokens).\n",
        "\n",
        "    print(\"MODEL INPUTS\\n\", inputs, \"\\n\")\n",
        "\n",
        "    # Decode helps inspecting the tokenization produced:\n",
        "    tokens = task.tokenizer.batch_decode(torch.flatten(inputs['input_ids']))\n",
        "\n",
        "    # Normally decode would give us a single string for each sentence but we would\n",
        "    # not be able to see some of the non-word tokens there. Flattening first gives\n",
        "    # us a string for each input_id.\n",
        "\n",
        "    print(\"TOKENS\\n\", tokens, \"\\n\")\n",
        "\n",
        "    # Each token is represented by a dense vector in the model.\n",
        "    toks = inputs['input_ids'].detach().cpu().numpy()\n",
        "    embs = np.array([task.embeddings[token_id] for token_id in toks])[0]\n",
        "\n",
        "    print(\"EMBEDDINGS\\n\", embs, \"\\n\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eT5MgR9c8cmw"
      },
      "source": [
        "## Running the model\n",
        "\n",
        "Evaluating huggingface models is straight-forward if we use the structure produced by the tokenizer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dk2_xQVs8cmw"
      },
      "outputs": [],
      "source": [
        "# AIQ\n",
        "# TODO: add comment on how the output is to be interpreted\n",
        "\n",
        "model_results = []\n",
        "\n",
        "\n",
        "@interact(text=textbox())\n",
        "def show_output(text):\n",
        "    global model_results\n",
        "    results = model_results\n",
        "\n",
        "    # Get the model appropriate inputs from a single text instance:\n",
        "    inputs = task.tokenize([text])\n",
        "\n",
        "    # Run the model on it:\n",
        "    outputs = task.model(**inputs)\n",
        "\n",
        "    # From logits we can extract the most likely class for each sentence and its\n",
        "    # readable label.\n",
        "    predictions = [task.labels[i] for i in outputs.logits.argmax(axis=1)]\n",
        "\n",
        "    results.insert(\n",
        "        0, (predictions[0], outputs.logits.detach().cpu().numpy()[0], text)\n",
        "    )\n",
        "    results = results[0:10]\n",
        "\n",
        "    for result in results:\n",
        "        print(*result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DSzU71Py8cmw"
      },
      "source": [
        "## Embedding Space\n",
        "\n",
        "This section visualizes the model's embedding space. It is based on TSNE dimensionality reduction that takes reduces 768 dimensional embedding vectors into just 2 dimensions. Ideally tokens that are nearby in the original space should show up nearby in the visualization but this naturally not exact. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uY9WCPPb8cmw"
      },
      "outputs": [],
      "source": [
        "# AIQ: The following computation takes too long on colab. The results should\n",
        "# have been downloaded for you earlier in this notebook.\n",
        "\n",
        "tsne_filename = Path(\"tsne_embedding.lzma\")\n",
        "if tsne_filename.exists():\n",
        "    print(\"loading\")\n",
        "    import pickle\n",
        "    import lzma\n",
        "    with lzma.open(tsne_filename, mode='rb') as fh:\n",
        "        tsne_embedding = pickle.load(fh)\n",
        "\n",
        "else:\n",
        "    print(\n",
        "        \"computing, if you are running this in colab, be prepared to wait a long time\"\n",
        "    )\n",
        "    from openTSNE import TSNE\n",
        "    import pickle\n",
        "    import lzma\n",
        "    man = TSNE(\n",
        "        n_jobs=mp.cpu_count(),\n",
        "        verbose=True,\n",
        "        n_iter=10000,\n",
        "        learning_rate=200,\n",
        "        negative_gradient_method='bh',\n",
        "        metric=\"cosine\"\n",
        "    )\n",
        "    tsne_embedding = man.fit(task.embeddings)\n",
        "\n",
        "    print(\"saving\")\n",
        "    with lzma.open(tsne_filename, mode='wb') as fh:\n",
        "        pickle.dump(obj=tsne_embedding, file=fh)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FVmxbI-b8cmx"
      },
      "outputs": [],
      "source": [
        "# AIQ: This is computationally intensive picture.\n",
        "\n",
        "fig = Figure(layout=dict(width=800, height=800))\n",
        "fig.add_scatter(\n",
        "    x=tsne_embedding[:, 0],\n",
        "    y=tsne_embedding[:, 1],\n",
        "    text=task.vocab[:],\n",
        "    mode='markers',\n",
        "    marker_size=2\n",
        ")\n",
        "display(fig)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U_gvi71u8cmx"
      },
      "source": [
        "## Performance\n",
        "\n",
        "We load a [rotten tomatoes movie review sentiment dataset](https://huggingface.co/datasets/rotten_tomatoes) as the first source of data. Later in the drift section we will load a different sentiment dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UAHkKXqH8cmx"
      },
      "outputs": [],
      "source": [
        "# https://huggingface.co/cardiffnlp/twitter-roberta-base-sentiment\n",
        "rotten_train = load_dataset(\"rotten_tomatoes\", split=\"train\")\n",
        "rotten_test = load_dataset(\"rotten_tomatoes\", split=\"test\")\n",
        "rotten_texts = rotten_train['text'] + rotten_test['text']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CL-oiXGf8cmx"
      },
      "outputs": [],
      "source": [
        "def accuracy(X: npt.NDArray[float], Y_true: npt.NDArray[int]) -> float:\n",
        "    \"\"\"\n",
        "    Determine model accuracy on the given dataset `X` with ground truth labels\n",
        "    `Y_true`. If this is running slowly, you might be running without GPU. \n",
        "    \"\"\"\n",
        "    \n",
        "    Y_probits = task.evaluate_to_probits(X).detach().cpu().numpy()\n",
        "\n",
        "    Y_pred = np.argmax(Y_probits, axis=1)\n",
        "\n",
        "    correct = Y_pred == Ytrue\n",
        "\n",
        "    return correct.mean()\n",
        "\n",
        "for dataset_name, X, Ytrue in [\n",
        "    (\"rotten train\", rotten_train['text'], rotten_train['label']),\n",
        "    (\"rotten test\", rotten_test['text'], rotten_test['label'])]:\n",
        "    print(dataset_name, f\"accuracy = {accuracy(X, Ytrue) * 100:0.2f} %\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pfPwNFdG8cmx"
      },
      "source": [
        "## Trulens: Model Wrapper\n",
        "\n",
        "As in the prior notebooks, we need to wrap the pytorch model with the appropriate Trulens functionality."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hw79gFcW8cmx"
      },
      "outputs": [],
      "source": [
        "task.wrapper = get_model_wrapper(task.model, device=task.device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FG-7G_Dd8cmy"
      },
      "outputs": [],
      "source": [
        "task.wrapper.print_layer_names()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NzPEjNgy8cmy"
      },
      "source": [
        "# Attributions/Explainability"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RhL_c8im8cmy"
      },
      "outputs": [],
      "source": [
        "# AIQ: TODO: Explain this.\n",
        "\n",
        "common_attributor_arguments = dict(\n",
        "    model=task.wrapper,\n",
        "    resolution=128,\n",
        "    rebatch_size=32,\n",
        "    doi_cut=Cut(task.embeddings_layer),\n",
        "    qoi=ClassQoI(task.POSITIVE),\n",
        "    qoi_cut=OutputCut(accessor=lambda o: o['logits'])\n",
        ")\n",
        "\n",
        "infl = IntegratedGradients(\n",
        "    **common_attributor_arguments\n",
        ")\n",
        "\n",
        "from trulens.visualizations import NLP\n",
        "\n",
        "V = NLP(\n",
        "    wrapper=task.wrapper,\n",
        "    labels=task.labels,\n",
        "    decode=task.token_str,\n",
        "    tokenize=lambda sentences: ModelInputs(kwargs=task.tokenize(sentences,)).\n",
        "    map(lambda t: t.to(task.device)),\n",
        "    # huggingface models can take as input the keyword args as per produced by\n",
        "    # their tokenizers.\n",
        "    input_accessor=lambda x: x.kwargs['input_ids'],\n",
        "    # for huggingface models, input/token ids are under input_ids key in the\n",
        "    # input dictionary\n",
        "    output_accessor=lambda x: x['logits'],\n",
        "    # and logits under 'logits' key in the output dictionary\n",
        "    hidden_tokens=set([task.tokenizer.pad_token_id])\n",
        "    # do not display these tokens\n",
        ")\n",
        "\n",
        "results = []\n",
        "\n",
        "@interact(text=textbox())\n",
        "def show_attribution(text):\n",
        "    global results\n",
        "\n",
        "    # Token attribution visualization takes in a list of sentences and the\n",
        "    # attribution method to compute the attributions.\n",
        "    token_attribution = V.tokens([text], infl) \n",
        "\n",
        "    results.insert(0, token_attribution)\n",
        "    results = results[:10]\n",
        "\n",
        "    for result in results:\n",
        "        display(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QmVXexNM8cmy"
      },
      "source": [
        "## Baselines\n",
        "\n",
        "We see in the above results that special tokens such as the sentence end **&lt;/s&gt;** contributes are found to contribute a lot to the model outputs. While this may be useful in some contexts, we are more interested in the contributions of the actual words in these sentences. To focus on the words more, we need to adjust the **baseline** used in the integrated gradients computation. By default in the instantiation so far, the baseline for each token is a zero vector of the same shape as its embedding. By making the basaeline be identicaly to the explained instances on special tokens, we can rid their impact from our measurement. Trulens provides a utility for this purpose in terms of `token_baseline` which constructs for you the methods to compute the appropriate baseline. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SxNN3f0T8cmy"
      },
      "outputs": [],
      "source": [
        "inputs_baseline_ids, inputs_baseline_embeddings = token_baseline(\n",
        "    keep_tokens=set([task.tokenizer.cls_token_id, task.tokenizer.sep_token_id]),\n",
        "    # Which tokens to preserve.\n",
        "    replacement_token=task.tokenizer.pad_token_id,\n",
        "\n",
        "    # AIQ: Try changing `replacement_token` parameter to other special or non\n",
        "    # special tokens.\n",
        "\n",
        "    # replacement_token=task.tokenizer.mask_token_id,\n",
        "    # replacement_token=task.tokenizer.vocab[\"happy\"],\n",
        "\n",
        "    # What to replace tokens with.\n",
        "    input_accessor=lambda x: x.kwargs['input_ids'],\n",
        "    ids_to_embeddings=task.model.get_input_embeddings()\n",
        "    # Callable to produce embeddings from token ids.\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MlRZmU_C8cmy"
      },
      "source": [
        "We can now inspect the baselines on some example sentences. The first method returned by `token_baseline` gives us token ids to inspect while the second gives us the embeddings of the baseline which we will pass to the attributions method."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GrOpj-hN8cmy"
      },
      "outputs": [],
      "source": [
        "# AIQ\n",
        "\n",
        "infl_positive_baseline = IntegratedGradients(\n",
        "    baseline=inputs_baseline_embeddings, **common_attributor_arguments\n",
        ")\n",
        "\n",
        "results2 = []\n",
        "\n",
        "\n",
        "@interact(text=textbox(c=False))\n",
        "def show_attribution(text):\n",
        "    global results2\n",
        "\n",
        "    default_result = widgets.HTML(V.tokens([text], infl).data)\n",
        "    baseline_result = widgets.HTML(\n",
        "        V.tokens([text], infl_positive_baseline).data\n",
        "    )\n",
        "\n",
        "    results2.insert(0, (default_result, baseline_result))\n",
        "    results2 = results2[:3]\n",
        "\n",
        "    parts = []\n",
        "\n",
        "    for result in results2:\n",
        "        parts.append(widgets.HBox(result))\n",
        "\n",
        "    display(widgets.VBox(parts))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R2x8vTQe8cmy"
      },
      "source": [
        "# Fairness"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z6_Vql4Z8cmy"
      },
      "outputs": [],
      "source": [
        "def word_pattern(word: str) -> str:\n",
        "    \"\"\"\n",
        "    Create a pattern that matches the given `word` as long as it is not\n",
        "    immediately next to an alpha-numeric character.\n",
        "    \"\"\"\n",
        "    return \"(?<!\\w)\" + re.escape(word) + \"(?!\\w)\"\n",
        "\n",
        "\n",
        "def swap(thing1: str, thing2: str) -> Callable[[str], str]:\n",
        "    \"\"\"\n",
        "    Create a method to swap occurances of `thing1` and `thing2`.\n",
        "    \"\"\"\n",
        "\n",
        "    pat_swapper = re.compile(r\":swapper:\")\n",
        "    pat1 = re.compile(word_pattern(thing1), re.IGNORECASE)\n",
        "    pat2 = re.compile(word_pattern(thing2), re.IGNORECASE)\n",
        "\n",
        "    def f(sentence: str):\n",
        "        \"\"\"\n",
        "        Swap instances of thing1 and thing2 in sentence.\n",
        "        \"\"\"\n",
        "\n",
        "        temp1 = pat1.sub(\":swapper:\", sentence)\n",
        "        temp2 = pat2.sub(thing1, temp1)\n",
        "        temp3 = pat_swapper.sub(thing2, temp2)\n",
        "        return temp3\n",
        "\n",
        "    return f\n",
        "\n",
        "\n",
        "def contains(s: str, pat: re.Pattern) -> bool:\n",
        "    \"\"\"\n",
        "    Determine whether the given string `s` satisfies regular expression `pat`.\n",
        "    \"\"\"\n",
        "    return pat.search(s) is not None\n",
        "\n",
        "\n",
        "def get_sentence_pairs(token_pairs: List[Tuple[str, str]],\n",
        "                       texts: List[str]) -> List[Tuple[str, str]]:\n",
        "    \"\"\"\n",
        "    Create sentence pairs from examples in `texts` that swap words from the\n",
        "    pairs list `token_pairs`.\n",
        "    \"\"\"\n",
        "\n",
        "    patterns = [\n",
        "        re.compile(\n",
        "            \"|\".join([word_pattern(tok) for tok in pair]), re.IGNORECASE\n",
        "        ) for pair in token_pairs\n",
        "    ]\n",
        "    swappers = [swap(*pair) for pair in token_pairs]\n",
        "\n",
        "    sentence_pairs = [\n",
        "        (sentence, swap(sentence))\n",
        "        for pattern, swap in\n",
        "        tqdm(zip(patterns, swappers), desc=\"finding swap pairs\", unit=\"pair\")\n",
        "        for sentence in texts\n",
        "        if contains(sentence, pattern)\n",
        "    ]\n",
        "\n",
        "    print(f\"found {len(sentence_pairs)} sentence pair(s)\")\n",
        "\n",
        "    return sentence_pairs\n",
        "\n",
        "\n",
        "def compute_pair_disparities(\n",
        "    sentence_pairs: List[Tuple[str, str]]\n",
        ") -> List[Tuple[Tuple[str, str], float]]:\n",
        "    \"\"\"\n",
        "    Given a collection of `sentence_pairs`, produce a list of tuples containing\n",
        "    the pairs as the first element and the disparity in model scores as the\n",
        "    second.\n",
        "    \"\"\"\n",
        "\n",
        "    diffs = []\n",
        "\n",
        "    a_probits = task.evaluate_to_probits([pair[0] for pair in sentence_pairs])\n",
        "    b_probits = task.evaluate_to_probits([pair[1] for pair in sentence_pairs])\n",
        "\n",
        "    for a_probit, b_probit in tqdm(zip(a_probits, b_probits),\n",
        "                                   desc=\"comparing probits\"):\n",
        "\n",
        "        diffs.append(\n",
        "            torch.nn.functional.cross_entropy(\n",
        "                torch.unsqueeze(a_probit, dim=0),\n",
        "                torch.unsqueeze(b_probit, dim=0)\n",
        "            ).detach().cpu().numpy()\n",
        "        )\n",
        "\n",
        "    diffs = np.array(diffs)\n",
        "    diffs_pairs = list(\n",
        "        reversed(sorted(zip(sentence_pairs, diffs), key=lambda pair: pair[1]))\n",
        "    )\n",
        "\n",
        "    return diffs_pairs\n",
        "\n",
        "\n",
        "def show_biggest_disparities(\n",
        "    diffs: List[Tuple[Tuple[str, str], float]],\n",
        "    attributor=infl_positive_baseline,\n",
        "    n=3\n",
        ") -> None:\n",
        "    \"\"\"\n",
        "    Display the top disparate pairs along with their attributions.\n",
        "    \"\"\"\n",
        "\n",
        "    display(\n",
        "        V.tokens_stability(\n",
        "            texts1=[p[0][0] for p in diffs][0:n],\n",
        "            texts2=[p[0][1] for p in diffs][0:n],\n",
        "            attributor=attributor\n",
        "        )\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KMsoRyJQ8cmy"
      },
      "outputs": [],
      "source": [
        "gender_pairs = [\n",
        "    ('he', 'she'),\n",
        "    ('guy', 'gal'),\n",
        "    ('himself', 'herself'),\n",
        "    ('boy', 'girl'),\n",
        "    ('husband', 'wife'),\n",
        "    ('man', 'woman'),\n",
        "    ('men', 'women'),\n",
        "    ('brother', 'sister'),\n",
        "    ('uncle', 'aunt'),\n",
        "    ('nephew', 'niece'),\n",
        "    ('dad', 'mom'),\n",
        "    ('father', 'mother'),\n",
        "    ('son', 'daughter'),\n",
        "    ('actor', 'actress'),\n",
        "    ('male', 'female'),\n",
        "    ('hero', 'heroine'),\n",
        "]\n",
        "\n",
        "sentence_pairs_gender = get_sentence_pairs(gender_pairs, rotten_texts)\n",
        "diffs_pairs_gender = compute_pair_disparities(sentence_pairs_gender)\n",
        "\n",
        "show_biggest_disparities(diffs_pairs_gender)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UfTf1YeB8cmy"
      },
      "outputs": [],
      "source": [
        "# AIQ\n",
        "\n",
        "\n",
        "@interact(\n",
        "    token1=textbox(\"hero\", d=\"token1\", c=False),\n",
        "    token2=textbox(\"heroine\", d=\"token2\", c=False)\n",
        ")\n",
        "def show_disparities(token1, token2):\n",
        "    if token1 == \"\" or token2 == \"\":\n",
        "        return\n",
        "\n",
        "    sentence_pairs = get_sentence_pairs([(token1, token2)], rotten_texts)\n",
        "\n",
        "    if len(sentence_pairs) == 0:\n",
        "        return\n",
        "\n",
        "    diffs_pairs = compute_pair_disparities(sentence_pairs)\n",
        "    show_biggest_disparities(diffs_pairs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VOMP3Ka68cmz"
      },
      "source": [
        "## Gender in embedding space"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sZMW-xJJ8cmz"
      },
      "outputs": [],
      "source": [
        "embeddings: npt.NDArray[float] = task.embeddings\n",
        "\n",
        "# A vector approximating the difference between embeddings of pairs of words\n",
        "# of the opposite gender. This one is for the token embedding used in\n",
        "# distilbert.\n",
        "gender_vector: npt.NDArray['float16'] = np.frombuffer(\n",
        "    base64.b85decode(\n",
        "        b'?4-;r+n_BbxT&BWWUIlV^Qc9YM<?$ffFvBOyeVWPZ6QZ1ttD(IWT3Vs@g>z1(IHALq#<!8`6&IFovN59FQQ<rjVfRxf2awiYAAOf0xHHLzn)JeL8GLalpxcndmiqk3oS&V=c3uGKP<ea=_dCgN-Vq~^C<SDBB=f;{-m#?lBazp6`Fi20ii4{HmQFaeJNI`eJG!zN2p7nT&MsSPpc;*^d-=!XDFVgZ7N3{HKO`0uqm#msif;Ew5=Wq#ib{lJ)>zVdn~jjwWg4$E-Hm9mL8ELf+nV@>7XN}EFh#J6Du&8d89uqq^KIJJfWAH<s(?A83iFILnMlrgei)r<*S^fJgHDAWT^b8{i%wqmaLPhbS9T6Jt#99Sf6L8+bydo;3dAJS}Uz4E1CW%<)@0Q!Ypv2O(|R<c%D?4xR(l{UL)hBP^^)syPV#omZOCyB&3O+Iv%o~8KW|tw5a_oU?h^MQ!4KuK_zc2qAUp_g(MRpzbQy4vZ$V-e<o?1A1Fqg-lZKX51h559ip|Y3@9lngeeCdV4_MV1tujcN-QR*NT-IaRil6?RICoHU?Y?#zN8MQpeo*^uBGuJ$tk-V*C>ytF{UOeM4I9z{VUa<Vx*-d-l$9`fGFf76r}JNuPLCYkTBb<^`&<w5-3!r4Wiwe2&9!9nI?EFcBnfjS1ipX@gcS+XD6R2W3AF1h>!iL@|kC$dla6YHl}MM<0kVa@~U*KA0pVH3?n6;I4Dsj{2DZ*bRJnH^CRFT$f!e}dniDvR;=tH3Mn0=KqeCw)1Lk)mZ)|kEUBrWAE-g9fhH>_i>J6EXBbkVJ|R9RW2H-=R-}(7FDB6^EvphMpez}nRGA2_{iNV0kf_8hG_1QR;-b?fwIDDpnI}W4-5<av4k0q42p)f_%Ay~t-K6*-@+Mjz%$YHy51(}(H7Uv>(<l=m@*IUHzoEq}BB-CJ^(USw-Yae=oTara=C19iiYMJ3?yV>$*qY=iC!+l-R;E;>I4FCe5iVFIf2tZC5UZpoNG!mnxS~QM38iYMm?*WNN1_!Zh^a27Eu`Qoc$}IfR+ikN7@WSNXQ}EcY!wqH2PuOl#HIx#l_0O7ogf&f@u_Slmm`>_AgeQ^RwvCWk}20BY9RBYb|sFgpeAH0Dx#e#tt9TASt-Dq@1$NOs3duoK`EM`FQsOske;`wT_Z0g0w<>+xh5K%eJIJG%cHs?)}7U(Vx_Yrcpk?fF(C#ZaHY4W)TXW`zn~3}NvZsnOQu36Q6W#L)T+R%yeKOyr=sMY+ou^QsHsz@^&6EdH6?hZOqr#q&8vec0VOpo7b`0ylcE<TLM8XAv7>h-WRQC!;i)z*1gXcS@g$NXP%J#EOdu8^39HJf-!4$5t1F|X>nB1bOs2G`_@1C8;w3n$h$<i`f+X;tZX^b#;Ucys8?I0(!zlPFE1h{F1FV`R&#Ig!^d}%BvmpE}lBB68#i@S|a3H`YHy+|3qoeSwl&35tv!2JIffi^f_$lxpN~Hd%My0>0I32|%;S;zZ38EJ*&ZM^~8z+q-g_sARQK)mI6e2np%BPPeTP$EE%&YOJg(wxF_94wE;wph611IgKq9+HaTdc1oA14&4ysA2+%p*K3yQG9HBPA3nuBm+}Bc<mgCL^*Z>ztA#$1O7^A1jL_&?9yoLLzG;4W}ldVyJc}@u_hmODydxda9wBL7j9dl?n!@W-8FCn5dm7l`5;HN2rgX&7LBtm8K*teWQw>D=p^^x~S!>GaR6#Mkv3kktndAR2qe)DyAc)iKy+Q&ZhH}K&En`@+qt+lOfouwj+$EbEZh08mM%hgeI${1Sv!*_$5}TS09xr!ljccx+a^awVhe0JfIpJSSmoK&7Kw@hbW;WB_D1h{;R}`OQ|d^%d6>`KqUt#-K4;xLadW0q^d!nF(UV!=_;$CJ1X3%@2E4Yk*i)Mz>y9k<fnnC5Gh|R<|O+jB#_&s2B9h)'\n",
        "    ),\n",
        "    dtype='float16'\n",
        ")\n",
        "\n",
        "\n",
        "def normalize(v: npt.NDArray[float]) -> npt.NDArray[float]:\n",
        "    \"\"\"\n",
        "    Normalize a single vector.\n",
        "    \"\"\"\n",
        "    return v / np.linalg.norm(v, ord=2)\n",
        "\n",
        "\n",
        "def normalize_many(v: npt.NDArray[float]) -> npt.NDArray[float]:\n",
        "    \"\"\"\n",
        "    Normalize an array of vectors.\n",
        "    \"\"\"\n",
        "    return v / np.linalg.norm(v, axis=1, ord=2)[:, np.newaxis]\n",
        "\n",
        "\n",
        "all_embs_norm = normalize_many(embeddings)\n",
        "baseline_penalties = np.abs(np.dot(all_embs_norm, gender_vector))\n",
        "\n",
        "direction_vector: np.ndarray = gender_vector\n",
        "\n",
        "\n",
        "def embedding_opposite_id(emb: np.ndarray) -> Tuple[int, float]:\n",
        "    \"\"\"\n",
        "    Get the token id of the token closest to the gender-opposite of the given\n",
        "    `emb`.\n",
        "    \"\"\"\n",
        "\n",
        "    emb = normalize(emb)\n",
        "    scores = np.abs(\n",
        "        np.dot(\n",
        "            normalize_many(emb - all_embs_norm + 0.000000001), direction_vector\n",
        "        )\n",
        "    ) - 0.55 * baseline_penalties\n",
        "\n",
        "    best = np.argmax(scores)\n",
        "\n",
        "    return best, scores[best]\n",
        "\n",
        "\n",
        "def embedding_opposite(emb: np.ndarray) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Try to find the embedding close to the opposite gender relative to the given\n",
        "    `emb`. \n",
        "    \"\"\"\n",
        "\n",
        "    best_id, best_score = embedding_opposite_id(emb)\n",
        "\n",
        "    if best_score > 0.25:\n",
        "        return embeddings[best_id]\n",
        "    else:\n",
        "        return emb\n",
        "\n",
        "\n",
        "def embedding_neutralize(emb: np.ndarray) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Remove the component of the given embedding that points in the gender\n",
        "    direction.\n",
        "    \"\"\"\n",
        "    return emb - np.dot(emb, direction_vector) * direction_vector\n",
        "\n",
        "\n",
        "@functools.lru_cache(maxsize=len(embeddings))\n",
        "def token_id_opposite(token_id: int):\n",
        "    \"\"\"\n",
        "    Try to find the opposite of `token_id` according to the direction of\n",
        "    `direction_vector`. If a good candidate is not found, returns the given\n",
        "    `token_id` instead.\n",
        "    \"\"\"\n",
        "    best_id, best_score = embedding_opposite_id(all_embs_norm[token_id])\n",
        "\n",
        "    if best_score > 0.20:\n",
        "        return best_id\n",
        "    else:\n",
        "        return token_id\n",
        "\n",
        "\n",
        "def swap_token(token: str) -> str:\n",
        "    \"\"\"\n",
        "    Attempts to find a token of the opposite gender of the given `token`.\n",
        "    \"\"\"\n",
        "\n",
        "    a_id = task.id_of_token[token]\n",
        "    b_id = token_id_opposite(a_id)\n",
        "    return task.token_of_id[b_id]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RWh4ing48cmz"
      },
      "outputs": [],
      "source": [
        "# geometry of gender in embedding space\n",
        "\n",
        "# AIQ: This is computationally intensive picture. It is only useful if you use\n",
        "# the tsne reduction.\n",
        "\n",
        "color = np.dot(normalize_many(task.embeddings), direction_vector)\n",
        "cmin = color.min()\n",
        "cmax = color.max()\n",
        "# adjust this to focus on tokens with highest gender components\n",
        "most_gendered = np.abs(color) >= 0.0 \n",
        "\n",
        "fig = Figure(layout=dict(width=1000, height=1000))\n",
        "fig.add_scatter(\n",
        "    x=tsne_embedding[most_gendered, 0],\n",
        "    y=tsne_embedding[most_gendered, 1],\n",
        "    text=task.vocab[most_gendered],\n",
        "    mode='markers',\n",
        "    marker={\n",
        "        'cmin': cmin,\n",
        "        'cmax': cmax,\n",
        "        'colorscale': \"Picnic\",\n",
        "        'color': color[most_gendered],\n",
        "        'colorbar': dict(thickness=20)\n",
        "    },\n",
        "    marker_size=4,\n",
        ")\n",
        "\n",
        "display(fig)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4DWx1E3u8cmz"
      },
      "outputs": [],
      "source": [
        "def baseline_neutralize(z: torch.Tensor) -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    Given input tensor of embeddings, produce a baseline that removes their\n",
        "    gender component.\n",
        "    \"\"\"\n",
        "\n",
        "    if isinstance(z, torch.Tensor):\n",
        "        z = z.detach().cpu().numpy()\n",
        "\n",
        "    return torch.tensor(\n",
        "        np.array(\n",
        "            [[embedding_neutralize(emb) for emb in instance] for instance in z]\n",
        "        )\n",
        "    ).to(task.device)\n",
        "\n",
        "\n",
        "infl_neutralize_gender = IntegratedGradients(\n",
        "    baseline=baseline_neutralize, **common_attributor_arguments\n",
        ")\n",
        "\n",
        "show_biggest_disparities(diffs_pairs_gender, attributor=infl_positive_baseline)\n",
        "show_biggest_disparities(diffs_pairs_gender, attributor=infl_neutralize_gender)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_gEpAKSy8cmz"
      },
      "outputs": [],
      "source": [
        "# AIQ\n",
        "\n",
        "results3 = []\n",
        "\n",
        "\n",
        "@interact(\n",
        "    text=textbox(\n",
        "        t=\n",
        "        \"Johnson has, in his first film, set himself a task he is not nearly up to.\",\n",
        "        c=False\n",
        "    )\n",
        ")\n",
        "def show_attribution(text):\n",
        "    global results3\n",
        "\n",
        "    default_result = widgets.HTML(V.tokens([text], infl_positive_baseline).data)\n",
        "    baseline_result = widgets.HTML(\n",
        "        V.tokens([text], infl_neutralize_gender).data\n",
        "    )\n",
        "\n",
        "    results3.insert(0, (default_result, baseline_result))\n",
        "    results3 = results3[:3]\n",
        "\n",
        "    parts = []\n",
        "\n",
        "    for result in results3:\n",
        "        parts.append(widgets.HBox(result))\n",
        "\n",
        "    display(widgets.VBox(parts))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U0JNeCce8cmz"
      },
      "source": [
        "# Drift"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q_JVNrjK8cmz"
      },
      "outputs": [],
      "source": [
        "# Get another dataset to compare to.\n",
        "\n",
        "# IMDB dataset is large, will take only a portion for speed:\n",
        "n = 3000\n",
        "\n",
        "imdb_train = load_dataset(\"imdb\", \"plain_text\", split=\"train\")\n",
        "imdb_test = load_dataset(\"imdb\", \"plain_text\", split=\"test\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W-mvzEHPW1BI"
      },
      "outputs": [],
      "source": [
        "# Check performance.\n",
        "\n",
        "for dataset_name, X, Ytrue in [\n",
        "    (\"imdb train\", imdb_train['text'][:n], imdb_train['label'][:n]),\n",
        "    (\"imdb test\", imdb_test['text'][:n], imdb_test['label'][:n])]:\n",
        "    print(dataset_name, f\"accuracy = {accuracy(X, Ytrue) * 100:0.2f} %\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y2oD02St8cmz"
      },
      "source": [
        "### Model score drift\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YW3PHg188cmz"
      },
      "outputs": [],
      "source": [
        "def show_model_score_drift(\n",
        "    texts1: List[str],\n",
        "    texts2: List[str],\n",
        "    n1: str,\n",
        "    n2: str,\n",
        "    score: str = \"positive\"\n",
        ") -> None:\n",
        "    \"\"\"\n",
        "    Given two collections of texts, display model `score` histogram over those\n",
        "    two texts. The other arguments are for labeling the collections. \n",
        "    \"\"\"\n",
        "\n",
        "    scores1 = task.evaluate_to_logits(texts1).detach().cpu().numpy()\n",
        "    scores2 = task.evaluate_to_logits(texts2).detach().cpu().numpy()\n",
        "\n",
        "    data_scores1 = dict(\n",
        "        negative=scores1[:, 0],\n",
        "        positive=scores1[:, 1],\n",
        "    )\n",
        "\n",
        "    data_scores2 = dict(negative=scores2[:, 0], positive=scores2[:, 1])\n",
        "\n",
        "    df1 = pd.DataFrame(data_scores1)\n",
        "    df2 = pd.DataFrame(data_scores2)\n",
        "\n",
        "    s1 = df1[score]\n",
        "    s2 = df2[score]\n",
        "\n",
        "    counts1, bin_edges = np.histogram(s1, bins=20, density=True)\n",
        "    counts2, _ = np.histogram(s2, bins=bin_edges, density=True)\n",
        "\n",
        "    fig = Figure(layout=dict(title=\"model score distributions\"))\n",
        "    bar1 = fig.add_scatter(x=bin_edges, y=counts1, name=n1)\n",
        "    bar2 = fig.add_scatter(x=bin_edges, y=counts2, name=n2)\n",
        "\n",
        "    display(fig)\n",
        "\n",
        "\n",
        "show_model_score_drift(\n",
        "    rotten_train['text'][:n], rotten_test['text'][:n], 'rotten train',\n",
        "    'rotten test'\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z3L3Ylrr8cmz"
      },
      "outputs": [],
      "source": [
        "show_model_score_drift(\n",
        "    rotten_train['text'][:n], imdb_train['text'][:n], 'rotten train',\n",
        "    'imdb train'\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wYszoaB08cmz"
      },
      "outputs": [],
      "source": [
        "def tokenize(portion: List[str]) -> Dict:\n",
        "    \"\"\"\n",
        "    Tokenize into just token_ids, not any of the other model inputs.\n",
        "    \"\"\"\n",
        "    return task.tokenizer.batch_encode_plus(\n",
        "        portion,\n",
        "        add_special_tokens=True,\n",
        "        return_attention_mask=False,\n",
        "        max_length=task.max_length,\n",
        "        truncation=True\n",
        "    )['input_ids']\n",
        "\n",
        "\n",
        "p = mp.Pool(mp.cpu_count())\n",
        "\n",
        "\n",
        "def toks_of_texts(texts: List[str]) -> npt.NDArray[int]:\n",
        "    toks = p.map(\n",
        "        tokenize,\n",
        "        [texts[1000 * i:1000 * (i + 1)] for i in range(len(texts) // 1000)]\n",
        "    )\n",
        "    all = np.array([i for tok in toks for t in tok for i in t])\n",
        "\n",
        "    return all\n",
        "\n",
        "\n",
        "def dists_of_texts(\n",
        "    texts: List[str]\n",
        ") -> Tuple[npt.NDArray[int], npt.NDArray[float]]:\n",
        "    all = toks_of_texts(texts)\n",
        "\n",
        "    counts = np.zeros(task.tokenizer.vocab_size)\n",
        "    total = len(all)\n",
        "    for i in all:\n",
        "        counts[i] += 1\n",
        "\n",
        "    dist = counts / total\n",
        "\n",
        "    return counts, dist\n",
        "\n",
        "\n",
        "def tops_of_texts(texts: List[str], n: int = 10) -> List[int]:\n",
        "    \"\"\"\n",
        "    Get the indices of the most frequent tokens in the collection of `texts`.\n",
        "    \"\"\"\n",
        "\n",
        "    counts, dist = dists_of_texts(texts)\n",
        "\n",
        "    return tops_of_dists(counts, dist, n=n)\n",
        "\n",
        "\n",
        "def tops_of_dists(c: npt.NDArray[int],\n",
        "                  d: npt.NDArray[float],\n",
        "                  n=10) -> List[int]:\n",
        "    sortindex = np.argsort(d)\n",
        "    top = []\n",
        "\n",
        "    for idx in sortindex[0:n]:\n",
        "        top.append((idx, c[idx], d[idx], task.tokenizer.decode(idx)))\n",
        "\n",
        "    crest_pos = 0\n",
        "    crest_neg = 0\n",
        "    drest_pos = 0\n",
        "    drest_neg = 0\n",
        "\n",
        "    for idx in sortindex[n:-n]:\n",
        "        if c[idx] >= 0:\n",
        "            crest_pos += c[idx]\n",
        "            drest_pos += d[idx]\n",
        "        else:\n",
        "            crest_neg += c[idx]\n",
        "            drest_neg += d[idx]\n",
        "\n",
        "    top.append((-1, crest_neg, drest_neg, \"*\"))\n",
        "    top.append((-1, crest_pos, drest_pos, \"*\"))\n",
        "\n",
        "    for idx in sortindex[-n:]:\n",
        "        top.append((idx, c[idx], d[idx], task.tokenizer.decode(idx)))\n",
        "\n",
        "    return top"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "leejbanU8cmz"
      },
      "outputs": [],
      "source": [
        "def plotdist(\n",
        "    d1: npt.NDArray[float], d2: npt.NDArray[float], top, l1: str, l2: str\n",
        ") -> None:\n",
        "\n",
        "    n = len(top)\n",
        "\n",
        "    dprobs = pd.DataFrame(\n",
        "        {\n",
        "            \"token\": [t[3] for t in top] * 2,\n",
        "            \"dataset\": ([l1] * n) + ([l2] * n),\n",
        "            \"prob\": [d1[t[0]] for t in top] + [d2[t[0]] for t in top]\n",
        "        }\n",
        "    )\n",
        "    fig = px.bar(dprobs, x=\"token\", y=\"prob\", color=\"dataset\", barmode='group')\n",
        "    display(fig)\n",
        "\n",
        "    ddiff = pd.DataFrame(\n",
        "        {\n",
        "            \"token\": [t[3] for t in top],\n",
        "            \"prob\": [t[2] for t in top]\n",
        "        }\n",
        "    )\n",
        "    fig = px.bar(ddiff, x=\"token\", y=\"prob\")\n",
        "    display(fig)\n",
        "\n",
        "\n",
        "c1, d1 = dists_of_texts(imdb_train['text'][:n])\n",
        "c2, d2 = dists_of_texts(rotten_train['text'][:n])\n",
        "top = tops_of_dists(c1 - c2, d1 - d2, n=20)\n",
        "\n",
        "plotdist(d1, d2, top, l1='imdb', l2='rotten')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qRSHyeg_8cmz"
      },
      "outputs": [],
      "source": [
        "c1, d1 = dists_of_texts(imdb_train['text'][:n])\n",
        "c2, d2 = dists_of_texts(imdb_test['text'][:n])\n",
        "top = tops_of_dists(c1 - c2, d1 - d2, n=20)\n",
        "\n",
        "plotdist(d1=d1, d2=d2, top=top, l1='imdb train', l2='imdb test')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u2oHZBJt8cmz"
      },
      "source": [
        "## Drift in embedding distribution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nlZIYs-d8cm0"
      },
      "outputs": [],
      "source": [
        "c1, d1 = dists_of_texts(rotten_train['text'][:n])\n",
        "c2, d2 = dists_of_texts(imdb_train['text'][:n])\n",
        "\n",
        "data1 = dict(prob=d1, token_id=range(len(task.embeddings)))\n",
        "data1.update({f\"dim{did}\": task.embeddings[:, did] for did in range(768)})\n",
        "\n",
        "df1 = pd.DataFrame(data1)\n",
        "\n",
        "data2 = dict(prob=d2, token_id=range(len(task.embeddings)))\n",
        "\n",
        "data2.update({f\"dim{did}\": task.embeddings[:, did] for did in range(768)})\n",
        "\n",
        "df2 = pd.DataFrame(data2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c_pKegDP8cm0"
      },
      "outputs": [],
      "source": [
        "def show_hists(\n",
        "    s1: pd.Series, s2: pd.Series, df1: pd.DataFrame, df2: pd.DataFrame,\n",
        "    title: str\n",
        ") -> None:\n",
        "    counts1, bin_edges = np.histogram(s1, bins=20, weights=df1.prob.values)\n",
        "    counts2, _ = np.histogram(s2, bins=bin_edges, weights=df2.prob.values)\n",
        "\n",
        "    fig = go.Figure(layout=dict(title=title))\n",
        "    fig.update_layout(xaxis_title=\"Dimension's value\", yaxis_title=\"Density\")\n",
        "    bar1 = fig.add_scatter(x=bin_edges, y=counts1, name=\"rotten\")\n",
        "    bar2 = fig.add_scatter(x=bin_edges, y=counts2, name=\"imdb\")\n",
        "\n",
        "    display(fig)\n",
        "\n",
        "\n",
        "@interact(dim=widgets.IntSlider(value=0, min=0, max=767, layout=aiq_layout))\n",
        "def show_dim_hist(dim):\n",
        "    show_hists(\n",
        "        df1[f'dim{dim}'],\n",
        "        df2[f'dim{dim}'],\n",
        "        df1,\n",
        "        df2,\n",
        "        title=f\"embedding dimension {dim}\"\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nhLC0ctD8cm0"
      },
      "source": [
        "## Drift in gender dimension distribution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mPBWRuw78cm0"
      },
      "outputs": [],
      "source": [
        "data1g = dict(\n",
        "    gender=np.dot(task.embeddings, direction_vector),\n",
        "    prob=d1,\n",
        "    token_id=range(len(task.embeddings))\n",
        ")\n",
        "\n",
        "df1g = pd.DataFrame(data1g)\n",
        "\n",
        "data2g = dict(\n",
        "    gender=np.dot(task.embeddings, direction_vector),\n",
        "    prob=d2,\n",
        "    token_id=range(len(task.embeddings))\n",
        ")\n",
        "\n",
        "df2g = pd.DataFrame(data2g)\n",
        "\n",
        "show_hists(\n",
        "    df1g.gender, df2g.gender, df1g, df2g, title=\"gender dimension histogram\"\n",
        ")\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "anaconda-cloud": {},
    "colab": {
      "collapsed_sections": [
        "pfPwNFdG8cmx",
        "NzPEjNgy8cmy",
        "QmVXexNM8cmy",
        "R2x8vTQe8cmy",
        "VOMP3Ka68cmz",
        "U0JNeCce8cmz",
        "Y2oD02St8cmz",
        "u2oHZBJt8cmz",
        "nhLC0ctD8cm0"
      ],
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3.7.13 ('python37_pytorch_cuda')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.13"
    },
    "vscode": {
      "interpreter": {
        "hash": "5c887fbf053ca26987bbc53439e0f6b5aed35740853e2f251f93c3825002a8ec"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
