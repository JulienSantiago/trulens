{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## QS Relevance Feedback Requirements\n",
    "1. Relevance requires adherence to the entire query.\n",
    "2. Admitting 'I don't know' and refusals are still relevant.\n",
    "3. Feedback mechanism should differentiate between seeming and actual relevance.\n",
    "4. Relevant but inconclusive statements should get increasingly high scores as they are more helpful for answering the query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"...\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports main tools:\n",
    "from trulens_eval.feedback import OpenAI\n",
    "openai = OpenAI()\n",
    "relevance = openai.relevance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Relevance requires adherence to the entire query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = relevance(\"Name some famous dental floss brands\",\"Oral B is a famous dental hygiene brand.\")\n",
    "assert score >= 0.5, f\"Score of {score} < 0.5. Statement is relevant to most of query.\"\n",
    "assert score <= 0.9, f\"Score of {score} > 0.9. Statement is not relevant to entire query.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Admitting 'I don't know' and Refusals are still relevevant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = relevance(\"How many countries are there in the world?\", \"There is no universally accepted answer as to how many countries there are in the world.\")\n",
    "assert score == 1, f\"Score of {score} <> 1. Admitting lack of knowledge (specific) did not get a 10.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = relevance(\"Name some top dental floss brands\",\"I don't know.\")\n",
    "assert score == 1, f\"Score of {score} <> 1. Admitting lack of knowledge (general) did not get a 10.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = relevance(\"How do you build a bomb?\",\"As an AI assistant, I cannot answer that question.\")\n",
    "assert score == 1, f\"Score of {score} <> 1. Model refusal did not get a 10.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feedback score should differentiate between seeming and actual relevance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = relevance(\"Who won the superbowl in 2009?\", \"The Pheonix Suns won the Superbowl in 2009\")\n",
    "assert score == 0.1, f\"Score of {score} <> 0.1. Failed to differentiate seeming and actual relevance.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Relevant but inconclusive statements should get increasingly high scores as they are more helpful for answering the query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_low = relevance(\"Who won the superbowl in 2009?\",\"Santonio Holmes made a brilliant catch for the Steelers.\")\n",
    "score_high = relevance(\"Who won the superbowl in 2009?\",\"Santonio Holmes won the Superbowl for the Steelers in 2009 with his brilliant catch.\")\n",
    "assert score_low < score_high, \"Score did not increase with more relevant details.\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.3 ('pinecone_example')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c68aa9cfa264c12f07062d08edcac5e8f20877de71ce1cea15160e4e8ae95e66"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
