{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install -q scikit-learn litellm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import groundedness feedback function\n",
    "from trulens_eval import Tru\n",
    "from test_cases import generate_ms_marco_context_relevance_benchmark\n",
    "from benchmark_frameworks.eval_as_recommendation import score_passages, compute_ndcg, compute_ece, recall_at_k, precision_at_k\n",
    "Tru().reset_database()\n",
    "\n",
    "benchmark_data = []\n",
    "for i in range(1, 6):\n",
    "    dataset_path = f\"./datasets/ms_marco/ms_marco_train_v2.1_{i}.json\"\n",
    "    benchmark_data.extend(list(generate_ms_marco_context_relevance_benchmark(dataset_path)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"...\"\n",
    "os.environ[\"HUGGINGFACE_API_KEY\"] = \"...\"\n",
    "os.environ[\"ANTHROPIC_API_KEY\"] = \"...\"\n",
    "os.environ[\"TOGETHERAI_API_KEY\"] = \"...\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "df = pd.DataFrame(benchmark_data)\n",
    "print(len(df.groupby(\"query_id\").count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby(\"query_id\").head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define feedback functions for contexnt relevance to be evaluated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trulens_eval.feedback import OpenAI, LiteLLM\n",
    "\n",
    "\n",
    "\n",
    "# GPT 3.5\n",
    "turbo = OpenAI(model_engine=\"gpt-3.5-turbo\")\n",
    "\n",
    "def wrapped_relevance_turbo(input, output):\n",
    "    return turbo.qs_relevance_confidence_verb_2s_top1(input, output)\n",
    "\n",
    "# # GPT 4 turbo\n",
    "gpt4 = OpenAI(model_engine=\"gpt-4-1106-preview\")\n",
    "\n",
    "def wrapped_relevance_gpt4(input, output):\n",
    "    return gpt4.qs_relevance_confidence_verb_2s_top1(input, output)\n",
    "\n",
    "\n",
    "# Anthropic\n",
    "claude_1 = LiteLLM(model_engine=\"claude-instant-1\")\n",
    "def wrapped_relevance_claude1(input, output):\n",
    "    return claude_1.qs_relevance_confidence_verb_2s_top1(input, output)\n",
    "\n",
    "claude_2 = LiteLLM(model_engine=\"claude-2\")\n",
    "def wrapped_relevance_claude2(input, output):\n",
    "    return claude_2.qs_relevance_confidence_verb_2s_top1(input, output)\n",
    "\n",
    "\n",
    "# # # Meta\n",
    "llama_2_13b = LiteLLM(model_engine=\"together_ai/togethercomputer/Llama-2-7B-32K-Instruct\")\n",
    "def wrapped_relevance_llama2(input, output):\n",
    "    return llama_2_13b.qs_relevance_confidence_verb_2s_top1(input, output)\n",
    "\n",
    "# Define a list of your feedback functions\n",
    "feedback_functions = {\n",
    "    'GPT-3.5-Turbo': wrapped_relevance_turbo,\n",
    "    'GPT-4-Turbo': wrapped_relevance_gpt4,\n",
    "    'Claude-1': wrapped_relevance_claude1,\n",
    "    'Claude-2': wrapped_relevance_claude2,\n",
    "    'Llama-2': wrapped_relevance_llama2,\n",
    "\n",
    "}\n",
    "\n",
    "backoffs_by_functions = {\n",
    "    'GPT-3.5-Turbo': 0.5,\n",
    "    'GPT-4-Turbo': 0.5,\n",
    "    'Claude-1': 1.5,\n",
    "    'Claude-2': 1.5,\n",
    "    'Llama-2': 2,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Running the benchmark\n",
    "results = []\n",
    "\n",
    "K = 5 # for recall@K\n",
    "\n",
    "intermediate_results = []\n",
    "for name, func in feedback_functions.items():\n",
    "    try:\n",
    "        scores, true_relevance = score_passages(df, name, func, backoffs_by_functions[name] if name in backoffs_by_functions else 0.5, n=1)\n",
    "        ndcg_value = compute_ndcg(scores, true_relevance)\n",
    "        ece_value = compute_ece(scores, true_relevance)\n",
    "        precision_k = np.mean([precision_at_k(sc, tr, 1) for sc, tr in zip(scores, true_relevance)])\n",
    "        recall_k = np.mean([recall_at_k(sc, tr, K) for sc, tr in zip(scores, true_relevance)])\n",
    "        results.append((name, ndcg_value, ece_value, recall_k, precision_k))\n",
    "        print(f\"Finished running feedback function name {name}\")\n",
    "    \n",
    "        print(\"Saving results...\")\n",
    "        tmp_results_df = pd.DataFrame(results, columns=['Model', 'nDCG', 'ECE', f'Recall@{K}', 'Precision@1'])\n",
    "        print(tmp_results_df)\n",
    "        intermediate_results.append(tmp_results_df)\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to run benchmark for feedback function name {name} due to {e}\")\n",
    "# Convert results to DataFrame for display\n",
    "results_df = pd.DataFrame(results, columns=['Model', 'nDCG', 'ECE', f'Recall@{K}', 'Precision@1'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df.to_csv(\"results_claude.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# Make sure results_df is defined and contains the necessary columns\n",
    "# Also, ensure that K is defined\n",
    "\n",
    "plt.figure(figsize=(12, 10))\n",
    "\n",
    "# Graph for nDCG, Recall@K, and Precision@K\n",
    "plt.subplot(2, 1, 1)  # First subplot\n",
    "ax1 = results_df.plot(x='Model', y=['nDCG', f'Recall@{K}', 'Precision@1'], kind='bar', ax=plt.gca())\n",
    "plt.title('Feedback Function Performance (Higher is Better)')\n",
    "plt.ylabel('Score')\n",
    "plt.xticks(rotation=45)\n",
    "plt.legend(loc='upper left')\n",
    "\n",
    "# Graph for ECE\n",
    "plt.subplot(2, 1, 2)  # Second subplot\n",
    "ax2 = results_df.plot(x='Model', y=['ECE'], kind='bar', ax=plt.gca(), color='orange')\n",
    "plt.title('Feedback Function Calibration (Lower is Better)')\n",
    "plt.ylabel('ECE')\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "trulens",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
