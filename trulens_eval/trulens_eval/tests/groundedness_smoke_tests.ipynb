{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Groundedness Evaluations\n",
    "\n",
    "In many ways, feedbacks can be thought of as LLM apps themselves. Given text, they return some result. Thinking in this way, we can use TruLens to evaluate and track our feedback quality. We can even do this for different models (e.g. gpt-3.5 and gpt-4) or prompting schemes (such as chain-of-thought reasoning).\n",
    "\n",
    "This notebook follows an evaluation of a set of test cases generated from human annotated datasets. In particular, we generate test cases from [SummEval](https://arxiv.org/abs/2007.12626) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import groundedness feedback function\n",
    "from trulens_eval.feedback import GroundTruthAgreement, Groundedness\n",
    "from trulens_eval import TruBasicApp, Feedback, Tru, Select, feedback, TruCustomApp\n",
    "from test_cases import generate_summeval_groundedness_golden_set\n",
    "\n",
    "\n",
    "Tru().reset_database()\n",
    "\n",
    "# generator for groundedness golden set\n",
    "test_cases_gen = generate_summeval_groundedness_golden_set(\"./datasets/summeval_test_100.json\")\n",
    "\n",
    "# generate x number of test cases\n",
    "groundedness_golden_set = []\n",
    "for i in range(50):\n",
    "    groundedness_golden_set.append(next(test_cases_gen))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"...\"\n",
    "os.environ[\"HUGGINGFACE_API_KEY\"] = \"...\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Benchmarking various Groundedness feedback function providers (OpenAI vs Hugginface)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trulens_eval.feedback.provider.hugs import Huggingface\n",
    "import numpy as np\n",
    "\n",
    "huggingface_provider = Huggingface()\n",
    "groundedness_hug = Groundedness(groundedness_provider=huggingface_provider)\n",
    "f_groundedness_hug = Feedback(groundedness_hug.groundedness_measure, name = \"Groundedness Huggingface\").on_input().on_output().aggregate(groundedness_hug.grounded_statements_aggregator)\n",
    "def wrapped_groundedness_hug(input, output):\n",
    "    return np.mean(list(f_groundedness_hug(input, output)[0].values()))\n",
    "     \n",
    "    \n",
    "    \n",
    "groundedness_openai = Groundedness()\n",
    "f_groundedness_openai = Feedback(groundedness_openai.groundedness_measure, name = \"Groundedness OpenAI\").on_input().on_output().aggregate(groundedness_openai.grounded_statements_aggregator)\n",
    "def wrapped_groundedness_openai(input, output):\n",
    "    return f_groundedness_openai(input, output)[0]['full_doc_score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Feedback object using the numeric_difference method of the ground_truth object\n",
    "ground_truth = GroundTruthAgreement(groundedness_golden_set)\n",
    "# Call the numeric_difference method with app and record\n",
    "f_groundtruth = Feedback(ground_truth.numeric_difference, name = \"Groundedness Smoke Test\").on(Select.Record.calls[0].args.args[0]).on(Select.Record.calls[0].args.args[1]).on_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tru_wrapped_groundedness_hug = TruBasicApp(wrapped_groundedness_hug, app_id = \"groundedness huggingface\", feedbacks=[f_groundtruth])\n",
    "tru_wrapped_groundedness_openai = TruBasicApp(wrapped_groundedness_openai, app_id = \"groundedness openai\", feedbacks=[f_groundtruth])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "for i in range(len(groundedness_golden_set[:10])):\n",
    "    source = groundedness_golden_set[i][\"query\"]\n",
    "    response = groundedness_golden_set[i][\"response\"]\n",
    "    with tru_wrapped_groundedness_hug as recording:\n",
    "        tru_wrapped_groundedness_hug.app(source, response)\n",
    "    with tru_wrapped_groundedness_openai as recording:\n",
    "        tru_wrapped_groundedness_openai.app(source, response)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Groundedness Smoke Test</th>\n",
       "      <th>latency</th>\n",
       "      <th>total_cost</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>app_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>groundedness huggingface</th>\n",
       "      <td>0.810252</td>\n",
       "      <td>54.571429</td>\n",
       "      <td>0.001080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>groundedness openai</th>\n",
       "      <td>0.473333</td>\n",
       "      <td>42.157895</td>\n",
       "      <td>0.001863</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          Groundedness Smoke Test    latency  total_cost\n",
       "app_id                                                                  \n",
       "groundedness huggingface                 0.810252  54.571429    0.001080\n",
       "groundedness openai                      0.473333  42.157895    0.001863"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Tru().get_leaderboard(app_ids=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
