{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Context Relevance Feedback Requirements\n",
    "1. Relevance requires adherence to the entire query.\n",
    "2. Long context with small relevant chunks are relevant.\n",
    "3. Context that provides no answer can still be relevant.\n",
    "4. Feedback mechanism should differentiate between seeming and actual relevance.\n",
    "5. Relevant but inconclusive statements should get increasingly high scores as they are more helpful for answering the query.\n",
    "\n",
    "Below are examples of usage. Find the [results](#test-results) of the tests tabulated at the bottom."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import relevance feedback function\n",
    "from trulens_eval.feedback import GroundTruthAgreement, OpenAI as fOpenAI\n",
    "from trulens_eval import TruBasicApp, Feedback, Tru, Select\n",
    "from test_cases import context_relevance_golden_set\n",
    "fopenai = fOpenAI()\n",
    "\n",
    "import openai\n",
    "\n",
    "Tru().reset_database()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"...\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer rlevance\n",
    "def wrapped_relevance(input, output):\n",
    "    return fopenai.relevance(input, output)\n",
    "\n",
    "# Relevancw with chain of thought reasoning\n",
    "def wrapped_relevance_with_cot(input, output):\n",
    "    return fopenai.relevance_with_cot_reasons(input, output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Feedback object using the numeric_difference method of the ground_truth object\n",
    "ground_truth = GroundTruthAgreement(context_relevance_golden_set)\n",
    "# Call the numeric_difference method with app and record\n",
    "f_groundtruth = Feedback(ground_truth.numeric_difference, name = \"Relevance Smoke Test\").on_input().on(Select.Record.calls[0].args.args[1]).on_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tru_wrapped_relevance = TruBasicApp(wrapped_relevance, app_id = \"context relevance\", feedbacks=[f_groundtruth])\n",
    "tru_wrapped_relevance_with_cot = TruBasicApp(wrapped_relevance_with_cot, app_id = \"context relevance with cot reasoning\", feedbacks=[f_groundtruth])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(context_relevance_golden_set)):\n",
    "    prompt = context_relevance_golden_set[i][\"query\"]\n",
    "    response = context_relevance_golden_set[i][\"response\"]\n",
    "    tru_wrapped_relevance.call_with_record(prompt, response)\n",
    "    tru_wrapped_relevance_with_cot.call_with_record(prompt, response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tru().run_dashboard()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.4 ('agents')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7d153714b979d5e6d08dd8ec90712dd93bff2c9b6c1f0c118169738af3430cd4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
