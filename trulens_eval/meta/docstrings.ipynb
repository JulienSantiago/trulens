{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Volumes/dev_new/trulens/trulens_eval\n",
      "✅ Key OPENAI_API_KEY set from environment (same value found in .env file at /Volumes/dev_new/.env).\n",
      "✅ Key HUGGINGFACE_API_KEY set from environment (same value found in .env file at /Volumes/dev_new/.env).\n"
     ]
    }
   ],
   "source": [
    "# pip uninstall -y trulens_eval\n",
    "# pip install git+https://github.com/truera/trulens@piotrm/azure_bugfixes#subdirectory=trulens_eval\n",
    "\n",
    "# trulens_eval notebook dev\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "base = Path().cwd()\n",
    "while not (base / \"trulens_eval\").exists():\n",
    "    base = base.parent\n",
    "\n",
    "print(base)\n",
    "\n",
    "# If running from github repo, can use this:\n",
    "sys.path.append(str(base))\n",
    "\n",
    "# Uncomment for more debugging printouts.\n",
    "\"\"\"\n",
    "import logging\n",
    "root = logging.getLogger()\n",
    "root.setLevel(logging.DEBUG)\n",
    "\n",
    "handler = logging.StreamHandler(sys.stdout)\n",
    "handler.setLevel(logging.DEBUG)\n",
    "formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "handler.setFormatter(formatter)\n",
    "root.addHandler(handler)\n",
    "\"\"\"\n",
    "\n",
    "from trulens_eval.keys import check_keys\n",
    "\n",
    "check_keys(\n",
    "    \"OPENAI_API_KEY\",\n",
    "    \"HUGGINGFACE_API_KEY\"\n",
    ")\n",
    "\n",
    "# from trulens_eval import Tru\n",
    "# tru = Tru()\n",
    "# tru.reset_database()\n",
    "# tru.run_dashboard(_dev=base, force=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trulens_eval.feedback.v2.feedback import Template, Insensitivity\n",
    "\n",
    "t = Template.from_template(\"hello there\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trulens_eval.feedback.provider.hugs import Dummy\n",
    "from trulens_eval import Select\n",
    "from trulens_eval.feedback.feedback import Feedback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import inspect\n",
    "from typing import Tuple\n",
    "from trulens_eval.utils.text import make_retab\n",
    "\n",
    "fimp = Dummy().language_match\n",
    "\n",
    "invert_template = \"\"\"\n",
    "You are a fuzzing tool. Your job is to provide inputs to a function that will\n",
    "achieve a desired output. You are given a description of the function. Some\n",
    "inputs may be fixed, others are to be determined by you.\n",
    "\n",
    "BEGIN FUNCTION DESCRIPTION\n",
    "{imp_doc}\n",
    "END FUNCTION DESCRIPTION\n",
    "\n",
    "BEGIN TARGET OUTPUT\n",
    "{target}\n",
    "END TARGET OUTPUT\n",
    "\n",
    "BEGIN FIXED INPUTS\n",
    "{fixed_inputs}\n",
    "END FIXED INPUTS\n",
    "\"\"\"\n",
    "\n",
    "def invert_feedback(imp, target, **kwargs):\n",
    "    \"\"\"Try to fill in input values to feedback function `imp` to achieve output\n",
    "    value `target`. Any `kwargs` provided fix `imp` arguments to the given\n",
    "    values.\n",
    "    \"\"\"\n",
    "\n",
    "    doc = imp.__doc__\n",
    "\n",
    "    filled_template = invert_template.format(\n",
    "        imp_doc=doc,\n",
    "        target=target,\n",
    "        fixed_inputs=\"\\n\".join(\n",
    "            f\"{k}={v}\" for k, v in kwargs.items()\n",
    "        )\n",
    "    )\n",
    "\n",
    "    return filled_template\n",
    "\n",
    "#BEGIN FUNCTION DOCSTRING\n",
    "#{imp_doc}\n",
    "#END FUNCTION DOCSTRING\n",
    "\n",
    "doc_template = \"\"\"\n",
    "You are a python method summarization tool. Your job is to summarize the\n",
    "purpose, implementation, arguments, and returns of a given method based on its\n",
    "signature and source code.\n",
    "\n",
    "BEGIN FUNCTION SIGNATURE\n",
    "{sig}\n",
    "END FUNCTION SIGNATURE\n",
    "\n",
    "BEGIN FUNCTION SOURCE\n",
    "{src}\n",
    "END FUNCTION SOURCE\n",
    "\n",
    "Summarize the purpose of the method. Please be as concise as possible and avoid\n",
    "mentioning how the method is implemented or what tools it uses to achieve its\n",
    "purpose. Only summarize the purpose.\n",
    "\n",
    "PURPOSE: <overall method purpose>\n",
    "\n",
    "Summarize how the method is implemented:\n",
    "IMPLEMENTATION: <how the method achieves its purpose>\n",
    "\n",
    "Summarize each of these arguments in this form, one per line:\n",
    "{args_templates}\n",
    "\n",
    "Summarize the function's return value. List its type, overall interpretation,\n",
    "and an interpretation extremal values it could achieve.\n",
    "{rets_templates}\n",
    "\"\"\"\n",
    "\n",
    "arg_template = \"\"\"ARGUMENT({name}: {type}): <argument_description>\"\"\"\n",
    "\n",
    "ret_template = \"\"\"\n",
    "RETURN({type}): <return1_description>\n",
    "RETURNVALUE(<return1_value1>): <interpretation for this return value>\n",
    "RETURNVALUE(<return1_value2>): <interpretation for this return value>\n",
    "\"\"\"\n",
    "\n",
    "def doc_feedback(imp):\n",
    "    \"\"\"Try to fill in input values to feedback function `imp` to achieve output\n",
    "    value `target`. Any `kwargs` provided fix `imp` arguments to the given\n",
    "    values.\n",
    "    \"\"\"\n",
    "\n",
    "    doc = imp.__doc__\n",
    "    sig = inspect.signature(imp)\n",
    "\n",
    "    rt = make_retab(\"  \")\n",
    "\n",
    "    arg_templates = \"\\n\".join(\n",
    "        arg_template.format(name=arg.name, type=arg.annotation.__name__)\n",
    "        for arg in sig.parameters.values() if arg.name != \"self\"\n",
    "    )\n",
    "\n",
    "    ret_annot = sig.return_annotation\n",
    "\n",
    "    if hasattr(ret_annot, \"__args__\"):\n",
    "        ret_types = [ret_annot.__args__[0]]\n",
    "    else:\n",
    "        ret_types = [ret_annot]\n",
    "\n",
    "    rets_templates = \"\\n\".join(\n",
    "        ret_template.format(type=ret_type.__name__)\n",
    "        for ret_type in ret_types\n",
    "    )\n",
    "    \n",
    "    filled_template = doc_template.format(\n",
    "        imp_doc=rt(doc),\n",
    "        sig=rt(imp.__name__ + str(sig)),\n",
    "        src=rt(inspect.getsource(fimp)),\n",
    "        args_templates=arg_templates,\n",
    "        rets_templates=rets_templates\n",
    "    )\n",
    "\n",
    "    return filled_template\n",
    "\n",
    "\n",
    "# invert_feedback(fimp, 1.0, text1=\"How are you?\")\n",
    "prompt = doc_feedback(Dummy().toxic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    \"\"\"Calculate the likelihood that two texts are in the same natural\n",
      "    language.\n",
      "\n",
      "    The method uses a language detection API to get language scores for the\n",
      "    input texts, calculates the difference in scores, and returns a value\n",
      "    based on the difference.\n",
      "\n",
      "    Args:\n",
      "        text1: The first text to evaluate.\n",
      "\n",
      "        text2: The second text to evaluate.\n",
      "\n",
      "\n",
      "\n",
      "    Returns:\n",
      "        float: A value between 0.0 and 1.0. The value 0.0 indicates that\n",
      "            the input texts were of different languages, and 1.0\n",
      "            indicates they are in the same language.\n",
      "\n",
      "            0.0: Input texts are of different languages.\n",
      "\n",
      "            1.0: Input texts are in the same language.\n",
      "\n",
      "\n",
      "    \"\"\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import dataclasses\n",
    "from typing import Dict, Any, Type\n",
    "import re\n",
    "from textwrap import wrap\n",
    "\n",
    "@dataclasses.dataclass\n",
    "class LLMDoc():\n",
    "    purpose: str\n",
    "\n",
    "    details: str\n",
    "\n",
    "    args: Dict[Tuple[str, Type], str]\n",
    "\n",
    "    ret: str\n",
    "    ret_type: Type\n",
    "    rets: Dict[Any, str]\n",
    "\n",
    "    def as_docstring(self, initial_indent=0, tabsize=4):\n",
    "        tab1 = make_retab(\"  \")\n",
    "\n",
    "        space = 80 - initial_indent\n",
    "\n",
    "        doc_purpose = \"\\n\".join(wrap(\n",
    "            \"\\\"\\\"\\\"\" + self.purpose,\n",
    "            width=space,\n",
    "            initial_indent=\" \" * initial_indent,\n",
    "            subsequent_indent=\" \" * initial_indent\n",
    "        ))\n",
    "\n",
    "        doc_details = \"\\n\".join(wrap(\n",
    "            self.details,\n",
    "            width=space,\n",
    "            initial_indent=\" \" * initial_indent,\n",
    "            subsequent_indent=\" \" * initial_indent\n",
    "        ))\n",
    "\n",
    "        doc_args = \" \" * initial_indent + \"Args:\\n\"\n",
    "        for (name, type_), desc in self.args.items():\n",
    "            doc_args += \"\\n\".join(wrap(\n",
    "                f\"{name}: {desc}\",\n",
    "                width=space - initial_indent - tabsize,\n",
    "                initial_indent=\" \" * (initial_indent + tabsize),\n",
    "                subsequent_indent=\" \" * (initial_indent + 2*tabsize)\n",
    "            )) + \"\\n\\n\"\n",
    "\n",
    "        doc_rets = \" \" * initial_indent + \"Returns:\\n\"\n",
    "        doc_rets += \"\\n\".join(wrap(\n",
    "            f\"{self.ret_type}: {self.ret}\",\n",
    "            width=space - tabsize,\n",
    "            initial_indent=\" \" * (initial_indent + tabsize),\n",
    "            subsequent_indent=\" \" * (initial_indent + 2*tabsize)\n",
    "        )) + \"\\n\\n\"\n",
    "\n",
    "        for ret, desc in self.rets.items():\n",
    "            doc_rets += \"\\n\".join(wrap(\n",
    "                f\"{ret}: {desc}\",\n",
    "                width=space - 2*tabsize,\n",
    "                initial_indent=\" \" * (initial_indent + 2*tabsize),\n",
    "                subsequent_indent=\" \" * (initial_indent + 3*tabsize)\n",
    "            )) + \"\\n\\n\"\n",
    "\n",
    "        \n",
    "        doc = f\"\"\"{doc_purpose}\n",
    "\n",
    "{doc_details}\n",
    "\n",
    "{doc_args}\n",
    "\n",
    "{doc_rets}\n",
    "{\" \" * initial_indent}\\\"\\\"\\\"\n",
    "\"\"\"\n",
    "        return doc\n",
    "\n",
    "re_purpose = re.compile(r\"PURPOSE: (.*)\")\n",
    "re_arg = re.compile(r\"ARGUMENT\\((.*): (.*)\\): (.*)\")\n",
    "re_returnvalue = re.compile(r\"RETURNVALUE\\((.*)\\): (.*)\")\n",
    "re_return = re.compile(r\"RETURN\\((.*)\\): (.*)\")\n",
    "re_details = re.compile(r\"DETAILS: (.*)\")\n",
    "\n",
    "def parse_result(res: str) -> LLMDoc:\n",
    "    lines = res.split(\"\\n\")\n",
    "\n",
    "    doc_args = {}\n",
    "    doc_rets = {}\n",
    "    doc = {'args': doc_args, 'rets': doc_rets}\n",
    "\n",
    "    for line in lines:\n",
    "        if match := re_purpose.fullmatch(line):\n",
    "            doc['purpose'] = match.group(1)\n",
    "\n",
    "        elif match := re_details.fullmatch(line):\n",
    "            doc['details'] = match.group(1)\n",
    "\n",
    "        elif match := re_returnvalue.fullmatch(line):\n",
    "            arg, desc = match.groups()\n",
    "            doc_rets[arg] = desc\n",
    "\n",
    "        elif match := re_return.fullmatch(line):\n",
    "            doc['ret'] = match.group(2)\n",
    "            doc['ret_type'] = match.group(1)\n",
    "\n",
    "        elif match := re_arg.fullmatch(line):\n",
    "            name, type_, desc = match.groups()\n",
    "            doc_args[(name, type_)] = desc\n",
    "\n",
    "    return LLMDoc(**doc)\n",
    "\n",
    "from pprint import pprint\n",
    "\n",
    "doc = parse_result(\"\"\"PURPOSE: Calculate the likelihood that two texts are in the same natural language.\n",
    "\n",
    "DETAILS: The method uses a language detection API to get language scores for the input texts, calculates the difference in scores, and returns a value based on the difference.\n",
    "\n",
    "ARGUMENT(text1: str): The first text to evaluate.\n",
    "ARGUMENT(text2: str): The second text to evaluate.\n",
    "\n",
    "RETURN(float): A value between 0.0 and 1.0. The value 0.0 indicates that the input texts were of different languages, and 1.0 indicates they are in the same language.\n",
    "RETURNVALUE(0.0): Input texts are of different languages.\n",
    "RETURNVALUE(1.0): Input texts are in the same language.\"\"\")\n",
    "\n",
    "print(doc.as_docstring(initial_indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# res = c.completions.create#(model=\"gpt-3.5-turbo\", prompt=prompt)\n",
    "# help(c.chat.completions.create)\n",
    "\n",
    "import openai\n",
    "c = openai.OpenAI()\n",
    "\n",
    "d = Dummy()\n",
    "\n",
    "for fimp in [d.language_match]:#, d.positive_sentiment, d.toxic, d.pii_detection, d.hallucination_evaluator]:\n",
    "    print(fimp.__name__)\n",
    "    prompt = doc_feedback(fimp)\n",
    "    res = c.chat.completions.create(model=\"gpt-3.5-turbo\", messages=[{\"role\": \"system\", \"content\": prompt}], temperature=0.0)\n",
    "    print(res.choices[0].message.content)\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py311_trulens",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
