
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
      
      
      
        <link rel="prev" href="../install/">
      
      
        <link rel="next" href="../api/tru/">
      
      <link rel="icon" href="../../img/favicon.ico">
      <meta name="generator" content="mkdocs-1.4.3, mkdocs-material-9.1.14">
    
    
      
        <title>Quickstart - TruLens</title>
      
    
    
      <link rel="stylesheet" href="../../assets/stylesheets/main.85bb2934.min.css">
      
        
        <link rel="stylesheet" href="../../assets/stylesheets/palette.a6bdf11c.min.css">
      
      

    
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
      <link rel="stylesheet" href="../../assets/_mkdocstrings.css">
    
      <link rel="stylesheet" href="../../stylesheets/extra.css">
    
      <link rel="stylesheet" href="../../stylesheets/cover.css">
    
    <script>__md_scope=new URL("../..",location),__md_hash=e=>[...e].reduce((e,_)=>(e<<5)-e+_.charCodeAt(0),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
    
  </head>
  
  
    
    
    
    
    
    <body dir="ltr" data-md-color-scheme="trulens" data-md-color-primary="indigo" data-md-color-accent="indigo">
  
    
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#quickstart" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

  

<header class="md-header md-header--shadow" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href="../.." title="TruLens" class="md-header__button md-logo" aria-label="TruLens" data-md-component="logo">
      
  <img src="../../img/squid.png" alt="logo">

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3V6m0 5h18v2H3v-2m0 5h18v2H3v-2Z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            TruLens
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              Quickstart
            
          </span>
        </div>
      </div>
    </div>
    
      
    
    
    
      <label class="md-header__button md-icon" for="__search">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"/></svg>
      </label>
      <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"/></svg>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12Z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12 19 6.41Z"/></svg>
        </button>
      </nav>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
    
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    


<nav class="md-nav md-nav--primary" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="../.." title="TruLens" class="md-nav__button md-logo" aria-label="TruLens" data-md-component="logo">
      
  <img src="../../img/squid.png" alt="logo">

    </a>
    TruLens
  </label>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href="../.." class="md-nav__link">
        Home
      </a>
    </li>
  

    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href="../../welcome/" class="md-nav__link">
        Welcome to TruLens!
      </a>
    </li>
  

    
      
      
      

  
  
    
  
  
    
      
    
    <li class="md-nav__item md-nav__item--active md-nav__item--section md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3" checked>
      
      
      
        <label class="md-nav__link" for="__nav_3" id="__nav_3_label" tabindex="0">
          Eval
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_3_label" aria-expanded="true">
        <label class="md-nav__title" for="__nav_3">
          <span class="md-nav__icon md-icon"></span>
          Eval
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../install/" class="md-nav__link">
        Installation
      </a>
    </li>
  

            
          
            
              
  
  
    
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" type="checkbox" id="__toc">
      
      
      
        <label class="md-nav__link md-nav__link--active" for="__toc">
          Quickstart
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <a href="./" class="md-nav__link md-nav__link--active">
        Quickstart
      </a>
      
        

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#quickstart" class="md-nav__link">
    Quickstart
  </a>
  
    <nav class="md-nav" aria-label="Quickstart">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#playground" class="md-nav__link">
    Playground
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#install-use" class="md-nav__link">
    Install &amp; Use
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#api-keys" class="md-nav__link">
    API Keys
  </a>
  
    <nav class="md-nav" aria-label="API Keys">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#in-python" class="md-nav__link">
    In Python
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#in-terminal" class="md-nav__link">
    In Terminal
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#create-a-basic-llm-chain-to-evaluate" class="md-nav__link">
    Create a basic LLM chain to evaluate
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#set-up-logging-and-instrumentation" class="md-nav__link">
    Set up logging and instrumentation
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#evaluate-quality" class="md-nav__link">
    Evaluate Quality
  </a>
  
    <nav class="md-nav" aria-label="Evaluate Quality">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#automatic-logging" class="md-nav__link">
    Automatic logging
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#out-of-band-feedback-evaluation" class="md-nav__link">
    Out-of-band Feedback evaluation
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#run-the-dashboard" class="md-nav__link">
    Run the dashboard!
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#chain-leaderboard-quickly-identify-quality-issues" class="md-nav__link">
    Chain Leaderboard: Quickly identify quality issues.
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#understand-chain-performance-with-evaluations" class="md-nav__link">
    Understand chain performance with Evaluations
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#out-of-the-box-feedback-functions" class="md-nav__link">
    Out-of-the-box Feedback Functions
  </a>
  
    <nav class="md-nav" aria-label="Out-of-the-box Feedback Functions">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#relevance" class="md-nav__link">
    Relevance
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sentiment" class="md-nav__link">
    Sentiment
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#model-agreement" class="md-nav__link">
    Model Agreement
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#language-match" class="md-nav__link">
    Language Match
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#toxicity" class="md-nav__link">
    Toxicity
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#moderation" class="md-nav__link">
    Moderation
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#adding-new-feedback-functions" class="md-nav__link">
    Adding new feedback functions
  </a>
  
</li>
      
    </ul>
  
</nav>
      
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3_3" >
      
      
      
        <label class="md-nav__link" for="__nav_3_3" id="__nav_3_3_label" tabindex="0">
          API Reference
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_3_3_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_3_3">
          <span class="md-nav__icon md-icon"></span>
          API Reference
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../api/tru/" class="md-nav__link">
        Tru
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../api/truchain/" class="md-nav__link">
        TruChain
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../api/tru_feedback/" class="md-nav__link">
        Feedback Functions
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

    
      
      
      

  
  
  
    
      
    
    <li class="md-nav__item md-nav__item--section md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_4" >
      
      
      
        <label class="md-nav__link" for="__nav_4" id="__nav_4_label" tabindex="0">
          Explain
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_4_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_4">
          <span class="md-nav__icon md-icon"></span>
          Explain
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../trulens_explain/install/" class="md-nav__link">
        Installation
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../trulens_explain/quickstart/" class="md-nav__link">
        Quickstart
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../trulens_explain/attribution_parameterization/" class="md-nav__link">
        Attributions for Different Use Cases
      </a>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_4_4" >
      
      
      
        <label class="md-nav__link" for="__nav_4_4" id="__nav_4_4_label" tabindex="0">
          API Reference
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_4_4_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_4_4">
          <span class="md-nav__icon md-icon"></span>
          API Reference
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../trulens_explain/api/attribution/" class="md-nav__link">
        Attribution
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../trulens_explain/api/model_wrappers/" class="md-nav__link">
        Models
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../trulens_explain/api/slices/" class="md-nav__link">
        Slices
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../trulens_explain/api/quantities/" class="md-nav__link">
        Quantities
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../trulens_explain/api/distributions/" class="md-nav__link">
        Distributions
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../trulens_explain/api/visualizations/" class="md-nav__link">
        Visualizations
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#quickstart" class="md-nav__link">
    Quickstart
  </a>
  
    <nav class="md-nav" aria-label="Quickstart">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#playground" class="md-nav__link">
    Playground
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#install-use" class="md-nav__link">
    Install &amp; Use
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#api-keys" class="md-nav__link">
    API Keys
  </a>
  
    <nav class="md-nav" aria-label="API Keys">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#in-python" class="md-nav__link">
    In Python
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#in-terminal" class="md-nav__link">
    In Terminal
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#create-a-basic-llm-chain-to-evaluate" class="md-nav__link">
    Create a basic LLM chain to evaluate
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#set-up-logging-and-instrumentation" class="md-nav__link">
    Set up logging and instrumentation
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#evaluate-quality" class="md-nav__link">
    Evaluate Quality
  </a>
  
    <nav class="md-nav" aria-label="Evaluate Quality">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#automatic-logging" class="md-nav__link">
    Automatic logging
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#out-of-band-feedback-evaluation" class="md-nav__link">
    Out-of-band Feedback evaluation
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#run-the-dashboard" class="md-nav__link">
    Run the dashboard!
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#chain-leaderboard-quickly-identify-quality-issues" class="md-nav__link">
    Chain Leaderboard: Quickly identify quality issues.
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#understand-chain-performance-with-evaluations" class="md-nav__link">
    Understand chain performance with Evaluations
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#out-of-the-box-feedback-functions" class="md-nav__link">
    Out-of-the-box Feedback Functions
  </a>
  
    <nav class="md-nav" aria-label="Out-of-the-box Feedback Functions">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#relevance" class="md-nav__link">
    Relevance
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sentiment" class="md-nav__link">
    Sentiment
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#model-agreement" class="md-nav__link">
    Model Agreement
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#language-match" class="md-nav__link">
    Language Match
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#toxicity" class="md-nav__link">
    Toxicity
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#moderation" class="md-nav__link">
    Moderation
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#adding-new-feedback-functions" class="md-nav__link">
    Adding new feedback functions
  </a>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  


  <h1>Quickstart</h1>

<h2 id="quickstart">Quickstart<a class="headerlink" href="#quickstart" title="Permanent link">&para;</a></h2>
<h3 id="playground">Playground<a class="headerlink" href="#playground" title="Permanent link">&para;</a></h3>
<p>To quickly play around with the TruLens Eval library, download this notebook: <a href="https://github.com/truera/trulens/blob/main/trulens_eval/trulens_eval_quickstart.ipynb">trulens_eval_quickstart.ipynb</a>.</p>
<h3 id="install-use">Install &amp; Use<a class="headerlink" href="#install-use" title="Permanent link">&para;</a></h3>
<p>Install trulens-eval from pypi.</p>
<div class="highlight"><pre><span></span><code>pip install trulens-eval
</code></pre></div>
<p>Imports from langchain to build app, trulens for evaluation</p>
<div class="highlight"><pre><span></span><code><span class="kn">from</span> <span class="nn">IPython.display</span> <span class="kn">import</span> <span class="n">JSON</span>
<span class="c1"># imports from langchain to build app</span>
<span class="kn">from</span> <span class="nn">langchain</span> <span class="kn">import</span> <span class="n">PromptTemplate</span>
<span class="kn">from</span> <span class="nn">langchain.chains</span> <span class="kn">import</span> <span class="n">LLMChain</span>
<span class="kn">from</span> <span class="nn">langchain.chat_models</span> <span class="kn">import</span> <span class="n">ChatOpenAI</span>
<span class="kn">from</span> <span class="nn">langchain.prompts.chat</span> <span class="kn">import</span> <span class="n">ChatPromptTemplate</span>
<span class="kn">from</span> <span class="nn">langchain.prompts.chat</span> <span class="kn">import</span> <span class="n">HumanMessagePromptTemplate</span>
<span class="c1"># imports from trulens to log and get feedback on chain</span>
<span class="kn">from</span> <span class="nn">trulens_eval</span> <span class="kn">import</span> <span class="n">tru</span>
<span class="kn">from</span> <span class="nn">trulens_eval</span> <span class="kn">import</span> <span class="n">tru_chain</span>
<span class="n">tru</span> <span class="o">=</span> <span class="n">Tru</span><span class="p">()</span>
</code></pre></div>
<h3 id="api-keys">API Keys<a class="headerlink" href="#api-keys" title="Permanent link">&para;</a></h3>
<p>Our example chat app and feedback functions call external APIs such as OpenAI or Huggingface. You can add keys by setting the environment variables. </p>
<h4 id="in-python">In Python<a class="headerlink" href="#in-python" title="Permanent link">&para;</a></h4>
<div class="highlight"><pre><span></span><code><span class="kn">import</span> <span class="nn">os</span>
<span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s2">&quot;OPENAI_API_KEY&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;...&quot;</span>
</code></pre></div>
<h4 id="in-terminal">In Terminal<a class="headerlink" href="#in-terminal" title="Permanent link">&para;</a></h4>
<div class="highlight"><pre><span></span><code><span class="nb">export</span><span class="w"> </span><span class="nv">OPENAI_API_KEY</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">&quot;...&quot;</span>
</code></pre></div>
<h3 id="create-a-basic-llm-chain-to-evaluate">Create a basic LLM chain to evaluate<a class="headerlink" href="#create-a-basic-llm-chain-to-evaluate" title="Permanent link">&para;</a></h3>
<p>This example uses langchain and OpenAI, but the same process can be followed with any framework and model provider. Once you've created your chain, just call TruChain to wrap it. Doing so allows you to capture the chain metadata for logging.</p>
<div class="highlight"><pre><span></span><code><span class="n">full_prompt</span> <span class="o">=</span> <span class="n">HumanMessagePromptTemplate</span><span class="p">(</span>
    <span class="n">prompt</span><span class="o">=</span><span class="n">PromptTemplate</span><span class="p">(</span>
        <span class="n">template</span><span class="o">=</span><span class="s2">&quot;Provide a helpful response with relevant background information for the following: </span><span class="si">{prompt}</span><span class="s2">&quot;</span><span class="p">,</span>
            <span class="n">input_variables</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;prompt&quot;</span><span class="p">],</span>
        <span class="p">)</span>
    <span class="p">)</span>
<span class="n">chat_prompt_template</span> <span class="o">=</span> <span class="n">ChatPromptTemplate</span><span class="o">.</span><span class="n">from_messages</span><span class="p">([</span><span class="n">full_prompt</span><span class="p">])</span>

<span class="n">chat</span> <span class="o">=</span> <span class="n">ChatOpenAI</span><span class="p">(</span><span class="n">model_name</span><span class="o">=</span><span class="s1">&#39;gpt-3.5-turbo&#39;</span><span class="p">,</span> <span class="n">temperature</span><span class="o">=</span><span class="mf">0.9</span><span class="p">)</span>

<span class="n">chain</span> <span class="o">=</span> <span class="n">LLMChain</span><span class="p">(</span><span class="n">llm</span><span class="o">=</span><span class="n">chat</span><span class="p">,</span> <span class="n">prompt</span><span class="o">=</span><span class="n">chat_prompt_template</span><span class="p">)</span>

<span class="c1"># wrap with truchain to instrument your chain</span>
<span class="n">tc</span> <span class="o">=</span> <span class="n">tru_chain</span><span class="o">.</span><span class="n">TruChain</span><span class="p">(</span><span class="n">chain</span><span class="p">)</span>
</code></pre></div>
<h3 id="set-up-logging-and-instrumentation">Set up logging and instrumentation<a class="headerlink" href="#set-up-logging-and-instrumentation" title="Permanent link">&para;</a></h3>
<p>Make the first call to your LLM Application. The instrumented chain can operate like the original but can also produce a log or "record" of the chain execution.</p>
<div class="highlight"><pre><span></span><code><span class="n">prompt_input</span> <span class="o">=</span> <span class="s1">&#39;que hora es?&#39;</span>
<span class="n">gpt3_response</span><span class="p">,</span> <span class="n">record</span> <span class="o">=</span> <span class="n">tc</span><span class="p">(</span><span class="n">prompt_input</span><span class="p">)</span>
</code></pre></div>
<p>We can log the records but first we need to log the chain itself.</p>
<div class="highlight"><pre><span></span><code><span class="n">tru</span><span class="o">.</span><span class="n">add_chain</span><span class="p">(</span><span class="n">chain_json</span><span class="o">=</span><span class="n">truchain</span><span class="o">.</span><span class="n">json</span><span class="p">)</span>
</code></pre></div>
<p>Now we can log the record:
<div class="highlight"><pre><span></span><code><span class="n">tru</span><span class="o">.</span><span class="n">add_record</span><span class="p">(</span>
    <span class="n">prompt</span><span class="o">=</span><span class="n">prompt_input</span><span class="p">,</span> <span class="c1"># prompt input</span>
    <span class="n">response</span><span class="o">=</span><span class="n">gpt3_response</span><span class="p">[</span><span class="s1">&#39;text&#39;</span><span class="p">],</span> <span class="c1"># LLM response</span>
    <span class="n">record_json</span><span class="o">=</span><span class="n">record</span> <span class="c1"># record is returned by the TruChain wrapper</span>
<span class="p">)</span>
</code></pre></div></p>
<h2 id="evaluate-quality">Evaluate Quality<a class="headerlink" href="#evaluate-quality" title="Permanent link">&para;</a></h2>
<p>Following the request to your app, you can then evaluate LLM quality using feedback functions. This is completed in a sequential call to minimize latency for your application, and evaluations will also be logged to your local machine.</p>
<p>To get feedback on the quality of your LLM, you can use any of the provided feedback functions or add your own.</p>
<p>To assess your LLM quality, you can provide the feedback functions to tru.run_feedback() in a list as shown below. Here we'll just add a simple language match checker.
<div class="highlight"><pre><span></span><code><span class="kn">from</span> <span class="nn">trulens_eval.tru_feedback</span> <span class="kn">import</span> <span class="n">Feedback</span><span class="p">,</span> <span class="n">Huggingface</span>

<span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s2">&quot;HUGGINGFACE_API_KEY&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;...&quot;</span>

<span class="c1"># Initialize Huggingface-based feedback function collection class:</span>
<span class="n">hugs</span> <span class="o">=</span> <span class="n">Huggingface</span><span class="p">()</span>

<span class="c1"># Define a language match feedback function using HuggingFace.</span>
<span class="n">f_lang_match</span> <span class="o">=</span> <span class="n">Feedback</span><span class="p">(</span><span class="n">hugs</span><span class="o">.</span><span class="n">language_match</span><span class="p">)</span><span class="o">.</span><span class="n">on</span><span class="p">(</span>
    <span class="n">text1</span><span class="o">=</span><span class="s2">&quot;prompt&quot;</span><span class="p">,</span> <span class="n">text2</span><span class="o">=</span><span class="s2">&quot;response&quot;</span>
<span class="p">)</span>

<span class="c1"># Run feedack functions. This might take a moment if the public api needs to load the language model used by the feedback function.</span>
<span class="n">feedback_result</span> <span class="o">=</span> <span class="n">f_lang_match</span><span class="o">.</span><span class="n">run_on_record</span><span class="p">(</span>
    <span class="n">chain_json</span><span class="o">=</span><span class="n">truchain</span><span class="o">.</span><span class="n">json</span><span class="p">,</span> <span class="n">record_json</span><span class="o">=</span><span class="n">record</span>
<span class="p">)</span>

<span class="n">JSON</span><span class="p">(</span><span class="n">feedback_result</span><span class="p">)</span>

<span class="c1"># We can also run a collection of feedback functions</span>
<span class="n">feedback_results</span> <span class="o">=</span> <span class="n">tru</span><span class="o">.</span><span class="n">run_feedback_functions</span><span class="p">(</span>
    <span class="n">record_json</span><span class="o">=</span><span class="n">record</span><span class="p">,</span>
    <span class="n">feedback_functions</span><span class="o">=</span><span class="p">[</span><span class="n">f_lang_match</span><span class="p">]</span>
<span class="p">)</span>
<span class="n">display</span><span class="p">(</span><span class="n">feedback_results</span><span class="p">)</span>
</code></pre></div></p>
<p>After capturing feedback, you can then log it to your local database
<div class="highlight"><pre><span></span><code><span class="n">tru</span><span class="o">.</span><span class="n">add_feedback</span><span class="p">(</span><span class="n">feedback_results</span><span class="p">)</span>
</code></pre></div></p>
<h3 id="automatic-logging">Automatic logging<a class="headerlink" href="#automatic-logging" title="Permanent link">&para;</a></h3>
<p>The above logging and feedback function evaluation steps can be done by TruChain.
<div class="highlight"><pre><span></span><code><span class="n">truchain</span> <span class="o">=</span> <span class="n">TruChain</span><span class="p">(</span>
    <span class="n">chain</span><span class="p">,</span>
    <span class="n">chain_id</span><span class="o">=</span><span class="s1">&#39;Chain1_ChatApplication&#39;</span><span class="p">,</span>
    <span class="n">feedbacks</span><span class="o">=</span><span class="p">[</span><span class="n">f_lang_match</span><span class="p">],</span>
    <span class="n">tru</span><span class="o">=</span><span class="n">tru</span>
<span class="p">)</span>
<span class="c1"># Note: providing `db: TruDB` causes the above constructor to log the wrapped chain in the database specified.</span>
<span class="c1"># Note: any `feedbacks` specified here will be evaluated and logged whenever the chain is used.</span>

<span class="n">truchain</span><span class="p">(</span><span class="s2">&quot;This will be automatically logged.&quot;</span><span class="p">)</span>
</code></pre></div></p>
<h3 id="out-of-band-feedback-evaluation">Out-of-band Feedback evaluation<a class="headerlink" href="#out-of-band-feedback-evaluation" title="Permanent link">&para;</a></h3>
<p>In the above example, the feedback function evaluation is done in the same process as the chain evaluation. The alternative approach is the use the provided persistent evaluator started via <code>tru.start_deferred_feedback_evaluator</code>. Then specify the <code>feedback_mode</code> for <code>TruChain</code> as <code>deferred</code> to let the evaluator handle the feedback functions.</p>
<p>For demonstration purposes, we start the evaluator here but it can be started in another process.
<div class="highlight"><pre><span></span><code><span class="n">truchain</span><span class="p">:</span> <span class="n">TruChain</span> <span class="o">=</span> <span class="n">TruChain</span><span class="p">(</span>
    <span class="n">chain</span><span class="p">,</span>
    <span class="n">chain_id</span><span class="o">=</span><span class="s1">&#39;Chain1_ChatApplication&#39;</span><span class="p">,</span>
    <span class="n">feedbacks</span><span class="o">=</span><span class="p">[</span><span class="n">f_lang_match</span><span class="p">],</span>
    <span class="n">tru</span><span class="o">=</span><span class="n">tru</span><span class="p">,</span>
    <span class="n">feedback_mode</span><span class="o">=</span><span class="s2">&quot;deferred&quot;</span>
<span class="p">)</span>

<span class="n">tru</span><span class="o">.</span><span class="n">start_evaluator</span><span class="p">()</span>
<span class="n">truchain</span><span class="p">(</span><span class="s2">&quot;This will be logged by deferred evaluator.&quot;</span><span class="p">)</span>
<span class="n">tru</span><span class="o">.</span><span class="n">stop_evaluator</span><span class="p">()</span>
</code></pre></div></p>
<h3 id="run-the-dashboard">Run the dashboard!<a class="headerlink" href="#run-the-dashboard" title="Permanent link">&para;</a></h3>
<div class="highlight"><pre><span></span><code><span class="n">tru</span><span class="o">.</span><span class="n">run_dashboard</span><span class="p">()</span> <span class="c1"># open a streamlit app to explore</span>
<span class="c1"># tru.stop_dashboard() # stop if needed</span>
</code></pre></div>
<h3 id="chain-leaderboard-quickly-identify-quality-issues">Chain Leaderboard: Quickly identify quality issues.<a class="headerlink" href="#chain-leaderboard-quickly-identify-quality-issues" title="Permanent link">&para;</a></h3>
<p>Understand how your LLM application is performing at a glance. Once you've set up logging and evaluation in your application, you can view key performance statistics including cost and average feedback value across all of your LLM apps using the chain leaderboard. As you iterate new versions of your LLM application, you can compare their performance across all of the different quality metrics you've set up.</p>
<p>Note: Average feedback values are returned and displayed in a range from 0 (worst) to 1 (best).</p>
<p><img alt="Chain Leaderboard" src="../Assets/image/Leaderboard.png" /></p>
<p>To dive deeper on a particular chain, click "Select Chain".</p>
<h3 id="understand-chain-performance-with-evaluations">Understand chain performance with Evaluations<a class="headerlink" href="#understand-chain-performance-with-evaluations" title="Permanent link">&para;</a></h3>
<p>To learn more about the performance of a particular chain or LLM model, we can select it to view its evaluations at the record level. LLM quality is assessed through the use of feedback functions. Feedback functions are extensible methods for determining the quality of LLM responses and can be applied to any downstream LLM task. Out of the box we provide a number of feedback functions for assessing model agreement, sentiment, relevance and more.</p>
<p>The evaluations tab provides record-level metadata and feedback on the quality of your LLM application.</p>
<p><img alt="Evaluations" src="../Assets/image/Leaderboard.png" /></p>
<p>Click on a record to dive deep into all of the details of your chain stack and underlying LLM, captured by tru_chain.</p>
<p><img alt="Explore a Chain" src="../Assets/image/Chain_Explore.png" /></p>
<p>If you prefer the raw format, you can quickly get it using the "Display full chain json" or "Display full record json" buttons at the bottom of the page.</p>
<h3 id="out-of-the-box-feedback-functions">Out-of-the-box Feedback Functions<a class="headerlink" href="#out-of-the-box-feedback-functions" title="Permanent link">&para;</a></h3>
<p>See: <a href="https://www.trulens.org/trulens_eval/api/tru_feedback/">https://www.trulens.org/trulens_eval/api/tru_feedback/</a></p>
<h4 id="relevance">Relevance<a class="headerlink" href="#relevance" title="Permanent link">&para;</a></h4>
<p>This evaluates the <em>relevance</em> of the LLM response to the given text by LLM prompting.</p>
<p>Relevance is currently only available with OpenAI ChatCompletion API.</p>
<h4 id="sentiment">Sentiment<a class="headerlink" href="#sentiment" title="Permanent link">&para;</a></h4>
<p>This evaluates the <em>positive sentiment</em> of either the prompt or response.</p>
<p>Sentiment is currently available to use with OpenAI, HuggingFace or Cohere as the model provider.</p>
<ul>
<li>The OpenAI sentiment feedback function prompts a Chat Completion model to rate the sentiment from 1 to 10, and then scales the response down to 0-1.</li>
<li>The HuggingFace sentiment feedback function returns a raw score from 0 to 1.</li>
<li>The Cohere sentiment feedback function uses the classification endpoint and a small set of examples stored in feedback_prompts.py to return either a 0 or a 1.</li>
</ul>
<h4 id="model-agreement">Model Agreement<a class="headerlink" href="#model-agreement" title="Permanent link">&para;</a></h4>
<p>Model agreement uses OpenAI to attempt an honest answer at your prompt with system prompts for correctness, and then evaluates the aggreement of your LLM response to this model on a scale from 1 to 10. The agreement with each honest bot is then averaged and scaled from 0 to 1.</p>
<h4 id="language-match">Language Match<a class="headerlink" href="#language-match" title="Permanent link">&para;</a></h4>
<p>This evaluates if the language of the prompt and response match.</p>
<p>Language match is currently only available to use with HuggingFace as the model provider. This feedback function returns a score in the range from 0 to 1, where 1 indicates match and 0 indicates mismatch.</p>
<h4 id="toxicity">Toxicity<a class="headerlink" href="#toxicity" title="Permanent link">&para;</a></h4>
<p>This evaluates the toxicity of the prompt or response.</p>
<p>Toxicity is currently only available to be used with HuggingFace, and uses a classification endpoint to return a score from 0 to 1. The feedback function is negated as not_toxicity, and returns a 1 if not toxic and a 0 if toxic.</p>
<h4 id="moderation">Moderation<a class="headerlink" href="#moderation" title="Permanent link">&para;</a></h4>
<p>The OpenAI Moderation API is made available for use as feedback functions. This includes hate, hate/threatening, self-harm, sexual, sexual/minors, violence, and violence/graphic. Each is negated (ex: not_hate) so that a 0 would indicate that the moderation rule is violated. These feedback functions return a score in the range 0 to 1.</p>
<h2 id="adding-new-feedback-functions">Adding new feedback functions<a class="headerlink" href="#adding-new-feedback-functions" title="Permanent link">&para;</a></h2>
<p>Feedback functions are an extensible framework for evaluating LLMs. You can add your own feedback functions to evaluate the qualities required by your application by updating trulens_eval/tru_feedback.py. If your contributions would be useful for others, we encourage you to contribute to trulens!</p>
<p>Feedback functions are organized by model provider into Provider classes.</p>
<p>The process for adding new feedback functions is:
1. Create a new Provider class or locate an existing one that applies to your feedback function. If your feedback function does not rely on a model provider, you can create a standalone class:</p>
<div class="highlight"><pre><span></span><code><span class="k">class</span> <span class="nc">StandAlone</span><span class="p">(</span><span class="n">Provider</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">pass</span>
</code></pre></div>
<ol>
<li>Add a new feedback function method to your selected class. Your new method can either take a single text (str) as a parameter or both promopt (str) and response (str). It should return a float between 0 (worst) and 1 (best).</li>
</ol>
<div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">feedback</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">text</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">float</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Describe how the model works</span>

<span class="sd">        Parameters:</span>
<span class="sd">            text (str): Text to evaluate.</span>
<span class="sd">            Can also be prompt (str) and response (str).</span>

<span class="sd">        Returns:</span>
<span class="sd">            float: A value between 0 (worst) and 1 (best).</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="nb">float</span>
</code></pre></div>





                
              </article>
            </div>
          
          
        </div>
        
      </main>
      
        <footer class="md-footer">
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    <script id="__config" type="application/json">{"base": "../..", "features": ["navigation.sections"], "search": "../../assets/javascripts/workers/search.208ed371.min.js", "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}}</script>
    
    
      <script src="../../assets/javascripts/bundle.b4d07000.min.js"></script>
      
        <script src="../../javascript/config.js"></script>
      
        <script src="../../javascript/app.js"></script>
      
        <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
      
        <script src="../../javascript/tex-mml-chtml-3.0.0.js"></script>
      
        <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_HTML"></script>
      
    
  </body>
</html>