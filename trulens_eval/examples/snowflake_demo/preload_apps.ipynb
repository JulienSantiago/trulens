{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Streamlit example development tuning\n",
    "\n",
    "In this notebook, we load a few configurations of the streamlit core app and run a test set through TruLens evaluations. This shows how to test and find the optimal configuration of any LLM app, carefully keeping track of configuration metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/snowday/lib/python3.11/site-packages/_distutils_hack/__init__.py:26: UserWarning: Setuptools is replacing distutils.\n",
      "  warnings.warn(\"Setuptools is replacing distutils.\")\n",
      "Package snowflake-snowpark-python not present in requirements.\n",
      "/opt/anaconda3/envs/snowday/lib/python3.11/site-packages/snowflake/sqlalchemy/base.py:1068: SAWarning: The GenericFunction 'flatten' is already registered and is going to be overridden.\n",
      "  functions.register_function(\"flatten\", flatten)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦‘ Tru initialized with db url snowflake://jreini:***@fab02971/dkurokawa/trulens_demo?role=ENGINEER&warehouse=dkurokawa .\n",
      "ðŸ›‘ Secret keys may be written to the database. See the `database_redact_keys` option of `Tru` to prevent this.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-12 10:55:05.638 \n",
      "  \u001b[33m\u001b[1mWarning:\u001b[0m to view this Streamlit app on a browser, run it with the following\n",
      "  command:\n",
      "\n",
      "    streamlit run /opt/anaconda3/envs/snowday/lib/python3.11/site-packages/ipykernel_launcher.py [ARGUMENTS]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… In Context Relevance, input question will be set to __record__.main_input or `Select.RecordInput` .\n",
      "âœ… In Context Relevance, input context will be set to __record__.app.retrieve_context.rets[:] .\n",
      "âœ… In [Small Local Model] Context Relevance, input question will be set to __record__.main_input or `Select.RecordInput` .\n",
      "âœ… In [Small Local Model] Context Relevance, input context will be set to __record__.app.retrieve_context.rets[:] .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tru was already initialized. Cannot change database configuration after initialization.\n",
      "Singleton instance of type Tru already created at:\n",
      "/Users/jreini/Desktop/development/trulens/trulens_eval/examples/snowflake_demo/feedback.py:23\n",
      "\ttru = Tru(database_url=db_url)\n",
      "\n",
      "You can delete the singleton by calling `<instance>.delete_singleton()` or \n",
      "  ```python\n",
      "  from trulens_eval.utils.python import SingletonPerName\n",
      "  SingletonPerName.delete_singleton_by_name(name=\"None\", cls=Tru)\n",
      "  ```\n",
      "            \n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from common_ui import TruCustomApp, generator, ModelConfig\n",
    "import streamlit as st\n",
    "\n",
    "from trulens_eval import Tru\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "db_url = 'snowflake://{user}:{password}@{account}/{dbname}/{schema}?warehouse={warehouse}&role={role}'.format(\n",
    "        user=os.environ['SNOWFLAKE_USER'],\n",
    "        account=os.environ['SNOWFLAKE_ACCOUNT'],\n",
    "        password=os.environ['SNOWFLAKE_USER_PASSWORD'],\n",
    "        dbname=os.environ['SNOWFLAKE_DATABASE'],\n",
    "        schema=os.environ['SNOWFLAKE_SCHEMA'],\n",
    "        warehouse=os.environ['SNOWFLAKE_WAREHOUSE'],\n",
    "        role=os.environ['SNOWFLAKE_ROLE']\n",
    "    )\n",
    "\n",
    "tru = Tru(database_url=db_url)\n",
    "\n",
    "tru.reset_database()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from schema import Conversation, Message\n",
    "\n",
    "def create_conversation_from_user_message(user_message: str) -> Conversation:\n",
    "    conversation = Conversation()\n",
    "    conversation.add_message(Message(role=\"user\", content=user_message))\n",
    "    return conversation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = [\n",
    "    \"How do I launch a streamlit app?\",\n",
    "    \"How can I capture the state of my session in streamlit?\",\n",
    "    \"How do I install streamlit?\",\n",
    "    \"How do I change the background color of a streamlit app?\",\n",
    "    \"What's the advantage of using a streamlit form?\",\n",
    "    \"What are some ways I should use checkboxes?\",\n",
    "    \"How can I conserve space and hide away content?\",\n",
    "    \"Can you recommend some resources for learning Streamlit?\",\n",
    "    \"What are some common use cases for Streamlit?\",\n",
    "    \"How can I deploy a streamlit app on the cloud?\",\n",
    "    \"How do I add a logo to streamlit?\",\n",
    "    \"What is the best way to deploy a Streamlit app?\",\n",
    "    \"How should I use a streamlit toggle?\",\n",
    "    \"How do I add new pages to my streamlit app?\",\n",
    "    \"How do I write a dataframe to display in my dashboard?\",\n",
    "    \"Can I plot a map in streamlit? If so, how?\",\n",
    "    \"How do vector stores enable efficient similarity search?\",\n",
    "    \"How do I prevent my child from using the internet?\",\n",
    "    \"What should I pack for a camping trip this weekend?\",\n",
    "    \"How do I defend myself against bear attacks?\"\n",
    "]\n",
    "\n",
    "conversations = [create_conversation_from_user_message(prompt) for prompt in prompts]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the first app"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… In Groundedness, input source will be set to __record__.app.retrieve_context.rets[:].collect() .\n",
      "âœ… In Groundedness, input statement will be set to __record__.main_output or `Select.RecordOutput` .\n",
      "âœ… In Context Relevance, input question will be set to __record__.main_input or `Select.RecordInput` .\n",
      "âœ… In Context Relevance, input context will be set to __record__.app.retrieve_context.rets[:] .\n",
      "âœ… In Answer Relevance, input prompt will be set to __record__.main_input or `Select.RecordInput` .\n",
      "âœ… In Answer Relevance, input response will be set to __record__.main_output or `Select.RecordOutput` .\n"
     ]
    }
   ],
   "source": [
    "from feedback import get_feedbacks\n",
    "\n",
    "feedbacks = get_feedbacks(provider_name=\"Cortex\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/snowday/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/snowday/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/snowday/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/snowday/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/snowday/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/snowday/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/snowday/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/snowday/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/snowday/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/snowday/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/snowday/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/snowday/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/snowday/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/snowday/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/snowday/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/snowday/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/snowday/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/Users/jreini/Desktop/development/trulens/trulens_eval/trulens_eval/feedback/feedback.py:915: UserWarning: Feedback function Context Relevance with aggregation <function mean at 0x1082d7100> had no inputs.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/snowday/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/snowday/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/snowday/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/Users/jreini/Desktop/development/trulens/trulens_eval/trulens_eval/feedback/feedback.py:915: UserWarning: Feedback function Context Relevance with aggregation <function mean at 0x1082d7100> had no inputs.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "tru_recorder_1 = TruCustomApp(generator, app_id=\"app-dev-1-pinecone\", metadata={\"model\": \"Mistral Large\", \"use_rag\": True, \"temperature\": 0.2, \"top_p\": 0.1, \"retriever\": \"pinecone\"},  feedbacks=feedbacks)\n",
    "\n",
    "with tru_recorder_1:\n",
    "    for conversation in conversations:\n",
    "        model_config_1 = ModelConfig(provider = \"Cortex\", model=\"Mistral Large\", use_rag=True, temperature=0.2, top_p=0.1, retriever=\"Pinecone\", retrieval_filter=0)\n",
    "        chat = st.chat_message(\"assistant\")\n",
    "        conversation.model_config = model_config_1\n",
    "        user_message, prompt = generator.prepare_prompt(conversation)\n",
    "        if conversation.model_config.use_rag:\n",
    "            text_response = generator.retrieve_and_generate_response(user_message, prompt, conversation, chat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jreini/Desktop/development/trulens/trulens_eval/trulens_eval/feedback/feedback.py:915: UserWarning: Feedback function Context Relevance with aggregation <function mean at 0x1082d7100> had no inputs.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "tru_recorder_2 = TruCustomApp(generator, app_id=\"app-dev-2-cortex-search\", metadata={\"model\": \"Mistral Large\", \"use_rag\": True, \"temperature\": 0.2, \"top_p\": 0.1, \"retriever\": \"cortex\"},  feedbacks=feedbacks)\n",
    "\n",
    "with tru_recorder_2:\n",
    "    for conversation in conversations:\n",
    "        model_config_2 = ModelConfig(provider = \"Cortex\", model=\"Mistral Large\",  use_rag=True, temperature=0.2, top_p=0.1, retriever=\"Cortex Search\", retrieval_filter=0)\n",
    "        chat = st.chat_message(\"assistant\")\n",
    "        conversation.model_config = model_config_2\n",
    "        user_message, prompt = generator.prepare_prompt(conversation)\n",
    "        if conversation.model_config.use_rag:\n",
    "            text_response = generator.retrieve_and_generate_response(user_message, prompt, conversation, chat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tru_recorder_3 = TruCustomApp(generator, app_id=\"app-dev-3-cortex-search-w-filters\", metadata={\"model\": \"Mistral Large\",  \"use_rag\": True, \"temperature\": 0.2, \"top_p\": 0.1, \"retriever\": \"cortex\", \"retrieval_filter\":0.75, \"filter_feedback_function\": \"Context Relevance (LLM-as-Judge)\"},  feedbacks=feedbacks)\n",
    "\n",
    "with tru_recorder_3:\n",
    "    for conversation in conversations:\n",
    "        model_config_3 = ModelConfig(provider = \"Cortex\", model=\"Mistral Large\",  use_rag=True, temperature=0.2, top_p=0.1, retriever=\"Cortex Search\", retrieval_filter=0.75, filter_feedback_function=\"Context Relevance (LLM-as-Judge)\")\n",
    "        chat = st.chat_message(\"assistant\")\n",
    "        conversation.model_config = model_config_3\n",
    "        user_message, prompt = generator.prepare_prompt(conversation)\n",
    "        if conversation.model_config.use_rag:\n",
    "            text_response = generator.retrieve_and_generate_response(user_message, prompt, conversation, chat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tru_recorder_prod = TruCustomApp(generator, app_id=\"app-prod\", metadata={\"model\": \"Mistral Large\", \"use_rag\": True, \"temperature\": 0.2, \"top_p\": 0.1, \"retriever\": \"cortex\", \"retrieval_filter\":0.75, \"filter_feedback_function\": \"Context Relevance (LLM-as-Judge)\"},  feedbacks=feedbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('st_127_132_prompt.txt', 'r') as file:\n",
    "    st_127_132_prompts = file.readlines()\n",
    "\n",
    "\n",
    "st_127_132_conversations = [create_conversation_from_user_message(prompt) for prompt in st_127_132_prompts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tru_recorder_prod:\n",
    "    import random\n",
    "    for _ in range(30):\n",
    "        conversation = random.choice(st_127_132_conversations)\n",
    "        model_config_prod = ModelConfig(provider = \"Cortex\", model=\"Mistral Large\", use_rag=True, temperature=0.2, top_p=0.1, retriever=\"Cortex Search\", retrieval_filter=0.75, filter_feedback_function=\"Context Relevance (LLM-as-Judge)\")\n",
    "        chat = st.chat_message(\"assistant\")\n",
    "        conversation.model_config = model_config_prod\n",
    "        user_message, prompt = generator.prepare_prompt(conversation)\n",
    "        if conversation.model_config.use_rag:\n",
    "            text_response = generator.retrieve_and_generate_response(user_message, prompt, conversation, chat)\n",
    "        import random\n",
    "        time.sleep(random.randint(2, 6))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('st_133_136_prompt.txt', 'r') as file:\n",
    "    st_133_136_prompts = file.readlines()\n",
    "\n",
    "st_133_136_conversations = [create_conversation_from_user_message(prompt) for prompt in st_133_136_prompts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tru_recorder_prod:\n",
    "    import random\n",
    "    for _ in range(100):\n",
    "        conversation = random.choice(st_133_136_conversations)\n",
    "        model_config_prod = ModelConfig(provider = \"Cortex\", model=\"Mistral Large\", temperature=0.2, top_p=0.1, retriever=\"Cortex Search\", retrieval_filter=0.5)\n",
    "        chat = st.chat_message(\"assistant\")\n",
    "        conversation.model_config = model_config_prod\n",
    "        user_message, prompt = generator.prepare_prompt(conversation)\n",
    "        if conversation.model_config.use_rag:\n",
    "            text_response = generator.retrieve_and_generate_response(user_message, prompt, conversation, chat)\n",
    "        time.sleep(random.randint(2, 6))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "streamlit_demo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
