{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Streamlit example development tuning\n",
    "\n",
    "In this notebook, we load a few configurations of the streamlit core app and run a test set through TruLens evaluations. This shows how to test and find the optimal configuration of any LLM app, carefully keeping track of configuration metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from common_ui import TruCustomApp, generator, ModelConfig, get_feedbacks\n",
    "import streamlit as st\n",
    "\n",
    "from trulens_eval import Tru\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "db_url = 'snowflake://{user}:{password}@{account}/{dbname}/{schema}?warehouse={warehouse}&role={role}'.format(\n",
    "        user=os.environ['SNOWFLAKE_USER'],\n",
    "        account=os.environ['SNOWFLAKE_ACCOUNT'],\n",
    "        password=os.environ['SNOWFLAKE_USER_PASSWORD'],\n",
    "        dbname=os.environ['SNOWFLAKE_DATABASE'],\n",
    "        schema=os.environ['SNOWFLAKE_SCHEMA'],\n",
    "        warehouse=os.environ['SNOWFLAKE_WAREHOUSE'],\n",
    "        role=os.environ['SNOWFLAKE_ROLE']\n",
    "    )\n",
    "\n",
    "tru = Tru(database_url=db_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feedbacks = get_feedbacks(provider_name=\"Cortex\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from schema import Conversation, Message\n",
    "\n",
    "def create_conversation_from_user_message(user_message: str) -> Conversation:\n",
    "    conversation = Conversation()\n",
    "    conversation.add_message(Message(role=\"user\", content=user_message))\n",
    "    return conversation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = [\n",
    "    \"How do I launch a streamlit app?\",\n",
    "    \"How can I capture the state of my session in streamlit?\",\n",
    "    \"How do I install streamlit?\",\n",
    "    \"How do I change the background color of a streamlit app?\",\n",
    "    \"What's the advantage of using a streamlit form?\",\n",
    "    \"What are some ways I should use checkboxes?\",\n",
    "    \"How can I conserve space and hide away content?\",\n",
    "    \"Can you recommend some resources for learning Streamlit?\",\n",
    "    \"What are some common use cases for Streamlit?\",\n",
    "    \"How can I deploy a streamlit app on the cloud?\",\n",
    "    \"How do I add a logo to streamlit?\",\n",
    "    \"What is the best way to deploy a Streamlit app?\",\n",
    "    \"How should I use a streamlit toggle?\",\n",
    "    \"How do I add new pages to my streamlit app?\",\n",
    "    \"How do I write a dataframe to display in my dashboard?\",\n",
    "    \"Can I plot a map in streamlit? If so, how?\"\n",
    "]\n",
    "\n",
    "conversations = [create_conversation_from_user_message(prompt) for prompt in prompts]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the first app"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from feedback import AVAILABLE_PROVIDERS\n",
    "from llm import AVAILABLE_FEEDBACK_FUNCTION_FILTERS\n",
    "from llm import PROVIDER_MODELS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tru_recorder_1 = TruCustomApp(generator, app_id=\"0TEST-app-dev-1-pinecone\", metadata={\"model\": \"Snowflake Arctic Instruct\", \"use_rag\": True, \"temperature\": 0.2, \"top_p\": 0.1, \"retriever\": \"pinecone\", \"provider\": \"Cortex\"},  feedbacks=feedbacks)\n",
    "\n",
    "with tru_recorder_1:\n",
    "    for conversation in conversations:\n",
    "        model_config_1 = ModelConfig(model=\"Snowflake Arctic Instruct\", use_rag=True, temperature=0.2, top_p=0.1, retriever=\"Pinecone\", retrieval_filter=0, provider=\"Cortex\")\n",
    "        chat = st.chat_message(\"assistant\")\n",
    "        conversation.model_config = model_config_1\n",
    "        user_message, prompt = generator.prepare_prompt(conversation)\n",
    "        if conversation.model_config.use_rag:\n",
    "            text_response = generator.retrieve_and_generate_response(user_message, prompt, conversation, chat)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tru_recorder_2 = TruCustomApp(generator, app_id=\"0TEST-app-dev-2-cortex-search\", metadata={\"model\": \"Snowflake Arctic\", \"use_rag\": True, \"temperature\": 0.2, \"top_p\": 0.1, \"retriever\": \"cortex\",},  feedbacks=feedbacks)\n",
    "\n",
    "with tru_recorder_2:\n",
    "    for conversation in conversations:\n",
    "        model_config_2 = ModelConfig(model=\"Snowflake Arctic\", use_rag=True, temperature=0.2, top_p=0.1, retriever=\"Cortex Search\", retrieval_filter=0, provider=\"Replicate\")\n",
    "        chat = st.chat_message(\"assistant\")\n",
    "        conversation.model_config = model_config_2\n",
    "        user_message, prompt = generator.prepare_prompt(conversation)\n",
    "        if conversation.model_config.use_rag:\n",
    "            text_response = generator.retrieve_and_generate_response(user_message, prompt, conversation, chat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tru_recorder_3 = TruCustomApp(generator, app_id=\"TEST-app-dev-3-cortex-search-w-filters\", metadata={\"model\": \"Snowflake Arctic\", \"use_rag\": True, \"temperature\": 0.2, \"top_p\": 0.1, \"retriever\": \"cortex\", \"retrieval_filter\":0.75, \"filter_feedback_function\": \"Context Relevance (LLM-as-Judge)\"},  feedbacks=feedbacks)\n",
    "\n",
    "with tru_recorder_3:\n",
    "    for conversation in conversations:\n",
    "        model_config_3 = ModelConfig(model=\"Snowflake Arctic\", use_rag=True, temperature=0.2, top_p=0.1, retriever=\"Cortex Search\", retrieval_filter=0.75, filter_feedback_function=\"Context Relevance (LLM-as-Judge)\", provider=\"Cortex\")\n",
    "        chat = st.chat_message(\"assistant\")\n",
    "        conversation.model_config = model_config_3\n",
    "        user_message, prompt = generator.prepare_prompt(conversation)\n",
    "        if conversation.model_config.use_rag:\n",
    "            text_response = generator.retrieve_and_generate_response(user_message, prompt, conversation, chat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tru_recorder_4 = TruCustomApp(generator, app_id=\"TEST-app-dev-4-cortex-search-w-filters-small_model\", metadata={\"model\": \"Snowflake Arctic\", \"use_rag\": True, \"temperature\": 0.2, \"top_p\": 0.1, \"retriever\": \"cortex\", \"retrieval_filter\":0.75, \"filter_feedback_function\": \"Context Relevance (small)\"},  feedbacks=feedbacks)\n",
    "\n",
    "with tru_recorder_4:\n",
    "    for conversation in conversations:\n",
    "        model_config_4= ModelConfig(model=\"Snowflake Arctic\", use_rag=True, temperature=0.2, top_p=0.1, retriever=\"Cortex Search\", retrieval_filter=0.75, filter_feedback_function=\"Context Relevance (small)\", provider=\"Cortex\")\n",
    "        chat = st.chat_message(\"assistant\")\n",
    "        conversation.model_config = model_config_4\n",
    "        user_message, prompt = generator.prepare_prompt(conversation)\n",
    "        if conversation.model_config.use_rag:\n",
    "            text_response = generator.retrieve_and_generate_response(user_message, prompt, conversation, chat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tru_recorder_prod = TruCustomApp(generator, app_id=\"TEST-app-prod\", metadata={\"model\": \"Snowflake Arctic\", \"use_rag\": True, \"temperature\": 0.2, \"top_p\": 0.1, \"retriever\": \"cortex\", \"retrieval_filter\":0.75, \"filter_feedback_function\": \"Context Relevance (LLM-as-Judge)\"},  feedbacks=feedbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tru_recorder_prod:\n",
    "    import random\n",
    "    for _ in range(100):\n",
    "        conversation = random.choice(conversations)\n",
    "        model_config_prod = ModelConfig(model=\"Snowflake Arctic\", use_rag=True, temperature=0.2, top_p=0.1, retriever=\"Cortex Search\", retrieval_filter=0.75, filter_feedback_function=\"Context Relevance (LLM-as-Judge)\")\n",
    "        chat = st.chat_message(\"assistant\")\n",
    "        conversation.model_config = model_config_prod\n",
    "        user_message, prompt = generator.prepare_prompt(conversation)\n",
    "        if conversation.model_config.use_rag:\n",
    "            text_response = generator.retrieve_and_generate_response(user_message, prompt, conversation, chat)\n",
    "        import random\n",
    "        time.sleep(random.randint(2, 6))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hard_prompts = [\n",
    "    \"How do I use streamlit pages with st.Page and st.navigation?\",\n",
    "    \"Is protobuf version 5 supported by streamlit?\",\n",
    "    \"Can I set the gap size between streamlit columns?\",\n",
    "    \"Can I set vertical alignment for streamlit columns?\",\n",
    "    \"How do I set vertical alignment for streamlit columns?\",\n",
    "    \"Can I add a dialog box to my streamlit app?\",\n",
    "    \"How do I add a dialog box to my streamlit app?\",\n",
    "    \"How do I make st.audio loop?\",\n",
    "    \"Can st.video play in a loop?\"\n",
    "]\n",
    "\n",
    "hard_conversations = [create_conversation_from_user_message(hard_prompt) for hard_prompt in hard_prompts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tru_recorder_prod:\n",
    "    import random\n",
    "    for _ in range(100):\n",
    "        conversation = random.choice(hard_conversations)\n",
    "        model_config_prod = ModelConfig(model=\"Snowflake Arctic\", use_rag=True, temperature=0.2, top_p=0.1, retriever=\"Cortex Search\", retrieval_filter=0.5)\n",
    "        chat = st.chat_message(\"assistant\")\n",
    "        conversation.model_config = model_config_prod\n",
    "        user_message, prompt = generator.prepare_prompt(conversation)\n",
    "        if conversation.model_config.use_rag:\n",
    "            text_response = generator.retrieve_and_generate_response(user_message, prompt, conversation, chat)\n",
    "        time.sleep(random.randint(2, 6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "streamlit_demo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
