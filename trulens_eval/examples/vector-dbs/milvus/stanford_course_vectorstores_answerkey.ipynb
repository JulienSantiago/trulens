{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 1\n",
    "\n",
    "Question 0.\n",
    "To get up and running, you'll first need to install Docker and Milvus. Find instructions below:\n",
    "* Docker Compose ([Instructions](https://docs.docker.com/compose/install/))\n",
    "* Milvus Standalone ([Instructions](https://milvus.io/docs/install_standalone-docker.md))\n",
    "\n",
    "Question 1.\n",
    "Build a prototype version of your RAG application. Choose any embedding/LLM/parameter set you like.\n",
    "\n",
    "Question 2.\n",
    "Set up evaluations. Answer relevance is set up for you. Add evaluations for context relevance and groundedness.\n",
    "\n",
    "Question 3.\n",
    "Try different index types, embeddings, parameters and LLMs. Find the best performing application for the evaluation set, and explain why it performed the best.\n",
    "\n",
    "Note. You may not prune the dataset stored in the vector database to improve performance."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "### Install dependencies\n",
    "Let's install some of the dependencies for this notebook if we don't have them already"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! pip install trulens-eval==0.10.0 llama_index==0.8.4 pymilvus==2.3.0 nltk==3.8.1 html2text==2020.1.16 tenacity==8.2.3"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add API keys\n",
    "For this quickstart, you will need Open AI and Huggingface keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"...\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import from LlamaIndex and TruLens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.storage.storage_context import StorageContext\n",
    "from llama_index.vector_stores import MilvusVectorStore\n",
    "from llama_index.llms import OpenAI\n",
    "from llama_index import (\n",
    "    VectorStoreIndex,\n",
    "    SimpleWebPageReader,\n",
    "    LLMPredictor,\n",
    "    ServiceContext\n",
    ")\n",
    "\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "\n",
    "from tenacity import retry, stop_after_attempt, wait_exponential\n",
    "\n",
    "from trulens_eval import TruLlama, Feedback, Tru, feedback\n",
    "from trulens_eval.feedback import Groundedness\n",
    "tru = Tru()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First we need to load documents. We can use SimpleWebPageReader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import WikipediaReader\n",
    "\n",
    "cities = [\n",
    "    \"Atlanta\", \"Los Angeles\", \"Chicago\", \"Houston\", \"Phoenix, Arizona\", \n",
    "    \"Philadelphia\", \"San Antonio\", \"Honolulu\", \"Tucson\", \"Mexico City\", \n",
    "    \"Austin\", \"Jacksonville\", \"Fort Worth\", \"Cincinatti\", \"Charlotte\", \n",
    "    \"San Francisco\", \"Indianapolis\", \"Seattle\", \"Salt Lake City\", \"Washington D.C.\"\n",
    "]\n",
    "\n",
    "wiki_docs = []\n",
    "for city in cities:\n",
    "    try:\n",
    "        doc = WikipediaReader().load_data(pages=[city])\n",
    "        wiki_docs.extend(doc)\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading page for city {city}: {e}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now write down our test prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_prompts = [\n",
    "    \"What's the best national park near Honolulu\",\n",
    "    \"What are some famous universities in Tucson?\",\n",
    "    \"What bodies of water are near Chicago?\",\n",
    "    \"What is the name of Chicago's central business district?\",\n",
    "    \"What are the two most famous universities in Los Angeles?\",\n",
    "    \"What are some famous festivals in Mexico City?\",\n",
    "    \"What are some famous festivals in Los Angeles?\",\n",
    "    \"What professional sports teams are located in Los Angeles\",\n",
    "    \"How do you classify Houston's climate?\",\n",
    "    \"What landmarks should I know about in Cincinatti\"\n",
    "]\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1. Build a prototype RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store = MilvusVectorStore(index_params={\n",
    "        \"index_type\": index_param,\n",
    "        \"metric_type\": \"L2\"\n",
    "        },\n",
    "        search_params={\"nprobe\": 20},\n",
    "        overwrite=True)\n",
    "llm = OpenAI(model=\"gpt-3.5-turbo\")\n",
    "storage_context = StorageContext.from_defaults(vector_store = vector_store)\n",
    "service_context = ServiceContext.from_defaults(embed_model = embed_model, llm = llm)\n",
    "index = VectorStoreIndex.from_documents(wiki_docs,\n",
    "            service_context=service_context,\n",
    "            storage_context=storage_context)\n",
    "query_engine = index.as_query_engine(top_k = top_k)\n",
    "@retry(stop=stop_after_attempt(10), wait=wait_exponential(multiplier=1, min=4, max=10))\n",
    "def call_query_engine(prompt):\n",
    "        return query_engine.query(prompt)\n",
    "for prompt in test_prompts:\n",
    "    call_query_engine(prompt)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2. Set up Evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Initialize OpenAI-based feedback function collection class:\n",
    "openai = feedback.OpenAI(model_engine=\"gpt-4\")\n",
    "\n",
    "# Define groundedness\n",
    "grounded = Groundedness(groundedness_provider=openai)\n",
    "f_groundedness = Feedback(grounded.groundedness_measure, name = \"Groundedness\").on(\n",
    "    TruLlama.select_source_nodes().node.text # context\n",
    ").on_output().aggregate(grounded.grounded_statements_aggregator)\n",
    "\n",
    "# Question/answer relevance between overall question and answer.\n",
    "f_qa_relevance = Feedback(openai.relevance, name = \"Answer Relevance\").on_input_output()\n",
    "\n",
    "# Question/statement relevance between question and each context chunk.\n",
    "f_qs_relevance = Feedback(openai.qs_relevance, name = \"Context Relevance\").on_input().on(\n",
    "    TruLlama.select_source_nodes().node.text\n",
    ").aggregate(np.mean)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3. Find the best configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_params = [\"IVF_FLAT\",\"HNSW\"]\n",
    "embed_v12 = HuggingFaceEmbeddings(model_name = \"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\")\n",
    "embed_ft3_v12 = HuggingFaceEmbeddings(model_name = \"Sprylab/paraphrase-multilingual-MiniLM-L12-v2-fine-tuned-3\")\n",
    "embed_ada = OpenAIEmbeddings(model_name = \"text-embedding-ada-002\")\n",
    "embed_models = [embed_v12, embed_ft3_v12, embed_ft5_v12]\n",
    "top_ks = [1,3,7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "for index_param, embed_model, top_k in itertools.product(\n",
    "    index_params, embed_models, top_ks\n",
    "    ):\n",
    "    if embed_model == embed_v12:\n",
    "        embed_model_name = \"v12\"\n",
    "    elif embed_model == embed_ft3_v12:\n",
    "        embed_model_name = \"ft3_v12\"\n",
    "    elif embed_model == embed_ada:\n",
    "        embed_model_name = \"ada\"\n",
    "    vector_store = MilvusVectorStore(index_params={\n",
    "        \"index_type\": index_param,\n",
    "        \"metric_type\": \"L2\"\n",
    "        },\n",
    "        search_params={\"nprobe\": 20},\n",
    "        overwrite=True)\n",
    "    llm = OpenAI(model=\"gpt-3.5-turbo\")\n",
    "    storage_context = StorageContext.from_defaults(vector_store = vector_store)\n",
    "    service_context = ServiceContext.from_defaults(embed_model = embed_model, llm = llm)\n",
    "    index = VectorStoreIndex.from_documents(wiki_docs,\n",
    "            service_context=service_context,\n",
    "            storage_context=storage_context)\n",
    "    query_engine = index.as_query_engine(top_k = top_k)\n",
    "    tru_query_engine = TruLlama(query_engine,\n",
    "                    app_id=f\"gpt4eval-App-{index_param}-{embed_model_name}-{top_k}\",\n",
    "                    feedbacks=[f_groundedness, f_qa_relevance, f_qs_relevance],\n",
    "                    metadata={\n",
    "                        'index_param':index_param,\n",
    "                        'embed_model':embed_model_name,\n",
    "                        'top_k':top_k\n",
    "                        })\n",
    "    @retry(stop=stop_after_attempt(10), wait=wait_exponential(multiplier=1, min=4, max=10))\n",
    "    def call_tru_query_engine(prompt):\n",
    "        return tru_query_engine.query(prompt)\n",
    "    for prompt in test_prompts:\n",
    "        call_tru_query_engine(prompt)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore in a Dashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tru.run_dashboard() # open a local streamlit app to explore\n",
    "\n",
    "# tru.stop_dashboard() # stop if needed"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, you can run `trulens-eval` from a command line in the same folder to start the dashboard."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Or view results directly in your notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tru.get_records_and_feedback(app_ids=[])[0] # pass an empty list of app_ids to get all"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.4 ('milvus')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "12da0033b6ee0a044900ff965f51baf1f826c79f2500e7fd02d2f79bac1ea7cb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
