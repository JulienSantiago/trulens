{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TruBot\n",
    "\n",
    "Example setup and monitoring of a conversational bot with context made up of the\n",
    "TruEra website. This example requires either a pinecone vector db set up with\n",
    "some contexts to answer questions with or alternatively can use the local\n",
    "database for use with hnswlib provided here. To use hnswlib, some additional\n",
    "requirements need to be installed with pip. Regardless of the vector db\n",
    "provider, the example feedback functions here use openai and huggingface free\n",
    "inference APIs and need their respective keys to be provided in a .env file.\n",
    "\n",
    "## HNSWLIB additional requirements\n",
    "\n",
    "Run the following in your shell or the equivalent in the following cell to\n",
    "install additional requirements for use with HNSWLIB. This is not required if\n",
    "you are running this example with a pinecone db.\n",
    "\n",
    "```bash\n",
    "pip install docarray hnswlib\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install docarray hnswlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "# If running from github repo, can use this:\n",
    "sys.path.append(str(Path().cwd().parent.parent.resolve()))\n",
    "\n",
    "# Uncomment for more debugging printouts.\n",
    "\"\"\"\n",
    "import logging\n",
    "root = logging.getLogger()\n",
    "root.setLevel(logging.DEBUG)\n",
    "\n",
    "handler = logging.StreamHandler(sys.stdout)\n",
    "handler.setLevel(logging.DEBUG)\n",
    "formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "handler.setFormatter(formatter)\n",
    "root.addHandler(handler)\n",
    "\"\"\"\n",
    "None"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## API keys setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Key OPENAI_API_KEY set from environment (same value found in .env file at /Users/piotrm/Dropbox/repos/github/trulens/.env).\n",
      "âœ… Key HUGGINGFACE_API_KEY set from environment (same value found in .env file at /Users/piotrm/Dropbox/repos/github/trulens/.env).\n"
     ]
    }
   ],
   "source": [
    "from trulens_eval.keys import check_keys\n",
    "\n",
    "check_keys(\n",
    "    \"OPENAI_API_KEY\",\n",
    "    \"HUGGINGFACE_API_KEY\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦‘ Tru initialized with db url sqlite:///default.sqlite .\n",
      "Force stopping dashboard ...\n",
      "Starting dashboard ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91ca485fd3994fdf82e6c1a4d10b0aab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Accordion(children=(VBox(children=(VBox(children=(Label(value='STDOUT'), Output())), VBox(children=(Label(valuâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dashboard started at http://192.168.86.40:8501 .\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<subprocess.Popen at 0x16aa35f40>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from pprint import PrettyPrinter\n",
    "\n",
    "# Imports from langchain to build app:\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.memory import ConversationSummaryBufferMemory\n",
    "import numpy as np\n",
    "\n",
    "# Imports main tools:\n",
    "from trulens_eval import Feedback\n",
    "from trulens_eval import feedback\n",
    "from trulens_eval import FeedbackMode\n",
    "from trulens_eval import Select\n",
    "from trulens_eval import TP\n",
    "from trulens_eval import Tru\n",
    "from trulens_eval.utils.langchain import WithFeedbackFilterDocuments\n",
    "\n",
    "pp = PrettyPrinter()\n",
    "\n",
    "# Tru object manages the database of apps, records, and feedbacks; and the\n",
    "# dashboard to display these.\n",
    "tru = Tru()\n",
    "\n",
    "# Start the dasshboard. If you running from github repo, you will need to adjust\n",
    "# the path the dashboard streamlit app starts in by providing the _dev argument.\n",
    "tru.start_dashboard(\n",
    "    force = True,\n",
    "    _dev=Path().cwd().parent.parent.resolve()\n",
    ")\n",
    "\n",
    "# If needed, you can reset the trulens_eval dashboard database by running the\n",
    "# below line:\n",
    "\n",
    "# tru.reset_database()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… In language_match, input text1 will be set to *.__record__.main_input or `Select.RecordInput` .\n",
      "âœ… In language_match, input text2 will be set to *.__record__.main_output or `Select.RecordOutput` .\n",
      "âœ… In relevance, input prompt will be set to *.__record__.main_input or `Select.RecordInput` .\n",
      "âœ… In relevance, input response will be set to *.__record__.main_output or `Select.RecordOutput` .\n",
      "âœ… In qs_relevance, input question will be set to *.__record__.main_input or `Select.RecordInput` .\n",
      "âœ… In qs_relevance, input statement will be set to *.__record__.app.combine_docs_chain._call.args.inputs.input_documents[:].page_content .\n"
     ]
    }
   ],
   "source": [
    "# Select vector db provider. Pinecone requires setting up a pinecone database\n",
    "# first while the hnsw database is included with trulens_eval.\n",
    "# db_host = \"pinecone\"\n",
    "db_host = \"hnsw\"\n",
    "\n",
    "model_name = \"gpt-3.5-turbo\"\n",
    "app_id = \"TruBot\"\n",
    "\n",
    "# Embedding for vector db.\n",
    "embedding = OpenAIEmbeddings(model='text-embedding-ada-002')  # 1536 dims\n",
    "\n",
    "if db_host == \"pinecone\":\n",
    "    check_keys(\n",
    "        \"PINECONE_API_KEY\",\n",
    "        \"PINECONE_ENV\"\n",
    "    )\n",
    "\n",
    "    # Pinecone configuration if using pinecone.\n",
    "\n",
    "    from langchain.vectorstores import Pinecone\n",
    "    import pinecone\n",
    "\n",
    "    pinecone.init(\n",
    "        api_key=os.environ.get(\"PINECONE_API_KEY\"),  # find at app.pinecone.io\n",
    "        environment=os.environ.get(\"PINECONE_ENV\")  # next to api key in console\n",
    "    )\n",
    "\n",
    "    # If using pinecone, make sure you create your index under name 'llmdemo' or\n",
    "    # change the below.\n",
    "\n",
    "    def get_doc_search():\n",
    "\n",
    "        docsearch = Pinecone.from_existing_index(\n",
    "            index_name=\"llmdemo\", embedding=embedding\n",
    "        )\n",
    "\n",
    "        return docsearch\n",
    "\n",
    "elif db_host == \"hnsw\":\n",
    "    # Local pinecone alternative. Requires precomputed 'hnswlib_truera' folder.\n",
    "\n",
    "    from langchain.vectorstores import DocArrayHnswSearch\n",
    "\n",
    "    def get_doc_search():\n",
    "        # We need to create this object in the thread in which it is used so we\n",
    "        # wrap it in this function for later usage.\n",
    "\n",
    "        docsearch = DocArrayHnswSearch.from_params(\n",
    "            embedding=embedding,\n",
    "            work_dir='hnswlib_trubot',\n",
    "            n_dim=1536,\n",
    "            max_elements=1024\n",
    "        )\n",
    "\n",
    "        return docsearch\n",
    "else:\n",
    "    raise RuntimeError(\"Unhandled db_host, select either 'pinecone' or 'hnsw'.\")\n",
    "\n",
    "# LLM for completing prompts, and other tasks.\n",
    "llm = OpenAI(temperature=0, max_tokens=256)\n",
    "\n",
    "# Construct feedback functions.\n",
    "\n",
    "# API endpoints for models used in feedback functions:\n",
    "hugs = feedback.Huggingface()\n",
    "openai = feedback.OpenAI()\n",
    "\n",
    "# Language match between question/answer.\n",
    "f_lang_match = Feedback(hugs.language_match).on_input_output()\n",
    "# By default this will evaluate feedback on main app input and main app output.\n",
    "\n",
    "# Question/answer relevance between overall question and answer.\n",
    "f_qa_relevance = Feedback(openai.relevance).on_input_output()\n",
    "# By default this will evaluate feedback on main app input and main app output.\n",
    "\n",
    "# Question/statement relevance between question and each context chunk.\n",
    "f_qs_relevance = feedback.Feedback(openai.qs_relevance).on_input().on(\n",
    "    Select.Record.app.combine_docs_chain._call.args.inputs.input_documents[:].page_content\n",
    ").aggregate(np.min)\n",
    "# First feedback argument is set to main app input, and the second is taken from\n",
    "# the context sources as passed to an internal `combine_docs_chain._call`.\n",
    "\n",
    "all_feedbacks = [f_lang_match, f_qa_relevance, f_qs_relevance]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TruBot Version 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def v1_new_conversation(feedback_mode=FeedbackMode.WITH_APP):\n",
    "    \"\"\"\n",
    "    Create a langchain app for a new conversation with a question-answering bot.\n",
    "\n",
    "    Feedback_mode controls when feedback is evaluated:\n",
    "\n",
    "    - FeedbackMode.WITH_APP -- app will wait until feedback is evaluated before\n",
    "      returning from calls.\n",
    "\n",
    "    - FeedbackMode.WITH_APP_THREAD -- app will return from calls and evaluate\n",
    "      feedback in a new thread.\n",
    "\n",
    "    - FeedbackMode.DEFERRED -- app will return and a separate runner thread (see\n",
    "      usage later in this notebook) will evaluate feedback.\n",
    "    \"\"\"\n",
    "\n",
    "    # Blank conversation memory.\n",
    "    memory = ConversationSummaryBufferMemory(\n",
    "        max_token_limit=650,\n",
    "        llm=llm,\n",
    "        memory_key=\"chat_history\",\n",
    "        output_key='answer'\n",
    "    )\n",
    "\n",
    "    docsearch = get_doc_search()\n",
    "\n",
    "    # Context retriever.\n",
    "    retriever = docsearch.as_retriever()\n",
    "\n",
    "    # Conversational app puts it all together.\n",
    "    app = ConversationalRetrievalChain.from_llm(\n",
    "        llm=llm,\n",
    "        retriever=retriever,\n",
    "        return_source_documents=True,\n",
    "        memory=memory,\n",
    "        get_chat_history=lambda a: a,\n",
    "        max_tokens_limit=4096\n",
    "    )\n",
    "\n",
    "    # Trulens instrumentation.\n",
    "    tc = Tru().Chain(\n",
    "        app_id=f\"{app_id}/v1\",\n",
    "        chain=app,\n",
    "        feedbacks=all_feedbacks,\n",
    "        feedback_mode=feedback_mode, \n",
    "    )\n",
    "\n",
    "    return app, tc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating new recording context\n",
      "hello <function Chain.__call__ at 0x134701430>\n",
      "hello <function BaseConversationalRetrievalChain._call at 0x135270ee0>\n",
      "hello <function VectorStoreRetriever._get_relevant_documents at 0x127f59280>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsure what the main input string is for the call to _get_relevant_documents with args ['Who is Shayak?', <langchain.callbacks.manager.CallbackManagerForRetrieverRun object at 0x6b1f3a820>].\n",
      "Unsure what the main output string is for the call to _get_relevant_documents.\n",
      "Error calling wrapped function _call.\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/piotrm/Dropbox/repos/github/trulens/trulens_eval/trulens_eval/instruments.py\", line 789, in wrapper\n",
      "    rets, cost = Endpoint.track_all_costs_tally(\n",
      "  File \"/Users/piotrm/Dropbox/repos/github/trulens/trulens_eval/trulens_eval/feedback/provider/endpoint/base.py\", line 331, in track_all_costs_tally\n",
      "    result, cbs = Endpoint.track_all_costs(\n",
      "  File \"/Users/piotrm/Dropbox/repos/github/trulens/trulens_eval/trulens_eval/feedback/provider/endpoint/base.py\", line 274, in track_all_costs\n",
      "    return Endpoint._track_costs(thunk, with_endpoints=endpoints)\n",
      "  File \"/Users/piotrm/Dropbox/repos/github/trulens/trulens_eval/trulens_eval/feedback/provider/endpoint/base.py\", line 426, in _track_costs\n",
      "    result: T = thunk()\n",
      "  File \"/Users/piotrm/Dropbox/repos/github/trulens/trulens_eval/trulens_eval/instruments.py\", line 790, in <lambda>\n",
      "    lambda: func(*bindings.args, **bindings.kwargs)\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/py38_trulens/lib/python3.8/site-packages/langchain/chains/conversational_retrieval/base.py\", line 135, in _call\n",
      "    docs = self._get_docs(new_question, inputs, run_manager=_run_manager)\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/py38_trulens/lib/python3.8/site-packages/langchain/chains/conversational_retrieval/base.py\", line 287, in _get_docs\n",
      "    docs = self.retriever.get_relevant_documents(\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/py38_trulens/lib/python3.8/site-packages/langchain/schema/retriever.py\", line 205, in get_relevant_documents\n",
      "    raise e\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/py38_trulens/lib/python3.8/site-packages/langchain/schema/retriever.py\", line 198, in get_relevant_documents\n",
      "    result = self._get_relevant_documents(\n",
      "  File \"/Users/piotrm/Dropbox/repos/github/trulens/trulens_eval/trulens_eval/instruments.py\", line 837, in wrapper\n",
      "    self.callbacks._on_add_record(\n",
      "  File \"/Users/piotrm/Dropbox/repos/github/trulens/trulens_eval/trulens_eval/app.py\", line 616, in _on_add_record\n",
      "    records = self.records.get()\n",
      "LookupError: <ContextVar name='records' at 0x296306e00>\n",
      "\n",
      "Error calling wrapped function __call__.\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/piotrm/Dropbox/repos/github/trulens/trulens_eval/trulens_eval/instruments.py\", line 789, in wrapper\n",
      "    rets, cost = Endpoint.track_all_costs_tally(\n",
      "  File \"/Users/piotrm/Dropbox/repos/github/trulens/trulens_eval/trulens_eval/feedback/provider/endpoint/base.py\", line 331, in track_all_costs_tally\n",
      "    result, cbs = Endpoint.track_all_costs(\n",
      "  File \"/Users/piotrm/Dropbox/repos/github/trulens/trulens_eval/trulens_eval/feedback/provider/endpoint/base.py\", line 274, in track_all_costs\n",
      "    return Endpoint._track_costs(thunk, with_endpoints=endpoints)\n",
      "  File \"/Users/piotrm/Dropbox/repos/github/trulens/trulens_eval/trulens_eval/feedback/provider/endpoint/base.py\", line 426, in _track_costs\n",
      "    result: T = thunk()\n",
      "  File \"/Users/piotrm/Dropbox/repos/github/trulens/trulens_eval/trulens_eval/instruments.py\", line 790, in <lambda>\n",
      "    lambda: func(*bindings.args, **bindings.kwargs)\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/py38_trulens/lib/python3.8/site-packages/langchain/chains/base.py\", line 282, in __call__\n",
      "    raise e\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/py38_trulens/lib/python3.8/site-packages/langchain/chains/base.py\", line 276, in __call__\n",
      "    self._call(inputs, run_manager=run_manager)\n",
      "  File \"/Users/piotrm/Dropbox/repos/github/trulens/trulens_eval/trulens_eval/instruments.py\", line 845, in wrapper\n",
      "    cost=cost\n",
      "UnboundLocalError: local variable 'cost' referenced before assignment\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_on_add_record, bindings=<BoundArguments (self=VectorStoreRetriever(tags=['DocArrayHnswSearch'], metadata=None, vectorstore=<langchain.vectorstores.docarray.hnsw.DocArrayHnswSearch object at 0x29f99e280>, search_type='similarity', search_kwargs={}), query='Who is Shayak?', run_manager=<langchain.callbacks.manager.CallbackManagerForRetrieverRun object at 0x6b1f3a820>)>\n",
      "adding a record\n",
      "finishing context with 0 records.\n"
     ]
    },
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'cost' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/piotrm/Dropbox/repos/github/trulens/trulens_eval/examples/trubot/trubot_example.ipynb Cell 10\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/piotrm/Dropbox/repos/github/trulens/trulens_eval/examples/trubot/trubot_example.ipynb#X12sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m app1, tc1 \u001b[39m=\u001b[39m v1_new_conversation()\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/piotrm/Dropbox/repos/github/trulens/trulens_eval/examples/trubot/trubot_example.ipynb#X12sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39m# Call the app:\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/piotrm/Dropbox/repos/github/trulens/trulens_eval/examples/trubot/trubot_example.ipynb#X12sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m res, record \u001b[39m=\u001b[39m tc1\u001b[39m.\u001b[39;49mcall_with_record(\u001b[39m\"\u001b[39;49m\u001b[39mWho is Shayak?\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/piotrm/Dropbox/repos/github/trulens/trulens_eval/examples/trubot/trubot_example.ipynb#X12sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m res\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/piotrm/Dropbox/repos/github/trulens/trulens_eval/examples/trubot/trubot_example.ipynb#X12sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39m# Notice the `source_documents` returned include chunks about Shameek and the\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/piotrm/Dropbox/repos/github/trulens/trulens_eval/examples/trubot/trubot_example.ipynb#X12sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39m# answer includes bits about Shameek as a result.\u001b[39;00m\n",
      "File \u001b[0;32m~/Dropbox/repos/github/trulens/trulens_eval/trulens_eval/tru_chain.py:189\u001b[0m, in \u001b[0;36mTruChain.call_with_record\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    185\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcall_with_record\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[Any, Record]:\n\u001b[1;32m    186\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    187\u001b[0m \u001b[39m    Run the chain call method and also return a record metadata object.\u001b[39;00m\n\u001b[1;32m    188\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 189\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mwith_record(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mapp\u001b[39m.\u001b[39;49m\u001b[39m__call__\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/Dropbox/repos/github/trulens/trulens_eval/trulens_eval/app.py:744\u001b[0m, in \u001b[0;36mApp.with_record\u001b[0;34m(self, func, *args, **kwargs)\u001b[0m\n\u001b[1;32m    738\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    739\u001b[0m \u001b[39mCall the given instrumented function `func` with the given `args`,\u001b[39;00m\n\u001b[1;32m    740\u001b[0m \u001b[39m`kwargs`, producing its results as well as a record.\u001b[39;00m\n\u001b[1;32m    741\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    743\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m \u001b[39mas\u001b[39;00m records:\n\u001b[0;32m--> 744\u001b[0m     ret \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    746\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mlen\u001b[39m(records) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m, (\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mDid not create any records. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    747\u001b[0m                           \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mThis means that no instrumented methods were invoked in the process of calling \u001b[39m\u001b[39m{\u001b[39;00mfunc\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    750\u001b[0m \u001b[39mreturn\u001b[39;00m ret, records[\u001b[39m0\u001b[39m]\n",
      "File \u001b[0;32m~/Dropbox/repos/github/trulens/trulens_eval/trulens_eval/app.py:559\u001b[0m, in \u001b[0;36mApp.__exit__\u001b[0;34m(self, exc_type, exc_value, exc_tb)\u001b[0m\n\u001b[1;32m    554\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrecords\u001b[39m.\u001b[39mreset(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrecords_token)\n\u001b[1;32m    558\u001b[0m \u001b[39mif\u001b[39;00m exc_type \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 559\u001b[0m     \u001b[39mraise\u001b[39;00m exc_value\n\u001b[1;32m    561\u001b[0m \u001b[39mreturn\u001b[39;00m\n",
      "File \u001b[0;32m~/Dropbox/repos/github/trulens/trulens_eval/trulens_eval/app.py:744\u001b[0m, in \u001b[0;36mApp.with_record\u001b[0;34m(self, func, *args, **kwargs)\u001b[0m\n\u001b[1;32m    738\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    739\u001b[0m \u001b[39mCall the given instrumented function `func` with the given `args`,\u001b[39;00m\n\u001b[1;32m    740\u001b[0m \u001b[39m`kwargs`, producing its results as well as a record.\u001b[39;00m\n\u001b[1;32m    741\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    743\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m \u001b[39mas\u001b[39;00m records:\n\u001b[0;32m--> 744\u001b[0m     ret \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    746\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mlen\u001b[39m(records) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m, (\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mDid not create any records. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    747\u001b[0m                           \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mThis means that no instrumented methods were invoked in the process of calling \u001b[39m\u001b[39m{\u001b[39;00mfunc\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    750\u001b[0m \u001b[39mreturn\u001b[39;00m ret, records[\u001b[39m0\u001b[39m]\n",
      "File \u001b[0;32m~/Dropbox/repos/github/trulens/trulens_eval/trulens_eval/instruments.py:845\u001b[0m, in \u001b[0;36mInstrument.tracked_method_wrapper.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    832\u001b[0m     calls\u001b[39m.\u001b[39mappend(call)\n\u001b[1;32m    834\u001b[0m     \u001b[39mif\u001b[39;00m is_root_call:\n\u001b[1;32m    835\u001b[0m         \u001b[39m# If this is a root call, notify app to add the completed record\u001b[39;00m\n\u001b[1;32m    836\u001b[0m         \u001b[39m# into its containers:\u001b[39;00m\n\u001b[1;32m    837\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallbacks\u001b[39m.\u001b[39m_on_add_record(\n\u001b[1;32m    838\u001b[0m             calls\u001b[39m=\u001b[39mcalls,\n\u001b[1;32m    839\u001b[0m             func\u001b[39m=\u001b[39mfunc,\n\u001b[1;32m    840\u001b[0m             sig\u001b[39m=\u001b[39msig,\n\u001b[1;32m    841\u001b[0m             bindings\u001b[39m=\u001b[39mbindings,\n\u001b[1;32m    842\u001b[0m             ret\u001b[39m=\u001b[39mrets,\n\u001b[1;32m    843\u001b[0m             error\u001b[39m=\u001b[39merror,\n\u001b[1;32m    844\u001b[0m             perf\u001b[39m=\u001b[39mPerf(start_time\u001b[39m=\u001b[39mstart_time, end_time\u001b[39m=\u001b[39mend_time),\n\u001b[0;32m--> 845\u001b[0m             cost\u001b[39m=\u001b[39mcost\n\u001b[1;32m    846\u001b[0m         )\n\u001b[1;32m    848\u001b[0m \u001b[39mif\u001b[39;00m error \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    849\u001b[0m     \u001b[39mraise\u001b[39;00m error\n",
      "\u001b[0;31mUnboundLocalError\u001b[0m: local variable 'cost' referenced before assignment"
     ]
    }
   ],
   "source": [
    "# Instantiate the app with fresh memory:\n",
    "\n",
    "app1, tc1 = v1_new_conversation()\n",
    "\n",
    "# Call the app:\n",
    "\n",
    "res, record = tc1.call_with_record(\"Who is Shayak?\")\n",
    "res\n",
    "\n",
    "# Notice the `source_documents` returned include chunks about Shameek and the\n",
    "# answer includes bits about Shameek as a result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'record_id': 'record_hash_51c35ce87e4c0bcb312ca7ffd1f269d8',\n",
       " 'app_id': 'TruBot/v1',\n",
       " 'cost': {'n_requests': 1,\n",
       "  'n_successful_requests': 1,\n",
       "  'n_classes': 0,\n",
       "  'n_tokens': 5,\n",
       "  'n_stream_chunks': 0,\n",
       "  'n_prompt_tokens': 5,\n",
       "  'n_completion_tokens': 0,\n",
       "  'cost': 0.0},\n",
       " 'perf': {'start_time': datetime.datetime(2023, 8, 22, 16, 12, 36, 526082),\n",
       "  'end_time': datetime.datetime(2023, 8, 22, 16, 12, 36, 951764)},\n",
       " 'ts': datetime.datetime(2023, 8, 22, 16, 12, 36, 952883),\n",
       " 'tags': '-',\n",
       " 'main_input': 'Who is Shayak?',\n",
       " 'main_output': {'page_content': 'When Shayak started building production grade machine learning models for algorithmic trading 10 years ago, he realized the need for putting the â€˜scienceâ€™ back in â€˜data scienceâ€™. Since then, he has been building systems and leading research to make machine learning and big data systems more explainable, privacy compliant, and fair. Shayakâ€™s research at Carnegie Mellon University introduced a number of pioneering breakthroughs to the field of explainable AI. Shayak obtained his PhD in Computer Science from Carnegie Mellon University and BTech in Computer Science from the Indian Institute of Technology, Delhi.',\n",
       "  'metadata': {'source': 'https://truera.com/ai-quality-leader/'}},\n",
       " 'main_error': None,\n",
       " 'calls': [{'stack': ({'path': {'path': ({'item_or_attribute': 'app'},\n",
       "       {'item_or_attribute': 'retriever'})},\n",
       "     'method': {'obj': {'cls': {'name': 'VectorStoreRetriever',\n",
       "        'module': {'package_name': 'langchain.vectorstores',\n",
       "         'module_name': 'langchain.vectorstores.base'},\n",
       "        'bases': None},\n",
       "       'id': 11115085888},\n",
       "      'name': '_get_relevant_documents'}},),\n",
       "   'args': {'query': 'Who is Shayak?',\n",
       "    'run_manager': {'__tru_non_serialized_object': {'cls': {'name': 'CallbackManagerForRetrieverRun',\n",
       "       'module': {'package_name': 'langchain.callbacks',\n",
       "        'module_name': 'langchain.callbacks.manager'},\n",
       "       'bases': None},\n",
       "      'id': 11165596016}}},\n",
       "   'rets': [{'page_content': 'When Shayak started building production grade machine learning models for algorithmic trading 10 years ago, he realized the need for putting the â€˜scienceâ€™ back in â€˜data scienceâ€™. Since then, he has been building systems and leading research to make machine learning and big data systems more explainable, privacy compliant, and fair. Shayakâ€™s research at Carnegie Mellon University introduced a number of pioneering breakthroughs to the field of explainable AI. Shayak obtained his PhD in Computer Science from Carnegie Mellon University and BTech in Computer Science from the Indian Institute of Technology, Delhi.',\n",
       "     'metadata': {'source': 'https://truera.com/ai-quality-leader/'}},\n",
       "    {'page_content': 'When Shayak started building production grade machine learning models for algorithmic trading 10 years ago, he realized the need for putting the â€˜scienceâ€™ back in â€˜data scienceâ€™. Since then, he has been building systems and leading research to make machine learning and big data systems more explainable, privacy compliant, and fair. Shayakâ€™s research at Carnegie Mellon University introduced a number of pioneering breakthroughs to the field of explainable AI. Shayak obtained his PhD in Computer Science from Carnegie Mellon University and BTech in Computer Science from the Indian Institute of Technology, Delhi.',\n",
       "     'metadata': {'source': 'https://truera.com/ai-quality-research/'}},\n",
       "    {'page_content': 'Anupam is passionate about enabling responsible adoption of artificial intelligence. As a Professor of Electrical & Computer Engineering and Computer Science at Carnegie Mellon University for over a decade, he has led groundbreaking research in the areas of AI explainability and governance as well as privacy and data protection. Anupam obtained PhD and MS degrees from Stanford University and a BTech from the Indian Institute of Technology, Kharagpur, all in Computer Science.\\n\\nLinkedin\\n\\nShayak Sen',\n",
       "     'metadata': {'source': 'https://truera.com/ai-quality-leader/'}},\n",
       "    {'page_content': 'Most recently, Shameek was Group Chief Data Officer at Standard Chartered Bank, where he helped the bank explore and adopt AI in multiple areas (e.g., credit, financial crime compliance, customer analytics, surveillance), and shaped the bankâ€™s internal approach to responsible AI',\n",
       "     'metadata': {'source': 'https://marketing.truera.com/webinar-eu-ai-law'}}],\n",
       "   'error': None,\n",
       "   'perf': {'start_time': datetime.datetime(2023, 8, 22, 16, 12, 36, 526082),\n",
       "    'end_time': datetime.datetime(2023, 8, 22, 16, 12, 36, 951764)},\n",
       "   'pid': 24804,\n",
       "   'tid': 962150}]}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "record.dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating new recording context\n",
      "hello <function Chain.__call__ at 0x134701430>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A new object of type <class 'langchain.chains.conversational_retrieval.base.ConversationalRetrievalChain'> at 0x29fa3b300 is calling an instrumented method <function Chain.__call__ at 0x134701430>. The path of this call may be incorrect.\n",
      "Guessing path of new object is *.app based on other object (0x295ca4340) using this function.\n",
      "A new object of type <class 'langchain.chains.conversational_retrieval.base.ConversationalRetrievalChain'> at 0x29fa3b300 is calling an instrumented method <function BaseConversationalRetrievalChain._call at 0x135270ee0>. The path of this call may be incorrect.\n",
      "Guessing path of new object is *.app based on other object (0x295ca4340) using this function.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello <function BaseConversationalRetrievalChain._call at 0x135270ee0>\n",
      "hello <function VectorStoreRetriever._get_relevant_documents at 0x127f59280>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsure what the main input string is for the call to _get_relevant_documents with args ['Who is Shayak?', <langchain.callbacks.manager.CallbackManagerForRetrieverRun object at 0x29fa35d00>].\n",
      "Unsure what the main output string is for the call to _get_relevant_documents.\n",
      "Error calling wrapped function _call.\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/piotrm/Dropbox/repos/github/trulens/trulens_eval/trulens_eval/instruments.py\", line 789, in wrapper\n",
      "    rets, cost = Endpoint.track_all_costs_tally(\n",
      "  File \"/Users/piotrm/Dropbox/repos/github/trulens/trulens_eval/trulens_eval/feedback/provider/endpoint/base.py\", line 331, in track_all_costs_tally\n",
      "    result, cbs = Endpoint.track_all_costs(\n",
      "  File \"/Users/piotrm/Dropbox/repos/github/trulens/trulens_eval/trulens_eval/feedback/provider/endpoint/base.py\", line 274, in track_all_costs\n",
      "    return Endpoint._track_costs(thunk, with_endpoints=endpoints)\n",
      "  File \"/Users/piotrm/Dropbox/repos/github/trulens/trulens_eval/trulens_eval/feedback/provider/endpoint/base.py\", line 426, in _track_costs\n",
      "    result: T = thunk()\n",
      "  File \"/Users/piotrm/Dropbox/repos/github/trulens/trulens_eval/trulens_eval/instruments.py\", line 790, in <lambda>\n",
      "    lambda: func(*bindings.args, **bindings.kwargs)\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/py38_trulens/lib/python3.8/site-packages/langchain/chains/conversational_retrieval/base.py\", line 135, in _call\n",
      "    docs = self._get_docs(new_question, inputs, run_manager=_run_manager)\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/py38_trulens/lib/python3.8/site-packages/langchain/chains/conversational_retrieval/base.py\", line 287, in _get_docs\n",
      "    docs = self.retriever.get_relevant_documents(\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/py38_trulens/lib/python3.8/site-packages/langchain/schema/retriever.py\", line 205, in get_relevant_documents\n",
      "    raise e\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/py38_trulens/lib/python3.8/site-packages/langchain/schema/retriever.py\", line 198, in get_relevant_documents\n",
      "    result = self._get_relevant_documents(\n",
      "  File \"/Users/piotrm/Dropbox/repos/github/trulens/trulens_eval/trulens_eval/instruments.py\", line 837, in wrapper\n",
      "    self.callbacks._on_add_record(\n",
      "  File \"/Users/piotrm/Dropbox/repos/github/trulens/trulens_eval/trulens_eval/app.py\", line 616, in _on_add_record\n",
      "    records = self.records.get()\n",
      "LookupError: <ContextVar name='records' at 0x296306e00>\n",
      "\n",
      "Error calling wrapped function __call__.\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/piotrm/Dropbox/repos/github/trulens/trulens_eval/trulens_eval/instruments.py\", line 789, in wrapper\n",
      "    rets, cost = Endpoint.track_all_costs_tally(\n",
      "  File \"/Users/piotrm/Dropbox/repos/github/trulens/trulens_eval/trulens_eval/feedback/provider/endpoint/base.py\", line 331, in track_all_costs_tally\n",
      "    result, cbs = Endpoint.track_all_costs(\n",
      "  File \"/Users/piotrm/Dropbox/repos/github/trulens/trulens_eval/trulens_eval/feedback/provider/endpoint/base.py\", line 274, in track_all_costs\n",
      "    return Endpoint._track_costs(thunk, with_endpoints=endpoints)\n",
      "  File \"/Users/piotrm/Dropbox/repos/github/trulens/trulens_eval/trulens_eval/feedback/provider/endpoint/base.py\", line 426, in _track_costs\n",
      "    result: T = thunk()\n",
      "  File \"/Users/piotrm/Dropbox/repos/github/trulens/trulens_eval/trulens_eval/instruments.py\", line 790, in <lambda>\n",
      "    lambda: func(*bindings.args, **bindings.kwargs)\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/py38_trulens/lib/python3.8/site-packages/langchain/chains/base.py\", line 282, in __call__\n",
      "    raise e\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/py38_trulens/lib/python3.8/site-packages/langchain/chains/base.py\", line 276, in __call__\n",
      "    self._call(inputs, run_manager=run_manager)\n",
      "  File \"/Users/piotrm/Dropbox/repos/github/trulens/trulens_eval/trulens_eval/instruments.py\", line 845, in wrapper\n",
      "    cost=cost\n",
      "UnboundLocalError: local variable 'cost' referenced before assignment\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_on_add_record, bindings=<BoundArguments (self=VectorStoreRetriever(tags=['DocArrayHnswSearch'], metadata=None, vectorstore=<langchain.vectorstores.docarray.hnsw.DocArrayHnswSearch object at 0x10762afa0>, search_type='similarity', search_kwargs={}), query='Who is Shayak?', run_manager=<langchain.callbacks.manager.CallbackManagerForRetrieverRun object at 0x29fa35d00>)>\n",
      "adding a record\n",
      "finishing context with 0 records.\n"
     ]
    },
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'cost' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/piotrm/Dropbox/repos/github/trulens/trulens_eval/examples/trubot/trubot_example.ipynb Cell 12\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/piotrm/Dropbox/repos/github/trulens/trulens_eval/examples/trubot/trubot_example.ipynb#X13sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m app, tc \u001b[39m=\u001b[39m v1_new_conversation()\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/piotrm/Dropbox/repos/github/trulens/trulens_eval/examples/trubot/trubot_example.ipynb#X13sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m res_with, record_with \u001b[39m=\u001b[39m tc\u001b[39m.\u001b[39;49mwith_record(app\u001b[39m.\u001b[39;49m\u001b[39m__call__\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mWho is Shayak?\u001b[39;49m\u001b[39m\"\u001b[39;49m) \u001b[39m# or app.__call__\u001b[39;00m\n",
      "File \u001b[0;32m~/Dropbox/repos/github/trulens/trulens_eval/trulens_eval/app.py:744\u001b[0m, in \u001b[0;36mApp.with_record\u001b[0;34m(self, func, *args, **kwargs)\u001b[0m\n\u001b[1;32m    738\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    739\u001b[0m \u001b[39mCall the given instrumented function `func` with the given `args`,\u001b[39;00m\n\u001b[1;32m    740\u001b[0m \u001b[39m`kwargs`, producing its results as well as a record.\u001b[39;00m\n\u001b[1;32m    741\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    743\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m \u001b[39mas\u001b[39;00m records:\n\u001b[0;32m--> 744\u001b[0m     ret \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    746\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mlen\u001b[39m(records) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m, (\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mDid not create any records. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    747\u001b[0m                           \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mThis means that no instrumented methods were invoked in the process of calling \u001b[39m\u001b[39m{\u001b[39;00mfunc\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    750\u001b[0m \u001b[39mreturn\u001b[39;00m ret, records[\u001b[39m0\u001b[39m]\n",
      "File \u001b[0;32m~/Dropbox/repos/github/trulens/trulens_eval/trulens_eval/app.py:559\u001b[0m, in \u001b[0;36mApp.__exit__\u001b[0;34m(self, exc_type, exc_value, exc_tb)\u001b[0m\n\u001b[1;32m    554\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrecords\u001b[39m.\u001b[39mreset(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrecords_token)\n\u001b[1;32m    558\u001b[0m \u001b[39mif\u001b[39;00m exc_type \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 559\u001b[0m     \u001b[39mraise\u001b[39;00m exc_value\n\u001b[1;32m    561\u001b[0m \u001b[39mreturn\u001b[39;00m\n",
      "File \u001b[0;32m~/Dropbox/repos/github/trulens/trulens_eval/trulens_eval/app.py:744\u001b[0m, in \u001b[0;36mApp.with_record\u001b[0;34m(self, func, *args, **kwargs)\u001b[0m\n\u001b[1;32m    738\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    739\u001b[0m \u001b[39mCall the given instrumented function `func` with the given `args`,\u001b[39;00m\n\u001b[1;32m    740\u001b[0m \u001b[39m`kwargs`, producing its results as well as a record.\u001b[39;00m\n\u001b[1;32m    741\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    743\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m \u001b[39mas\u001b[39;00m records:\n\u001b[0;32m--> 744\u001b[0m     ret \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    746\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mlen\u001b[39m(records) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m, (\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mDid not create any records. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    747\u001b[0m                           \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mThis means that no instrumented methods were invoked in the process of calling \u001b[39m\u001b[39m{\u001b[39;00mfunc\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    750\u001b[0m \u001b[39mreturn\u001b[39;00m ret, records[\u001b[39m0\u001b[39m]\n",
      "File \u001b[0;32m~/Dropbox/repos/github/trulens/trulens_eval/trulens_eval/instruments.py:845\u001b[0m, in \u001b[0;36mInstrument.tracked_method_wrapper.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    832\u001b[0m     calls\u001b[39m.\u001b[39mappend(call)\n\u001b[1;32m    834\u001b[0m     \u001b[39mif\u001b[39;00m is_root_call:\n\u001b[1;32m    835\u001b[0m         \u001b[39m# If this is a root call, notify app to add the completed record\u001b[39;00m\n\u001b[1;32m    836\u001b[0m         \u001b[39m# into its containers:\u001b[39;00m\n\u001b[1;32m    837\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallbacks\u001b[39m.\u001b[39m_on_add_record(\n\u001b[1;32m    838\u001b[0m             calls\u001b[39m=\u001b[39mcalls,\n\u001b[1;32m    839\u001b[0m             func\u001b[39m=\u001b[39mfunc,\n\u001b[1;32m    840\u001b[0m             sig\u001b[39m=\u001b[39msig,\n\u001b[1;32m    841\u001b[0m             bindings\u001b[39m=\u001b[39mbindings,\n\u001b[1;32m    842\u001b[0m             ret\u001b[39m=\u001b[39mrets,\n\u001b[1;32m    843\u001b[0m             error\u001b[39m=\u001b[39merror,\n\u001b[1;32m    844\u001b[0m             perf\u001b[39m=\u001b[39mPerf(start_time\u001b[39m=\u001b[39mstart_time, end_time\u001b[39m=\u001b[39mend_time),\n\u001b[0;32m--> 845\u001b[0m             cost\u001b[39m=\u001b[39mcost\n\u001b[1;32m    846\u001b[0m         )\n\u001b[1;32m    848\u001b[0m \u001b[39mif\u001b[39;00m error \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    849\u001b[0m     \u001b[39mraise\u001b[39;00m error\n",
      "\u001b[0;31mUnboundLocalError\u001b[0m: local variable 'cost' referenced before assignment"
     ]
    }
   ],
   "source": [
    "app, tc = v1_new_conversation()\n",
    "res_with, record_with = tc.with_record(app.__call__, \"Who is Shayak?\") # or app.__call__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "app, tc = v1_new_conversation()\n",
    "with tc as records:\n",
    "    res_context = app(\"Who is Shayak?\")  # should be same as res_with\n",
    "\n",
    "record_context = records[0] # should be same as record_with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The feedback should already be present in the dashboard, but we can check the\n",
    "# qs_relevance here manually as well:\n",
    "feedback = f_qs_relevance.run(record=record, app=tc1)\n",
    "feedback.dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now a question about QII (quantitative input influence is a base technology\n",
    "# employed in TruEra's products) question but in a non-English language:\n",
    "\n",
    "# Start a new conversation as the app keeps prior questions in its memory which\n",
    "# may cause you some testing woes.\n",
    "tc1 = v1_new_conversation()\n",
    "\n",
    "# res, record = tc1.call_with_record(\"Co jest QII?\") # Polish\n",
    "res, record = tc1.call_with_record(\"Was ist QII?\") # German\n",
    "res\n",
    "\n",
    "# Note here the response is in English. This example sometimes matches language\n",
    "# so other variants may need to be tested."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Language match failure can be seen using the f_lang_match (and is visible in\n",
    "# dashboard):\n",
    "feedback = f_lang_match.run(record=record, app=tc1)\n",
    "feedback.dict()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TruBot Version 2 - Language match fix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def v2_new_conversation(feedback_mode=FeedbackMode.WITH_APP):\n",
    "    \"\"\"\n",
    "    Create a langchain app for a new conversation with a question-answering bot.\n",
    "    \"\"\"\n",
    "\n",
    "    # Blank conversation memory.\n",
    "    memory = ConversationSummaryBufferMemory(\n",
    "        max_token_limit=650,\n",
    "        llm=llm,\n",
    "        memory_key=\"chat_history\",\n",
    "        output_key='answer'\n",
    "    )\n",
    "\n",
    "    docsearch = get_doc_search()\n",
    "\n",
    "    # Context retriever.\n",
    "    retriever = docsearch.as_retriever()\n",
    "\n",
    "    # Conversational app puts it all together.\n",
    "    app = ConversationalRetrievalChain.from_llm(\n",
    "        llm=llm,\n",
    "        retriever=retriever,\n",
    "        return_source_documents=True,\n",
    "        memory=memory,\n",
    "        get_chat_history=lambda a: a,\n",
    "        max_tokens_limit=4096\n",
    "    )\n",
    "\n",
    "    ### DIFFERENCES START HERE\n",
    "\n",
    "    # Need to copy these otherwise various apps will feature templates that\n",
    "    # point to the same objects.\n",
    "    app.combine_docs_chain.llm_chain.prompt = \\\n",
    "        app.combine_docs_chain.llm_chain.prompt.copy()\n",
    "    app.combine_docs_chain.document_prompt = \\\n",
    "        app.combine_docs_chain.document_prompt.copy()\n",
    "\n",
    "    # Language mismatch fix via a prompt adjustment:\n",
    "    app.combine_docs_chain.llm_chain.prompt.template = \\\n",
    "        \"Use the following pieces of context to answer the question at the end \" \\\n",
    "        \"in the same language as the question. If you don't know the answer, \" \\\n",
    "        \"just say that you don't know, don't try to make up an answer.\\n\\n\" \\\n",
    "        \"{context}\\n\\n\" \\\n",
    "        \"Question: {question}\\n\" \\\n",
    "        \"Helpful Answer: \"\n",
    "\n",
    "    ### END OF DIFFERENCES\n",
    "\n",
    "    # Trulens instrumentation.\n",
    "    tc = Tru().Chain(\n",
    "        app_id=f\"{app_id}/v2\",\n",
    "        chain=app,\n",
    "        feedbacks=all_feedbacks,\n",
    "        feedback_mode=feedback_mode\n",
    "    )\n",
    "\n",
    "    return tc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the version 2 app:\n",
    "\n",
    "tc2 = v2_new_conversation()\n",
    "\n",
    "# Now the non-English question again:\n",
    "\n",
    "res, record = tc2.call_with_record(\"Was ist QII?\")\n",
    "res\n",
    "\n",
    "# Note that the response is now the appropriate language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# And the language match feedback is happy:\n",
    "\n",
    "feedback = f_lang_match.run(record=record, app=tc2)\n",
    "feedback.dict()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TruBot Version 3: Context Filtering with Relevance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def v3_new_conversation(feedback_mode=FeedbackMode.WITH_APP):\n",
    "    \"\"\"\n",
    "    Create a langchain app for a new conversation with a question-answering bot.\n",
    "    \"\"\"\n",
    "\n",
    "    # Blank conversation memory.\n",
    "    memory = ConversationSummaryBufferMemory(\n",
    "        max_token_limit=650,\n",
    "        llm=llm,\n",
    "        memory_key=\"chat_history\",\n",
    "        output_key='answer'\n",
    "    )\n",
    "\n",
    "    docsearch = get_doc_search()\n",
    "\n",
    "    # Context retriever.\n",
    "    retriever = docsearch.as_retriever()\n",
    "\n",
    "    ### DIFFERENCES START HERE\n",
    "\n",
    "    # Modified retriever that first filters returned contexts using\n",
    "    # f_qs_relevance with a minimum relevance threshold (of 0.5):\n",
    "    retriever_filtered = WithFeedbackFilterDocuments.of_retriever(\n",
    "        retriever=retriever, feedback=f_qs_relevance, threshold = 0.5\n",
    "    )\n",
    "\n",
    "    ### END OF DIFFERENCES\n",
    "\n",
    "    # Conversational app puts it all together.\n",
    "    app = ConversationalRetrievalChain.from_llm(\n",
    "        llm=llm,\n",
    "        retriever=retriever_filtered,\n",
    "        return_source_documents=True,\n",
    "        memory=memory,\n",
    "        get_chat_history=lambda a: a,\n",
    "        max_tokens_limit=4096\n",
    "    )\n",
    "\n",
    "    # Trulens instrumentation.\n",
    "    tc = Tru().Chain(\n",
    "        app_id=f\"{app_id}/v3\",\n",
    "        chain=app,\n",
    "        feedbacks=all_feedbacks,\n",
    "        feedback_mode=feedback_mode\n",
    "    )\n",
    "\n",
    "    return tc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the version 3 app:\n",
    "\n",
    "tc3 = v3_new_conversation()\n",
    "\n",
    "# Call the app:\n",
    "\n",
    "res, record = tc3.call_with_record(\"Who is Shayak?\")\n",
    "res\n",
    "\n",
    "# Notice the `source_documents` returned now does not include the low-relevance\n",
    "# chunks and the answer likewise does not reference them."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TruBot Version 4: Lang match fix and context filter\n",
    "\n",
    "This is left as an exercise to the reader. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def v4_new_conversation(feedback_mode=FeedbackMode.WITH_APP):\n",
    "    \"\"\"\n",
    "    Create a langchain app for a new conversation with a question-answering bot.\n",
    "    \"\"\"\n",
    "\n",
    "    ### TO FILL IN HERE ###\n",
    "    app = ...\n",
    "    ### END OF TO FILL IN ###\n",
    "\n",
    "    # Trulens instrumentation.\n",
    "    tc = Tru().Chain(\n",
    "        app_id=f\"{app_id}/v4\",\n",
    "        chain=app,\n",
    "        feedbacks=all_feedbacks,\n",
    "        feedback_mode=feedback_mode\n",
    "    )\n",
    "\n",
    "    return tc"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test conversations\n",
    "\n",
    "Lets try out the 3 (or 4) trubot versions on a collection of test instances\n",
    "about Shayak and some technical terms in several languages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apps = [\n",
    "    v1_new_conversation,\n",
    "    v2_new_conversation,\n",
    "    v3_new_conversation,\n",
    "    # v4_new_conversation # include this if you completed the exercise\n",
    "]\n",
    "\n",
    "questions = [\n",
    "    \"Who is Shayak?\", \"Wer ist Shayak?\", \"Kim jest Shayak?\", \"Â¿QuiÃ©n es Shayak?\", \n",
    "    \"What is QII?\", \"Was ist QII?\", \"Co jest QII?\", \"Â¿Que es QII?\"\n",
    "]\n",
    "\n",
    "# Comment out the next two lines to try all of the version and question\n",
    "# combinations. Otherwise we select here only 2 questions and 2 models to start with.\n",
    "apps = apps[0:2]\n",
    "questions = questions[0:2]\n",
    "\n",
    "def test_app_on_question(app, question):\n",
    "    print(app.__name__, question)\n",
    "    app = app(feedback_mode=FeedbackMode.DEFERRED)\n",
    "    answer = app.call_with_record(question)\n",
    "    return answer\n",
    "\n",
    "# This asks all of the questions in parallel:\n",
    "for app in apps:\n",
    "    for question in questions:\n",
    "        TP().promise(\n",
    "            test_app_on_question,\n",
    "            app=app,\n",
    "            question=question\n",
    "        )\n",
    "\n",
    "TP().finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For deferred feedback evaluation. Start this:\n",
    "\n",
    "Tru().start_evaluator(restart=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "demo3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
