{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dev Notebook\n",
    "\n",
    "This notebook loads the version of trulens_eval from the enclosing repo folder. You can use this to debug or devlop trulens_eval features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Volumes/dev_new/trulens/trulens_eval\n",
      "âœ… Key OPENAI_API_KEY set from environment (same value found in .env file at /Volumes/dev_new/.env).\n",
      "âœ… Key HUGGINGFACE_API_KEY set from environment (same value found in .env file at /Volumes/dev_new/.env).\n",
      "ðŸ¦‘ Tru initialized with db url sqlite:///default.sqlite .\n",
      "ðŸ›‘ Secret keys may be written to the database. See the `database_redact_keys` option of Tru` to prevent this.\n"
     ]
    }
   ],
   "source": [
    "# pip uninstall -y trulens_eval\n",
    "# pip install git+https://github.com/truera/trulens@piotrm/azure_bugfixes#subdirectory=trulens_eval\n",
    "\n",
    "# trulens_eval notebook dev\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "base = Path().cwd()\n",
    "while not (base / \"trulens_eval\").exists():\n",
    "    base = base.parent\n",
    "\n",
    "print(base)\n",
    "\n",
    "# If running from github repo, can use this:\n",
    "sys.path.append(str(base))\n",
    "\n",
    "# Uncomment for more debugging printouts.\n",
    "\"\"\"\n",
    "import logging\n",
    "root = logging.getLogger()\n",
    "root.setLevel(logging.DEBUG)\n",
    "\n",
    "handler = logging.StreamHandler(sys.stdout)\n",
    "handler.setLevel(logging.DEBUG)\n",
    "formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "handler.setFormatter(formatter)\n",
    "root.addHandler(handler)\n",
    "\"\"\"\n",
    "\n",
    "from trulens_eval.keys import check_keys\n",
    "\n",
    "check_keys(\n",
    "    \"OPENAI_API_KEY\",\n",
    "    \"HUGGINGFACE_API_KEY\"\n",
    ")\n",
    "\n",
    "from trulens_eval import Tru\n",
    "tru = Tru()\n",
    "# tru.reset_database()\n",
    "\n",
    "# tru.run_dashboard(_dev=base, force=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trulens_eval.feedback.provider import LiteLLM\n",
    "from trulens_eval.feedback.provider.endpoint import Endpoint\n",
    "from litellm import completion\n",
    "\n",
    "from trulens_eval.keys import check_or_set_keys\n",
    "\n",
    "import os\n",
    "\n",
    "check_or_set_keys(\n",
    "    \"AZURE_OPENAI_ENDPOINT\",\n",
    "    \"AZURE_OPENAI_API_KEY\",\n",
    "    \"OPENAI_API_TYPE\",\n",
    "    \"OPENAI_API_VERSION\",\n",
    "    \"AZURE_OPENAI_DEPLOYMENT_NAME\"\n",
    ")\n",
    "\n",
    "import litellm\n",
    "from litellm.utils import get_llm_provider, completion_cost\n",
    "\n",
    "prov = get_llm_provider(model=\"azure/truera-gpt-35-turbo\")\n",
    "\n",
    "\"\"\"\n",
    "lm_res = litellm.completion(\n",
    "    model=\"azure/truera-gpt-35-turbo\",\n",
    "    api_base=\"https://truera-dev-azure-openai.openai.azure.com/\",\n",
    "    messages=[{\"role\": \"user\", \"content\": \"hello there\"}]\n",
    ")\n",
    "\"\"\"\n",
    "    #azure_client_params = { \n",
    "    #    \"api_key\": os.environ[\"AZURE_OPENAI_API_KEY\"], \n",
    "    #    \"azure_endpoint\": os.environ[\"AZURE_OPENAI_ENDPOINT\"], \n",
    "    #    \"api_version\": os.environ[\"OPENAI_API_VERSION\"], \n",
    "    #}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_llm_provider(\"azure/truera-gpt-35-turbo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prov = LiteLLM(\n",
    "#    model_engine=\"azure/\" + os.environ[\"AZURE_OPENAI_DEPLOYMENT_NAME\"],\n",
    "#    completion_kwargs=dict(api_base=\"https://truera-dev-azure-openai.openai.azure.com/\")\n",
    "#)\n",
    "from trulens_eval.feedback.provider.bedrock import Bedrock\n",
    "from trulens_eval.feedback.provider import LiteLLM\n",
    "\n",
    "# Have to delete litellm endpoint singleton as it may have been created\n",
    "# with the wrong underlying litellm provider in a prior test.\n",
    "# Endpoint.delete_singleton_by_name(\"litellm\")\n",
    "\n",
    "#provider = LiteLLM(f\"bedrock/{Bedrock.DEFAULT_MODEL_ID}\")\n",
    "provider = Bedrock()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "provider.endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "provider.sentiment(\"hello there\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res, costs = await Endpoint.atrack_all_costs_tally(prov.sentiment, text=\"This rocks!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "costs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "op.chat.completions.create(\"Hello?\", stream=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trulens_eval.feedback.provider import Huggingface\n",
    "from trulens_eval.feedback.provider import OpenAI\n",
    "from trulens_eval.feedback.provider.endpoint import Endpoint\n",
    "from openai import OpenAI\n",
    "op = OpenAI()\n",
    "\n",
    "hugs = Huggingface()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res, cost = Endpoint.track_all_costs_tally(hugs.positive_sentiment, text=\"This rocks!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "import boto3\n",
    "from langchain.docstore.document import Document\n",
    "from langchain_community.retrievers.bedrock import \\\n",
    "    AmazonKnowledgeBasesRetriever\n",
    "from langchain_community.retrievers.bedrock import RetrievalConfig\n",
    "\n",
    "\n",
    "class MyKnowledgeBaseRetriever(AmazonKnowledgeBasesRetriever): \n",
    "    def __init__(self, knowledge_base_id, region_name, retrieval_config):\n",
    "        super().__init__(\n",
    "            knowledge_base_id=knowledge_base_id,\n",
    "            region_name=region_name,\n",
    "            retrieval_config=retrieval_config\n",
    "        )\n",
    "        self.knowledge_base_id = knowledge_base_id\n",
    "        self.region_name = region_name\n",
    "        self.retrieval_config = retrieval_config\n",
    "\n",
    "    def get_relevant_documents(self, query):\n",
    "        # Create a session using your AWS credentials\n",
    "        session = boto3.Session(region_name=self.region_name)\n",
    "\n",
    "        # Create a client for the Bedrock Agent Runtime service\n",
    "        bedrock_client = session.client('bedrock-agent-runtime')\n",
    "\n",
    "        # Retrieve relevant documents from the Knowledge Base\n",
    "        response = bedrock_client.retrieve(\n",
    "            knowledgeBaseId=self.knowledge_base_id,\n",
    "            retrievalQuery={\n",
    "                'text': query\n",
    "            },\n",
    "            retrievalConfiguration=self.retrieval_config\n",
    "        )\n",
    "        \n",
    "        relevant_documents = response['retrievalResults']\n",
    "        docs = []\n",
    "\n",
    "        for result in relevant_documents:\n",
    "            content = result['content']['text']\n",
    "            location = result['location']['s3Location']['uri']\n",
    "            score = result['score']\n",
    "\n",
    "            doc = Document(page_content=content, metadata={'location': location, 'score': score})\n",
    "            docs.append(doc)\n",
    "\n",
    "        # Return the relevant documents\n",
    "        return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms.bedrock import Bedrock\n",
    "from langchain.chains.retrieval_qa.base import RetrievalQA\n",
    "\n",
    "retriever = MyKnowledgeBaseRetriever(\n",
    "    knowledge_base_id=\"MY KNOWLEDGE BASE ID\",\n",
    "    region_name=\"us-east-1\",\n",
    "    retrieval_config={\"vectorSearchConfiguration\": {\"numberOfResults\": 6}}\n",
    ")\n",
    "\n",
    "# Create an instance of the Bedrock LLM\n",
    "llm = Bedrock(\n",
    "    model_id='meta.llama2-70b-chat-v1',\n",
    "    model_kwargs={\"temperature\": 0.1, \"top_p\": 0.9, \"max_gen_len\": 1200}\n",
    ")\n",
    "\n",
    "# Create an instance of the RetrievalQA chain\n",
    "qa_chain = RetrievalQA.from_chain_type(llm=llm, retriever=retriever, return_source_documents=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Garbage collecting testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trulens_eval import TruChain, Tru\n",
    "tru = Tru()\n",
    "\n",
    "from operator import itemgetter\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.schema import StrOutputParser\n",
    "\n",
    "prompt1 = ChatPromptTemplate.from_template(\"what is the city {person} is from?\")\n",
    "prompt2 = ChatPromptTemplate.from_template(\n",
    "    \"what country is the city {city} in? respond in {language}\"\n",
    ")\n",
    "\n",
    "model = ChatOpenAI()\n",
    "\n",
    "chain1 = prompt1 | model | StrOutputParser()\n",
    "\n",
    "chain2 = (\n",
    "    {\"city\": chain1, \"language\": itemgetter(\"language\")}\n",
    "    | prompt2\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "chain2_input = {\"person\": \"obama\", \"language\": \"spanish\"}\n",
    "# chain2.invoke(chain2_input)\n",
    "\n",
    "tru_recorder2 = TruChain(\n",
    "    chain2,\n",
    "    app_id='Chain1'\n",
    ")\n",
    "\n",
    "with tru_recorder2 as recs:\n",
    "    llm_response = chain2.invoke(chain2_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain2 = (\n",
    "    {\"city\": chain1, \"language\": itemgetter(\"language\")}\n",
    "    | prompt2\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "tru_recorder2 = TruChain(\n",
    "    chain2,\n",
    "    app_id='Chain2'\n",
    ")\n",
    "\n",
    "with tru_recorder2 as recs:\n",
    "    llm_response = chain2.invoke(chain2_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain2 = (\n",
    "    {\"city\": chain1, \"language\": itemgetter(\"language\")}\n",
    "    | prompt2\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "tru_recorder2 = TruChain(\n",
    "    chain2,\n",
    "    app_id='Chain3'\n",
    ")\n",
    "\n",
    "with tru_recorder2 as recs:\n",
    "    llm_response = chain2.invoke(chain2_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain2 = (\n",
    "    {\"city\": chain1, \"language\": itemgetter(\"language\")}\n",
    "    | prompt2\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "tru_recorder2 = TruChain(\n",
    "    chain2,\n",
    "    app_id='Chain4'\n",
    ")\n",
    "\n",
    "with tru_recorder2 as recs:\n",
    "    llm_response = chain2.invoke(chain2_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain2 = (\n",
    "    {\"city\": chain1, \"language\": itemgetter(\"language\")}\n",
    "    | prompt2\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "tru_recorder2 = TruChain(\n",
    "    chain2,\n",
    "    app_id='Chain5'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tru_recorder2 as recs:\n",
    "    llm_response = chain2.invoke(chain2_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from concurrent.futures import as_completed\n",
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "from time import sleep\n",
    "\n",
    "import dotenv\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from trulens_eval import Feedback\n",
    "from trulens_eval import Tru\n",
    "from trulens_eval.feedback.provider.endpoint.base import Endpoint\n",
    "from trulens_eval.feedback.provider.hugs import Dummy\n",
    "from trulens_eval.schema import Cost\n",
    "from trulens_eval.schema import FeedbackMode\n",
    "from trulens_eval.schema import Record\n",
    "from trulens_eval.tru_custom_app import TruCustomApp\n",
    "from trulens_eval.utils.threading import TP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Context selection tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import json\n",
    "from langchain_community.retrievers.bedrock import (\n",
    "    AmazonKnowledgeBasesRetriever,\n",
    "    RetrievalConfig,\n",
    ")\n",
    "from langchain.docstore.document import Document\n",
    "\n",
    "from langchain.llms.bedrock import Bedrock\n",
    "from langchain.chains.retrieval_qa.base import RetrievalQA\n",
    "\n",
    "\n",
    "class MyKnowledgeBaseRetriever(AmazonKnowledgeBasesRetriever):\n",
    "\n",
    "    def __init__(self, knowledge_base_id, region_name, retrieval_config):\n",
    "        super().__init__(\n",
    "            knowledge_base_id=knowledge_base_id,\n",
    "            region_name=region_name,\n",
    "            retrieval_config=retrieval_config\n",
    "        )\n",
    "        self.knowledge_base_id = knowledge_base_id\n",
    "        self.region_name = region_name\n",
    "        self.retrieval_config = retrieval_config\n",
    "\n",
    "    def get_relevant_documents(self, query):\n",
    "        # Create a session using your AWS credentials\n",
    "        session = boto3.Session(region_name=self.region_name)\n",
    "\n",
    "        # Create a client for the Bedrock Agent Runtime service\n",
    "        bedrock_client = session.client('bedrock-agent-runtime')\n",
    "\n",
    "        # Retrieve relevant documents from the Knowledge Base\n",
    "        response = bedrock_client.retrieve(\n",
    "            knowledgeBaseId=self.knowledge_base_id,\n",
    "            retrievalQuery={'text': query},\n",
    "            retrievalConfiguration=self.retrieval_config\n",
    "        )\n",
    "\n",
    "        relevant_documents = response['retrievalResults']\n",
    "        docs = []\n",
    "\n",
    "        for result in relevant_documents:\n",
    "            content = result['content']['text']\n",
    "            location = result['location']['s3Location']['uri']\n",
    "            score = result['score']\n",
    "\n",
    "            doc = Document(\n",
    "                page_content=content,\n",
    "                metadata={\n",
    "                    'location': location,\n",
    "                    'score': score\n",
    "                }\n",
    "            )\n",
    "            docs.append(doc)\n",
    "\n",
    "        # Return the relevant documents\n",
    "        return docs\n",
    "\n",
    "\n",
    "# Configure the AWS credentials and region\n",
    "# config = Config(region_name='us-east-1')\n",
    "\n",
    "retriever = MyKnowledgeBaseRetriever(\n",
    "    knowledge_base_id=\"MY KNOWLEDGE BASE ID\",\n",
    "    region_name=\"us-east-1\",\n",
    "    retrieval_config={\"vectorSearchConfiguration\": {\n",
    "        \"numberOfResults\": 6\n",
    "    }}\n",
    ")\n",
    "\n",
    "# Create an instance of the Bedrock LLM\n",
    "llm = Bedrock(\n",
    "    model_id='meta.llama2-70b-chat-v1',\n",
    "    model_kwargs={\n",
    "        \"temperature\": 0.1,\n",
    "        \"top_p\": 0.9,\n",
    "        \"max_gen_len\": 1200\n",
    "    }\n",
    ")\n",
    "\n",
    "# Create an instance of the RetrievalQA chain\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm, retriever=retriever, return_source_documents=True\n",
    ")\n",
    "\n",
    "#qa_chain.invoke(\"What steps should be taken during the demo ride in the buying cycle to improve the likelihood of closing the deal?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trulens_eval.app import App\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "tru = Tru()\n",
    "\n",
    "context = App.select_context(qa_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup Tru and/or dashboard.\n",
    "\n",
    "tru = Tru()\n",
    "\n",
    "#tru.reset_database()\n",
    "\n",
    "tru.start_dashboard(\n",
    "    force = True,\n",
    "    _dev=Path().cwd().parent.parent.resolve()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pydantic testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any\n",
    "\n",
    "from pydantic import BaseModel\n",
    "from pydantic import Field\n",
    "from pydantic import field_validator\n",
    "from pydantic import model_validator\n",
    "from pydantic import PydanticUndefinedAnnotation\n",
    "from pydantic import SerializeAsAny\n",
    "from pydantic import ValidationInfo\n",
    "from pydantic import validator\n",
    "from pydantic_core import PydanticUndefined\n",
    "\n",
    "\n",
    "class CustomLoader(BaseModel):\n",
    "    cls: Any\n",
    "\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        kwargs['cls'] = type(self)\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "    @model_validator(mode='before')\n",
    "    @staticmethod\n",
    "    def my_model_validate(obj, info: ValidationInfo):\n",
    "        if not isinstance(obj, dict):\n",
    "            return obj\n",
    "\n",
    "        cls = obj['cls']\n",
    "        # print(cls, subcls, obj, info)\n",
    "\n",
    "        validated = dict()\n",
    "        for k, finfo in cls.model_fields.items():\n",
    "            print(k, finfo)\n",
    "            typ = finfo.annotation\n",
    "            val = finfo.get_default()\n",
    "\n",
    "            if val is PydanticUndefined:\n",
    "                val = obj[k]\n",
    "\n",
    "            print(typ, type(typ))\n",
    "            if isinstance(typ, type) \\\n",
    "            and issubclass(typ, CustomLoader) \\\n",
    "            and isinstance(val, dict) and \"cls\" in val:\n",
    "                subcls = val['cls']\n",
    "                val = subcls.model_validate(val)\n",
    "    \n",
    "            validated[k] = val\n",
    "            \n",
    "        return validated\n",
    "\n",
    "class SubModel(CustomLoader):\n",
    "    sm: int = 3\n",
    "\n",
    "class Model(CustomLoader):\n",
    "    m: int = 2\n",
    "    sub: SubModel\n",
    "\n",
    "class SubSubModelA(SubModel):\n",
    "    ssma: int = 42\n",
    "\n",
    "class SubModelA(SubModel):\n",
    "    sma: int = 0\n",
    "    subsub: SubSubModelA\n",
    "\n",
    "class SubModelB(SubModel):\n",
    "    smb: int = 1\n",
    "\n",
    "c = Model(sub=SubModelA(subsub=SubSubModelA()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c.model_dump()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Model.model_validate({'cls': Model, 'm': 2, 'sub': {'cls': SubModelA, 'sma':3, 'subsub': {'cls': SubSubModelA, 'ssma': 42}}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Model.model_validate({'c': 2, 'sub': {}})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keys testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show keys.\n",
    "\n",
    "import os\n",
    "for k in os.environ:\n",
    "    if \"KEY\" in k:\n",
    "        print(k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bedrock testing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trulens_eval import Bedrock\n",
    "bedrock = Bedrock(\n",
    "    model_id = \"amazon.titan-tg1-large\",\n",
    "    region_name=\"us-west-2\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Endpoint.print_instrumented()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bedrock_response, cost = Endpoint.track_all_costs_tally(\n",
    "    thunk=lambda: bedrock.endpoint.client.invoke_model_with_response_stream(\n",
    "    body=json.dumps({'inputText': \"Hello there.\"}), modelId=\"amazon.titan-tg1-large\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Huggingface testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trulens_eval import Huggingface\n",
    "Huggingface()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Endpoint.print_instrumented()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AzureOpenAI Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trulens_eval.keys import check_keys\n",
    "check_keys(\n",
    "    \"AZURE_OPENAI_API_KEY\",\n",
    "    \"AZURE_OPENAI_ENDPOINT\",\n",
    "    \"OPENAI_API_VERSION\",\n",
    "    \"OPENAI_API_TYPE\",\n",
    "    \"AZURE_OPENAI_DEPLOYMENT_NAME\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llms import AzureOpenAI as AzureOpenAIChat\n",
    "import os\n",
    "\n",
    "gpt_35_turbo = AzureOpenAIChat(\n",
    "    deployment_name=\"gpt-35-turbo\",\n",
    "    model=\"gpt-35-turbo\",\n",
    "    api_key=os.getenv(\"AZURE_OPENAI_API_KEY\"),\n",
    "    api_version=\"2023-05-15\",\n",
    "    model_version=\"0613\",\n",
    "    temperature=0.0,\n",
    ")\n",
    "c = gpt_35_turbo._get_client()\n",
    "gpt_35_turbo._get_credential_kwargs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c.base_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from trulens_eval import feedback\n",
    "azopenai = feedback.AzureOpenAI(\n",
    "    deployment_name=os.environ['AZURE_OPENAI_DEPLOYMENT_NAME']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "azopenai.endpoint.client.client_kwargs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# azopenai.relevance(prompt=\"Where is Germany?\", response=\"Germany is in Europe.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reval = feedback.AzureOpenAI.model_validate(azopenai.model_dump())\n",
    "# reval.relevance(prompt=\"Where is Germany?\", response=\"Poland is in Europe.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "azureOpenAI = azopenai\n",
    "\n",
    "from trulens_eval.feedback.provider import AzureOpenAI\n",
    "from trulens_eval.feedback import Groundedness, GroundTruthAgreement\n",
    "from trulens_eval import TruLlama, Feedback\n",
    "from trulens_eval.app import App\n",
    "import numpy as np\n",
    "# Initialize provider class\n",
    "#azureOpenAI = AzureOpenAI(deployment_name=\"gpt-35-turbo\")\n",
    "\n",
    "grounded = Groundedness(groundedness_provider=azureOpenAI)\n",
    "# Define a groundedness feedback function\n",
    "f_groundedness = (\n",
    "    Feedback(grounded.groundedness_measure_with_cot_reasons)\n",
    "    .on_input_output()\n",
    "    .aggregate(grounded.grounded_statements_aggregator)\n",
    ")\n",
    "\n",
    "# Question/answer relevance between overall question and answer.\n",
    "f_answer_relevance = Feedback(azureOpenAI.relevance_with_cot_reasons).on_input_output()\n",
    "# Question/statement relevance between question and each context chunk.\n",
    "f_context_relevance = (\n",
    "    Feedback(azureOpenAI.qs_relevance_with_cot_reasons)\n",
    "    .on_input_output()\n",
    "    .aggregate(np.mean)\n",
    ")\n",
    "\n",
    "# GroundTruth for comparing the Answer to the Ground-Truth Answer\n",
    "#ground_truth_collection = GroundTruthAgreement(golden_set, provider=azureOpenAI)\n",
    "#f_answer_correctness = (\n",
    "#    Feedback(ground_truth_collection.agreement_measure)\n",
    "#    .on_input_output()\n",
    "#)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_groundedness.model_dump()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trulens_eval.utils.pyschema import WithClassInfo\n",
    "from trulens_eval.utils.serial import SerialModel\n",
    "from trulens_eval.feedback.groundedness import Groundedness\n",
    "import pydantic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Groundedness.model_validate(grounded.model_dump())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f2 = Feedback.model_validate(f_groundedness.model_dump())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f2.implementation.obj.init_bindings.kwargs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f2.imp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_serial(f):\n",
    "    print(\"Before serialization:\")\n",
    "    print(f.imp(\"Where is Poland?\", \"Poland is in Europe\"))\n",
    "    f_dump = f.model_dump()\n",
    "    f = Feedback.model_validate(f_dump)\n",
    "    print(\"After serialization:\")\n",
    "    print(f.imp(\"Where is Poland?\", \"Germany is in Europe\"))\n",
    "    return f\n",
    "\n",
    "f2 = test_serial(f_groundedness)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_groundedness.imp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f2.imp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_answer_relevance = Feedback(azureOpenAI.relevance_with_cot_reasons).on_input_output()\n",
    "\n",
    "# test without serialization\n",
    "print(f_answer_relevance.imp(prompt=\"Where is Germany?\", response=\"Germany is in Europe.\"))\n",
    "\n",
    "# serialize/deserialize\n",
    "f_answer_relevance2 = Feedback.model_validate(f_answer_relevance.model_dump())\n",
    "\n",
    "# test after deserialization\n",
    "print(f_answer_relevance2.imp(prompt=\"Where is Germany?\", response=\"Poland is in Europe.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fr = feedback.Feedback.model_validate(f.model_dump())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fr.imp(prompt=\"Where is Germany?\", response=\"Germany is in Europe.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dummy endpoint testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tp = TP()\n",
    "\n",
    "d = Dummy(\n",
    "    loading_prob=0.1,\n",
    "    freeze_prob=0.0, # we expect requests to have their own timeouts so freeze should never happen\n",
    "    error_prob=0.01,\n",
    "    overloaded_prob=0.1,\n",
    "    rpm=6000\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Langchain testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports from langchain to build app. You may need to install langchain first\n",
    "# with the following:\n",
    "# ! pip install langchain>=0.0.170\n",
    "from langchain.chains import LLMChain\n",
    "from langchain_community.llms import OpenAI\n",
    "from langchain.prompts import ChatPromptTemplate, PromptTemplate\n",
    "from langchain.prompts import HumanMessagePromptTemplate\n",
    "\n",
    "# Initialize Huggingface-based feedback function collection class:\n",
    "# bedrock = Bedrock(model_engine=\"Bedrock\", model_id = \"anthropic.claude-v2\", region_name=\"us-west-2\")\n",
    "\n",
    "# Define a language match feedback function using HuggingFace.\n",
    "#f_relevance = Feedback(bedrock.relevance).on_input_output()\n",
    "# By default this will check language match on the main app input and main app\n",
    "# output.\n",
    "\n",
    "full_prompt = HumanMessagePromptTemplate(\n",
    "    prompt=PromptTemplate(\n",
    "        template=\n",
    "        \"Provide a helpful response with relevant background information for the following: {prompt}\",\n",
    "        input_variables=[\"prompt\"],\n",
    "    )\n",
    ")\n",
    "\n",
    "chat_prompt_template = ChatPromptTemplate.from_messages([full_prompt])\n",
    "\n",
    "llm = OpenAI(temperature=0.9, max_tokens=128)\n",
    "\n",
    "chain = LLMChain(llm=llm, prompt=chat_prompt_template, verbose=True)\n",
    "\n",
    "tru_recorder1 = tru.Chain(\n",
    "    chain,\n",
    "    app_id='Chain1_ChatApplication',\n",
    "    #feedbacks=[f_relevance]\n",
    ")\n",
    "\n",
    "with tru_recorder1 as recording:\n",
    "    llm_response = chain.run(\"What's the capital of the USA?\")\n",
    "\n",
    "display(llm_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Langchain expressions testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trulens_eval import TruChain, Tru\n",
    "tru = Tru()\n",
    "\n",
    "from operator import itemgetter\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.schema import StrOutputParser\n",
    "\n",
    "prompt1 = ChatPromptTemplate.from_template(\"what is the city {person} is from?\")\n",
    "prompt2 = ChatPromptTemplate.from_template(\n",
    "    \"what country is the city {city} in? respond in {language}\"\n",
    ")\n",
    "\n",
    "model = ChatOpenAI()\n",
    "\n",
    "chain1 = prompt1 | model | StrOutputParser()\n",
    "\n",
    "chain2 = (\n",
    "    {\"city\": chain1, \"language\": itemgetter(\"language\")}\n",
    "    | prompt2\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "chain2_input = {\"person\": \"obama\", \"language\": \"spanish\"}\n",
    "\n",
    "chain2.invoke(chain2_input)\n",
    "\n",
    "tru_recorder2 = TruChain(\n",
    "    chain2,\n",
    "    app_id='Chain2'\n",
    ")\n",
    "\n",
    "with tru_recorder2 as recs:\n",
    "    llm_response = chain2.invoke({\"person\": \"obama\", \"language\": \"spanish\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "record = recs.get()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "record.calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install 'llama_index<0.10'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Llama-Index testing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'global_handler' from 'llama_index.core' (/opt/homebrew/Caskroom/miniconda/base/envs/py311_trulens/lib/python3.11/site-packages/llama_index/core/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 13\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mllama_index\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mreaders\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mweb\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SimpleWebPageReader\n\u001b[1;32m     10\u001b[0m documents \u001b[38;5;241m=\u001b[39m SimpleWebPageReader(\n\u001b[1;32m     11\u001b[0m     html_to_text\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m     12\u001b[0m )\u001b[38;5;241m.\u001b[39mload_data([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttp://paulgraham.com/worked.html\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m---> 13\u001b[0m index \u001b[38;5;241m=\u001b[39m \u001b[43mVectorStoreIndex\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_documents\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdocuments\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m query_engine \u001b[38;5;241m=\u001b[39m index\u001b[38;5;241m.\u001b[39mas_query_engine()\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/py311_trulens/lib/python3.11/site-packages/llama_index/indices/base.py:96\u001b[0m, in \u001b[0;36mBaseIndex.from_documents\u001b[0;34m(cls, documents, storage_context, service_context, show_progress, **kwargs)\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Create index from documents.\u001b[39;00m\n\u001b[1;32m     89\u001b[0m \n\u001b[1;32m     90\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     93\u001b[0m \n\u001b[1;32m     94\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     95\u001b[0m storage_context \u001b[38;5;241m=\u001b[39m storage_context \u001b[38;5;129;01mor\u001b[39;00m StorageContext\u001b[38;5;241m.\u001b[39mfrom_defaults()\n\u001b[0;32m---> 96\u001b[0m service_context \u001b[38;5;241m=\u001b[39m service_context \u001b[38;5;129;01mor\u001b[39;00m \u001b[43mServiceContext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_defaults\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     97\u001b[0m docstore \u001b[38;5;241m=\u001b[39m storage_context\u001b[38;5;241m.\u001b[39mdocstore\n\u001b[1;32m     99\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m service_context\u001b[38;5;241m.\u001b[39mcallback_manager\u001b[38;5;241m.\u001b[39mas_trace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mindex_construction\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/py311_trulens/lib/python3.11/site-packages/llama_index/service_context.py:178\u001b[0m, in \u001b[0;36mServiceContext.from_defaults\u001b[0;34m(cls, llm_predictor, llm, prompt_helper, embed_model, node_parser, text_splitter, transformations, llama_logger, callback_manager, system_prompt, query_wrapper_prompt, pydantic_program_mode, chunk_size, chunk_overlap, context_window, num_output, chunk_size_limit)\u001b[0m\n\u001b[1;32m    176\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m llm_predictor \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    177\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLLMPredictor is deprecated, please use LLM instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 178\u001b[0m llm_predictor \u001b[38;5;241m=\u001b[39m llm_predictor \u001b[38;5;129;01mor\u001b[39;00m \u001b[43mLLMPredictor\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    179\u001b[0m \u001b[43m    \u001b[49m\u001b[43mllm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mllm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpydantic_program_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpydantic_program_mode\u001b[49m\n\u001b[1;32m    180\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    181\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(llm_predictor, LLMPredictor):\n\u001b[1;32m    182\u001b[0m     llm_predictor\u001b[38;5;241m.\u001b[39mllm\u001b[38;5;241m.\u001b[39mcallback_manager \u001b[38;5;241m=\u001b[39m callback_manager\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/py311_trulens/lib/python3.11/site-packages/llama_index/llm_predictor/base.py:109\u001b[0m, in \u001b[0;36mLLMPredictor.__init__\u001b[0;34m(self, llm, callback_manager, system_prompt, query_wrapper_prompt, pydantic_program_mode)\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n\u001b[1;32m    101\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    102\u001b[0m     llm: Optional[LLMType] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdefault\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    106\u001b[0m     pydantic_program_mode: PydanticProgramMode \u001b[38;5;241m=\u001b[39m PydanticProgramMode\u001b[38;5;241m.\u001b[39mDEFAULT,\n\u001b[1;32m    107\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    108\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Initialize params.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 109\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_llm \u001b[38;5;241m=\u001b[39m \u001b[43mresolve_llm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mllm\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    111\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m callback_manager:\n\u001b[1;32m    112\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_llm\u001b[38;5;241m.\u001b[39mcallback_manager \u001b[38;5;241m=\u001b[39m callback_manager\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/py311_trulens/lib/python3.11/site-packages/llama_index/llms/utils.py:28\u001b[0m, in \u001b[0;36mresolve_llm\u001b[0;34m(llm)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m llm \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdefault\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;66;03m# return default OpenAI model. If it fails, return LlamaCPP\u001b[39;00m\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 28\u001b[0m         llm \u001b[38;5;241m=\u001b[39m \u001b[43mOpenAI\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     29\u001b[0m         validate_openai_api_key(llm\u001b[38;5;241m.\u001b[39mapi_key)\n\u001b[1;32m     30\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/py311_trulens/lib/python3.11/site-packages/llama_index/llms/openai/base.py:153\u001b[0m, in \u001b[0;36mOpenAI.__init__\u001b[0;34m(self, model, temperature, max_tokens, additional_kwargs, max_retries, timeout, reuse_client, api_key, api_base, api_version, callback_manager, default_headers, http_client, system_prompt, messages_to_prompt, completion_to_prompt, pydantic_program_mode, output_parser, **kwargs)\u001b[0m\n\u001b[1;32m    145\u001b[0m additional_kwargs \u001b[38;5;241m=\u001b[39m additional_kwargs \u001b[38;5;129;01mor\u001b[39;00m {}\n\u001b[1;32m    147\u001b[0m api_key, api_base, api_version \u001b[38;5;241m=\u001b[39m resolve_openai_credentials(\n\u001b[1;32m    148\u001b[0m     api_key\u001b[38;5;241m=\u001b[39mapi_key,\n\u001b[1;32m    149\u001b[0m     api_base\u001b[38;5;241m=\u001b[39mapi_base,\n\u001b[1;32m    150\u001b[0m     api_version\u001b[38;5;241m=\u001b[39mapi_version,\n\u001b[1;32m    151\u001b[0m )\n\u001b[0;32m--> 153\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    154\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    155\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    156\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    157\u001b[0m \u001b[43m    \u001b[49m\u001b[43madditional_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madditional_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    158\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_retries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    159\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallback_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    160\u001b[0m \u001b[43m    \u001b[49m\u001b[43mapi_key\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mapi_key\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    161\u001b[0m \u001b[43m    \u001b[49m\u001b[43mapi_version\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mapi_version\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    162\u001b[0m \u001b[43m    \u001b[49m\u001b[43mapi_base\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mapi_base\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    163\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    164\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreuse_client\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreuse_client\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    165\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdefault_headers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdefault_headers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    166\u001b[0m \u001b[43m    \u001b[49m\u001b[43msystem_prompt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msystem_prompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    167\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmessages_to_prompt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmessages_to_prompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    168\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompletion_to_prompt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompletion_to_prompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    169\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpydantic_program_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpydantic_program_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    170\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_parser\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_parser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    171\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    172\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    174\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_client \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    175\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_aclient \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/py311_trulens/lib/python3.11/site-packages/pydantic/v1/main.py:339\u001b[0m, in \u001b[0;36mBaseModel.__init__\u001b[0;34m(__pydantic_self__, **data)\u001b[0m\n\u001b[1;32m    333\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    334\u001b[0m \u001b[38;5;124;03mCreate a new model by parsing and validating input data from keyword arguments.\u001b[39;00m\n\u001b[1;32m    335\u001b[0m \n\u001b[1;32m    336\u001b[0m \u001b[38;5;124;03mRaises ValidationError if the input data cannot be parsed to form a valid model.\u001b[39;00m\n\u001b[1;32m    337\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    338\u001b[0m \u001b[38;5;66;03m# Uses something other than `self` the first arg to allow \"self\" as a settable attribute\u001b[39;00m\n\u001b[0;32m--> 339\u001b[0m values, fields_set, validation_error \u001b[38;5;241m=\u001b[39m \u001b[43mvalidate_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m__pydantic_self__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__class__\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    340\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m validation_error:\n\u001b[1;32m    341\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m validation_error\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/py311_trulens/lib/python3.11/site-packages/pydantic/v1/main.py:1074\u001b[0m, in \u001b[0;36mvalidate_model\u001b[0;34m(model, input_data, cls)\u001b[0m\n\u001b[1;32m   1071\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m check_extra:\n\u001b[1;32m   1072\u001b[0m         names_used\u001b[38;5;241m.\u001b[39madd(field\u001b[38;5;241m.\u001b[39mname \u001b[38;5;28;01mif\u001b[39;00m using_name \u001b[38;5;28;01melse\u001b[39;00m field\u001b[38;5;241m.\u001b[39malias)\n\u001b[0;32m-> 1074\u001b[0m v_, errors_ \u001b[38;5;241m=\u001b[39m \u001b[43mfield\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalidate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfield\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43malias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcls_\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1075\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(errors_, ErrorWrapper):\n\u001b[1;32m   1076\u001b[0m     errors\u001b[38;5;241m.\u001b[39mappend(errors_)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/py311_trulens/lib/python3.11/site-packages/pydantic/v1/fields.py:864\u001b[0m, in \u001b[0;36mModelField.validate\u001b[0;34m(self, v, values, loc, cls)\u001b[0m\n\u001b[1;32m    862\u001b[0m errors: Optional[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mErrorList\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m    863\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpre_validators:\n\u001b[0;32m--> 864\u001b[0m     v, errors \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply_validators\u001b[49m\u001b[43m(\u001b[49m\u001b[43mv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpre_validators\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    865\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m errors:\n\u001b[1;32m    866\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m v, errors\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/py311_trulens/lib/python3.11/site-packages/pydantic/v1/fields.py:1154\u001b[0m, in \u001b[0;36mModelField._apply_validators\u001b[0;34m(self, v, values, loc, cls, validators)\u001b[0m\n\u001b[1;32m   1152\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m validator \u001b[38;5;129;01min\u001b[39;00m validators:\n\u001b[1;32m   1153\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1154\u001b[0m         v \u001b[38;5;241m=\u001b[39m \u001b[43mvalidator\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_config\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1155\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mValueError\u001b[39;00m, \u001b[38;5;167;01mTypeError\u001b[39;00m, \u001b[38;5;167;01mAssertionError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m   1156\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m v, ErrorWrapper(exc, loc)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/py311_trulens/lib/python3.11/site-packages/pydantic/v1/class_validators.py:304\u001b[0m, in \u001b[0;36m_generic_validator_cls.<locals>.<lambda>\u001b[0;34m(cls, v, values, field, config)\u001b[0m\n\u001b[1;32m    302\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mlambda\u001b[39;00m \u001b[38;5;28mcls\u001b[39m, v, values, field, config: validator(\u001b[38;5;28mcls\u001b[39m, v, values\u001b[38;5;241m=\u001b[39mvalues, field\u001b[38;5;241m=\u001b[39mfield, config\u001b[38;5;241m=\u001b[39mconfig)\n\u001b[1;32m    303\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m args \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mset\u001b[39m():\n\u001b[0;32m--> 304\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mlambda\u001b[39;00m \u001b[38;5;28mcls\u001b[39m, v, values, field, config: \u001b[43mvalidator\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    305\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m args \u001b[38;5;241m==\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvalues\u001b[39m\u001b[38;5;124m'\u001b[39m}:\n\u001b[1;32m    306\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mlambda\u001b[39;00m \u001b[38;5;28mcls\u001b[39m, v, values, field, config: validator(\u001b[38;5;28mcls\u001b[39m, v, values\u001b[38;5;241m=\u001b[39mvalues)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/py311_trulens/lib/python3.11/site-packages/llama_index/core/base/llms/base.py:38\u001b[0m, in \u001b[0;36mBaseLLM._validate_callback_manager\u001b[0;34m(cls, v)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;129m@validator\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcallback_manager\u001b[39m\u001b[38;5;124m\"\u001b[39m, pre\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_validate_callback_manager\u001b[39m(\u001b[38;5;28mcls\u001b[39m, v: CallbackManager) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m CallbackManager:\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m v \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 38\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mCallbackManager\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     39\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m v\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/py311_trulens/lib/python3.11/site-packages/llama_index/core/callbacks/base.py:54\u001b[0m, in \u001b[0;36mCallbackManager.__init__\u001b[0;34m(self, handlers)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, handlers: Optional[List[BaseCallbackHandler]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m     53\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Initialize the manager with a list of handlers.\"\"\"\u001b[39;00m\n\u001b[0;32m---> 54\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mllama_index\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m global_handler\n\u001b[1;32m     56\u001b[0m     handlers \u001b[38;5;241m=\u001b[39m handlers \u001b[38;5;129;01mor\u001b[39;00m []\n\u001b[1;32m     58\u001b[0m     \u001b[38;5;66;03m# add eval handlers based on global defaults\u001b[39;00m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'global_handler' from 'llama_index.core' (/opt/homebrew/Caskroom/miniconda/base/envs/py311_trulens/lib/python3.11/site-packages/llama_index/core/__init__.py)"
     ]
    }
   ],
   "source": [
    "from trulens_eval import Feedback, Tru, TruLlama\n",
    "from trulens_eval.feedback import Groundedness\n",
    "from trulens_eval.feedback.provider.openai import OpenAI\n",
    "\n",
    "tru = Tru()\n",
    "\n",
    "from llama_index import VectorStoreIndex, QueryBundle\n",
    "from llama_index.readers.web import SimpleWebPageReader\n",
    "\n",
    "documents = SimpleWebPageReader(\n",
    "    html_to_text=True\n",
    ").load_data([\"http://paulgraham.com/worked.html\"])\n",
    "index = VectorStoreIndex.from_documents(documents)\n",
    "\n",
    "query_engine = index.as_query_engine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "llama-index                              0.9.48\n",
      "llama-index-agent-openai                 0.1.4\n",
      "llama-index-core                         0.10.9\n",
      "llama-index-embeddings-openai            0.1.5\n",
      "llama-index-indices-managed-llama-cloud  0.1.2\n",
      "llama-index-legacy                       0.9.48\n",
      "llama-index-llms-openai                  0.1.5\n",
      "llama-index-multi-modal-llms-openai      0.1.3\n",
      "llama-index-program-openai               0.1.3\n",
      "llama-index-question-gen-openai          0.1.2\n",
      "llama-index-readers-file                 0.1.4\n",
      "llama-index-readers-llama-parse          0.1.2\n",
      "llama-parse                              0.3.4\n",
      "llamaindex-py-client                     0.1.13\n",
      "pyllama                                  0.0.9\n"
     ]
    }
   ],
   "source": [
    "! pip list | grep llama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tru_query_engine_recorder = TruLlama(query_engine,\n",
    "    app_id='LlamaIndex_App1',\n",
    "    feedbacks=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# or as context manager\n",
    "with tru_query_engine_recorder as recording:\n",
    "    print(query_engine.query(QueryBundle(\"What did the author do growing up?\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rec = recording.get().layout_calls_as_app() # important\n",
    "from trulens_eval.utils.serial import Lens\n",
    "from trulens_eval.schema import Select\n",
    "all_args = next(Lens().app.query[0].args.str_or_query_bundle.get(rec))\n",
    "\n",
    "Select.render_for_dashboard(Select.RecordRets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TruBasicApp testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trulens_eval import TruBasicApp\n",
    "\n",
    "SCRIPT_DIR = Path().cwd()\n",
    "dotenv.load_dotenv(SCRIPT_DIR / \"my.env\")\n",
    "\n",
    "tru = Tru(database_redact_keys=True)#database_url=os.environ.get(\"database_url\"))\n",
    "\n",
    "def llm_standalone(prompt):\n",
    "    return prompt\n",
    "\n",
    "f_sentiment = Feedback(bedrock.sentiment).on_output()\n",
    "\n",
    "recorder = TruBasicApp(llm_standalone, app=\"default\", feedbacks=[f_sentiment])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py38_trulens",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
