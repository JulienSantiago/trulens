{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dev Notebook\n",
    "\n",
    "This notebook loads the version of trulens_eval from the enclosing repo folder. You can use this to debug or devlop trulens_eval features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Volumes/dev_new/trulens/trulens_eval\n",
      "WARNING: package pinecone not present in requirements.\n",
      "âœ… Key OPENAI_API_KEY set from environment (same value found in .env file at /Volumes/dev_new/.env).\n",
      "âœ… Key HUGGINGFACE_API_KEY set from environment (same value found in .env file at /Volumes/dev_new/.env).\n",
      "ðŸ¦‘ Tru initialized with db url sqlite:///default.sqlite .\n",
      "ðŸ›‘ Secret keys may be written to the database. See the `database_redact_keys` option of `Tru` to prevent this.\n",
      "Force stopping dashboard ...\n",
      "Starting dashboard ...\n",
      "Config file already exists. Skipping writing process.\n",
      "Credentials file already exists. Skipping writing process.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "299bf7b92d9f43419d1518cd2e38022e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Accordion(children=(VBox(children=(VBox(children=(Label(value='STDOUT'), Output())), VBox(children=(Label(valuâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dashboard started at http://10.0.0.35:8501 .\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Popen: returncode: None args: ['streamlit', 'run', '--server.headless=True'...>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pip uninstall -y trulens_eval\n",
    "# pip install git+https://github.com/truera/trulens@piotrm/azure_bugfixes#subdirectory=trulens_eval\n",
    "\n",
    "# trulens_eval notebook dev\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "base = Path().cwd()\n",
    "while not (base / \"trulens_eval\").exists():\n",
    "    base = base.parent\n",
    "\n",
    "print(base)\n",
    "\n",
    "# If running from github repo, can use this:\n",
    "sys.path.append(str(base))\n",
    "\n",
    "# Uncomment for more debugging printouts.\n",
    "\"\"\"\n",
    "import logging\n",
    "root = logging.getLogger()\n",
    "root.setLevel(logging.DEBUG)\n",
    "\n",
    "handler = logging.StreamHandler(sys.stdout)\n",
    "handler.setLevel(logging.DEBUG)\n",
    "formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "handler.setFormatter(formatter)\n",
    "root.addHandler(handler)\n",
    "\"\"\"\n",
    "\n",
    "from trulens_eval.keys import check_keys\n",
    "\n",
    "check_keys(\n",
    "    \"OPENAI_API_KEY\",\n",
    "    \"HUGGINGFACE_API_KEY\"\n",
    ")\n",
    "\n",
    "from trulens_eval import Tru\n",
    "tru = Tru()\n",
    "# tru.reset_database()\n",
    "\n",
    "# tru.run_dashboard(_dev=base, force=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoding type <class 'trulens_eval.tru_llama.TruLlama'>\n",
      "encoding type <class 'trulens_eval.utils.pyschema.Class'>\n",
      "encoding type <class 'trulens_eval.utils.pyschema.Module'>\n",
      "encoding type <class 'list'>\n",
      "encoding type <class 'trulens_eval.utils.pyschema.Class'>\n",
      "encoding type <class 'trulens_eval.utils.pyschema.Module'>\n",
      "encoding type <class 'trulens_eval.utils.pyschema.Class'>\n",
      "encoding type <class 'trulens_eval.utils.pyschema.Module'>\n",
      "encoding type <class 'trulens_eval.utils.pyschema.Class'>\n",
      "encoding type <class 'trulens_eval.utils.pyschema.Module'>\n",
      "encoding type <class 'trulens_eval.utils.pyschema.Class'>\n",
      "encoding type <class 'trulens_eval.utils.pyschema.Module'>\n",
      "encoding type <class 'trulens_eval.utils.pyschema.Class'>\n",
      "encoding type <class 'trulens_eval.utils.pyschema.Module'>\n",
      "encoding type <class 'trulens_eval.utils.pyschema.Class'>\n",
      "encoding type <class 'trulens_eval.utils.pyschema.Module'>\n",
      "encoding type <class 'trulens_eval.utils.pyschema.Class'>\n",
      "encoding type <class 'trulens_eval.utils.pyschema.Module'>\n",
      "encoding type <class 'trulens_eval.utils.pyschema.Class'>\n",
      "encoding type <class 'trulens_eval.utils.pyschema.Module'>\n",
      "encoding type <class 'trulens_eval.utils.pyschema.Class'>\n",
      "encoding type <class 'trulens_eval.utils.pyschema.Module'>\n",
      "encoding type <class 'trulens_eval.utils.pyschema.Class'>\n",
      "encoding type <class 'trulens_eval.utils.pyschema.Module'>\n",
      "encoding type <class 'dict'>\n",
      "encoding type <class 'list'>\n",
      "encoding type <class 'trulens_eval.utils.pyschema.Class'>\n",
      "encoding type <class 'trulens_eval.utils.pyschema.Module'>\n",
      "encoding type <class 'llama_index.query_engine.retriever_query_engine.RetrieverQueryEngine'>\n",
      "encoding type <class 'trulens_eval.utils.pyschema.Obj'>\n",
      "encoding type <class 'trulens_eval.utils.pyschema.Class'>\n",
      "encoding type <class 'trulens_eval.utils.pyschema.Module'>\n",
      "encoding type <class 'dict'>\n",
      "encoding type <class 'trulens_eval.utils.pyschema.Class'>\n",
      "encoding type <class 'trulens_eval.utils.pyschema.Module'>\n",
      "encoding type <class 'list'>\n",
      "encoding type <class 'trulens_eval.utils.pyschema.Class'>\n",
      "encoding type <class 'trulens_eval.utils.pyschema.Module'>\n",
      "encoding type <class 'trulens_eval.utils.pyschema.Class'>\n",
      "encoding type <class 'trulens_eval.utils.pyschema.Module'>\n",
      "encoding type <class 'trulens_eval.utils.pyschema.Class'>\n",
      "encoding type <class 'trulens_eval.utils.pyschema.Module'>\n",
      "encoding type <class 'trulens_eval.utils.pyschema.Class'>\n",
      "encoding type <class 'trulens_eval.utils.pyschema.Module'>\n",
      "encoding type <class 'trulens_eval.utils.pyschema.Class'>\n",
      "encoding type <class 'trulens_eval.utils.pyschema.Module'>\n",
      "encoding type <class 'trulens_eval.utils.pyschema.Class'>\n",
      "encoding type <class 'trulens_eval.utils.pyschema.Module'>\n",
      "encoding type <class 'trulens_eval.utils.pyschema.Class'>\n",
      "encoding type <class 'trulens_eval.utils.pyschema.Module'>\n",
      "encoding type <class 'trulens_eval.utils.pyschema.Class'>\n",
      "encoding type <class 'trulens_eval.utils.pyschema.Module'>\n",
      "encoding type <class 'trulens_eval.utils.pyschema.Class'>\n",
      "encoding type <class 'trulens_eval.utils.pyschema.Module'>\n",
      "encoding type <class 'trulens_eval.utils.pyschema.Class'>\n",
      "encoding type <class 'trulens_eval.utils.pyschema.Module'>\n",
      "Hello! How can I assist you today?\n"
     ]
    }
   ],
   "source": [
    "# from llama_index import get_response_synthesizer, ServiceContext\n",
    "from llama_index.query_engine import RetrieverQueryEngine\n",
    "from llama_index.retrievers import VectorIndexRetriever\n",
    "# from llama_index.response_synthesizers import ResponseMode\n",
    "\n",
    "# service_context = ServiceContext.from_defaults()\n",
    "\n",
    "from llama_index import VectorStoreIndex\n",
    "from llama_index.readers.web import SimpleWebPageReader\n",
    "\n",
    "documents = SimpleWebPageReader(\n",
    "    html_to_text=True\n",
    ").load_data([\"http://paulgraham.com/worked.html\"])\n",
    "index = VectorStoreIndex.from_documents(documents)\n",
    "\n",
    "retriever = VectorIndexRetriever(\n",
    "    index=index,\n",
    "    similarity_top_k=2\n",
    ")\n",
    "\n",
    "query_eengine = RetrieverQueryEngine.from_args(\n",
    "    retriever=retriever,\n",
    "    #streaming=True\n",
    ")\n",
    "\n",
    "from trulens_eval import TruLlama\n",
    "tru_query_engine_recorder = TruLlama(\n",
    "    query_eengine,\n",
    "    app_id='LlamaIndex_App2',\n",
    ")\n",
    "\n",
    "#with tru_query_engine_recorder as recording:\n",
    "print(query_eengine.query(\"Hello there.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error calling wrapped function aquery.\n",
      "Traceback (most recent call last):\n",
      "  File \"/Volumes/dev_new/trulens/trulens_eval/trulens_eval/instruments.py\", line 561, in tru_awrapper\n",
      "    rets, cost = await Endpoint.atrack_all_costs_tally(\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Volumes/dev_new/trulens/trulens_eval/trulens_eval/feedback/provider/endpoint/base.py\", line 438, in atrack_all_costs_tally\n",
      "    result, cbs = await Endpoint.atrack_all_costs(\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Volumes/dev_new/trulens/trulens_eval/trulens_eval/feedback/provider/endpoint/base.py\", line 421, in atrack_all_costs\n",
      "    return await Endpoint._atrack_costs(__func, *args, with_endpoints=endpoints, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Volumes/dev_new/trulens/trulens_eval/trulens_eval/feedback/provider/endpoint/base.py\", line 516, in _atrack_costs\n",
      "    result: T = await desync(__func, *args, **kwargs)\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Volumes/dev_new/trulens/trulens_eval/trulens_eval/utils/asynchro.py\", line 85, in desync\n",
      "    return await func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/py311_trulens/lib/python3.11/site-packages/llama_index/core/base_query_engine.py\", line 46, in aquery\n",
      "    return await self._aquery(str_or_query_bundle)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/py311_trulens/lib/python3.11/site-packages/llama_index/query_engine/retriever_query_engine.py\", line 188, in _aquery\n",
      "    response = await self._response_synthesizer.asynthesize(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/py311_trulens/lib/python3.11/site-packages/llama_index/response_synthesizers/base.py\", line 201, in asynthesize\n",
      "    response_str = await self.aget_response(\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/py311_trulens/lib/python3.11/site-packages/llama_index/response_synthesizers/compact_and_refine.py\", line 19, in aget_response\n",
      "    return await super().aget_response(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/py311_trulens/lib/python3.11/site-packages/llama_index/response_synthesizers/refine.py\", line 325, in aget_response\n",
      "    response = await self._agive_response_single(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/py311_trulens/lib/python3.11/site-packages/llama_index/response_synthesizers/refine.py\", line 445, in _agive_response_single\n",
      "    raise ValueError(\"Streaming not supported for async\")\n",
      "ValueError: Streaming not supported for async\n",
      "\n",
      "Unsure what the main output string is for the call to aquery with return type <class 'NoneType'>.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoding type <class 'ValueError'>\n",
      "encoding type <class 'trulens_eval.utils.pyschema.Obj'>\n",
      "encoding type <class 'trulens_eval.utils.pyschema.Class'>\n",
      "encoding type <class 'trulens_eval.utils.pyschema.Module'>\n",
      "encoding type <class 'trulens_eval.schema.Record'>\n",
      "encoding type <class 'trulens_eval.schema.Cost'>\n",
      "encoding type <class 'trulens_eval.schema.Perf'>\n",
      "encoding type <class 'datetime.datetime'>\n",
      "encoding type <class 'datetime.datetime'>\n",
      "encoding type <class 'datetime.datetime'>\n",
      "encoding type <class 'dict'>\n",
      "encoding type <class 'dict'>\n",
      "encoding type <class 'dict'>\n",
      "encoding type <class 'dict'>\n",
      "encoding type <class 'list'>\n",
      "encoding type <class 'trulens_eval.schema.RecordAppCall'>\n",
      "encoding type <class 'list'>\n",
      "encoding type <class 'trulens_eval.schema.RecordAppCallMethod'>\n",
      "encoding type <class 'trulens_eval.utils.serial.Lens'>\n",
      "encoding type <class 'trulens_eval.utils.pyschema.Method'>\n",
      "encoding type <class 'trulens_eval.utils.pyschema.Obj'>\n",
      "encoding type <class 'trulens_eval.utils.pyschema.Class'>\n",
      "encoding type <class 'trulens_eval.utils.pyschema.Module'>\n",
      "encoding type <class 'dict'>\n",
      "encoding type <class 'trulens_eval.schema.Perf'>\n",
      "encoding type <class 'datetime.datetime'>\n",
      "encoding type <class 'datetime.datetime'>\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Streaming not supported for async",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;43;01mwith\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtru_query_engine_recorder\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mas\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrecording\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mprint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mawait\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mquery_eengine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maquery\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mHello there.\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Volumes/dev_new/trulens/trulens_eval/trulens_eval/app.py:838\u001b[0m, in \u001b[0;36mApp.__exit__\u001b[0;34m(self, exc_type, exc_value, exc_tb)\u001b[0m\n\u001b[1;32m    835\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrecording_contexts\u001b[38;5;241m.\u001b[39mreset(ctx\u001b[38;5;241m.\u001b[39mtoken)\n\u001b[1;32m    837\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m exc_type \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 838\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exc_value\n\u001b[1;32m    840\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[6], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tru_query_engine_recorder \u001b[38;5;28;01mas\u001b[39;00m recording:\n\u001b[0;32m----> 2\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28;01mawait\u001b[39;00m query_eengine\u001b[38;5;241m.\u001b[39maquery(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHello there.\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n",
      "File \u001b[0;32m/Volumes/dev_new/trulens/trulens_eval/trulens_eval/instruments.py:610\u001b[0m, in \u001b[0;36mInstrument.tracked_method_wrapper.<locals>.tru_awrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    604\u001b[0m     \u001b[38;5;66;03m# If stack has only 1 thing on it, we are looking at a \"root\u001b[39;00m\n\u001b[1;32m    605\u001b[0m     \u001b[38;5;66;03m# call\". Create a record of the result and notify the app:\u001b[39;00m\n\u001b[1;32m    607\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(stack) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    608\u001b[0m         \u001b[38;5;66;03m# If this is a root call, notify app to add the completed record\u001b[39;00m\n\u001b[1;32m    609\u001b[0m         \u001b[38;5;66;03m# into its containers:\u001b[39;00m\n\u001b[0;32m--> 610\u001b[0m         \u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_on_add_record\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    611\u001b[0m \u001b[43m            \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    612\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfunc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    613\u001b[0m \u001b[43m            \u001b[49m\u001b[43msig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    614\u001b[0m \u001b[43m            \u001b[49m\u001b[43mbindings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbindings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    615\u001b[0m \u001b[43m            \u001b[49m\u001b[43mret\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrets\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    616\u001b[0m \u001b[43m            \u001b[49m\u001b[43merror\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merror\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    617\u001b[0m \u001b[43m            \u001b[49m\u001b[43mperf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mPerf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstart_time\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstart_time\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mend_time\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mend_time\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    618\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcost\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcost\u001b[49m\n\u001b[1;32m    619\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    621\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m error \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    622\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m error\n",
      "File \u001b[0;32m/Volumes/dev_new/trulens/trulens_eval/trulens_eval/app.py:886\u001b[0m, in \u001b[0;36mApp._on_add_record\u001b[0;34m(self, ctx, func, sig, bindings, ret, error, perf, cost)\u001b[0m\n\u001b[1;32m    883\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m error \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    884\u001b[0m     \u001b[38;5;66;03m# May block on DB.\u001b[39;00m\n\u001b[1;32m    885\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_handle_error(record\u001b[38;5;241m=\u001b[39mrecord, error\u001b[38;5;241m=\u001b[39merror)\n\u001b[0;32m--> 886\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m error\n\u001b[1;32m    888\u001b[0m \u001b[38;5;66;03m# Will block on DB, but not on feedback evaluation, depending on\u001b[39;00m\n\u001b[1;32m    889\u001b[0m \u001b[38;5;66;03m# FeedbackMode:\u001b[39;00m\n\u001b[1;32m    890\u001b[0m record\u001b[38;5;241m.\u001b[39mfeedback_results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_handle_record(record\u001b[38;5;241m=\u001b[39mrecord)\n",
      "File \u001b[0;32m/Volumes/dev_new/trulens/trulens_eval/trulens_eval/instruments.py:561\u001b[0m, in \u001b[0;36mInstrument.tracked_method_wrapper.<locals>.tru_awrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    556\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    557\u001b[0m     \u001b[38;5;66;03m# Using sig bind here so we can produce a list of key-value\u001b[39;00m\n\u001b[1;32m    558\u001b[0m     \u001b[38;5;66;03m# pairs even if positional arguments were provided.\u001b[39;00m\n\u001b[1;32m    559\u001b[0m     bindings: BoundArguments \u001b[38;5;241m=\u001b[39m sig\u001b[38;5;241m.\u001b[39mbind(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 561\u001b[0m     rets, cost \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m Endpoint\u001b[38;5;241m.\u001b[39matrack_all_costs_tally(\n\u001b[1;32m    562\u001b[0m         func, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    563\u001b[0m     )\n\u001b[1;32m    565\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    566\u001b[0m     error \u001b[38;5;241m=\u001b[39m e\n",
      "File \u001b[0;32m/Volumes/dev_new/trulens/trulens_eval/trulens_eval/feedback/provider/endpoint/base.py:438\u001b[0m, in \u001b[0;36mEndpoint.atrack_all_costs_tally\u001b[0;34m(_Endpoint__func, with_openai, with_hugs, with_litellm, with_bedrock, *args, **kwargs)\u001b[0m\n\u001b[1;32m    423\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[1;32m    424\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21matrack_all_costs_tally\u001b[39m(\n\u001b[1;32m    425\u001b[0m     __func: CallableMaybeAwaitable[A, T],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    431\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    432\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[T, Cost]:\n\u001b[1;32m    433\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    434\u001b[0m \u001b[38;5;124;03m    Track costs of all of the apis we can currently track, over the\u001b[39;00m\n\u001b[1;32m    435\u001b[0m \u001b[38;5;124;03m    execution of thunk.\u001b[39;00m\n\u001b[1;32m    436\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 438\u001b[0m     result, cbs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m Endpoint\u001b[38;5;241m.\u001b[39matrack_all_costs(\n\u001b[1;32m    439\u001b[0m         __func,\n\u001b[1;32m    440\u001b[0m         \u001b[38;5;241m*\u001b[39margs,\n\u001b[1;32m    441\u001b[0m         with_openai\u001b[38;5;241m=\u001b[39mwith_openai,\n\u001b[1;32m    442\u001b[0m         with_hugs\u001b[38;5;241m=\u001b[39mwith_hugs,\n\u001b[1;32m    443\u001b[0m         with_litellm\u001b[38;5;241m=\u001b[39mwith_litellm,\n\u001b[1;32m    444\u001b[0m         with_bedrock\u001b[38;5;241m=\u001b[39mwith_bedrock,\n\u001b[1;32m    445\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    446\u001b[0m     )\n\u001b[1;32m    448\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(cbs) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    449\u001b[0m         \u001b[38;5;66;03m# Otherwise sum returns \"0\" below.\u001b[39;00m\n\u001b[1;32m    450\u001b[0m         costs \u001b[38;5;241m=\u001b[39m Cost()\n",
      "File \u001b[0;32m/Volumes/dev_new/trulens/trulens_eval/trulens_eval/feedback/provider/endpoint/base.py:421\u001b[0m, in \u001b[0;36mEndpoint.atrack_all_costs\u001b[0;34m(_Endpoint__func, with_openai, with_hugs, with_litellm, with_bedrock, *args, **kwargs)\u001b[0m\n\u001b[1;32m    414\u001b[0m         \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    415\u001b[0m             logger\u001b[38;5;241m.\u001b[39mdebug(\n\u001b[1;32m    416\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not initiallize endpoint \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    417\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPossibly missing key(s). \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    418\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrulens_eval will not track costs/usage of this endpoint. \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    419\u001b[0m             )\n\u001b[0;32m--> 421\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m Endpoint\u001b[38;5;241m.\u001b[39m_atrack_costs(__func, \u001b[38;5;241m*\u001b[39margs, with_endpoints\u001b[38;5;241m=\u001b[39mendpoints, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/Volumes/dev_new/trulens/trulens_eval/trulens_eval/feedback/provider/endpoint/base.py:516\u001b[0m, in \u001b[0;36mEndpoint._atrack_costs\u001b[0;34m(_Endpoint__func, with_endpoints, *args, **kwargs)\u001b[0m\n\u001b[1;32m    513\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mappend(callback)\n\u001b[1;32m    515\u001b[0m \u001b[38;5;66;03m# Call the thunk and wait for result.\u001b[39;00m\n\u001b[0;32m--> 516\u001b[0m result: T \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m desync(__func, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    518\u001b[0m \u001b[38;5;66;03m# Return result and only the callbacks created here. Outer thunks might\u001b[39;00m\n\u001b[1;32m    519\u001b[0m \u001b[38;5;66;03m# return others.\u001b[39;00m\n\u001b[1;32m    520\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result, callbacks\n",
      "File \u001b[0;32m/Volumes/dev_new/trulens/trulens_eval/trulens_eval/utils/asynchro.py:85\u001b[0m, in \u001b[0;36mdesync\u001b[0;34m(func, *args, **kwargs)\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;124;03mRun the given function asynchronously with the given args. If it is not\u001b[39;00m\n\u001b[1;32m     78\u001b[0m \u001b[38;5;124;03masynchronous, will run in thread. Note: this has to be marked async since in\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[38;5;124;03mrun asynchronously.\u001b[39;00m\n\u001b[1;32m     82\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     84\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_really_coroutinefunction(func):\n\u001b[0;32m---> 85\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m     87\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     88\u001b[0m     res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m asyncio\u001b[38;5;241m.\u001b[39mto_thread(func, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/py311_trulens/lib/python3.11/site-packages/llama_index/core/base_query_engine.py:46\u001b[0m, in \u001b[0;36mBaseQueryEngine.aquery\u001b[0;34m(self, str_or_query_bundle)\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(str_or_query_bundle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m     45\u001b[0m     str_or_query_bundle \u001b[38;5;241m=\u001b[39m QueryBundle(str_or_query_bundle)\n\u001b[0;32m---> 46\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_aquery(str_or_query_bundle)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/py311_trulens/lib/python3.11/site-packages/llama_index/query_engine/retriever_query_engine.py:188\u001b[0m, in \u001b[0;36mRetrieverQueryEngine._aquery\u001b[0;34m(self, query_bundle)\u001b[0m\n\u001b[1;32m    183\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_manager\u001b[38;5;241m.\u001b[39mevent(\n\u001b[1;32m    184\u001b[0m     CBEventType\u001b[38;5;241m.\u001b[39mQUERY, payload\u001b[38;5;241m=\u001b[39m{EventPayload\u001b[38;5;241m.\u001b[39mQUERY_STR: query_bundle\u001b[38;5;241m.\u001b[39mquery_str}\n\u001b[1;32m    185\u001b[0m ) \u001b[38;5;28;01mas\u001b[39;00m query_event:\n\u001b[1;32m    186\u001b[0m     nodes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maretrieve(query_bundle)\n\u001b[0;32m--> 188\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_response_synthesizer\u001b[38;5;241m.\u001b[39masynthesize(\n\u001b[1;32m    189\u001b[0m         query\u001b[38;5;241m=\u001b[39mquery_bundle,\n\u001b[1;32m    190\u001b[0m         nodes\u001b[38;5;241m=\u001b[39mnodes,\n\u001b[1;32m    191\u001b[0m     )\n\u001b[1;32m    193\u001b[0m     query_event\u001b[38;5;241m.\u001b[39mon_end(payload\u001b[38;5;241m=\u001b[39m{EventPayload\u001b[38;5;241m.\u001b[39mRESPONSE: response})\n\u001b[1;32m    195\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/py311_trulens/lib/python3.11/site-packages/llama_index/response_synthesizers/base.py:201\u001b[0m, in \u001b[0;36mBaseSynthesizer.asynthesize\u001b[0;34m(self, query, nodes, additional_source_nodes, **response_kwargs)\u001b[0m\n\u001b[1;32m    196\u001b[0m     query \u001b[38;5;241m=\u001b[39m QueryBundle(query_str\u001b[38;5;241m=\u001b[39mquery)\n\u001b[1;32m    198\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_callback_manager\u001b[38;5;241m.\u001b[39mevent(\n\u001b[1;32m    199\u001b[0m     CBEventType\u001b[38;5;241m.\u001b[39mSYNTHESIZE, payload\u001b[38;5;241m=\u001b[39m{EventPayload\u001b[38;5;241m.\u001b[39mQUERY_STR: query\u001b[38;5;241m.\u001b[39mquery_str}\n\u001b[1;32m    200\u001b[0m ) \u001b[38;5;28;01mas\u001b[39;00m event:\n\u001b[0;32m--> 201\u001b[0m     response_str \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maget_response(\n\u001b[1;32m    202\u001b[0m         query_str\u001b[38;5;241m=\u001b[39mquery\u001b[38;5;241m.\u001b[39mquery_str,\n\u001b[1;32m    203\u001b[0m         text_chunks\u001b[38;5;241m=\u001b[39m[\n\u001b[1;32m    204\u001b[0m             n\u001b[38;5;241m.\u001b[39mnode\u001b[38;5;241m.\u001b[39mget_content(metadata_mode\u001b[38;5;241m=\u001b[39mMetadataMode\u001b[38;5;241m.\u001b[39mLLM) \u001b[38;5;28;01mfor\u001b[39;00m n \u001b[38;5;129;01min\u001b[39;00m nodes\n\u001b[1;32m    205\u001b[0m         ],\n\u001b[1;32m    206\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mresponse_kwargs,\n\u001b[1;32m    207\u001b[0m     )\n\u001b[1;32m    209\u001b[0m     additional_source_nodes \u001b[38;5;241m=\u001b[39m additional_source_nodes \u001b[38;5;129;01mor\u001b[39;00m []\n\u001b[1;32m    210\u001b[0m     source_nodes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(nodes) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mlist\u001b[39m(additional_source_nodes)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/py311_trulens/lib/python3.11/site-packages/llama_index/response_synthesizers/compact_and_refine.py:19\u001b[0m, in \u001b[0;36mCompactAndRefine.aget_response\u001b[0;34m(self, query_str, text_chunks, prev_response, **response_kwargs)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21maget_response\u001b[39m(\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m     13\u001b[0m     query_str: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mresponse_kwargs: Any,\n\u001b[1;32m     17\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m RESPONSE_TEXT_TYPE:\n\u001b[1;32m     18\u001b[0m     compact_texts \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_compact_text_chunks(query_str, text_chunks)\n\u001b[0;32m---> 19\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39maget_response(\n\u001b[1;32m     20\u001b[0m         query_str\u001b[38;5;241m=\u001b[39mquery_str,\n\u001b[1;32m     21\u001b[0m         text_chunks\u001b[38;5;241m=\u001b[39mcompact_texts,\n\u001b[1;32m     22\u001b[0m         prev_response\u001b[38;5;241m=\u001b[39mprev_response,\n\u001b[1;32m     23\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mresponse_kwargs,\n\u001b[1;32m     24\u001b[0m     )\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/py311_trulens/lib/python3.11/site-packages/llama_index/response_synthesizers/refine.py:325\u001b[0m, in \u001b[0;36mRefine.aget_response\u001b[0;34m(self, query_str, text_chunks, prev_response, **response_kwargs)\u001b[0m\n\u001b[1;32m    321\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m text_chunk \u001b[38;5;129;01min\u001b[39;00m text_chunks:\n\u001b[1;32m    322\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m prev_response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    323\u001b[0m         \u001b[38;5;66;03m# if this is the first chunk, and text chunk already\u001b[39;00m\n\u001b[1;32m    324\u001b[0m         \u001b[38;5;66;03m# is an answer, then return it\u001b[39;00m\n\u001b[0;32m--> 325\u001b[0m         response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_agive_response_single(\n\u001b[1;32m    326\u001b[0m             query_str, text_chunk, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mresponse_kwargs\n\u001b[1;32m    327\u001b[0m         )\n\u001b[1;32m    328\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    329\u001b[0m         response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_arefine_response_single(\n\u001b[1;32m    330\u001b[0m             prev_response, query_str, text_chunk, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mresponse_kwargs\n\u001b[1;32m    331\u001b[0m         )\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/py311_trulens/lib/python3.11/site-packages/llama_index/response_synthesizers/refine.py:445\u001b[0m, in \u001b[0;36mRefine._agive_response_single\u001b[0;34m(self, query_str, text_chunk, **response_kwargs)\u001b[0m\n\u001b[1;32m    441\u001b[0m         logger\u001b[38;5;241m.\u001b[39mwarning(\n\u001b[1;32m    442\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mValidation error on structured response: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, exc_info\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    443\u001b[0m         )\n\u001b[1;32m    444\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_streaming:\n\u001b[0;32m--> 445\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStreaming not supported for async\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    446\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    447\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_arefine_response_single(\n\u001b[1;32m    448\u001b[0m         cast(RESPONSE_TEXT_TYPE, response),\n\u001b[1;32m    449\u001b[0m         query_str,\n\u001b[1;32m    450\u001b[0m         cur_text_chunk,\n\u001b[1;32m    451\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mresponse_kwargs,\n\u001b[1;32m    452\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: Streaming not supported for async"
     ]
    }
   ],
   "source": [
    "with tru_query_engine_recorder as recording:\n",
    "    print(await query_eengine.aquery(\"Hello there.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import json\n",
    "from langchain_community.retrievers.bedrock import (\n",
    "    AmazonKnowledgeBasesRetriever,\n",
    "    RetrievalConfig,\n",
    ")\n",
    "from langchain.docstore.document import Document\n",
    "\n",
    "\n",
    "class MyKnowledgeBaseRetriever(AmazonKnowledgeBasesRetriever): \n",
    "    def __init__(self, knowledge_base_id, region_name, retrieval_config):\n",
    "        super().__init__(\n",
    "            knowledge_base_id=knowledge_base_id,\n",
    "            region_name=region_name,\n",
    "            retrieval_config=retrieval_config\n",
    "        )\n",
    "        self.knowledge_base_id = knowledge_base_id\n",
    "        self.region_name = region_name\n",
    "        self.retrieval_config = retrieval_config\n",
    "\n",
    "    def get_relevant_documents(self, query):\n",
    "        # Create a session using your AWS credentials\n",
    "        session = boto3.Session(region_name=self.region_name)\n",
    "\n",
    "        # Create a client for the Bedrock Agent Runtime service\n",
    "        bedrock_client = session.client('bedrock-agent-runtime')\n",
    "\n",
    "        # Retrieve relevant documents from the Knowledge Base\n",
    "        response = bedrock_client.retrieve(\n",
    "            knowledgeBaseId=self.knowledge_base_id,\n",
    "            retrievalQuery={\n",
    "                'text': query\n",
    "            },\n",
    "            retrievalConfiguration=self.retrieval_config\n",
    "        )\n",
    "        \n",
    "        relevant_documents = response['retrievalResults']\n",
    "        docs = []\n",
    "\n",
    "        for result in relevant_documents:\n",
    "            content = result['content']['text']\n",
    "            location = result['location']['s3Location']['uri']\n",
    "            score = result['score']\n",
    "\n",
    "            doc = Document(page_content=content, metadata={'location': location, 'score': score})\n",
    "            docs.append(doc)\n",
    "\n",
    "        # Return the relevant documents\n",
    "        return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms.bedrock import Bedrock\n",
    "from langchain.chains.retrieval_qa.base import RetrievalQA\n",
    "\n",
    "retriever = MyKnowledgeBaseRetriever(\n",
    "    knowledge_base_id=\"MY KNOWLEDGE BASE ID\",\n",
    "    region_name=\"us-east-1\",\n",
    "    retrieval_config={\"vectorSearchConfiguration\": {\"numberOfResults\": 6}}\n",
    ")\n",
    "\n",
    "# Create an instance of the Bedrock LLM\n",
    "llm = Bedrock(\n",
    "    model_id='meta.llama2-70b-chat-v1',\n",
    "    model_kwargs={\"temperature\": 0.1, \"top_p\": 0.9, \"max_gen_len\": 1200}\n",
    ")\n",
    "\n",
    "# Create an instance of the RetrievalQA chain\n",
    "qa_chain = RetrievalQA.from_chain_type(llm=llm, retriever=retriever, return_source_documents=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Garbage collecting testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trulens_eval import TruChain, Tru\n",
    "tru = Tru()\n",
    "\n",
    "from operator import itemgetter\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.schema import StrOutputParser\n",
    "\n",
    "prompt1 = ChatPromptTemplate.from_template(\"what is the city {person} is from?\")\n",
    "prompt2 = ChatPromptTemplate.from_template(\n",
    "    \"what country is the city {city} in? respond in {language}\"\n",
    ")\n",
    "\n",
    "model = ChatOpenAI()\n",
    "\n",
    "chain1 = prompt1 | model | StrOutputParser()\n",
    "\n",
    "chain2 = (\n",
    "    {\"city\": chain1, \"language\": itemgetter(\"language\")}\n",
    "    | prompt2\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "chain2_input = {\"person\": \"obama\", \"language\": \"spanish\"}\n",
    "# chain2.invoke(chain2_input)\n",
    "\n",
    "tru_recorder2 = TruChain(\n",
    "    chain2,\n",
    "    app_id='Chain1'\n",
    ")\n",
    "\n",
    "with tru_recorder2 as recs:\n",
    "    llm_response = chain2.invoke(chain2_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain2 = (\n",
    "    {\"city\": chain1, \"language\": itemgetter(\"language\")}\n",
    "    | prompt2\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "tru_recorder2 = TruChain(\n",
    "    chain2,\n",
    "    app_id='Chain2'\n",
    ")\n",
    "\n",
    "with tru_recorder2 as recs:\n",
    "    llm_response = chain2.invoke(chain2_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain2 = (\n",
    "    {\"city\": chain1, \"language\": itemgetter(\"language\")}\n",
    "    | prompt2\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "tru_recorder2 = TruChain(\n",
    "    chain2,\n",
    "    app_id='Chain3'\n",
    ")\n",
    "\n",
    "with tru_recorder2 as recs:\n",
    "    llm_response = chain2.invoke(chain2_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain2 = (\n",
    "    {\"city\": chain1, \"language\": itemgetter(\"language\")}\n",
    "    | prompt2\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "tru_recorder2 = TruChain(\n",
    "    chain2,\n",
    "    app_id='Chain4'\n",
    ")\n",
    "\n",
    "with tru_recorder2 as recs:\n",
    "    llm_response = chain2.invoke(chain2_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain2 = (\n",
    "    {\"city\": chain1, \"language\": itemgetter(\"language\")}\n",
    "    | prompt2\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "tru_recorder2 = TruChain(\n",
    "    chain2,\n",
    "    app_id='Chain5'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tru_recorder2 as recs:\n",
    "    llm_response = chain2.invoke(chain2_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from concurrent.futures import as_completed\n",
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "from time import sleep\n",
    "\n",
    "import dotenv\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from trulens_eval import Feedback\n",
    "from trulens_eval import Tru\n",
    "from trulens_eval.feedback.provider.endpoint.base import Endpoint\n",
    "from trulens_eval.feedback.provider.hugs import Dummy\n",
    "from trulens_eval.schema import Cost\n",
    "from trulens_eval.schema import FeedbackMode\n",
    "from trulens_eval.schema import Record\n",
    "from trulens_eval.tru_custom_app import TruCustomApp\n",
    "from trulens_eval.utils.threading import TP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Context selection tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import json\n",
    "from langchain_community.retrievers.bedrock import (\n",
    "    AmazonKnowledgeBasesRetriever,\n",
    "    RetrievalConfig,\n",
    ")\n",
    "from langchain.docstore.document import Document\n",
    "\n",
    "from langchain.llms.bedrock import Bedrock\n",
    "from langchain.chains.retrieval_qa.base import RetrievalQA\n",
    "\n",
    "\n",
    "class MyKnowledgeBaseRetriever(AmazonKnowledgeBasesRetriever):\n",
    "\n",
    "    def __init__(self, knowledge_base_id, region_name, retrieval_config):\n",
    "        super().__init__(\n",
    "            knowledge_base_id=knowledge_base_id,\n",
    "            region_name=region_name,\n",
    "            retrieval_config=retrieval_config\n",
    "        )\n",
    "        self.knowledge_base_id = knowledge_base_id\n",
    "        self.region_name = region_name\n",
    "        self.retrieval_config = retrieval_config\n",
    "\n",
    "    def get_relevant_documents(self, query):\n",
    "        # Create a session using your AWS credentials\n",
    "        session = boto3.Session(region_name=self.region_name)\n",
    "\n",
    "        # Create a client for the Bedrock Agent Runtime service\n",
    "        bedrock_client = session.client('bedrock-agent-runtime')\n",
    "\n",
    "        # Retrieve relevant documents from the Knowledge Base\n",
    "        response = bedrock_client.retrieve(\n",
    "            knowledgeBaseId=self.knowledge_base_id,\n",
    "            retrievalQuery={'text': query},\n",
    "            retrievalConfiguration=self.retrieval_config\n",
    "        )\n",
    "\n",
    "        relevant_documents = response['retrievalResults']\n",
    "        docs = []\n",
    "\n",
    "        for result in relevant_documents:\n",
    "            content = result['content']['text']\n",
    "            location = result['location']['s3Location']['uri']\n",
    "            score = result['score']\n",
    "\n",
    "            doc = Document(\n",
    "                page_content=content,\n",
    "                metadata={\n",
    "                    'location': location,\n",
    "                    'score': score\n",
    "                }\n",
    "            )\n",
    "            docs.append(doc)\n",
    "\n",
    "        # Return the relevant documents\n",
    "        return docs\n",
    "\n",
    "\n",
    "# Configure the AWS credentials and region\n",
    "# config = Config(region_name='us-east-1')\n",
    "\n",
    "retriever = MyKnowledgeBaseRetriever(\n",
    "    knowledge_base_id=\"MY KNOWLEDGE BASE ID\",\n",
    "    region_name=\"us-east-1\",\n",
    "    retrieval_config={\"vectorSearchConfiguration\": {\n",
    "        \"numberOfResults\": 6\n",
    "    }}\n",
    ")\n",
    "\n",
    "# Create an instance of the Bedrock LLM\n",
    "llm = Bedrock(\n",
    "    model_id='meta.llama2-70b-chat-v1',\n",
    "    model_kwargs={\n",
    "        \"temperature\": 0.1,\n",
    "        \"top_p\": 0.9,\n",
    "        \"max_gen_len\": 1200\n",
    "    }\n",
    ")\n",
    "\n",
    "# Create an instance of the RetrievalQA chain\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm, retriever=retriever, return_source_documents=True\n",
    ")\n",
    "\n",
    "#qa_chain.invoke(\"What steps should be taken during the demo ride in the buying cycle to improve the likelihood of closing the deal?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trulens_eval.app import App\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "tru = Tru()\n",
    "\n",
    "context = App.select_context(qa_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup Tru and/or dashboard.\n",
    "\n",
    "tru = Tru()\n",
    "\n",
    "#tru.reset_database()\n",
    "\n",
    "tru.start_dashboard(\n",
    "    force = True,\n",
    "    _dev=Path().cwd().parent.parent.resolve()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pydantic testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any\n",
    "\n",
    "from pydantic import BaseModel\n",
    "from pydantic import Field\n",
    "from pydantic import field_validator\n",
    "from pydantic import model_validator\n",
    "from pydantic import PydanticUndefinedAnnotation\n",
    "from pydantic import SerializeAsAny\n",
    "from pydantic import ValidationInfo\n",
    "from pydantic import validator\n",
    "from pydantic_core import PydanticUndefined\n",
    "\n",
    "\n",
    "class CustomLoader(BaseModel):\n",
    "    cls: Any\n",
    "\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        kwargs['cls'] = type(self)\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "    @model_validator(mode='before')\n",
    "    @staticmethod\n",
    "    def my_model_validate(obj, info: ValidationInfo):\n",
    "        if not isinstance(obj, dict):\n",
    "            return obj\n",
    "\n",
    "        cls = obj['cls']\n",
    "        # print(cls, subcls, obj, info)\n",
    "\n",
    "        validated = dict()\n",
    "        for k, finfo in cls.model_fields.items():\n",
    "            print(k, finfo)\n",
    "            typ = finfo.annotation\n",
    "            val = finfo.get_default()\n",
    "\n",
    "            if val is PydanticUndefined:\n",
    "                val = obj[k]\n",
    "\n",
    "            print(typ, type(typ))\n",
    "            if isinstance(typ, type) \\\n",
    "            and issubclass(typ, CustomLoader) \\\n",
    "            and isinstance(val, dict) and \"cls\" in val:\n",
    "                subcls = val['cls']\n",
    "                val = subcls.model_validate(val)\n",
    "    \n",
    "            validated[k] = val\n",
    "            \n",
    "        return validated\n",
    "\n",
    "class SubModel(CustomLoader):\n",
    "    sm: int = 3\n",
    "\n",
    "class Model(CustomLoader):\n",
    "    m: int = 2\n",
    "    sub: SubModel\n",
    "\n",
    "class SubSubModelA(SubModel):\n",
    "    ssma: int = 42\n",
    "\n",
    "class SubModelA(SubModel):\n",
    "    sma: int = 0\n",
    "    subsub: SubSubModelA\n",
    "\n",
    "class SubModelB(SubModel):\n",
    "    smb: int = 1\n",
    "\n",
    "c = Model(sub=SubModelA(subsub=SubSubModelA()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c.model_dump()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Model.model_validate({'cls': Model, 'm': 2, 'sub': {'cls': SubModelA, 'sma':3, 'subsub': {'cls': SubSubModelA, 'ssma': 42}}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Model.model_validate({'c': 2, 'sub': {}})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keys testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show keys.\n",
    "\n",
    "import os\n",
    "for k in os.environ:\n",
    "    if \"KEY\" in k:\n",
    "        print(k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bedrock testing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trulens_eval import Bedrock\n",
    "bedrock = Bedrock(\n",
    "    model_id = \"amazon.titan-tg1-large\",\n",
    "    region_name=\"us-west-2\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Endpoint.print_instrumented()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bedrock_response, cost = Endpoint.track_all_costs_tally(\n",
    "    thunk=lambda: bedrock.endpoint.client.invoke_model_with_response_stream(\n",
    "    body=json.dumps({'inputText': \"Hello there.\"}), modelId=\"amazon.titan-tg1-large\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Huggingface testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trulens_eval import Huggingface\n",
    "Huggingface()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Endpoint.print_instrumented()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AzureOpenAI Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trulens_eval.keys import check_keys\n",
    "check_keys(\n",
    "    \"AZURE_OPENAI_API_KEY\",\n",
    "    \"AZURE_OPENAI_ENDPOINT\",\n",
    "    \"OPENAI_API_VERSION\",\n",
    "    \"OPENAI_API_TYPE\",\n",
    "    \"AZURE_OPENAI_DEPLOYMENT_NAME\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llms import AzureOpenAI as AzureOpenAIChat\n",
    "import os\n",
    "\n",
    "gpt_35_turbo = AzureOpenAIChat(\n",
    "    deployment_name=\"gpt-35-turbo\",\n",
    "    model=\"gpt-35-turbo\",\n",
    "    api_key=os.getenv(\"AZURE_OPENAI_API_KEY\"),\n",
    "    api_version=\"2023-05-15\",\n",
    "    model_version=\"0613\",\n",
    "    temperature=0.0,\n",
    ")\n",
    "c = gpt_35_turbo._get_client()\n",
    "gpt_35_turbo._get_credential_kwargs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c.base_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from trulens_eval import feedback\n",
    "azopenai = feedback.AzureOpenAI(\n",
    "    deployment_name=os.environ['AZURE_OPENAI_DEPLOYMENT_NAME']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "azopenai.endpoint.client.client_kwargs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# azopenai.relevance(prompt=\"Where is Germany?\", response=\"Germany is in Europe.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reval = feedback.AzureOpenAI.model_validate(azopenai.model_dump())\n",
    "# reval.relevance(prompt=\"Where is Germany?\", response=\"Poland is in Europe.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "azureOpenAI = azopenai\n",
    "\n",
    "from trulens_eval.feedback.provider import AzureOpenAI\n",
    "from trulens_eval.feedback import Groundedness, GroundTruthAgreement\n",
    "from trulens_eval import TruLlama, Feedback\n",
    "from trulens_eval.app import App\n",
    "import numpy as np\n",
    "# Initialize provider class\n",
    "#azureOpenAI = AzureOpenAI(deployment_name=\"gpt-35-turbo\")\n",
    "\n",
    "grounded = Groundedness(groundedness_provider=azureOpenAI)\n",
    "# Define a groundedness feedback function\n",
    "f_groundedness = (\n",
    "    Feedback(grounded.groundedness_measure_with_cot_reasons)\n",
    "    .on_input_output()\n",
    "    .aggregate(grounded.grounded_statements_aggregator)\n",
    ")\n",
    "\n",
    "# Question/answer relevance between overall question and answer.\n",
    "f_answer_relevance = Feedback(azureOpenAI.relevance_with_cot_reasons).on_input_output()\n",
    "# Question/statement relevance between question and each context chunk.\n",
    "f_context_relevance = (\n",
    "    Feedback(azureOpenAI.qs_relevance_with_cot_reasons)\n",
    "    .on_input_output()\n",
    "    .aggregate(np.mean)\n",
    ")\n",
    "\n",
    "# GroundTruth for comparing the Answer to the Ground-Truth Answer\n",
    "#ground_truth_collection = GroundTruthAgreement(golden_set, provider=azureOpenAI)\n",
    "#f_answer_correctness = (\n",
    "#    Feedback(ground_truth_collection.agreement_measure)\n",
    "#    .on_input_output()\n",
    "#)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_groundedness.model_dump()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trulens_eval.utils.pyschema import WithClassInfo\n",
    "from trulens_eval.utils.serial import SerialModel\n",
    "from trulens_eval.feedback.groundedness import Groundedness\n",
    "import pydantic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Groundedness.model_validate(grounded.model_dump())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f2 = Feedback.model_validate(f_groundedness.model_dump())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f2.implementation.obj.init_bindings.kwargs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f2.imp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_serial(f):\n",
    "    print(\"Before serialization:\")\n",
    "    print(f.imp(\"Where is Poland?\", \"Poland is in Europe\"))\n",
    "    f_dump = f.model_dump()\n",
    "    f = Feedback.model_validate(f_dump)\n",
    "    print(\"After serialization:\")\n",
    "    print(f.imp(\"Where is Poland?\", \"Germany is in Europe\"))\n",
    "    return f\n",
    "\n",
    "f2 = test_serial(f_groundedness)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_groundedness.imp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f2.imp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_answer_relevance = Feedback(azureOpenAI.relevance_with_cot_reasons).on_input_output()\n",
    "\n",
    "# test without serialization\n",
    "print(f_answer_relevance.imp(prompt=\"Where is Germany?\", response=\"Germany is in Europe.\"))\n",
    "\n",
    "# serialize/deserialize\n",
    "f_answer_relevance2 = Feedback.model_validate(f_answer_relevance.model_dump())\n",
    "\n",
    "# test after deserialization\n",
    "print(f_answer_relevance2.imp(prompt=\"Where is Germany?\", response=\"Poland is in Europe.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fr = feedback.Feedback.model_validate(f.model_dump())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fr.imp(prompt=\"Where is Germany?\", response=\"Germany is in Europe.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dummy endpoint testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tp = TP()\n",
    "\n",
    "d = Dummy(\n",
    "    loading_prob=0.1,\n",
    "    freeze_prob=0.0, # we expect requests to have their own timeouts so freeze should never happen\n",
    "    error_prob=0.01,\n",
    "    overloaded_prob=0.1,\n",
    "    rpm=6000\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Langchain testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports from langchain to build app. You may need to install langchain first\n",
    "# with the following:\n",
    "# ! pip install langchain>=0.0.170\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.prompts import ChatPromptTemplate, PromptTemplate\n",
    "from langchain.prompts import HumanMessagePromptTemplate\n",
    "\n",
    "# Initialize Huggingface-based feedback function collection class:\n",
    "# bedrock = Bedrock(model_engine=\"Bedrock\", model_id = \"anthropic.claude-v2\", region_name=\"us-west-2\")\n",
    "\n",
    "# Define a language match feedback function using HuggingFace.\n",
    "#f_relevance = Feedback(bedrock.relevance).on_input_output()\n",
    "# By default this will check language match on the main app input and main app\n",
    "# output.\n",
    "\n",
    "full_prompt = HumanMessagePromptTemplate(\n",
    "    prompt=PromptTemplate(\n",
    "        template=\n",
    "        \"Provide a helpful response with relevant background information for the following: {prompt}\",\n",
    "        input_variables=[\"prompt\"],\n",
    "    )\n",
    ")\n",
    "\n",
    "chat_prompt_template = ChatPromptTemplate.from_messages([full_prompt])\n",
    "\n",
    "llm = OpenAI(temperature=0.9, max_tokens=128)\n",
    "\n",
    "chain = LLMChain(llm=llm, prompt=chat_prompt_template, verbose=True)\n",
    "\n",
    "tru_recorder1 = tru.Chain(\n",
    "    chain,\n",
    "    app_id='Chain1_ChatApplication',\n",
    "    #feedbacks=[f_relevance]\n",
    ")\n",
    "\n",
    "with tru_recorder1 as recording:\n",
    "    llm_response = chain.run(\"What's the capital of the USA?\")\n",
    "\n",
    "display(llm_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Langchain expressions testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trulens_eval import TruChain, Tru\n",
    "tru = Tru()\n",
    "\n",
    "from operator import itemgetter\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.schema import StrOutputParser\n",
    "\n",
    "prompt1 = ChatPromptTemplate.from_template(\"what is the city {person} is from?\")\n",
    "prompt2 = ChatPromptTemplate.from_template(\n",
    "    \"what country is the city {city} in? respond in {language}\"\n",
    ")\n",
    "\n",
    "model = ChatOpenAI()\n",
    "\n",
    "chain1 = prompt1 | model | StrOutputParser()\n",
    "\n",
    "chain2 = (\n",
    "    {\"city\": chain1, \"language\": itemgetter(\"language\")}\n",
    "    | prompt2\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "chain2_input = {\"person\": \"obama\", \"language\": \"spanish\"}\n",
    "\n",
    "chain2.invoke(chain2_input)\n",
    "\n",
    "tru_recorder2 = TruChain(\n",
    "    chain2,\n",
    "    app_id='Chain2'\n",
    ")\n",
    "\n",
    "with tru_recorder2 as recs:\n",
    "    llm_response = chain2.invoke({\"person\": \"obama\", \"language\": \"spanish\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "record = recs.get()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "record.calls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Llama-Index testing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trulens_eval import Feedback, Tru, TruLlama\n",
    "from trulens_eval.feedback import Groundedness\n",
    "from trulens_eval.feedback.provider.openai import OpenAI\n",
    "\n",
    "tru = Tru()\n",
    "\n",
    "from llama_index import VectorStoreIndex, QueryBundle\n",
    "from llama_index.readers.web import SimpleWebPageReader\n",
    "\n",
    "documents = SimpleWebPageReader(\n",
    "    html_to_text=True\n",
    ").load_data([\"http://paulgraham.com/worked.html\"])\n",
    "index = VectorStoreIndex.from_documents(documents)\n",
    "\n",
    "query_engine = index.as_query_engine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tru_query_engine_recorder = TruLlama(query_engine,\n",
    "    app_id='LlamaIndex_App1',\n",
    "    feedbacks=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# or as context manager\n",
    "with tru_query_engine_recorder as recording:\n",
    "    print(query_engine.query(QueryBundle(\"What did the author do growing up?\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rec = recording.get().layout_calls_as_app() # important\n",
    "from trulens_eval.utils.serial import Lens\n",
    "from trulens_eval.schema import Select\n",
    "all_args = next(Lens().app.query[0].args.str_or_query_bundle.get(rec))\n",
    "\n",
    "Select.render_for_dashboard(Select.RecordRets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TruBasicApp testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trulens_eval import TruBasicApp\n",
    "\n",
    "SCRIPT_DIR = Path().cwd()\n",
    "dotenv.load_dotenv(SCRIPT_DIR / \"my.env\")\n",
    "\n",
    "tru = Tru(database_redact_keys=True)#database_url=os.environ.get(\"database_url\"))\n",
    "\n",
    "def llm_standalone(prompt):\n",
    "    return prompt\n",
    "\n",
    "f_sentiment = Feedback(bedrock.sentiment).on_output()\n",
    "\n",
    "recorder = TruBasicApp(llm_standalone, app=\"default\", feedbacks=[f_sentiment])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py38_trulens",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
