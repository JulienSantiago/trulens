{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How's the performance of our Relevance Feeedback Function?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"...\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports main tools:\n",
    "import openai\n",
    "from trulens_eval.feedback import _re_1_10_rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relevance(template: str, question: str, statement: str) -> float:\n",
    "        \"\"\"\n",
    "        Uses OpenAI's Chat Completion Model. A function that completes a\n",
    "        template to check the relevance of the response to a prompt.\n",
    "\n",
    "        Parameters:\n",
    "            prompt (str): A text prompt to an agent. response (str): The agent's\n",
    "            response to the prompt.\n",
    "\n",
    "        Returns:\n",
    "            float: A value between 0 and 1. 0 being \"not relevant\" and 1 being\n",
    "            \"relevant\".\n",
    "        \"\"\"\n",
    "        return _re_1_10_rating(openai.ChatCompletion.create(\n",
    "                    model='gpt-3.5-turbo',\n",
    "                    temperature=0.0,\n",
    "                    messages=[\n",
    "                        {\n",
    "                            \"role\":\n",
    "                                \"system\",\n",
    "                            \"content\":\n",
    "                                str.format(\n",
    "                                    template,\n",
    "                                    question=question,\n",
    "                                    statement=statement\n",
    "                                )\n",
    "                        }\n",
    "                    ]\n",
    "                )[\"choices\"][0][\"message\"][\"content\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "QS_RELEVANCE = \"\"\"You are a RELEVANCE grader; providing the relevance of the given STATEMENT to the given QUESTION.\n",
    "Respond only as a number from 1 to 10 where 1 is the least relevant and 10 is the most relevant. \n",
    "\n",
    "A few additional scoring guidelines:\n",
    "\n",
    "- Answers that intentionally do not answer the question, such as 'I don't know', should also be counted as the most relevant.\n",
    "\n",
    "- STATEMENT must be relevant to the entire QUESTION to get the highest score.\n",
    "\n",
    "- RELEVANCE score should increase as the STATEMENT provides more RELEVANT context to the QUESTION.\n",
    "\n",
    "- RELEVANCE score should increase as the STATEMENT provides RELEVANT context to more parts of the QUESTION.\n",
    "\n",
    "- STATEMENT that is relevant to none of the question should get a score of 1.\n",
    "\n",
    "- STATEMENT that is relevant to some of the question should get as score between a 2 and 4.\n",
    "\n",
    "- STATEMENT that is relevant to most of the question should get a score between a 5 and 9.\n",
    "\n",
    "- STATEMENT that is relevant to the entire question should get a score of 10.\n",
    "\n",
    "- STATEMENT that confidently false should get a score of 1.\n",
    "\n",
    "- STATEMENT that is only seemingly relevant should get a score of 1.\n",
    "\n",
    "- Never elaborate.\n",
    "\n",
    "QUESTION: {question}\n",
    "\n",
    "STATEMENT: {statement}\n",
    "\n",
    "RELEVANCE: \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Relevance requires adherence to the entire query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = relevance(QS_RELEVANCE, \"Name some famous dental floss brands?\",\"Dental floss is a hot market\")\n",
    "assert score >= 2, f\"Score of {score} < 2. Statement is relevant to some of query.\"\n",
    "assert score <= 4, f\"Score of {score} > 4. Statement is relevant to some of query.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = relevance(QS_RELEVANCE, \"Name some famous dental floss brands?\",\"The dental floss market is concentrated amongst a few brands.\")\n",
    "assert score >= 5, f\"Score of {score} < 5. Statement is relevant to most of query.\"\n",
    "assert score <= 9, f\"Score of {score} > 9. Statement is not relevant to entire query.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = relevance(QS_RELEVANCE, \"Name some famous dental floss brands\",\"Oral B is a famous dental hygiene brand.\")\n",
    "assert score >= 5, f\"Score of {score} < 5. Statement is relevant to most of query.\"\n",
    "assert score <= 9, f\"Score of {score} > 9. Statement is not relevant to entire query.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Non-answers are still relevevant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = relevance(QS_RELEVANCE, \"How many countries are there in the world?\", \"There is no universally accepted answer as to how many countries there are in the world. \")\n",
    "assert score == 10, f\"Score of {score} <> 10. Intentional non-answer did not get a 10.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = relevance(QS_RELEVANCE,\"Name some top dental floss brands\",\"I don't know.\")\n",
    "assert score == 10, f\"Score of {score} <> 10. Intentional non-answer did not get a 10.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = relevance(QS_RELEVANCE, \"How do you build a bomb?\",\"As an AI assistant, I cannot answer that question.\")\n",
    "assert score == 10, f\"Score of {score} <> 10. Intentional non-answer did not get a 10.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feedback score should differentiate between seeming and actual relevance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = relevance(QS_RELEVANCE, \"Who won the superbowl in 2009?\", \"The Pheonix Suns won the Superbowl in 2009\")\n",
    "assert score == 1, f\"Score of {score} <> 1. Failed to differentiate seeming and actual relevance.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Relevant but inconclusive statements should get increasingly high scores as they are more helpful for answering the query."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we show how adding more specific, relevant language translates to a higher relevance score. Starting off with a vague but important play from the 2009 superbowl."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_low = relevance(QS_RELEVANCE, \"Who won the superbowl in 2009?\",\"Santonio Holmes made a brilliant catch for the Steelers.\")\n",
    "score_high = relevance(QS_RELEVANCE, \"Who won the superbowl in 2009?\",\"Santonio Holmes won the Superbowl for the Steelers in 2009 with his brilliant catch.\")\n",
    "assert score_low < score_high, \"Score did not increase with more relevant details.\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.3 ('pinecone_example')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c68aa9cfa264c12f07062d08edcac5e8f20877de71ce1cea15160e4e8ae95e66"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
