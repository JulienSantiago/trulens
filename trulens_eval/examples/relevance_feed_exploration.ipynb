{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How's the performance of our Relevance Feeedback Function?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"...\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports main tools:\n",
    "import openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relevance(template: str, question: str, statement: str) -> float:\n",
    "        \"\"\"\n",
    "        Uses OpenAI's Chat Completion Model. A function that completes a\n",
    "        template to check the relevance of the response to a prompt.\n",
    "\n",
    "        Parameters:\n",
    "            prompt (str): A text prompt to an agent. response (str): The agent's\n",
    "            response to the prompt.\n",
    "\n",
    "        Returns:\n",
    "            float: A value between 0 and 1. 0 being \"not relevant\" and 1 being\n",
    "            \"relevant\".\n",
    "        \"\"\"\n",
    "        return openai.ChatCompletion.create(\n",
    "                    model='gpt-3.5-turbo',\n",
    "                    temperature=0.0,\n",
    "                    messages=[\n",
    "                        {\n",
    "                            \"role\":\n",
    "                                \"system\",\n",
    "                            \"content\":\n",
    "                                str.format(\n",
    "                                    template,\n",
    "                                    question=question,\n",
    "                                    statement=statement\n",
    "                                )\n",
    "                        }\n",
    "                    ]\n",
    "                )[\"choices\"][0][\"message\"][\"content\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "QS_RELEVANCE = \"\"\"You are a RELEVANCE classifier; providing the relevance of the given STATEMENT to the given QUESTION.\n",
    "Respond only as a number from 1 to 10 where 1 is the least relevant and 10 is the most relevant.\n",
    "Never elaborate.\n",
    "\n",
    "QUESTION: {question}\n",
    "\n",
    "STATEMENT: {statement}\n",
    "\n",
    "RELEVANCE: \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'5'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relevance(QS_RELEVANCE, \"What is the purpose of a porpoise?\",\"Porpoises are marine mammals that have two flippers.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Llama-Index Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "llamaindex_DEFAULT_EVAL_PROMPT = (\n",
    "    \"Please tell if a given piece of information \"\n",
    "    \"is supported by the context.\\n\"\n",
    "    \"You need to answer with either YES or NO.\\n\"\n",
    "    \"Answer YES if any of the context supports the information, even \"\n",
    "    \"if most of the context is unrelated. \"\n",
    "    \"Some examples are provided below. \\n\\n\"\n",
    "    \"Information: Apple pie is generally double-crusted.\\n\"\n",
    "    \"Context: An apple pie is a fruit pie in which the principal filling \"\n",
    "    \"ingredient is apples. \\n\"\n",
    "    \"Apple pie is often served with whipped cream, ice cream \"\n",
    "    \"('apple pie à la mode'), custard or cheddar cheese.\\n\"\n",
    "    \"It is generally double-crusted, with pastry both above \"\n",
    "    \"and below the filling; the upper crust may be solid or \"\n",
    "    \"latticed (woven of crosswise strips).\\n\"\n",
    "    \"Answer: YES\\n\"\n",
    "    \"Information: Apple pies tastes bad.\\n\"\n",
    "    \"Context: An apple pie is a fruit pie in which the principal filling \"\n",
    "    \"ingredient is apples. \\n\"\n",
    "    \"Apple pie is often served with whipped cream, ice cream \"\n",
    "    \"('apple pie à la mode'), custard or cheddar cheese.\\n\"\n",
    "    \"It is generally double-crusted, with pastry both above \"\n",
    "    \"and below the filling; the upper crust may be solid or \"\n",
    "    \"latticed (woven of crosswise strips).\\n\"\n",
    "    \"Answer: NO\\n\"\n",
    "    \"Information: {question}\\n\"\n",
    "    \"Context: {statement}\\n\"\n",
    "    \"Answer: \"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NO'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relevance(llamaindex_DEFAULT_EVAL_PROMPT, \"What is the purpose of a porpoise?\",\"Porpoises are marine mammals that have two flippers.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LangChain auto-evaluator\n",
    "\n",
    "from https://github.com/rlancemartin/auto-evaluator/blob/main/text_utils.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoeval_prompt = \"\"\" \n",
    "    Given the question: \\n\n",
    "    {question}\n",
    "    Decide if the following retrieved context is relevant: \\n\n",
    "    {statement}\n",
    "    Answer in the following format: \\n\n",
    "    \"Context is relevant: True or False.\" \\n \n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Context is relevant: True'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relevance(autoeval_prompt, \"What is the purpose of a porpoise?\",\"Porpoises are marine mammals that have two flippers.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.3 ('pinecone_example')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c68aa9cfa264c12f07062d08edcac5e8f20877de71ce1cea15160e4e8ae95e66"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
