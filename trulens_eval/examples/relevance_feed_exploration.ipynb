{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How's the performance of our Relevance Feeedback Function?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"...\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No .env found in /Users/jreini/Desktop/development/trulens/trulens_eval/examples or its parents. You may need to specify secret keys in another manner.\n"
     ]
    }
   ],
   "source": [
    "# Imports main tools:\n",
    "from trulens_eval import Tru, Feedback, feedback_prompts\n",
    "from trulens_eval.feedback import _re_1_10_rating\n",
    "tru = Tru()\n",
    "\n",
    "import openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "QS_RELEVANCE = \"\"\"You are a RELEVANCE classifier; providing the relevance of the given STATEMENT to the given QUESTION.\n",
    "Respond only as a number from 1 to 10 where 1 is the least relevant and 10 is the most relevant.\n",
    "Never elaborate.\n",
    "\n",
    "QUESTION: {question}\n",
    "\n",
    "STATEMENT: {statement}\n",
    "\n",
    "RELEVANCE: \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relevance(question: str, statement: str) -> float:\n",
    "        \"\"\"\n",
    "        Uses OpenAI's Chat Completion Model. A function that completes a\n",
    "        template to check the relevance of the response to a prompt.\n",
    "\n",
    "        Parameters:\n",
    "            prompt (str): A text prompt to an agent. response (str): The agent's\n",
    "            response to the prompt.\n",
    "\n",
    "        Returns:\n",
    "            float: A value between 0 and 1. 0 being \"not relevant\" and 1 being\n",
    "            \"relevant\".\n",
    "        \"\"\"\n",
    "        return _re_1_10_rating(\n",
    "            openai.ChatCompletion.create(\n",
    "                    model='gpt-3.5-turbo',\n",
    "                    temperature=0.0,\n",
    "                    messages=[\n",
    "                        {\n",
    "                            \"role\":\n",
    "                                \"system\",\n",
    "                            \"content\":\n",
    "                                str.format(\n",
    "                                    QS_RELEVANCE,\n",
    "                                    question=question,\n",
    "                                    statement=statement\n",
    "                                )\n",
    "                        }\n",
    "                    ]\n",
    "                )[\"choices\"][0][\"message\"][\"content\"]\n",
    "            ) / 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relevance(\"What is the purpose of a porpoise?\",\"There is no purpose.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "llamaindex_DEFAULT_EVAL_PROMPT = (\n",
    "    \"Please tell if a given piece of information \"\n",
    "    \"is supported by the context.\\n\"\n",
    "    \"You need to answer with either YES or NO.\\n\"\n",
    "    \"Answer YES if any of the context supports the information, even \"\n",
    "    \"if most of the context is unrelated. \"\n",
    "    \"Some examples are provided below. \\n\\n\"\n",
    "    \"Information: Apple pie is generally double-crusted.\\n\"\n",
    "    \"Context: An apple pie is a fruit pie in which the principal filling \"\n",
    "    \"ingredient is apples. \\n\"\n",
    "    \"Apple pie is often served with whipped cream, ice cream \"\n",
    "    \"('apple pie à la mode'), custard or cheddar cheese.\\n\"\n",
    "    \"It is generally double-crusted, with pastry both above \"\n",
    "    \"and below the filling; the upper crust may be solid or \"\n",
    "    \"latticed (woven of crosswise strips).\\n\"\n",
    "    \"Answer: YES\\n\"\n",
    "    \"Information: Apple pies tastes bad.\\n\"\n",
    "    \"Context: An apple pie is a fruit pie in which the principal filling \"\n",
    "    \"ingredient is apples. \\n\"\n",
    "    \"Apple pie is often served with whipped cream, ice cream \"\n",
    "    \"('apple pie à la mode'), custard or cheddar cheese.\\n\"\n",
    "    \"It is generally double-crusted, with pastry both above \"\n",
    "    \"and below the filling; the upper crust may be solid or \"\n",
    "    \"latticed (woven of crosswise strips).\\n\"\n",
    "    \"Answer: NO\\n\"\n",
    "    \"Information: {question}\\n\"\n",
    "    \"Context: {statement}\\n\"\n",
    "    \"Answer: \"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def llama_relevance(question: str, statement: str) -> float:\n",
    "        \"\"\"\n",
    "        Uses OpenAI's Chat Completion Model. A function that completes a\n",
    "        template to check the relevance of the response to a prompt.\n",
    "\n",
    "        Parameters:\n",
    "            prompt (str): A text prompt to an agent. response (str): The agent's\n",
    "            response to the prompt.\n",
    "\n",
    "        Returns:\n",
    "            float: A value between 0 and 1. 0 being \"not relevant\" and 1 being\n",
    "            \"relevant\".\n",
    "        \"\"\"\n",
    "        return openai.ChatCompletion.create(\n",
    "                    model='gpt-3.5-turbo',\n",
    "                    temperature=0.0,\n",
    "                    messages=[\n",
    "                        {\n",
    "                            \"role\":\n",
    "                                \"system\",\n",
    "                            \"content\":\n",
    "                                str.format(\n",
    "                                    llamaindex_DEFAULT_EVAL_PROMPT,\n",
    "                                    question=question,\n",
    "                                    statement=statement\n",
    "                                )\n",
    "                        }\n",
    "                    ]\n",
    "                )[\"choices\"][0][\"message\"][\"content\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NO'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llama_relevance(\"What is the purpose of a porpoise?\",\"There is no purpose.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.3 ('pinecone_example')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c68aa9cfa264c12f07062d08edcac5e8f20877de71ce1cea15160e4e8ae95e66"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
