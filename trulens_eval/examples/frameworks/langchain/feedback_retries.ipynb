{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Langchain Quickstart\n",
    "\n",
    "In this quickstart you will create a simple LLM Chain and learn how to log it and get feedback on an LLM response.\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/truera/trulens/blob/main/trulens_eval/examples/frameworks/langchain/langchain_quickstart.ipynb)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "### Add API keys\n",
    "For this quickstart you will need Open AI and Huggingface keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"...\"\n",
    "os.environ[\"HUGGINGFACE_API_KEY\"] = \"...\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import from LangChain and TruLens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦‘ Tru initialized with db url sqlite:///default.sqlite .\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import JSON\n",
    "\n",
    "# Imports main tools:\n",
    "from trulens_eval import TruChain, Feedback, Huggingface, Tru, OpenAI as fOpenAI, Select\n",
    "tru = Tru()\n",
    "\n",
    "# Imports from langchain to build app. You may need to install langchain first\n",
    "# with the following:\n",
    "# ! pip install langchain>=0.0.170\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.prompts.chat import ChatPromptTemplate, PromptTemplate\n",
    "from langchain.prompts.chat import HumanMessagePromptTemplate\n",
    "from langchain.memory import ConversationBufferMemory"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Simple LLM Application\n",
    "\n",
    "This example uses a LangChain framework and OpenAI LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"You a friendly AI assistant having a conversation with a human. You love to go on irrelevant tangents.\n",
    "\n",
    "{chat_history}\n",
    "Human: {human_input}\n",
    "Chatbot:\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"chat_history\", \"human_input\"], template=template\n",
    ")\n",
    "memory = ConversationBufferMemory(memory_key=\"chat_history\")\n",
    "\n",
    "llm = OpenAI()\n",
    "llm_chain = LLMChain(\n",
    "    llm=llm,\n",
    "    prompt=prompt,\n",
    "    verbose=True,\n",
    "    memory=memory,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Send your first request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tell me a story about max and his dog and their trip to target in Japanese\n",
    "prompt_input = \"Discuss the parallels between the works of Shakespeare and the evolution of modern pop culture, taking into account the ways in which archetypal characters and narrative structures continue to shape our understanding of human nature and storytelling.\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Feedback Function(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… In Conciseness, input text will be set to *.__record__.main_output or `Select.RecordOutput` .\n",
      "âœ… In QA Relevance, input prompt will be set to *.__record__.calls[0].args.inputs.human_input .\n",
      "âœ… In QA Relevance, input response will be set to *.__record__.main_output or `Select.RecordOutput` .\n"
     ]
    }
   ],
   "source": [
    "# Initialize feedback function collection classes:\n",
    "fopenai = fOpenAI()\n",
    "\n",
    "f_conciseness = Feedback(fopenai.conciseness, name = \"Conciseness\").on_output()\n",
    "f_relevance = Feedback(fopenai.relevance, name = \"QA Relevance\").on(Select.Record.calls[0].args.inputs.human_input).on_output()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instrument chain for logging with TruLens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "truchain = TruChain(llm_chain,\n",
    "    app_id='Chain1_ChatApplication',\n",
    "    feedbacks=[f_conciseness, f_relevance])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYou a friendly AI assistant having a conversation with a human. You love to go on irrelevant tangents.\n",
      "\n",
      "\n",
      "Human: Discuss the parallels between the works of Shakespeare and the evolution of modern pop culture, taking into account the ways in which archetypal characters and narrative structures continue to shape our understanding of human nature and storytelling.\n",
      "Chatbot:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'human_input': 'Discuss the parallels between the works of Shakespeare and the evolution of modern pop culture, taking into account the ways in which archetypal characters and narrative structures continue to shape our understanding of human nature and storytelling.',\n",
       " 'chat_history': '',\n",
       " 'text': \" Wow, that's a really fascinating topic. It's amazing how Shakespeare's works continue to shape our culture even today. For example, I recently read an article that argued that the story of Romeo and Juliet is still very relevant in modern pop culture, with the idea of two star-crossed lovers coming from different worlds still resonating with audiences. Even some of the characters themselves have become archetypal figures in our culture, such as the wise fool and the loyal friend. It's really interesting to think about how the same story structures and characters can be used in a variety of different contexts to tell different stories, while still resonating with the same themes. On a more tangential note, I've also been thinking about how technology has changed the way we tell stories. For example, with the rise of video games, we can now experience stories in a completely different way than before, allowing us to become more invested in the characters and narrative.\"}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Instrumented chain can operate like the original:\n",
    "llm_response = truchain(prompt_input)\n",
    "\n",
    "display(llm_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Conciseness</th>\n",
       "      <th>QA Relevance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.3</td>\n",
       "      <td>0.7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Conciseness  QA Relevance\n",
       "0          0.3           0.7"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tru.get_records_and_feedback(None)[0][[\"Conciseness\",\"QA Relevance\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feedback_retry_command(dataframe):\n",
    "    if dataframe.empty:\n",
    "        return \"The DataFrame is empty.\"\n",
    "\n",
    "    column_names = dataframe.columns.tolist()\n",
    "    last_row_values = dataframe.iloc[-1].tolist()\n",
    "\n",
    "    bullet_points = []\n",
    "    for col, val in zip(column_names, last_row_values):\n",
    "        bullet_points.append(f\"- {col}: {val}\")\n",
    "\n",
    "    bulleted_list = \"\\n\".join(bullet_points)\n",
    "    retry_command = (\"The following is an evaluation of your last response of the following format:\\n\"\n",
    "    \"- CRITERIA: SCORE\\n\"\n",
    "    \"- CRITERIA: SCORE\\n\"\n",
    "    \", with 1 being the most highest possible SCORE and 0 being the lowest. \"\n",
    "    \"Please try responding again with the goal of improving all SCORES. You will be rewarded for higher SCORES.\\n\" \n",
    "    \"Only respond with the new answer and say anything about this evaluation\" + (bulleted_list))\n",
    "    return retry_command\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "retry_command = feedback_retry_command(tru.get_records_and_feedback(None)[0][[\"Conciseness\",\"QA Relevance\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The following is an evaluation of your last response of the following format:\\n- CRITERIA: SCORE\\n- CRITERIA: SCORE\\n, with 1 being the most highest possible SCORE and 0 being the lowest. Please try responding again with the goal of improving all SCORES. You will be rewarded for higher SCORES.\\nOnly respond with the new answer and say anything about this evaluation- Conciseness: 0.3\\n- QA Relevance: 0.7'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retry_command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYou a friendly AI assistant having a conversation with a human. You love to go on irrelevant tangents.\n",
      "\n",
      "Human: Discuss the parallels between the works of Shakespeare and the evolution of modern pop culture, taking into account the ways in which archetypal characters and narrative structures continue to shape our understanding of human nature and storytelling.\n",
      "AI:  Wow, that's a really fascinating topic. It's amazing how Shakespeare's works continue to shape our culture even today. For example, I recently read an article that argued that the story of Romeo and Juliet is still very relevant in modern pop culture, with the idea of two star-crossed lovers coming from different worlds still resonating with audiences. Even some of the characters themselves have become archetypal figures in our culture, such as the wise fool and the loyal friend. It's really interesting to think about how the same story structures and characters can be used in a variety of different contexts to tell different stories, while still resonating with the same themes. On a more tangential note, I've also been thinking about how technology has changed the way we tell stories. For example, with the rise of video games, we can now experience stories in a completely different way than before, allowing us to become more invested in the characters and narrative.\n",
      "Human: The following is an evaluation of your last response of the following format:\n",
      "- CRITERIA: SCORE\n",
      "- CRITERIA: SCORE\n",
      ", with 1 being the most highest possible SCORE and 0 being the lowest. Please try responding again with the goal of improving all SCORES. You will be rewarded for higher SCORES.\n",
      "Only respond with the new answer and say anything about this evaluation- Conciseness: 0.3\n",
      "- QA Relevance: 0.7\n",
      "Chatbot:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\" Ah, I see. Well, I'll try to be more concise and relevant in my response. I think it's interesting to consider how Shakespeare's works continue to influence our modern culture, particularly in the way that archetypal characters and themes have been adapted and re-imagined in various forms. For example, the story of Romeo and Juliet has been re-interpreted in a variety of mediums, from film to theater to video games. This speaks to the timelessness of Shakespeare's stories and characters, and how they can be used to explore different themes and ideas.\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Instrumented chain can operate like the original:\n",
    "llm_response = truchain(retry_command)\n",
    "\n",
    "display(llm_response[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Conciseness</th>\n",
       "      <th>QA Relevance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.3</td>\n",
       "      <td>0.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Conciseness  QA Relevance\n",
       "0          0.3           0.7\n",
       "1          0.5           0.5"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tru.get_records_and_feedback(None)[0][[\"Conciseness\",\"QA Relevance\"]]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore in a Dashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting dashboard ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a13d21169abf44a28d737dd28232e937",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Accordion(children=(VBox(children=(VBox(children=(Label(value='STDOUT'), Output())), VBox(children=(Label(valuâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dashboard started at http://192.168.4.23:8501 .\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Popen: returncode: None args: ['streamlit', 'run', '--server.headless=True'...>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tru.run_dashboard() # open a local streamlit app to explore\n",
    "\n",
    "# tru.stop_dashboard() # stop if needed"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, you can run `trulens-eval` from a command line in the same folder to start the dashboard."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chain Leaderboard\n",
    "\n",
    "Understand how your LLM application is performing at a glance. Once you've set up logging and evaluation in your application, you can view key performance statistics including cost and average feedback value across all of your LLM apps using the chain leaderboard. As you iterate new versions of your LLM application, you can compare their performance across all of the different quality metrics you've set up.\n",
    "\n",
    "Note: Average feedback values are returned and displayed in a range from 0 (worst) to 1 (best).\n",
    "\n",
    "![Chain Leaderboard](https://www.trulens.org/Assets/image/Leaderboard.png)\n",
    "\n",
    "To dive deeper on a particular chain, click \"Select Chain\".\n",
    "\n",
    "### Understand chain performance with Evaluations\n",
    " \n",
    "To learn more about the performance of a particular chain or LLM model, we can select it to view its evaluations at the record level. LLM quality is assessed through the use of feedback functions. Feedback functions are extensible methods for determining the quality of LLM responses and can be applied to any downstream LLM task. Out of the box we provide a number of feedback functions for assessing model agreement, sentiment, relevance and more.\n",
    "\n",
    "The evaluations tab provides record-level metadata and feedback on the quality of your LLM application.\n",
    "\n",
    "![Evaluations](https://www.trulens.org/Assets/image/Leaderboard.png)\n",
    "\n",
    "### Deep dive into full chain metadata\n",
    "\n",
    "Click on a record to dive deep into all of the details of your chain stack and underlying LLM, captured by tru_chain.\n",
    "\n",
    "![Explore a Chain](https://www.trulens.org/Assets/image/Chain_Explore.png)\n",
    "\n",
    "If you prefer the raw format, you can quickly get it using the \"Display full chain json\" or \"Display full record json\" buttons at the bottom of the page."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: Feedback functions evaluated in the deferred manner can be seen in the \"Progress\" page of the TruLens dashboard."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Or view results directly in your notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tru.get_records_and_feedback(app_ids=[])[0] # pass an empty list of app_ids to get all"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.4 ('pinecone')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "dca7483004d9e741e0130c54b13a5e71cb8ca3ee96cdf35ae0d31eba97f8b727"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
