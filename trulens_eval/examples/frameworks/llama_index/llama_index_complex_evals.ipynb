{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "X6-q-gTUaZU7"
      },
      "source": [
        "# Advanced Querying & Evaluation\n",
        "\n",
        "More complex embedding creations and query engines can be useful for complex queries in many domains, such as science and medicine. Using TruLens, we can create fine-grained evaluations for these complex queries. In this example, you'll also learn how to use deferred mode for evalations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RTSzmVFIaffU",
        "outputId": "0fe81f4b-80c5-4811-fba3-49c45cac2d90"
      },
      "outputs": [],
      "source": [
        "#!pip install trulens-eval==0.12.0 llama-index==0.8.29post1 sentence-transformers transformers pypdf"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "SnwSnBkSaZU8"
      },
      "source": [
        "## Query Engine Construction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IBfdyn3MaZU9"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import openai\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"...\"\n",
        "openai.api_key = os.environ[\"OPENAI_API_KEY\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bh43sV1eaZU9",
        "outputId": "f356b401-d4c2-4496-da7c-9fb9fe4c9b6a"
      },
      "outputs": [],
      "source": [
        "!curl https://www.ipcc.ch/report/ar6/wg2/downloads/report/IPCC_AR6_WGII_Chapter03.pdf --output IPCC_AR6_WGII_Chapter03.pdf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wMvq1q8yaZU-"
      },
      "outputs": [],
      "source": [
        "from llama_index import SimpleDirectoryReader\n",
        "\n",
        "documents = SimpleDirectoryReader(\n",
        "    input_files=[\"./IPCC_AR6_WGII_Chapter03.pdf\"]\n",
        ").load_data()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sY8Oui4taZU-"
      },
      "outputs": [],
      "source": [
        "# Merge into a single large document rather than one document per-page\n",
        "from llama_index import Document\n",
        "\n",
        "document = Document(text=\"\\n\\n\".join([doc.text for doc in documents]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MkbaDRJCaZU_"
      },
      "outputs": [],
      "source": [
        "from llama_index import ServiceContext\n",
        "from llama_index.llms import OpenAI\n",
        "from llama_index.node_parser import SentenceWindowNodeParser\n",
        "\n",
        "# create the sentence window node parser w/ default settings\n",
        "node_parser = SentenceWindowNodeParser.from_defaults(\n",
        "    window_size=3,\n",
        "    window_metadata_key=\"window\",\n",
        "    original_text_metadata_key=\"original_text\",\n",
        ")\n",
        "\n",
        "llm = OpenAI(model=\"gpt-3.5-turbo\", temperature=0.1)\n",
        "sentence_context = ServiceContext.from_defaults(\n",
        "    llm=llm,\n",
        "    embed_model=\"local:BAAI/bge-small-en-v1.5\",\n",
        "    node_parser=node_parser,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JQPRoF21aZU_"
      },
      "outputs": [],
      "source": [
        "from llama_index import VectorStoreIndex\n",
        "\n",
        "sentence_index = VectorStoreIndex.from_documents(\n",
        "    [document], service_context=sentence_context\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RAERQ_BeaZU_"
      },
      "outputs": [],
      "source": [
        "from llama_index.indices.postprocessor import (\n",
        "    MetadataReplacementPostProcessor,\n",
        "    SentenceTransformerRerank,\n",
        ")\n",
        "\n",
        "sentence_window_engine = sentence_index.as_query_engine(\n",
        "    similarity_top_k=6,\n",
        "    # the target key defaults to `window` to match the node_parser's default\n",
        "    node_postprocessors=[\n",
        "        MetadataReplacementPostProcessor(target_metadata_key=\"window\"),\n",
        "        SentenceTransformerRerank(top_n=2, model=\"BAAI/bge-reranker-base\"),\n",
        "    ],\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PCsOz-3ZaZVB"
      },
      "outputs": [],
      "source": [
        "from llama_index.tools import QueryEngineTool, ToolMetadata\n",
        "from llama_index.query_engine import SubQuestionQueryEngine\n",
        "\n",
        "sentence_sub_engine = SubQuestionQueryEngine.from_defaults(\n",
        "  [QueryEngineTool(\n",
        "    query_engine=sentence_window_engine,\n",
        "    metadata=ToolMetadata(name=\"climate_report\", description=\"Climate Report on Oceans.\")\n",
        "  )],\n",
        "  service_context=sentence_context,\n",
        "  verbose=False,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5KqV-IbQaZVB"
      },
      "outputs": [],
      "source": [
        "import nest_asyncio\n",
        "nest_asyncio.apply()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "gZyCMs25aZVB"
      },
      "source": [
        "## Custom Eval Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SN3k1zsXaZVB",
        "outputId": "41797123-8028-4165-d700-6b4a1a60f308"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "from trulens_eval import Feedback, OpenAI, Tru, TruLlama, feedback, Select, FeedbackMode\n",
        "\n",
        "tru = Tru()\n",
        "\n",
        "tru.reset_database()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kXJBD4gfaZVC",
        "outputId": "b4ebd2f9-1768-47be-d0eb-8963f7076ecd"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Initialize Huggingface-based feedback function collection class:\n",
        "openai = feedback.OpenAI()\n",
        "\n",
        "# Helpfulness\n",
        "f_helpfulness = feedback = Feedback(openai.helpfulness).on_output() \n",
        "\n",
        "# Question/answer relevance between overall question and answer.\n",
        "f_qa_relevance = Feedback(openai.relevance).on_input_output()\n",
        "\n",
        "# Question/statement relevance between question and each context chunk.\n",
        "# The context is located in a different place for the sub questions so we need to define that feedback separately\n",
        "f_qs_relevance_subquestions = (\n",
        "    Feedback(openai.qs_relevance)\n",
        "    .on_input()\n",
        "    .on(Select.Record.calls[0].rets.source_nodes[:].node.text)\n",
        "    .aggregate(np.mean))\n",
        "\n",
        "f_qs_relevance = (\n",
        "    Feedback(openai.qs_relevance)\n",
        "    .on_input()\n",
        "    .on(Select.Record.calls[0].args.prompt_args.context_str)\n",
        "    .aggregate(np.mean))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KUDHInR-aZVC"
      },
      "outputs": [],
      "source": [
        "# We'll use the recorder in deferred mode so we can log all of the subquestions before starting eval.\n",
        "# This approach will give us smoother handling for the evals + more consistent logging at high volume.\n",
        "# In addition, for our two different qs relevance definitions, deferred mode can just take the one that evaluates.\n",
        "tru_recorder = TruLlama(\n",
        "    sentence_sub_engine,\n",
        "    app_id=\"CustomEvalTest\",\n",
        "    feedbacks=[f_qa_relevance, f_qs_relevance, f_qs_relevance_subquestions, f_helpfulness],\n",
        "    feedback_mode=FeedbackMode.DEFERRED\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dsA3ziw1aZVD"
      },
      "outputs": [],
      "source": [
        "questions = [\n",
        "  \"Based on the provided text, discuss the impact of human activities on the natural carbon dynamics of estuaries, shelf seas, and other intertidal and shallow-water habitats. Provide examples from the text to support your answer.\",\n",
        "  \"Analyze the combined effects of exploitation and multi-decadal climate fluctuations on global fisheries yields. How do these factors make it difficult to assess the impacts of global climate change on fisheries yields? Use specific examples from the text to support your analysis.\",\n",
        "  \"Based on the study by Gutiérrez-Rodríguez, A.G., et al., 2018, what potential benefits do seaweeds have in the field of medicine, specifically in relation to cancer treatment?\",\n",
        "  \"According to the research conducted by Haasnoot, M., et al., 2020, how does the uncertainty in Antarctic mass-loss impact the coastal adaptation strategy of the Netherlands?\",\n",
        "  \"Based on the context, explain how the decline in warm water coral reefs is projected to impact the services they provide to society, particularly in terms of coastal protection.\",\n",
        "  \"Tell me something about the intricacies of tying a tie.\",\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "01_P6TxaaZVD",
        "outputId": "4f03da5b-34a3-4d41-ee78-9c09bc97368e"
      },
      "outputs": [],
      "source": [
        "for question in questions:\n",
        "  with tru_recorder as recording:\n",
        "    sentence_sub_engine.query(question)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Yp4_e4faZVD",
        "outputId": "d2ba9d2d-7e2a-46d2-8459-41ba3778eba3"
      },
      "outputs": [],
      "source": [
        "tru.run_dashboard()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Before we start the evaluator, note that we've logged all of the records including the sub-questions. However we haven't completed any evals yet.\n",
        "\n",
        "Start the evaluator to generate the feedback results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "tru.start_evaluator()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "milvus",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
