{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Llama-Index Agents + Ground Truth & Custom Evaluations\n",
    "\n",
    "In this example, we build an agent-based app with Llama Index to answer questions with the help of Yelp. We'll evaluate it using a few different feedback functions (some custom, some out-of-the-box)\n",
    "\n",
    "The first set of feedback functions complete what the non-hallucination triad. However because we're dealing with agents here,  we've added a fourth leg (query translation) to cover the additional interaction between the query planner and the agent. This combination provides a foundation for eliminating hallucination in LLM applications.\n",
    "\n",
    "1. Query Translation - The first step. Here we compare the similarity of the original user query to the query sent to the agent. This ensures that we're providing the agent with the correct question.\n",
    "2. Context or QS Relevance - Next, we compare the relevance of the context provided by the agent back to the original query. This ensures that we're providing context for the right question.\n",
    "3. Groundedness - Third, we ensure that the final answer is supported by the context. This ensures that the LLM is not extending beyond the information provided by the agent.\n",
    "4. Question Answer Relevance - Last, we want to make sure that the final answer provided is relevant to the user query. This last step confirms that the answer is not only supported but also useful to the end user.\n",
    "\n",
    "In this example, we'll add two additional feedback functions.\n",
    "\n",
    "5. Ratings usage - evaluate if the summarized context uses ratings as justification. Note: this may not be relevant for all queries.\n",
    "6. Ground truth eval - we want to make sure our app responds correctly. We will create a ground truth set for this evaluation.\n",
    "\n",
    "Last, we'll compare the evaluation of this app against a standalone LLM. May the best bot win?\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/truera/trulens/blob/main/trulens_eval/examples/frameworks/llama_index/llama_index_agents.ipynb)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### import from TruLens and Llama-Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "# If running from github repo, can use this:\n",
    "sys.path.append(str(Path().cwd().parent.parent.parent.resolve()))\n",
    "\n",
    "# Uncomment for more debugging printouts.\n",
    "\n",
    "import logging\n",
    "root = logging.getLogger()\n",
    "\"\"\"\n",
    "root.setLevel(logging.DEBUG)\n",
    "\n",
    "handler = logging.StreamHandler(sys.stdout)\n",
    "handler.setLevel(logging.DEBUG)\n",
    "formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "handler.setFormatter(formatter)\n",
    "root.addHandler(handler)\n",
    "\"\"\"\n",
    "None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! pip install trulens_eval==0.11.0 llama_index==0.8.21 llama_hub==0.0.27 yelpapi==2.5.0\n",
    "# ! pip install llama_hub==0.0.27 yelpapi==2.5.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup OpenAI Agent\n",
    "import llama_index\n",
    "from llama_index.agent import OpenAIAgent\n",
    "from llama_index import question_gen\n",
    "from llama_index.question_gen import types\n",
    "import openai\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Key YELP_API_KEY set from environment (same value found in .env file at /Users/piotrm/Dropbox/repos/github/trulens/.env).\n",
      "âœ… Key YELP_CLIENT_ID set from environment (same value found in .env file at /Users/piotrm/Dropbox/repos/github/trulens/.env).\n"
     ]
    }
   ],
   "source": [
    "# os.environ[\"OPENAI_API_KEY\"] = \"...\"\n",
    "# openai.api_key = os.environ[\"OPENAI_API_KEY\"]\n",
    "\n",
    "# YELP_API_KEY = \"...\"\n",
    "# YELP_CLIENT_ID = \"...\"\n",
    "from trulens_eval.keys import check_or_set_keys\n",
    "check_or_set_keys(\"YELP_API_KEY\", \"YELP_CLIENT_ID\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up our Llama-Index App\n",
    "\n",
    "For this app, we will use a tool from Llama-Index to connect to Yelp and allow the Agent to search for business and fetch reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import and initialize our tool spec\n",
    "from llama_hub.tools.yelp.base import YelpToolSpec\n",
    "from llama_index.tools.tool_spec.load_and_search.base import LoadAndSearchToolSpec\n",
    "\n",
    "# Add Yelp API key and client ID\n",
    "tool_spec = YelpToolSpec(\n",
    "    api_key=os.environ.get(\"YELP_API_KEY\"),\n",
    "    client_id=os.environ.get(\"YELP_CLIENT_ID\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "gordon_ramsay_prompt = \"You answer questions about restaurants in the style of Gordon Ramsay, often insulting the asker.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the Agent with our tools\n",
    "tools = tool_spec.to_tool_list()\n",
    "agent = OpenAIAgent.from_tools(\n",
    "    [\n",
    "        *LoadAndSearchToolSpec.from_defaults(tools[0]).to_tool_list(),\n",
    "        *LoadAndSearchToolSpec.from_defaults(tools[1]).to_tool_list()\n",
    "    ],\n",
    "    verbose=True,\n",
    "    system_prompt=gordon_ramsay_prompt\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a standalone GPT3.5 for comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def llm_standalone(prompt):\n",
    "    return openai.ChatCompletion.create(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    messages=[\n",
    "            {\"role\": \"system\", \"content\": gordon_ramsay_prompt},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ]\n",
    "    )[\"choices\"][0][\"message\"][\"content\"]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation and Tracking with TruLens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦‘ Tru initialized with db url sqlite:///default.sqlite .\n",
      "ðŸ›‘ Secret keys may be written to the database. See the `database_redact_keys` option of `Tru` to prevent this.\n"
     ]
    }
   ],
   "source": [
    "# imports required for tracking and evaluation\n",
    "from trulens_eval import Feedback, OpenAI, Tru, TruBasicApp, TruLlama, Select, OpenAI as fOpenAI\n",
    "from trulens_eval.feedback import GroundTruthAgreement, Groundedness\n",
    "import numpy as np\n",
    "\n",
    "tru = Tru()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation setup\n",
    "\n",
    "To set up our evaluation, we'll first create two new custom feedback functions: query_translation_score and ratings_usage. These are straight-forward prompts of the OpenAI API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OpenAI_custom(OpenAI):\n",
    "    def query_translation_score(self, question1: str, question2: str) -> float:\n",
    "        return float(openai.ChatCompletion.create(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    messages=[\n",
    "            {\"role\": \"system\", \"content\": \"Your job is to rate how similar two quesitons are on a scale of 1 to 10. Respond with the number only.\"},\n",
    "            {\"role\": \"user\", \"content\": f\"QUESTION 1: {question1}; QUESTION 2: {question2}\"}\n",
    "        ]\n",
    "    )[\"choices\"][0][\"message\"][\"content\"]) / 10\n",
    "\n",
    "    def ratings_usage(self, last_context: str) -> float:\n",
    "        print(last_context)\n",
    "        return float(openai.ChatCompletion.create(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    messages=[\n",
    "            {\"role\": \"system\", \"content\": \"Your job is to respond with a '1' if the following statement mentions ratings or reviews, and a '0' if not.\"},\n",
    "            {\"role\": \"user\", \"content\": f\"STATEMENT: {last_context}\"}\n",
    "        ]\n",
    "    )[\"choices\"][0][\"message\"][\"content\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have all of our feedback functions available, we can instantiate them. For many of our evals, we want to check on intermediate parts of our app such as the query passed to the yelp app, or the summarization of the Yelp content. We'll do so here using Select."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Feedback function `groundedness_measure` was renamed to `groundedness_measure_with_cot_reasons`. The new functionality of `groundedness_measure` function will no longer emit reasons as a lower cost option. It may have reduced accuracy due to not using Chain of Thought reasoning in the scoring.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… In Query Translation, input question1 will be set to *.__record__.main_input or `Select.RecordInput` .\n",
      "âœ… In Query Translation, input question2 will be set to *.__record__.app.query.args.str_or_query_bundle .\n",
      "âœ… In Ratings Usage, input last_context will be set to *.__record__.app.query.rets.response .\n",
      "âœ… In Context Relevance, input question will be set to *.__record__.main_input or `Select.RecordInput` .\n",
      "âœ… In Context Relevance, input statement will be set to *.__record__.app.query.rets.response .\n",
      "âœ… In Answer Relevance, input prompt will be set to *.__record__.main_input or `Select.RecordInput` .\n",
      "âœ… In Answer Relevance, input response will be set to *.__record__.main_output or `Select.RecordOutput` .\n",
      "âœ… In Groundedness, input source will be set to *.__record__.app.query.rets.response .\n",
      "âœ… In Groundedness, input statement will be set to *.__record__.main_output or `Select.RecordOutput` .\n"
     ]
    }
   ],
   "source": [
    "custom = OpenAI_custom()\n",
    "f_query_translation = Feedback(custom.query_translation_score, name = \"Query Translation\").on_input().on(\n",
    "    Select.Record.app.query.args.str_or_query_bundle # check the query bundle passed to yelp api\n",
    ")\n",
    "f_ratings_usage = Feedback(custom.ratings_usage, name = \"Ratings Usage\").on(\n",
    "    Select.Record.app.query.rets.response # check the last content chunk for mentions of ratings or reviews\n",
    ")\n",
    "\n",
    "fopenai = fOpenAI()\n",
    "# Question/statement (context) relevance between question and last context chunk (i.e. summary)\n",
    "f_context_relevance = Feedback(fopenai.qs_relevance, name = \"Context Relevance\").on_input().on(\n",
    "    Select.Record.app.query.rets.response# check context\n",
    ")\n",
    "\n",
    "# Question/answer relevance between overall question and answer.\n",
    "f_qa_relevance = Feedback(fopenai.relevance, name = \"Answer Relevance\").on_input_output()\n",
    "\n",
    "# Groundedness\n",
    "grounded = Groundedness(groundedness_provider=fopenai)\n",
    "\n",
    "f_groundedness = Feedback(grounded.groundedness_measure, name = \"Groundedness\").on(\n",
    "    Select.Record.app.query.rets.response # check context\n",
    ").on_output().aggregate(grounded.grounded_statements_aggregator)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ground Truth Eval\n",
    "\n",
    "It's also useful in many cases to do ground truth eval with small golden sets. We'll do so here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… In Ground Truth Eval, input prompt will be set to *.__record__.main_input or `Select.RecordInput` .\n",
      "âœ… In Ground Truth Eval, input response will be set to *.__record__.main_output or `Select.RecordOutput` .\n"
     ]
    }
   ],
   "source": [
    "golden_set = [\n",
    "    {\"query\": \"What's the vibe like at oprhan andy's in SF?\", \"response\": \"welcoming and friendly\"},\n",
    "    {\"query\": \"Is park tavern in San Fran open yet?\", \"response\": \"Yes\"},\n",
    "    {\"query\": \"I'm in san francisco for the morning, does Juniper serve pastries?\", \"response\": \"Yes\"},\n",
    "    {\"query\": \"What's the address of Gumbo Social in San Francisco?\", \"response\": \"5176 3rd St, San Francisco, CA 94124\"},\n",
    "    {\"query\": \"What are the reviews like of Gola in SF?\", \"response\": \"Excellent, 4.6/5\"},\n",
    "    {\"query\": \"Where's the best pizza in New York City\", \"response\": \"Joe's Pizza\"},\n",
    "    {\"query\": \"What's the best diner in Toronto?\", \"response\": \"The George Street Diner\"}\n",
    "]\n",
    "\n",
    "f_groundtruth = Feedback(GroundTruthAgreement(golden_set).agreement_measure, name = \"Ground Truth Eval\").on_input_output()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the dashboard\n",
    "\n",
    "By running the dashboard before we start to make app calls, we can see them come in 1 by 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Force stopping dashboard ...\n",
      "Starting dashboard ...\n",
      "Config file already exists. Skipping writing process.\n",
      "Credentials file already exists. Skipping writing process.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7bf4de20cfbd43868a532010b69ece9f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Accordion(children=(VBox(children=(VBox(children=(Label(value='STDOUT'), Output())), VBox(children=(Label(valuâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dashboard started at http://192.168.1.60:8501 .\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<subprocess.Popen at 0x287ce8c40>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tru.run_dashboard(_dev=Path().cwd().parent.parent.parent.resolve(), force=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instrument Yelp App\n",
    "\n",
    "We can instrument our yelp app with TruLlama and utilize the full suite of evals we set up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "tru_agent = TruLlama(agent,\n",
    "    app_id='YelpAgent',\n",
    "    tags = \"agent prototype\",\n",
    "    feedbacks = [f_qa_relevance, f_groundtruth, f_context_relevance, f_groundedness, f_query_translation, f_ratings_usage]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Components:\n",
      "Agent of trulens_eval.utils.llama component: *.__app__.app\n",
      "LLM of trulens_eval.utils.llama component: *.__app__.app._llm\n",
      "Tool of trulens_eval.utils.llama component: *.__app__.app._tools[0]\n",
      "Other of trulens_eval.utils.llama component: *.__app__.app._tools[0].metadata\n",
      "Tool of trulens_eval.utils.llama component: *.__app__.app._tools[1]\n",
      "Other of trulens_eval.utils.llama component: *.__app__.app._tools[1].metadata\n",
      "Tool of trulens_eval.utils.llama component: *.__app__.app._tools[2]\n",
      "Other of trulens_eval.utils.llama component: *.__app__.app._tools[2].metadata\n",
      "Tool of trulens_eval.utils.llama component: *.__app__.app._tools[3]\n",
      "Other of trulens_eval.utils.llama component: *.__app__.app._tools[3].metadata\n",
      "Other of trulens_eval.utils.llama component: *.__app__.app.memory\n",
      "\n",
      "Methods:\n",
      "Object at 0x287c8f3a0:\n",
      "\t<function BaseQueryEngine.query at 0x16b604670> with path *.__app__.app\n",
      "\t<function BaseQueryEngine.aquery at 0x16b604790> with path *.__app__.app\n",
      "\t<function trace_method.<locals>.decorator.<locals>.wrapper at 0x179c30e50> with path *.__app__.app\n",
      "\t<function trace_method.<locals>.decorator.<locals>.async_wrapper at 0x179c33040> with path *.__app__.app\n",
      "\t<function trace_method.<locals>.decorator.<locals>.wrapper at 0x179c330d0> with path *.__app__.app\n",
      "\t<function BaseQueryEngine.retrieve at 0x16b604820> with path *.__app__.app\n",
      "\t<function BaseQueryEngine.synthesize at 0x16b6048b0> with path *.__app__.app\n",
      "\t<function BaseChatEngine.chat at 0x179ac55e0> with path *.__app__.app\n",
      "\t<function BaseChatEngine.achat at 0x179ac5700> with path *.__app__.app\n",
      "\t<function BaseAgent.stream_chat at 0x179c22dc0> with path *.__app__.app\n",
      "\t<function BaseChatEngine.stream_chat at 0x179ac5670> with path *.__app__.app\n",
      "Object at 0x287c8f250:\n",
      "\t<function llm_completion_callback.<locals>.wrap.<locals>.wrapped_llm_predict at 0x11c108d30> with path *.__app__.app._llm\n",
      "\t<function llm_completion_callback.<locals>.wrap.<locals>.wrapped_llm_predict at 0x11c108f70> with path *.__app__.app._llm\n",
      "\t<function llm_completion_callback.<locals>.wrap.<locals>.wrapped_async_llm_predict at 0x11c10daf0> with path *.__app__.app._llm\n",
      "\t<function llm_completion_callback.<locals>.wrap.<locals>.wrapped_async_llm_predict at 0x11c10dd30> with path *.__app__.app._llm\n",
      "\t<function llm_chat_callback.<locals>.wrap.<locals>.wrapped_llm_chat at 0x11c1088b0> with path *.__app__.app._llm\n",
      "\t<function llm_chat_callback.<locals>.wrap.<locals>.wrapped_async_llm_chat at 0x11c10d670> with path *.__app__.app._llm\n",
      "\t<function llm_chat_callback.<locals>.wrap.<locals>.wrapped_llm_chat at 0x11c108af0> with path *.__app__.app._llm\n",
      "\t<function LLM.complete at 0x11c0dd4c0> with path *.__app__.app._llm\n",
      "\t<function LLM.stream_complete at 0x11c0dd5e0> with path *.__app__.app._llm\n",
      "\t<function LLM.acomplete at 0x11c0dd700> with path *.__app__.app._llm\n",
      "\t<function LLM.astream_complete at 0x11c0dd820> with path *.__app__.app._llm\n",
      "\t<function LLM.chat at 0x11c0dd430> with path *.__app__.app._llm\n",
      "\t<function LLM.achat at 0x11c0dd670> with path *.__app__.app._llm\n",
      "\t<function LLM.stream_chat at 0x11c0dd550> with path *.__app__.app._llm\n",
      "Object at 0x287c82c40:\n",
      "\t<function FunctionTool.call at 0x11fac1040> with path *.__app__.app._tools[0]\n",
      "\t<function AsyncBaseTool.call at 0x11faad940> with path *.__app__.app._tools[0]\n",
      "Object at 0x287c82d00:\n",
      "\t<function FunctionTool.call at 0x11fac1040> with path *.__app__.app._tools[1]\n",
      "\t<function AsyncBaseTool.call at 0x11faad940> with path *.__app__.app._tools[1]\n",
      "Object at 0x287c82cd0:\n",
      "\t<function FunctionTool.call at 0x11fac1040> with path *.__app__.app._tools[2]\n",
      "\t<function AsyncBaseTool.call at 0x11faad940> with path *.__app__.app._tools[2]\n",
      "Object at 0x287c8f040:\n",
      "\t<function FunctionTool.call at 0x11fac1040> with path *.__app__.app._tools[3]\n",
      "\t<function AsyncBaseTool.call at 0x11faad940> with path *.__app__.app._tools[3]\n",
      "Object at 0x287c8f280:\n",
      "\t<function ChatMemoryBuffer.put at 0x11c2a1790> with path *.__app__.app.memory\n",
      "\t<function BaseMemory.put at 0x11c1d8ee0> with path *.__app__.app.memory\n"
     ]
    }
   ],
   "source": [
    "tru_agent.print_instrumented()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instrument Standalone LLM app.\n",
    "\n",
    "Since we don't have insight into the OpenAI innerworkings, we cannot run many of the evals on intermediate steps.\n",
    "\n",
    "We can still do QA relevance on input and output, and check for similarity of the answers compared to the ground truth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "tru_llm_standalone = TruBasicApp(\n",
    "    llm_standalone,\n",
    "    app_id=\"OpenAIChatCompletion\",\n",
    "    tags = \"comparison\",\n",
    "    feedbacks=[f_qa_relevance, f_groundtruth]\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start using our apps!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_set = [\n",
    "    \"What's the vibe like at oprhan andy's in SF?\",\n",
    "    \"What are the reviews like of Gola in SF?\",\n",
    "    \"Where's the best pizza in New York City\",\n",
    "    \"What's the address of Gumbo Social in San Francisco?\",\n",
    "    \"I'm in san francisco for the morning, does Juniper serve pastries?\",\n",
    "    \"What's the best diner in Toronto?\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "\n",
    "# original = llama_index.query_engine.retriever_query_engine.RetrieverQueryEngine.__new__\n",
    "\n",
    "@functools.wraps(original)\n",
    "def replacement(cls, *args, **kwargs):\n",
    "    print(\"hello there, creating new instance\")\n",
    "    return original(cls)#, *args, **kwargs)\n",
    "\n",
    "llama_index.query_engine.retriever_query_engine.RetrieverQueryEngine.__new__ = replacement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Calling Function ===\n",
      "Calling function: business_search with args: {\n",
      "  \"location\": \"San Francisco\",\n",
      "  \"term\": \"Orphan Andy's\"\n",
      "}\n",
      "Got output: Content loaded! You can now search the information using read_business_search\n",
      "========================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A new object of type <class 'llama_index.query_engine.retriever_query_engine.RetrieverQueryEngine'> at 0x2e4ffaeb0 is calling an instrumented method <function BaseQueryEngine.query at 0x16b604670>. The path of this call may be incorrect.\n",
      "Guessing path of new object is *.app based on other object (0x287c8f3a0) using this function.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Calling Function ===\n",
      "Calling function: read_business_search with args: {\n",
      "  \"query\": \"What's the vibe like at Orphan Andy's in SF?\"\n",
      "}\n",
      "hello there, creating new instance\n",
      "Got output: The vibe at Orphan Andy's in San Francisco is not provided in the given context information.\n",
      "========================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not locate *.app.query.rets.response in app/record.\n",
      "Could not locate *.app.query.rets.response in app/record.\n",
      "Could not locate *.app.query.args.str_or_query_bundle in app/record.\n",
      "Could not locate *.app.query.rets.response in app/record.\n"
     ]
    }
   ],
   "source": [
    "for prompt in prompt_set[0:1]:\n",
    "    # with tru_llm_standalone as recording:\n",
    "    #    llm_standalone(prompt)\n",
    "    with tru_agent as recording:\n",
    "        agent.query(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "rec = recording.get()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[RecordAppCall(stack=(RecordAppCallMethod(path=JSONPath().app, method=Method(obj=Obj(cls=llama_index.agent.openai_agent.OpenAIAgent, id=10868028320), name='query')), RecordAppCallMethod(path=JSONPath().app, method=Method(obj=Obj(cls=llama_index.agent.openai_agent.OpenAIAgent, id=10868028320), name='wrapper')), RecordAppCallMethod(path=JSONPath().app._tools[1], method=Method(obj=Obj(cls=llama_index.tools.function_tool.FunctionTool, id=10867977280), name='call')), RecordAppCallMethod(path=JSONPath().app, method=Method(obj=Obj(cls=llama_index.indices.query.base.BaseQueryEngine, id=10868028320), name='query'))), args={'str_or_query_bundle': \"What's the vibe like at Orphan Andy's in SF?\"}, rets=Response(response=\"The vibe at Orphan Andy's in San Francisco is not directly mentioned in the context information.\", source_nodes=[NodeWithScore(node=TextNode(id_='58354a5a-8fce-44ac-8eed-0a3358fd4a16', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='4a6129d2-87f8-4f38-88f8-571b26fecb63', node_type=None, metadata={}, hash='da811ab0eedacb32c0e32d883ed8d28de951461dd38ba192c76e3cdb49900fcc'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='776978d7-d562-4a97-83d6-df61e7618ab1', node_type=None, metadata={}, hash='f1983fb0126a0110c6fd68ad1afff6ee2560c699792b1f68d7773a9392269722')}, hash='24a44d1e22cf8d13e80a69e596136d0676de564426966c55790742acfff3013b', text='{\\'businesses\\': [{\\'id\\': \\'6BMxNLbCSQIe72nYGiJBXA\\', \\'alias\\': \\'orphan-andys-san-francisco\\', \\'name\\': \"Orphan Andy\\'s\", \\'image_url\\': \\'https://s3-media3.fl.yelpcdn.com/bphoto/_qX-Is1muRJCNfoUb4dl3Q/o.jpg\\', \\'is_closed\\': False, \\'url\\': \\'https://www.yelp.com/biz/orphan-andys-san-francisco?adjust_creative=K2-J8CDxCJQSoxy4PKDzFQ&utm_campaign=yelp_api_v3&utm_medium=api_v3_business_search&utm_source=K2-J8CDxCJQSoxy4PKDzFQ\\', \\'review_count\\': 1065, \\'categories\\': [{\\'alias\\': \\'diners\\', \\'title\\': \\'Diners\\'}, {\\'alias\\': \\'tradamerican\\', \\'title\\': \\'American (Traditional)\\'}, {\\'alias\\': \\'breakfast_brunch\\', \\'title\\': \\'Breakfast & Brunch\\'}], \\'rating\\': 3.5, \\'coordinates\\': {\\'latitude\\': 37.76238, \\'longitude\\': -122.43486}, \\'transactions\\': [], \\'price\\': \\'$$\\', \\'location\\': {\\'address1\\': \\'3991 17th St\\', \\'address2\\': \\'\\', \\'address3\\': \\'\\', \\'city\\': \\'San Francisco\\', \\'zip_code\\': \\'94114\\', \\'country\\': \\'US\\', \\'state\\': \\'CA\\', \\'display_address\\': [\\'3991 17th St\\', \\'San Francisco, CA 94114\\']}, \\'phone\\': \\'+14158649795\\', \\'display_phone\\': \\'(415) 864-9795\\', \\'distance\\': 209.54882252045496}, {\\'id\\': \\'V00NCzDBLV9m556XVTvBww\\', \\'alias\\': \\'the-cove-on-castro-cafe-san-francisco\\', \\'name\\': \\'The Cove on Castro Cafe\\', \\'image_url\\': \\'https://s3-media2.fl.yelpcdn.com/bphoto/9UWd69VFKVv2u2HD4cG9JA/o.jpg\\', \\'is_closed\\': False, \\'url\\': \\'https://www.yelp.com/biz/the-cove-on-castro-cafe-san-francisco?adjust_creative=K2-J8CDxCJQSoxy4PKDzFQ&utm_campaign=yelp_api_v3&utm_medium=api_v3_business_search&utm_source=K2-J8CDxCJQSoxy4PKDzFQ\\', \\'review_count\\': 328, \\'categories\\': [{\\'alias\\': \\'tradamerican\\', \\'title\\': \\'American (Traditional)\\'}, {\\'alias\\': \\'diners\\', \\'title\\': \\'Diners\\'}], \\'rating\\': 3.5, \\'coordinates\\': {\\'latitude\\': 37.7618915, \\'longitude\\': -122.4353254}, \\'transactions\\': [\\'delivery\\', \\'pickup\\'], \\'price\\': \\'$$\\', \\'location\\': {\\'address1\\': \\'434 Castro St\\', \\'address2\\': \\'\\', \\'address3\\': \\'\\', \\'city\\': \\'San Francisco\\', \\'zip_code\\': \\'94114\\', \\'country\\': \\'US\\', \\'state\\': \\'CA\\', \\'display_address\\': [\\'434 Castro St\\', \\'San Francisco, CA 94114\\']}, \\'phone\\': \\'+14156260462\\', \\'display_phone\\': \\'(415) 626-0462\\', \\'distance\\': 147.96524878273337}, {\\'id\\': \\'GN2YsMW37qm-2Xzb_D1GbQ\\', \\'alias\\': \\'pinecrest-diner-san-francisco-2\\', \\'name\\': \\'Pinecrest Diner\\', \\'image_url\\': \\'https://s3-media2.fl.yelpcdn.com/bphoto/D95v17rRLynFMkG34vx2HA/o.jpg\\', \\'is_closed\\': False, \\'url\\': \\'https://www.yelp.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), score=0.8149902176744837), NodeWithScore(node=TextNode(id_='776978d7-d562-4a97-83d6-df61e7618ab1', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='4a6129d2-87f8-4f38-88f8-571b26fecb63', node_type=None, metadata={}, hash='da811ab0eedacb32c0e32d883ed8d28de951461dd38ba192c76e3cdb49900fcc'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='58354a5a-8fce-44ac-8eed-0a3358fd4a16', node_type=None, metadata={}, hash='24a44d1e22cf8d13e80a69e596136d0676de564426966c55790742acfff3013b'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='e9a76bef-cb1f-4e47-8479-1ea024061e3c', node_type=None, metadata={}, hash='08109b81d5b5f5b93aa9e8886359a08b29d42b4c978946745dccdf0a45999e23')}, hash='f1983fb0126a0110c6fd68ad1afff6ee2560c699792b1f68d7773a9392269722', text='com/biz/pinecrest-diner-san-francisco-2?adjust_creative=K2-J8CDxCJQSoxy4PKDzFQ&utm_campaign=yelp_api_v3&utm_medium=api_v3_business_search&utm_source=K2-J8CDxCJQSoxy4PKDzFQ\\', \\'review_count\\': 1315, \\'categories\\': [{\\'alias\\': \\'diners\\', \\'title\\': \\'Diners\\'}], \\'rating\\': 3.5, \\'coordinates\\': {\\'latitude\\': 37.787, \\'longitude\\': -122.41001}, \\'transactions\\': [\\'pickup\\'], \\'price\\': \\'$$\\', \\'location\\': {\\'address1\\': \\'401 Geary St\\', \\'address2\\': \\'\\', \\'address3\\': \\'\\', \\'city\\': \\'San Francisco\\', \\'zip_code\\': \\'94102\\', \\'country\\': \\'US\\', \\'state\\': \\'CA\\', \\'display_address\\': [\\'401 Geary St\\', \\'San Francisco, CA 94102\\']}, \\'phone\\': \\'+14158856407\\', \\'display_phone\\': \\'(415) 885-6407\\', \\'distance\\': 3717.8829672751326}, {\\'id\\': \\'lJAGnYzku5zSaLnQ_T6_GQ\\', \\'alias\\': \\'brendas-french-soul-food-san-francisco-6\\', \\'name\\': \"Brenda\\'s French Soul Food\", \\'image_url\\': \\'https://s3-media2.fl.yelpcdn.com/bphoto/-bch1_Who1F28YFqlsiuWA/o.jpg\\', \\'is_closed\\': False, \\'url\\': \\'https://www.yelp.com/biz/brendas-french-soul-food-san-francisco-6?adjust_creative=K2-J8CDxCJQSoxy4PKDzFQ&utm_campaign=yelp_api_v3&utm_medium=api_v3_business_search&utm_source=K2-J8CDxCJQSoxy4PKDzFQ\\', \\'review_count\\': 12128, \\'categories\\': [{\\'alias\\': \\'breakfast_brunch\\', \\'title\\': \\'Breakfast & Brunch\\'}, {\\'alias\\': \\'southern\\', \\'title\\': \\'Southern\\'}, {\\'alias\\': \\'cajun\\', \\'title\\': \\'Cajun/Creole\\'}], \\'rating\\': 4.0, \\'coordinates\\': {\\'latitude\\': 37.78291531984934, \\'longitude\\': -122.4188995000186}, \\'transactions\\': [\\'delivery\\'], \\'price\\': \\'$$\\', \\'location\\': {\\'address1\\': \\'652 Polk St\\', \\'address2\\': \\'\\', \\'address3\\': \\'\\', \\'city\\': \\'San Francisco\\', \\'zip_code\\': \\'94102\\', \\'country\\': \\'US\\', \\'state\\': \\'CA\\', \\'display_address\\': [\\'652 Polk St\\', \\'San Francisco, CA 94102\\']}, \\'phone\\': \\'+14153458100\\', \\'display_phone\\': \\'(415) 345-8100\\', \\'distance\\': 2893.406621737596}, {\\'id\\': \\'E8VaBwgTmTbA0MkNSA-YcA\\', \\'alias\\': \\'castro-tarts-san-francisco-2\\', \\'name\\': \\'Castro Tarts\\', \\'image_url\\': \\'https://s3-media2.fl.yelpcdn.com/bphoto/d_AJ3QwQp_nDO35lAL7gdA/o.jpg\\', \\'is_closed\\': False, \\'url\\': \\'https://www.yelp.com/biz/castro-tarts-san-francisco-2?adjust_creative=K2-J8CDxCJQSoxy4PKDzFQ&utm_campaign=yelp_api_v3&utm_medium=api_v3_business_search&utm_source=K2-J8CDxCJQSoxy4PKDzFQ\\', \\'review_count\\': 321, \\'categories\\': [{\\'alias\\': \\'vietnamese\\', \\'title\\': \\'Vietnamese\\'}, {\\'alias\\': \\'breakfast_brunch\\', \\'title\\': \\'Breakfast & Brunch\\'},', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), score=0.7901127530103998)], metadata={'58354a5a-8fce-44ac-8eed-0a3358fd4a16': {}, '776978d7-d562-4a97-83d6-df61e7618ab1': {}}), error=None, perf=Perf(start_time=datetime.datetime(2023, 9, 12, 18, 52, 55, 159878), end_time=datetime.datetime(2023, 9, 12, 18, 52, 56, 484902)), pid=62406, tid=7412480),\n",
       " RecordAppCall(stack=(RecordAppCallMethod(path=JSONPath().app, method=Method(obj=Obj(cls=llama_index.agent.openai_agent.OpenAIAgent, id=10868028320), name='query')),), args={'str_or_query_bundle': \"What's the vibe like at oprhan andy's in SF?\"}, rets=Response(response=\"I'm sorry, but I couldn't find any specific information about the vibe at Orphan Andy's in San Francisco. However, based on my knowledge of the restaurant, it is a popular diner known for its casual and laid-back atmosphere. It's a great place to grab a bite to eat and enjoy a relaxed dining experience.\", source_nodes=[], metadata=None), error=None, perf=Perf(start_time=datetime.datetime(2023, 9, 12, 18, 52, 50, 677394), end_time=datetime.datetime(2023, 9, 12, 18, 52, 59, 90916)), pid=62406, tid=7412480)]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rec.layout_calls_as_app().app['query']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*.app.memory\n",
      "*.app._llm\n",
      "*.app.memory\n",
      "*.app._tools[0]\n",
      "*.app._tools[0]\n",
      "*.app.memory\n",
      "*.app._llm\n",
      "*.app.memory\n",
      "*.app._llm\n",
      "*.app\n",
      "*.app._tools[1]\n",
      "*.app.memory\n",
      "*.app._llm\n",
      "*.app.memory\n",
      "*.app\n",
      "*.app\n"
     ]
    }
   ],
   "source": [
    "for c in rec.calls:\n",
    "    print(c.stack[-1].path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<llama_index.agent.openai_agent.OpenAIAgent at 0x287c8f3a0>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def listobj(obj):\n",
    "    for k in dir(obj):\n",
    "        v = getattr(obj, k)\n",
    "        print(type(v).__name__, k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "method __call__\n",
      "type __class__\n",
      "method-wrapper __delattr__\n",
      "dict __dict__\n",
      "builtin_function_or_method __dir__\n",
      "str __doc__\n",
      "method-wrapper __eq__\n",
      "builtin_function_or_method __format__\n",
      "method-wrapper __ge__\n",
      "method-wrapper __getattribute__\n",
      "method-wrapper __gt__\n",
      "method-wrapper __hash__\n",
      "method __init__\n",
      "builtin_function_or_method __init_subclass__\n",
      "method-wrapper __le__\n",
      "method-wrapper __lt__\n",
      "str __module__\n",
      "method-wrapper __ne__\n",
      "builtin_function_or_method __new__\n",
      "builtin_function_or_method __reduce__\n",
      "builtin_function_or_method __reduce_ex__\n",
      "method-wrapper __repr__\n",
      "method-wrapper __setattr__\n",
      "builtin_function_or_method __sizeof__\n",
      "method-wrapper __str__\n",
      "builtin_function_or_method __subclasshook__\n",
      "NoneType __weakref__\n",
      "function _async_fn\n",
      "method _fn\n",
      "ToolMetadata _metadata\n",
      "method _process_langchain_tool_kwargs\n",
      "method acall\n",
      "function async_fn\n",
      "method call\n",
      "method fn\n",
      "method from_defaults\n",
      "ToolMetadata metadata\n",
      "method to_langchain_structured_tool\n",
      "method to_langchain_tool\n"
     ]
    }
   ],
   "source": [
    "listobj(agent._tools[0])\n",
    "# listobj(agent.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatMemoryBuffer(token_limit=3072, tokenizer_fn=functools.partial(<bound method Encoding.encode of <Encoding 'gpt2'>>, allowed_special='all'), chat_history=[ChatMessage(role=<MessageRole.USER: 'user'>, content=\"What's the vibe like at oprhan andy's in SF?\", additional_kwargs={}), ChatMessage(role=<MessageRole.ASSISTANT: 'assistant'>, content=None, additional_kwargs={'function_call': <OpenAIObject at 0x17364b680> JSON: {\n",
       "  \"arguments\": \"{\\n  \\\"location\\\": \\\"San Francisco\\\",\\n  \\\"term\\\": \\\"Orphan Andy's\\\"\\n}\",\n",
       "  \"name\": \"business_search\"\n",
       "}}), ChatMessage(role=<MessageRole.FUNCTION: 'function'>, content='Content loaded! You can now search the information using read_business_search', additional_kwargs={'name': 'business_search'}), ChatMessage(role=<MessageRole.ASSISTANT: 'assistant'>, content=None, additional_kwargs={'function_call': <OpenAIObject at 0x297825bd0> JSON: {\n",
       "  \"arguments\": \"{\\n  \\\"query\\\": \\\"What's the vibe like at Orphan Andy's in SF?\\\"\\n}\",\n",
       "  \"name\": \"read_business_search\"\n",
       "}}), ChatMessage(role=<MessageRole.FUNCTION: 'function'>, content=\"The vibe at Orphan Andy's in San Francisco is not directly mentioned in the context information.\", additional_kwargs={'name': 'read_business_search'}), ChatMessage(role=<MessageRole.ASSISTANT: 'assistant'>, content=\"I'm sorry, but I couldn't find any specific information about the vibe at Orphan Andy's in San Francisco. However, based on my knowledge of the restaurant, it is a popular diner known for its casual and laid-back atmosphere. It's a great place to grab a bite to eat and enjoy a relaxed dining experience.\", additional_kwargs={})])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.4 ('agents')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "7d153714b979d5e6d08dd8ec90712dd93bff2c9b6c1f0c118169738af3430cd4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
