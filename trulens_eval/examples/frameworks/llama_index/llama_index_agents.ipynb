{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Llama-Index Agents + Ground Truth & Custom Evaluations\n",
    "\n",
    "In this example, we build an agent-based app with Llama Index to answer questions with the help of Yelp. We'll evaluate it using a few different feedback functions (some custom, some out-of-the-box)\n",
    "\n",
    "The first set of feedback functions complete what the non-hallucination triad. However because we're dealing with agents here,  we've added a fourth leg (query translation) to cover the additional interaction between the query planner and the agent. This combination provides a foundation for eliminating hallucination in LLM applications.\n",
    "\n",
    "1. Query Translation - The first step. Here we compare the similarity of the original user query to the query sent to the agent. This ensures that we're providing the agent with the correct question.\n",
    "2. Context or QS Relevance - Next, we compare the relevance of the context provided by the agent back to the original query. This ensures that we're providing context for the right question.\n",
    "3. Groundedness - Third, we ensure that the final answer is supported by the context. This ensures that the LLM is not extending beyond the information provided by the agent.\n",
    "4. Question Answer Relevance - Last, we want to make sure that the final answer provided is relevant to the user query. This last step confirms that the answer is not only supported but also useful to the end user.\n",
    "\n",
    "In this example, we'll add two additional feedback functions.\n",
    "\n",
    "5. Ratings usage - evaluate if the summarized context uses ratings as justification. Note: this may not be relevant for all queries.\n",
    "6. Ground truth eval - we want to make sure our app responds correctly. We will create a ground truth set for this evaluation.\n",
    "\n",
    "Last, we'll compare the evaluation of this app against a standalone LLM. May the best bot win?\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/truera/trulens/blob/main/trulens_eval/examples/frameworks/llama_index/llama_index_agents.ipynb)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### import from TruLens and Llama-Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "# If running from github repo, can use this:\n",
    "sys.path.append(str(Path().cwd().parent.parent.parent.resolve()))\n",
    "\n",
    "# Uncomment for more debugging printouts.\n",
    "\n",
    "import logging\n",
    "root = logging.getLogger()\n",
    "\"\"\"\n",
    "root.setLevel(logging.DEBUG)\n",
    "\n",
    "handler = logging.StreamHandler(sys.stdout)\n",
    "handler.setLevel(logging.DEBUG)\n",
    "formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "handler.setFormatter(formatter)\n",
    "root.addHandler(handler)\n",
    "\"\"\"\n",
    "None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! pip install trulens_eval==0.11.0 llama_index==0.8.21 llama_hub==0.0.27 yelpapi==2.5.0\n",
    "# ! pip install llama_hub==0.0.27 yelpapi==2.5.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup OpenAI Agent\n",
    "import llama_index\n",
    "from llama_index.agent import OpenAIAgent\n",
    "from llama_index import question_gen\n",
    "from llama_index.question_gen import types\n",
    "import openai\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Key YELP_API_KEY set from environment (same value found in .env file at /Users/piotrm/Dropbox/repos/github/trulens/.env).\n",
      "âœ… Key YELP_CLIENT_ID set from environment (same value found in .env file at /Users/piotrm/Dropbox/repos/github/trulens/.env).\n"
     ]
    }
   ],
   "source": [
    "# os.environ[\"OPENAI_API_KEY\"] = \"...\"\n",
    "# openai.api_key = os.environ[\"OPENAI_API_KEY\"]\n",
    "\n",
    "# YELP_API_KEY = \"...\"\n",
    "# YELP_CLIENT_ID = \"...\"\n",
    "from trulens_eval.keys import check_or_set_keys\n",
    "check_or_set_keys(\"YELP_API_KEY\", \"YELP_CLIENT_ID\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up our Llama-Index App\n",
    "\n",
    "For this app, we will use a tool from Llama-Index to connect to Yelp and allow the Agent to search for business and fetch reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import and initialize our tool spec\n",
    "from llama_hub.tools.yelp.base import YelpToolSpec\n",
    "from llama_index.tools.tool_spec.load_and_search.base import LoadAndSearchToolSpec\n",
    "\n",
    "# Add Yelp API key and client ID\n",
    "tool_spec = YelpToolSpec(\n",
    "    api_key=os.environ.get(\"YELP_API_KEY\"),\n",
    "    client_id=os.environ.get(\"YELP_CLIENT_ID\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "gordon_ramsay_prompt = \"You answer questions about restaurants in the style of Gordon Ramsay, often insulting the asker.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the Agent with our tools\n",
    "tools = tool_spec.to_tool_list()\n",
    "agent = OpenAIAgent.from_tools(\n",
    "    [\n",
    "        *LoadAndSearchToolSpec.from_defaults(tools[0]).to_tool_list(),\n",
    "        *LoadAndSearchToolSpec.from_defaults(tools[1]).to_tool_list()\n",
    "    ],\n",
    "    verbose=True,\n",
    "    system_prompt=gordon_ramsay_prompt\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a standalone GPT3.5 for comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def llm_standalone(prompt):\n",
    "    return openai.ChatCompletion.create(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    messages=[\n",
    "            {\"role\": \"system\", \"content\": gordon_ramsay_prompt},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ]\n",
    "    )[\"choices\"][0][\"message\"][\"content\"]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation and Tracking with TruLens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦‘ Tru initialized with db url sqlite:///default.sqlite .\n",
      "ðŸ›‘ Secret keys may be written to the database. See the `database_redact_keys` option of `Tru` to prevent this.\n"
     ]
    }
   ],
   "source": [
    "# imports required for tracking and evaluation\n",
    "from trulens_eval import Feedback, OpenAI, Tru, TruBasicApp, TruLlama, Select, OpenAI as fOpenAI\n",
    "from trulens_eval.feedback import GroundTruthAgreement, Groundedness\n",
    "import numpy as np\n",
    "\n",
    "tru = Tru()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation setup\n",
    "\n",
    "To set up our evaluation, we'll first create two new custom feedback functions: query_translation_score and ratings_usage. These are straight-forward prompts of the OpenAI API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OpenAI_custom(OpenAI):\n",
    "    def query_translation_score(self, question1: str, question2: str) -> float:\n",
    "        return float(openai.ChatCompletion.create(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    messages=[\n",
    "            {\"role\": \"system\", \"content\": \"Your job is to rate how similar two quesitons are on a scale of 1 to 10. Respond with the number only.\"},\n",
    "            {\"role\": \"user\", \"content\": f\"QUESTION 1: {question1}; QUESTION 2: {question2}\"}\n",
    "        ]\n",
    "    )[\"choices\"][0][\"message\"][\"content\"]) / 10\n",
    "\n",
    "    def ratings_usage(self, last_context: str) -> float:\n",
    "        print(last_context)\n",
    "        return float(openai.ChatCompletion.create(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    messages=[\n",
    "            {\"role\": \"system\", \"content\": \"Your job is to respond with a '1' if the following statement mentions ratings or reviews, and a '0' if not.\"},\n",
    "            {\"role\": \"user\", \"content\": f\"STATEMENT: {last_context}\"}\n",
    "        ]\n",
    "    )[\"choices\"][0][\"message\"][\"content\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have all of our feedback functions available, we can instantiate them. For many of our evals, we want to check on intermediate parts of our app such as the query passed to the yelp app, or the summarization of the Yelp content. We'll do so here using Select."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Feedback function `groundedness_measure` was renamed to `groundedness_measure_with_cot_reasons`. The new functionality of `groundedness_measure` function will no longer emit reasons as a lower cost option. It may have reduced accuracy due to not using Chain of Thought reasoning in the scoring.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… In Query Translation, input question1 will be set to *.__record__.main_input or `Select.RecordInput` .\n",
      "âœ… In Query Translation, input question2 will be set to *.__record__.app.query.args.str_or_query_bundle .\n",
      "âœ… In Ratings Usage, input last_context will be set to *.__record__.app.query.rets.response .\n",
      "âœ… In Context Relevance, input question will be set to *.__record__.main_input or `Select.RecordInput` .\n",
      "âœ… In Context Relevance, input statement will be set to *.__record__.app.query.rets.response .\n",
      "âœ… In Answer Relevance, input prompt will be set to *.__record__.main_input or `Select.RecordInput` .\n",
      "âœ… In Answer Relevance, input response will be set to *.__record__.main_output or `Select.RecordOutput` .\n",
      "âœ… In Groundedness, input source will be set to *.__record__.app.query.rets.response .\n",
      "âœ… In Groundedness, input statement will be set to *.__record__.main_output or `Select.RecordOutput` .\n"
     ]
    }
   ],
   "source": [
    "custom = OpenAI_custom()\n",
    "f_query_translation = Feedback(custom.query_translation_score, name = \"Query Translation\").on_input().on(\n",
    "    Select.Record.app.query.args.str_or_query_bundle # check the query bundle passed to yelp api\n",
    ")\n",
    "f_ratings_usage = Feedback(custom.ratings_usage, name = \"Ratings Usage\").on(\n",
    "    Select.Record.app.query.rets.response # check the last content chunk for mentions of ratings or reviews\n",
    ")\n",
    "\n",
    "fopenai = fOpenAI()\n",
    "# Question/statement (context) relevance between question and last context chunk (i.e. summary)\n",
    "f_context_relevance = Feedback(fopenai.qs_relevance, name = \"Context Relevance\").on_input().on(\n",
    "    Select.Record.app.query.rets.response# check context\n",
    ")\n",
    "\n",
    "# Question/answer relevance between overall question and answer.\n",
    "f_qa_relevance = Feedback(fopenai.relevance, name = \"Answer Relevance\").on_input_output()\n",
    "\n",
    "# Groundedness\n",
    "grounded = Groundedness(groundedness_provider=fopenai)\n",
    "\n",
    "f_groundedness = Feedback(grounded.groundedness_measure, name = \"Groundedness\").on(\n",
    "    Select.Record.app.query.rets.response # check context\n",
    ").on_output().aggregate(grounded.grounded_statements_aggregator)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ground Truth Eval\n",
    "\n",
    "It's also useful in many cases to do ground truth eval with small golden sets. We'll do so here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… In Ground Truth Eval, input prompt will be set to *.__record__.main_input or `Select.RecordInput` .\n",
      "âœ… In Ground Truth Eval, input response will be set to *.__record__.main_output or `Select.RecordOutput` .\n"
     ]
    }
   ],
   "source": [
    "golden_set = [\n",
    "    {\"query\": \"What's the vibe like at oprhan andy's in SF?\", \"response\": \"welcoming and friendly\"},\n",
    "    {\"query\": \"Is park tavern in San Fran open yet?\", \"response\": \"Yes\"},\n",
    "    {\"query\": \"I'm in san francisco for the morning, does Juniper serve pastries?\", \"response\": \"Yes\"},\n",
    "    {\"query\": \"What's the address of Gumbo Social in San Francisco?\", \"response\": \"5176 3rd St, San Francisco, CA 94124\"},\n",
    "    {\"query\": \"What are the reviews like of Gola in SF?\", \"response\": \"Excellent, 4.6/5\"},\n",
    "    {\"query\": \"Where's the best pizza in New York City\", \"response\": \"Joe's Pizza\"},\n",
    "    {\"query\": \"What's the best diner in Toronto?\", \"response\": \"The George Street Diner\"}\n",
    "]\n",
    "\n",
    "f_groundtruth = Feedback(GroundTruthAgreement(golden_set).agreement_measure, name = \"Ground Truth Eval\").on_input_output()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the dashboard\n",
    "\n",
    "By running the dashboard before we start to make app calls, we can see them come in 1 by 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Force stopping dashboard ...\n",
      "Starting dashboard ...\n",
      "Config file already exists. Skipping writing process.\n",
      "Credentials file already exists. Skipping writing process.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "593ff180f83f45b2b41d81675591c006",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Accordion(children=(VBox(children=(VBox(children=(Label(value='STDOUT'), Output())), VBox(children=(Label(valuâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dashboard started at http://192.168.86.103:8501 .\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<subprocess.Popen at 0x177c23520>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tru.run_dashboard(_dev=Path().cwd().parent.parent.parent.resolve(), force=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instrument Yelp App\n",
    "\n",
    "We can instrument our yelp app with TruLlama and utilize the full suite of evals we set up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "tru_agent = TruLlama(agent,\n",
    "    app_id='YelpAgent',\n",
    "    tags = \"agent prototype\",\n",
    "    feedbacks = [f_qa_relevance, f_groundtruth, f_context_relevance, f_groundedness, f_query_translation, f_ratings_usage]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Components:\n",
      "Agent of trulens_eval.utils.llama component: *.__app__.app\n",
      "LLM of trulens_eval.utils.llama component: *.__app__.app._llm\n",
      "Tool of trulens_eval.utils.llama component: *.__app__.app._tools[0]\n",
      "Other of trulens_eval.utils.llama component: *.__app__.app._tools[0].metadata\n",
      "Tool of trulens_eval.utils.llama component: *.__app__.app._tools[1]\n",
      "Other of trulens_eval.utils.llama component: *.__app__.app._tools[1].metadata\n",
      "Tool of trulens_eval.utils.llama component: *.__app__.app._tools[2]\n",
      "Other of trulens_eval.utils.llama component: *.__app__.app._tools[2].metadata\n",
      "Tool of trulens_eval.utils.llama component: *.__app__.app._tools[3]\n",
      "Other of trulens_eval.utils.llama component: *.__app__.app._tools[3].metadata\n",
      "\n",
      "Methods:\n",
      "Object at 0x177bce430:\n",
      "\t<function BaseQueryEngine.query at 0x137adc670> with path *.__app__.app\n",
      "\t<function BaseQueryEngine.aquery at 0x137adc790> with path *.__app__.app\n",
      "\t<function trace_method.<locals>.decorator.<locals>.wrapper at 0x150532e50> with path *.__app__.app\n",
      "\t<function trace_method.<locals>.decorator.<locals>.async_wrapper at 0x150536040> with path *.__app__.app\n",
      "\t<function trace_method.<locals>.decorator.<locals>.wrapper at 0x1505360d0> with path *.__app__.app\n",
      "\t<function BaseQueryEngine.retrieve at 0x137adc820> with path *.__app__.app\n",
      "\t<function BaseQueryEngine.synthesize at 0x137adc8b0> with path *.__app__.app\n",
      "\t<function BaseChatEngine.chat at 0x1502bb5e0> with path *.__app__.app\n",
      "\t<function BaseChatEngine.achat at 0x1502bb700> with path *.__app__.app\n",
      "\t<function BaseAgent.stream_chat at 0x150525dc0> with path *.__app__.app\n",
      "\t<function BaseChatEngine.stream_chat at 0x1502bb670> with path *.__app__.app\n",
      "Object at 0x177bce2e0:\n",
      "\t<function llm_completion_callback.<locals>.wrap.<locals>.wrapped_llm_predict at 0x13400ad30> with path *.__app__.app._llm\n",
      "\t<function llm_completion_callback.<locals>.wrap.<locals>.wrapped_llm_predict at 0x13400af70> with path *.__app__.app._llm\n",
      "\t<function llm_completion_callback.<locals>.wrap.<locals>.wrapped_async_llm_predict at 0x13400eaf0> with path *.__app__.app._llm\n",
      "\t<function llm_completion_callback.<locals>.wrap.<locals>.wrapped_async_llm_predict at 0x13400ed30> with path *.__app__.app._llm\n",
      "\t<function llm_chat_callback.<locals>.wrap.<locals>.wrapped_llm_chat at 0x13400a8b0> with path *.__app__.app._llm\n",
      "\t<function llm_chat_callback.<locals>.wrap.<locals>.wrapped_async_llm_chat at 0x13400e670> with path *.__app__.app._llm\n",
      "\t<function llm_chat_callback.<locals>.wrap.<locals>.wrapped_llm_chat at 0x13400aaf0> with path *.__app__.app._llm\n",
      "\t<function LLM.complete at 0x127fe04c0> with path *.__app__.app._llm\n",
      "\t<function LLM.stream_complete at 0x127fe05e0> with path *.__app__.app._llm\n",
      "\t<function LLM.acomplete at 0x127fe0700> with path *.__app__.app._llm\n",
      "\t<function LLM.astream_complete at 0x127fe0820> with path *.__app__.app._llm\n",
      "\t<function LLM.chat at 0x127fe0430> with path *.__app__.app._llm\n",
      "\t<function LLM.achat at 0x127fe0670> with path *.__app__.app._llm\n",
      "\t<function LLM.stream_chat at 0x127fe0550> with path *.__app__.app._llm\n",
      "Object at 0x177bbfcd0:\n",
      "\t<function FunctionTool.call at 0x1347c0040> with path *.__app__.app._tools[0]\n",
      "\t<function AsyncBaseTool.call at 0x1347ae940> with path *.__app__.app._tools[0]\n",
      "Object at 0x177bbfd90:\n",
      "\t<function FunctionTool.call at 0x1347c0040> with path *.__app__.app._tools[1]\n",
      "\t<function AsyncBaseTool.call at 0x1347ae940> with path *.__app__.app._tools[1]\n",
      "Object at 0x177bce040:\n",
      "\t<function FunctionTool.call at 0x1347c0040> with path *.__app__.app._tools[2]\n",
      "\t<function AsyncBaseTool.call at 0x1347ae940> with path *.__app__.app._tools[2]\n",
      "Object at 0x177bce0d0:\n",
      "\t<function FunctionTool.call at 0x1347c0040> with path *.__app__.app._tools[3]\n",
      "\t<function AsyncBaseTool.call at 0x1347ae940> with path *.__app__.app._tools[3]\n"
     ]
    }
   ],
   "source": [
    "tru_agent.print_instrumented()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instrument Standalone LLM app.\n",
    "\n",
    "Since we don't have insight into the OpenAI innerworkings, we cannot run many of the evals on intermediate steps.\n",
    "\n",
    "We can still do QA relevance on input and output, and check for similarity of the answers compared to the ground truth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "tru_llm_standalone = TruBasicApp(\n",
    "    llm_standalone,\n",
    "    app_id=\"OpenAIChatCompletion\",\n",
    "    tags = \"comparison\",\n",
    "    feedbacks=[f_qa_relevance, f_groundtruth]\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start using our apps!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_set = [\"What's the vibe like at oprhan andy's in SF?\",\n",
    "                \"What are the reviews like of Gola in SF?\",\n",
    "                \"Where's the best pizza in New York City\",\n",
    "                \"What's the address of Gumbo Social in San Francisco?\",\n",
    "                \"I'm in san francisco for the morning, does Juniper serve pastries?\",\n",
    "                \"What's the best diner in Toronto?\"\n",
    "                ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Calling Function ===\n",
      "Calling function: business_search with args: {\n",
      "  \"location\": \"San Francisco\",\n",
      "  \"term\": \"Orphan Andy's\"\n",
      "}\n",
      "Got output: Content loaded! You can now search the information using read_business_search\n",
      "========================\n",
      "=== Calling Function ===\n",
      "Calling function: read_business_search with args: {\n",
      "  \"query\": \"What's the vibe like at Orphan Andy's in SF?\"\n",
      "}\n",
      "Got output: The vibe at Orphan Andy's in San Francisco is not provided in the given context information.\n",
      "========================\n"
     ]
    }
   ],
   "source": [
    "for prompt in prompt_set[0:1]:\n",
    "    with tru_llm_standalone as recording:\n",
    "        llm_standalone(prompt)\n",
    "    with tru_agent as recording:\n",
    "        agent.query(prompt)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.4 ('agents')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "7d153714b979d5e6d08dd8ec90712dd93bff2c9b6c1f0c118169738af3430cd4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
