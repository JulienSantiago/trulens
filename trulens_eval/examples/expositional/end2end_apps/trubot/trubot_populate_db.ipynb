{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TruBot Populate DB\n",
    "\n",
    "This notebook tests a conversation bot with vector-store context of TruEra website. The database is reset and several pre-defined queries are made to test the four chain variants."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute this cell if running from github repo.\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "sys.path.append(str(Path().cwd().parent.parent.parent.parent.resolve()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Key OPENAI_API_KEY set from environment (same value found in .env file at /Volumes/dev_new/.env).\n",
      "‚úÖ Key HUGGINGFACE_API_KEY set from environment (same value found in .env file at /Volumes/dev_new/.env).\n",
      "‚úÖ Key PINECONE_API_KEY set from environment (same value found in .env file at /Volumes/dev_new/.env).\n",
      "‚úÖ Key PINECONE_ENV set from environment (same value found in .env file at /Volumes/dev_new/.env).\n",
      "‚úÖ Key LAMINI_API_KEY set from environment (same value found in .env file at /Volumes/dev_new/.env).\n"
     ]
    }
   ],
   "source": [
    "from trulens_eval.keys import check_or_set_keys\n",
    "\n",
    "check_or_set_keys(\n",
    "    OPENAI_API_KEY=\"to fill in\",\n",
    "    HUGGINGFACE_API_KEY=\"to fill in\",\n",
    "    PINECONE_API_KEY=\"to fill in\",\n",
    "    PINECONE_ENV=\"to fill in\",\n",
    "    LAMINI_API_KEY=\"to fill in\"\n",
    ")\n",
    "\n",
    "import os\n",
    "import lamini\n",
    "lamini.api_key = os.environ[\"LAMINI_API_KEY\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü¶ë Tru initialized with db url sqlite:///default.sqlite .\n",
      "üõë Secret keys may be written to the database. See the `database_redact_keys` option of Tru` to prevent this.\n"
     ]
    }
   ],
   "source": [
    "from trulens_eval.utils.threading import TP\n",
    "from trulens_eval import Tru\n",
    "\n",
    "from pprint import PrettyPrinter\n",
    "pp = PrettyPrinter()\n",
    "\n",
    "# Reset the database if needed:\n",
    "Tru().reset_database()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Key OPENAI_API_KEY set from environment (same value found in .env file at /Volumes/dev_new/.env).\n",
      "‚úÖ Key HUGGINGFACE_API_KEY set from environment (same value found in .env file at /Volumes/dev_new/.env).\n",
      "‚úÖ Key PINECONE_API_KEY set from environment (same value found in .env file at /Volumes/dev_new/.env).\n",
      "‚úÖ Key PINECONE_ENV set from environment (same value found in .env file at /Volumes/dev_new/.env).\n",
      "‚úÖ In language_match, input text1 will be set to __record__.main_input or `Select.RecordInput` .\n",
      "‚úÖ In language_match, input text2 will be set to __record__.main_output or `Select.RecordOutput` .\n",
      "'Starting a new conversation with 3/filtered_context.'\n"
     ]
    }
   ],
   "source": [
    "from examples.expositional.end2end_apps.trubot.trubot import get_or_make_app, get_answer\n",
    "\n",
    "from trulens_eval.feedback.provider.lamini import Lamini\n",
    "\n",
    "app = get_or_make_app(cid=None, selector=3, llmprovider=Lamini())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prompt= You are a RELEVANCE grader; providing the relevance of the given STATEMENT to the given QUESTION.\n",
      "Respond only as a number from 0 to 10 where 0 is the least relevant and 10 is the most relevant. \n",
      "\n",
      "A few additional scoring guidelines:\n",
      "\n",
      "- Long STATEMENTS should score equally well as short STATEMENTS.\n",
      "\n",
      "- RELEVANCE score should increase as the STATEMENT provides more RELEVANT context to the QUESTION.\n",
      "\n",
      "- RELEVANCE score should increase as the STATEMENT provides RELEVANT context to more parts of the QUESTION.\n",
      "\n",
      "- STATEMENT that is RELEVANT to some of the QUESTION should score of 2, 3 or 4. Higher score indicates more RELEVANCE.\n",
      "\n",
      "- STATEMENT that is RELEVANT to most of the QUESTION should get a score of 5, 6, 7 or 8. Higher score indicates more RELEVANCE.\n",
      "\n",
      "- STATEMENT that is RELEVANT to the entire QUESTION should get a score of 9 or 10. Higher score indicates more RELEVANCE.\n",
      "\n",
      "- STATEMENT must be relevant and helpful for answering the entire QUESTION to get a score of 10.\n",
      "\n",
      "- Answers that intentionally do not answer the question, such as 'I don't know', should also be counted as the most relevant.\n",
      "\n",
      "- Never elaborate.\n",
      "\n",
      "QUESTION: who are you?\n",
      "\n",
      "STATEMENT: Personal Information: You can visit the Site without submitting any information that we can use to identify you personally. However, if you use certain features on the Site, such as ‚ÄúContact Us‚Äù, ‚ÄúRequest Demo‚Äù, ‚ÄúSubscribe to Blog‚Äù, or when you submit an application for a position with us, you will be required to provide personal information. Such information could include, for example, your name, email address, phone number, or resume. Our online contact form may also contain a ‚Äúcomments‚Äù field through which you may submit additional personal information.\n",
      "\n",
      "RELEVANCE: \n",
      "args {'output_type': {'output': 'int'}}\n",
      "prompt= You are a RELEVANCE grader; providing the relevance of the given STATEMENT to the given QUESTION.\n",
      "Respond only as a number from 0 to 10 where 0 is the least relevant and 10 is the most relevant. \n",
      "\n",
      "A few additional scoring guidelines:\n",
      "\n",
      "- Long STATEMENTS should score equally well as short STATEMENTS.\n",
      "\n",
      "- RELEVANCE score should increase as the STATEMENT provides more RELEVANT context to the QUESTION.\n",
      "\n",
      "- RELEVANCE score should increase as the STATEMENT provides RELEVANT context to more parts of the QUESTION.\n",
      "\n",
      "- STATEMENT that is RELEVANT to some of the QUESTION should score of 2, 3 or 4. Higher score indicates more RELEVANCE.\n",
      "\n",
      "- STATEMENT that is RELEVANT to most of the QUESTION should get a score of 5, 6, 7 or 8. Higher score indicates more RELEVANCE.\n",
      "\n",
      "- STATEMENT that is RELEVANT to the entire QUESTION should get a score of 9 or 10. Higher score indicates more RELEVANCE.\n",
      "\n",
      "- STATEMENT must be relevant and helpful for answering the entire QUESTION to get a score of 10.\n",
      "\n",
      "- Answers that intentionally do not answer the question, such as 'I don't know', should also be counted as the most relevant.\n",
      "\n",
      "- Never elaborate.\n",
      "\n",
      "QUESTION: who are you?\n",
      "\n",
      "STATEMENT: Stand out from data scientists who all too often employ a \"set it and forget it\" approach to machine learning. With the newsletter, you'll stay up to date on:\n",
      "\n",
      "How interpretability can enhance model performance\n",
      "\n",
      "Different aspects and methods of explanation\n",
      "\n",
      "Where different AI systems fall on the sliding scale of societal impact\n",
      "\n",
      "More ways to advance trust such as how humans can work effectively with AI systems\n",
      "\n",
      "Who are we?\n",
      "\n",
      "RELEVANCE: \n",
      "args {'output_type': {'output': 'int'}}\n",
      "prompt= You are a RELEVANCE grader; providing the relevance of the given STATEMENT to the given QUESTION.\n",
      "Respond only as a number from 0 to 10 where 0 is the least relevant and 10 is the most relevant. \n",
      "\n",
      "A few additional scoring guidelines:\n",
      "\n",
      "- Long STATEMENTS should score equally well as short STATEMENTS.\n",
      "\n",
      "- RELEVANCE score should increase as the STATEMENT provides more RELEVANT context to the QUESTION.\n",
      "\n",
      "- RELEVANCE score should increase as the STATEMENT provides RELEVANT context to more parts of the QUESTION.\n",
      "\n",
      "- STATEMENT that is RELEVANT to some of the QUESTION should score of 2, 3 or 4. Higher score indicates more RELEVANCE.\n",
      "\n",
      "- STATEMENT that is RELEVANT to most of the QUESTION should get a score of 5, 6, 7 or 8. Higher score indicates more RELEVANCE.\n",
      "\n",
      "- STATEMENT that is RELEVANT to the entire QUESTION should get a score of 9 or 10. Higher score indicates more RELEVANCE.\n",
      "\n",
      "- STATEMENT must be relevant and helpful for answering the entire QUESTION to get a score of 10.\n",
      "\n",
      "- Answers that intentionally do not answer the question, such as 'I don't know', should also be counted as the most relevant.\n",
      "\n",
      "- Never elaborate.\n",
      "\n",
      "QUESTION: who are you?\n",
      "\n",
      "STATEMENT: Anupam Datta is passionate about enabling responsible adoption of artificial intelligence. As a Professor of Electrical & Computer Engineering and Computer Science at Carnegie Mellon University for over a decade, he has led groundbreaking research in the areas of AI explainability and governance as well as privacy and data protection.\n",
      "\n",
      "Moderator\n",
      "\n",
      "Lofred MadzouDirector of Strategy and Business Development, TruEra\n",
      "\n",
      "RELEVANCE: \n",
      "args {'output_type': {'output': 'int'}}\n",
      "prompt= You are a RELEVANCE grader; providing the relevance of the given STATEMENT to the given QUESTION.\n",
      "Respond only as a number from 0 to 10 where 0 is the least relevant and 10 is the most relevant. \n",
      "\n",
      "A few additional scoring guidelines:\n",
      "\n",
      "- Long STATEMENTS should score equally well as short STATEMENTS.\n",
      "\n",
      "- RELEVANCE score should increase as the STATEMENT provides more RELEVANT context to the QUESTION.\n",
      "\n",
      "- RELEVANCE score should increase as the STATEMENT provides RELEVANT context to more parts of the QUESTION.\n",
      "\n",
      "- STATEMENT that is RELEVANT to some of the QUESTION should score of 2, 3 or 4. Higher score indicates more RELEVANCE.\n",
      "\n",
      "- STATEMENT that is RELEVANT to most of the QUESTION should get a score of 5, 6, 7 or 8. Higher score indicates more RELEVANCE.\n",
      "\n",
      "- STATEMENT that is RELEVANT to the entire QUESTION should get a score of 9 or 10. Higher score indicates more RELEVANCE.\n",
      "\n",
      "- STATEMENT must be relevant and helpful for answering the entire QUESTION to get a score of 10.\n",
      "\n",
      "- Answers that intentionally do not answer the question, such as 'I don't know', should also be counted as the most relevant.\n",
      "\n",
      "- Never elaborate.\n",
      "\n",
      "QUESTION: who are you?\n",
      "\n",
      "STATEMENT: Kay Firth-Butterfield is Head of Artificial Intelligence and a member of the Executive Committee at the World Economic Forum. One of the foremost experts in the world on the governance of artificial intelligence (AI), Kay is a barrister, former judge and professor, technologist, and entrepreneur who has an abiding interest in how humanity can equitably benefit from new technologies, especially AI. In this discussion with Shameek Kundu, TruEra's Chief Strategy Officer, Kay Firth-Butterfield will share her thoughts on emerging practices around AI governance around the world, and the roles that corporations, technology firms and other stakeholders can play in enabling the safe and responsible use of AI. The discussion will cover both formal regulation, such as those proposed in the European Union, as well as ‚Äúsoft governance,‚Äù such as industry-level initiatives to define common standards. Finally, the discussion will cover the emerging roles of tools and external certification/auditing in ensuring sustainability. Join this live webinar on May 5th at 9AM Pacific/Noon Eastern Time and you will:\n",
      "\n",
      "RELEVANCE: \n",
      "args {'output_type': {'output': 'int'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unrecognized lamini response format. It did not have usage information:\n",
      "{'output': 8}\n",
      "Unrecognized lamini response format. It did not have usage information:\n",
      "{'output': 8}\n",
      "Unrecognized lamini response format. It did not have usage information:\n",
      "{'output': 8}\n",
      "Unrecognized lamini response format. It did not have usage information:\n",
      "{'output': 8}\n",
      "Unrecognized lamini response format. It did not have usage information:\n",
      "{'output': 8}\n",
      "Unrecognized lamini response format. It did not have usage information:\n",
      "{'output': 8}\n",
      "Unrecognized lamini response format. It did not have usage information:\n",
      "{'output': 8}\n",
      "Unrecognized lamini response format. It did not have usage information:\n",
      "{'output': 8}\n",
      "Unrecognized lamini response format. It did not have usage information:\n",
      "{'output': 8}\n",
      "Unrecognized lamini response format. It did not have usage information:\n",
      "{'output': 8}\n",
      "Unrecognized lamini response format. It did not have usage information:\n",
      "{'output': 10}\n",
      "Unrecognized lamini response format. It did not have usage information:\n",
      "{'output': 10}\n",
      "Unrecognized lamini response format. It did not have usage information:\n",
      "{'output': 10}\n",
      "Unrecognized lamini response format. It did not have usage information:\n",
      "{'output': 10}\n",
      "Unrecognized lamini response format. It did not have usage information:\n",
      "{'output': 10}\n",
      "Unrecognized lamini response format. It did not have usage information:\n",
      "{'output': 10}\n",
      "Unrecognized lamini response format. It did not have usage information:\n",
      "{'output': 10}\n",
      "Unrecognized lamini response format. It did not have usage information:\n",
      "{'output': 10}\n",
      "Unrecognized lamini response format. It did not have usage information:\n",
      "{'output': 10}\n",
      "Unrecognized lamini response format. It did not have usage information:\n",
      "{'output': 10}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "comp=8\n",
      "comp=10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unrecognized lamini response format. It did not have usage information:\n",
      "{'output': 9}\n",
      "Unrecognized lamini response format. It did not have usage information:\n",
      "{'output': 9}\n",
      "Unrecognized lamini response format. It did not have usage information:\n",
      "{'output': 9}\n",
      "Unrecognized lamini response format. It did not have usage information:\n",
      "{'output': 9}\n",
      "Unrecognized lamini response format. It did not have usage information:\n",
      "{'output': 9}\n",
      "Unrecognized lamini response format. It did not have usage information:\n",
      "{'output': 9}\n",
      "Unrecognized lamini response format. It did not have usage information:\n",
      "{'output': 9}\n",
      "Unrecognized lamini response format. It did not have usage information:\n",
      "{'output': 9}\n",
      "Unrecognized lamini response format. It did not have usage information:\n",
      "{'output': 9}\n",
      "Unrecognized lamini response format. It did not have usage information:\n",
      "{'output': 9}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "comp=9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unrecognized lamini response format. It did not have usage information:\n",
      "{'output': 0}\n",
      "Unrecognized lamini response format. It did not have usage information:\n",
      "{'output': 0}\n",
      "Unrecognized lamini response format. It did not have usage information:\n",
      "{'output': 0}\n",
      "Unrecognized lamini response format. It did not have usage information:\n",
      "{'output': 0}\n",
      "Unrecognized lamini response format. It did not have usage information:\n",
      "{'output': 0}\n",
      "Unrecognized lamini response format. It did not have usage information:\n",
      "{'output': 0}\n",
      "Unrecognized lamini response format. It did not have usage information:\n",
      "{'output': 0}\n",
      "Unrecognized lamini response format. It did not have usage information:\n",
      "{'output': 0}\n",
      "Unrecognized lamini response format. It did not have usage information:\n",
      "{'output': 0}\n",
      "Unrecognized lamini response format. It did not have usage information:\n",
      "{'output': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "comp=0\n"
     ]
    }
   ],
   "source": [
    "with app as recorder:\n",
    "    res = app.app(dict(question=\"who are you?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Components:\n",
      "\tTruChain (Other) at 0x2a0855bd0 with path __app__\n",
      "\tConversationalRetrievalChain (Other) at 0x2a0855db0 with path __app__.app\n",
      "\tConversationSummaryBufferMemory (Other) at 0x2a49b5c50 with path __app__.app.memory\n",
      "\tOpenAI (Custom) at 0x29dd43110 with path __app__.app.memory.llm\n",
      "\tPromptTemplate (Custom) at 0x29dd432f0 with path __app__.app.memory.prompt\n",
      "\tChatMessageHistory (Custom) at 0x29fe4add0 with path __app__.app.memory.chat_memory\n",
      "\tHumanMessage (Custom) at 0x2a49dd5d0 with path __app__.app.memory.chat_memory.messages[0]\n",
      "\tAIMessage (Custom) at 0x2a736cbd0 with path __app__.app.memory.chat_memory.messages[1]\n",
      "\tStuffDocumentsChain (Other) at 0x2a08555e0 with path __app__.app.combine_docs_chain\n",
      "\tLLMChain (Other) at 0x2a0855e00 with path __app__.app.combine_docs_chain.llm_chain\n",
      "\tPromptTemplate (Custom) at 0x2a0855c20 with path __app__.app.combine_docs_chain.llm_chain.prompt\n",
      "\tOpenAI (Custom) at 0x2a46dd9a0 with path __app__.app.combine_docs_chain.llm_chain.llm\n",
      "\tStrOutputParser (Custom) at 0x2a0856080 with path __app__.app.combine_docs_chain.llm_chain.output_parser\n",
      "\tPromptTemplate (Custom) at 0x2a08564e0 with path __app__.app.combine_docs_chain.document_prompt\n",
      "\tLLMChain (Other) at 0x2a0855540 with path __app__.app.question_generator\n",
      "\tPromptTemplate (Custom) at 0x2a0855d10 with path __app__.app.question_generator.prompt\n",
      "\tOpenAI (Custom) at 0x2a46dd9a0 with path __app__.app.question_generator.llm\n",
      "\tStrOutputParser (Custom) at 0x2a0855c70 with path __app__.app.question_generator.output_parser\n",
      "\tWithFeedbackFilterDocuments (Other) at 0x2a0854dc0 with path __app__.app.retriever\n",
      "\tFeedback (Other) at 0x2a0a0f2f0 with path __app__.app.retriever.feedback\n",
      "\tdict (Other) at 0x2a19afc00 with path __app__.app.retriever.feedback.implementation.obj.init_bindings.kwargs\n",
      "\tdict (Other) at 0x2a49b7000 with path __app__.app.retriever.feedback.implementation.obj.init_bindings.kwargs.endpoint\n",
      "\n",
      "Methods:\n",
      "Object at 0x2a49b5c50:\n",
      "\t<function ConversationSummaryBufferMemory.save_context at 0x29e468900> with path __app__.app.memory\n",
      "\t<function ConversationSummaryBufferMemory.clear at 0x29e468a40> with path __app__.app.memory\n",
      "\t<function BaseChatMemory.save_context at 0x29e3e1d00> with path __app__.app.memory\n",
      "\t<function BaseChatMemory.clear at 0x29e3e1e40> with path __app__.app.memory\n",
      "\t<function BaseMemory.save_context at 0x29dd15300> with path __app__.app.memory\n",
      "\t<function BaseMemory.clear at 0x29dd15440> with path __app__.app.memory\n",
      "Object at 0x2a0855e00:\n",
      "\t<function Chain.__call__ at 0x29dd17c40> with path __app__.app.combine_docs_chain.llm_chain\n",
      "\t<function Chain.invoke at 0x29dd174c0> with path __app__.app.combine_docs_chain.llm_chain\n",
      "\t<function Chain.ainvoke at 0x29dd17560> with path __app__.app.combine_docs_chain.llm_chain\n",
      "\t<function Chain.run at 0x29dd2c220> with path __app__.app.combine_docs_chain.llm_chain\n",
      "\t<function Chain.arun at 0x29dd2c4a0> with path __app__.app.combine_docs_chain.llm_chain\n",
      "\t<function LLMChain._call at 0x29dd709a0> with path __app__.app.combine_docs_chain.llm_chain\n",
      "\t<function LLMChain._acall at 0x29dd70f40> with path __app__.app.combine_docs_chain.llm_chain\n",
      "\t<function Chain.acall at 0x29dd17ec0> with path __app__.app.combine_docs_chain.llm_chain\n",
      "\t<function Chain._call at 0x29dd17a60> with path __app__.app.combine_docs_chain.llm_chain\n",
      "\t<function Chain._acall at 0x29dd17b00> with path __app__.app.combine_docs_chain.llm_chain\n",
      "\t<function Runnable.invoke at 0x29d7a8180> with path __app__.app.combine_docs_chain.llm_chain\n",
      "\t<function Runnable.ainvoke at 0x29d7a8220> with path __app__.app.combine_docs_chain.llm_chain\n",
      "Object at 0x2a08555e0:\n",
      "\t<function Chain.__call__ at 0x29dd17c40> with path __app__.app.combine_docs_chain\n",
      "\t<function Chain.invoke at 0x29dd174c0> with path __app__.app.combine_docs_chain\n",
      "\t<function Chain.ainvoke at 0x29dd17560> with path __app__.app.combine_docs_chain\n",
      "\t<function Chain.run at 0x29dd2c220> with path __app__.app.combine_docs_chain\n",
      "\t<function Chain.arun at 0x29dd2c4a0> with path __app__.app.combine_docs_chain\n",
      "\t<function BaseCombineDocumentsChain._call at 0x29de193a0> with path __app__.app.combine_docs_chain\n",
      "\t<function BaseCombineDocumentsChain._acall at 0x29de19440> with path __app__.app.combine_docs_chain\n",
      "\t<function Chain.acall at 0x29dd17ec0> with path __app__.app.combine_docs_chain\n",
      "\t<function Chain._call at 0x29dd17a60> with path __app__.app.combine_docs_chain\n",
      "\t<function Chain._acall at 0x29dd17b00> with path __app__.app.combine_docs_chain\n",
      "\t<function Runnable.invoke at 0x29d7a8180> with path __app__.app.combine_docs_chain\n",
      "\t<function Runnable.ainvoke at 0x29d7a8220> with path __app__.app.combine_docs_chain\n",
      "Object at 0x2a0855540:\n",
      "\t<function Chain.__call__ at 0x29dd17c40> with path __app__.app.question_generator\n",
      "\t<function Chain.invoke at 0x29dd174c0> with path __app__.app.question_generator\n",
      "\t<function Chain.ainvoke at 0x29dd17560> with path __app__.app.question_generator\n",
      "\t<function Chain.run at 0x29dd2c220> with path __app__.app.question_generator\n",
      "\t<function Chain.arun at 0x29dd2c4a0> with path __app__.app.question_generator\n",
      "\t<function LLMChain._call at 0x29dd709a0> with path __app__.app.question_generator\n",
      "\t<function LLMChain._acall at 0x29dd70f40> with path __app__.app.question_generator\n",
      "\t<function Chain.acall at 0x29dd17ec0> with path __app__.app.question_generator\n",
      "\t<function Chain._call at 0x29dd17a60> with path __app__.app.question_generator\n",
      "\t<function Chain._acall at 0x29dd17b00> with path __app__.app.question_generator\n",
      "\t<function Runnable.invoke at 0x29d7a8180> with path __app__.app.question_generator\n",
      "\t<function Runnable.ainvoke at 0x29d7a8220> with path __app__.app.question_generator\n",
      "Object at 0x2a0a0f2f0:\n",
      "\t<function Feedback.__call__ at 0x29fea2b60> with path __app__.app.retriever.feedback\n",
      "Object at 0x2a0854dc0:\n",
      "\t<function WithFeedbackFilterDocuments._get_relevant_documents at 0x2a197bd80> with path __app__.app.retriever\n",
      "\t<function BaseRetriever.get_relevant_documents at 0x29dd16a20> with path __app__.app.retriever\n",
      "\t<function BaseRetriever.aget_relevant_documents at 0x29dd16ac0> with path __app__.app.retriever\n",
      "\t<function VectorStoreRetriever._aget_relevant_documents at 0x29e46b1a0> with path __app__.app.retriever\n",
      "\t<function VectorStoreRetriever._get_relevant_documents at 0x29e46afc0> with path __app__.app.retriever\n",
      "\t<function BaseRetriever._get_relevant_documents at 0x29dd168e0> with path __app__.app.retriever\n",
      "\t<function BaseRetriever._aget_relevant_documents at 0x29dd16980> with path __app__.app.retriever\n",
      "Object at 0x2a0855db0:\n",
      "\t<function Chain.__call__ at 0x29dd17c40> with path __app__.app\n",
      "\t<function Chain.invoke at 0x29dd174c0> with path __app__.app\n",
      "\t<function Chain.ainvoke at 0x29dd17560> with path __app__.app\n",
      "\t<function Chain.run at 0x29dd2c220> with path __app__.app\n",
      "\t<function Chain.arun at 0x29dd2c4a0> with path __app__.app\n",
      "\t<function BaseConversationalRetrievalChain._call at 0x29e4a13a0> with path __app__.app\n",
      "\t<function BaseConversationalRetrievalChain._acall at 0x29e4a18a0> with path __app__.app\n",
      "\t<function Chain.acall at 0x29dd17ec0> with path __app__.app\n",
      "\t<function Chain._call at 0x29dd17a60> with path __app__.app\n",
      "\t<function Chain._acall at 0x29dd17b00> with path __app__.app\n",
      "\t<function Runnable.invoke at 0x29d7a8180> with path __app__.app\n",
      "\t<function Runnable.ainvoke at 0x29d7a8220> with path __app__.app\n"
     ]
    }
   ],
   "source": [
    "# List the app components that got instrumented along with their categories.\n",
    "\n",
    "app.print_instrumented()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proc = Tru().start_dashboard(force=True, _dev=Path.cwd().parent.parent.parent.parent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trulens_eval.schema import FeedbackMode\n",
    "\n",
    "selectors = [0,1,3,4]\n",
    "messages = [\"Who is Shayak?\", \"Wer ist Shayak?\", \"Kim jest Shayak?\", \"¬øQui√©n es Shayak?\", \"Was ist QII?\", \"Co jest QII?\"]\n",
    "\n",
    "# Comment this out to run all chain variants and all test queries:\n",
    "selectors = selectors[0:1]\n",
    "messages = messages[0:1]\n",
    "\n",
    "def test_bot(selector, question):\n",
    "    print(selector, question)\n",
    "    app = get_or_make_app(cid=question + str(selector), selector=selector, feedback_mode=FeedbackMode.WITH_APP)\n",
    "    answer = get_answer(app=app, question=question)\n",
    "    return answer\n",
    "\n",
    "results = []\n",
    "\n",
    "for s in selectors:\n",
    "    for m in messages:\n",
    "        results.append(TP().submit(test_bot, selector=s, question=m))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thread = Tru().start_evaluator(restart=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "demo3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
