{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ““ Text to Text Quickstart\n",
    "\n",
    "In this quickstart you will create a simple text to text application and learn how to log it and get feedback.\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/truera/trulens/blob/main/trulens_eval/examples/quickstart/text2text_quickstart.ipynb)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "### Add API keys\n",
    "For this quickstart you will need an OpenAI Key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install trulens_eval openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"sk-...\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import from TruLens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create openai client\n",
    "from openai import OpenAI\n",
    "client = OpenAI()\n",
    "\n",
    "# Imports main tools:\n",
    "from trulens_eval import Feedback, OpenAI as fOpenAI, Tru\n",
    "tru = Tru()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Simple Text to Text Application\n",
    "\n",
    "This example uses a bare bones OpenAI LLM, and a non-LLM just for demonstration purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def llm_standalone(prompt):\n",
    "    return client.chat.completions.create(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a question and answer bot, and you answer super upbeat.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ]\n",
    "    ).choices[0].message.content\n",
    "    \n",
    "def llm_standalone_2(prompt):\n",
    "    return \"stuff\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Send your first request"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Feedback Function(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… In relevance, input prompt will be set to __record__.main_input or `Select.RecordInput` .\n",
      "âœ… In relevance, input response will be set to __record__.main_output or `Select.RecordOutput` .\n"
     ]
    }
   ],
   "source": [
    "# Initialize OpenAI-based feedback function collection class:\n",
    "fopenai = fOpenAI()\n",
    "\n",
    "# Define a relevance function from openai\n",
    "f_answer_relevance = Feedback(fopenai.relevance).on_input_output()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instrument the callable for logging with TruLens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trulens_eval import TruBasicApp\n",
    "\n",
    "# Daniel - notice a regression behavior here, where only the recorder defined last (tru_llm_standalone_recorder_2) will be recording, anything above does not perform recording at all.\n",
    "tru_llm_standalone_recorder = TruBasicApp(llm_standalone, app_id=\"Happy Bot 1\", feedbacks=[f_answer_relevance])\n",
    "\n",
    "tru_llm_standalone_recorder_2 = TruBasicApp(llm_standalone_2, app_id=\"Happy Bot 2\", feedbacks=[f_answer_relevance])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_input = \"what is language models\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "tru.reset_database()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tru_llm_standalone_recorder as recording:\n",
    "    tru_llm_standalone_recorder.app(prompt_input)\n",
    "\n",
    "recording.get() # Daniel: this is not adding records to the DB\n",
    "with tru_llm_standalone_recorder_2 as recording:\n",
    "    tru_llm_standalone_recorder_2.app(prompt_input)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore in a Dashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tru.run_dashboard() # open a local streamlit app to explore\n",
    "\n",
    "# tru.stop_dashboard() # stop if needed"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, you can run `trulens-eval` from a command line in the same folder to start the dashboard."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Or view results directly in your notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>app_id</th>\n",
       "      <th>app_json</th>\n",
       "      <th>type</th>\n",
       "      <th>record_id</th>\n",
       "      <th>input</th>\n",
       "      <th>output</th>\n",
       "      <th>tags</th>\n",
       "      <th>record_json</th>\n",
       "      <th>cost_json</th>\n",
       "      <th>perf_json</th>\n",
       "      <th>ts</th>\n",
       "      <th>relevance</th>\n",
       "      <th>relevance_calls</th>\n",
       "      <th>latency</th>\n",
       "      <th>total_tokens</th>\n",
       "      <th>total_cost</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Happy Bot 2</td>\n",
       "      <td>{\"tru_class_info\": {\"name\": \"TruBasicApp\", \"mo...</td>\n",
       "      <td>TruWrapperApp(trulens_eval.tru_basic_app)</td>\n",
       "      <td>record_hash_e0b15a13df4f519a37f348a905ad3d15</td>\n",
       "      <td>\"How good is language AI?\"</td>\n",
       "      <td>\"Language AI is absolutely amazing! It has com...</td>\n",
       "      <td>-</td>\n",
       "      <td>{\"record_id\": \"record_hash_e0b15a13df4f519a37f...</td>\n",
       "      <td>{\"n_requests\": 1, \"n_successful_requests\": 1, ...</td>\n",
       "      <td>{\"start_time\": \"2024-06-07T14:43:28.236744\", \"...</td>\n",
       "      <td>2024-06-07T14:43:29.270988</td>\n",
       "      <td>0.8</td>\n",
       "      <td>[{'args': {'prompt': 'How good is language AI?...</td>\n",
       "      <td>1</td>\n",
       "      <td>68</td>\n",
       "      <td>0.000121</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        app_id                                           app_json  \\\n",
       "0  Happy Bot 2  {\"tru_class_info\": {\"name\": \"TruBasicApp\", \"mo...   \n",
       "\n",
       "                                        type  \\\n",
       "0  TruWrapperApp(trulens_eval.tru_basic_app)   \n",
       "\n",
       "                                      record_id                       input  \\\n",
       "0  record_hash_e0b15a13df4f519a37f348a905ad3d15  \"How good is language AI?\"   \n",
       "\n",
       "                                              output tags  \\\n",
       "0  \"Language AI is absolutely amazing! It has com...    -   \n",
       "\n",
       "                                         record_json  \\\n",
       "0  {\"record_id\": \"record_hash_e0b15a13df4f519a37f...   \n",
       "\n",
       "                                           cost_json  \\\n",
       "0  {\"n_requests\": 1, \"n_successful_requests\": 1, ...   \n",
       "\n",
       "                                           perf_json  \\\n",
       "0  {\"start_time\": \"2024-06-07T14:43:28.236744\", \"...   \n",
       "\n",
       "                           ts  relevance  \\\n",
       "0  2024-06-07T14:43:29.270988        0.8   \n",
       "\n",
       "                                     relevance_calls  latency  total_tokens  \\\n",
       "0  [{'args': {'prompt': 'How good is language AI?...        1            68   \n",
       "\n",
       "   total_cost  \n",
       "0    0.000121  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tru.get_records_and_feedback(app_ids=[])[0] # pass an empty list of app_ids to get all"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "milvus",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
