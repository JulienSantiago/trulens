{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add API keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install trulens_eval==0.18.3 openai==1.3.7 torch transformers peft==0.6.2 gdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trulens_eval import Tru, TruLlama\n",
    "\n",
    "tru = Tru()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import VectorStoreIndex\n",
    "from llama_index.readers.web import SimpleWebPageReader\n",
    "\n",
    "documents = SimpleWebPageReader(\n",
    "    html_to_text=True\n",
    ").load_data([\"http://paulgraham.com/worked.html\"])\n",
    "index = VectorStoreIndex.from_documents(documents)\n",
    "\n",
    "query_engine = index.as_query_engine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = query_engine.query(\"What did the author do growing up?\")\n",
    "print(response)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Feedback Function(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gdown\n",
    "import torch\n",
    "from torch import nn\n",
    "from transformers import AutoModel\n",
    "from peft.tuners.lora.config import LoraConfig\n",
    "from peft.mapping import get_peft_model\n",
    "from peft.utils.peft_types import TaskType"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassifierHead(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_dim: int,\n",
    "        inner_dim: int,\n",
    "        num_classes: int,\n",
    "        p_dropout: float,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.dense = nn.Linear(input_dim, inner_dim)\n",
    "        self.dropout = nn.Dropout(p=p_dropout)\n",
    "        self.out_proj = nn.Linear(inner_dim, num_classes)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        activation = self.dropout(x)\n",
    "        activation = self.dense(activation)\n",
    "        activation = torch.tanh(activation)\n",
    "        activation = self.dropout(activation)\n",
    "        return self.out_proj(activation)\n",
    "    \n",
    "\n",
    "class FeedbackClassifier(nn.Module):\n",
    "    def __init__(self, backbone_model: nn.Module, n_classes: int = 1):\n",
    "        super().__init__()\n",
    "        self.n_classes = n_classes\n",
    "        self._backbone = backbone_model\n",
    "        \n",
    "        self.eos_token_id = self._backbone.config.eos_token_id\n",
    "\n",
    "        self._classifier = ClassifierHead(\n",
    "            input_dim=self._backbone.config.hidden_size,\n",
    "            inner_dim=self._backbone.config.hidden_size,\n",
    "            num_classes=n_classes,\n",
    "            p_dropout=self._backbone.config.hidden_dropout_prob,\n",
    "        )\n",
    "        if n_classes == 1:\n",
    "            self.probits_proj = nn.Sigmoid()\n",
    "        else:\n",
    "            self.probits_proj = nn.Softmax(dim=-1)\n",
    "\n",
    "    def forward(self, input_ids: torch.Tensor, attention_mask: torch.Tensor) -> torch.Tensor:\n",
    "        outputs = self._backbone(input_ids=input_ids, attention_mask=attention_mask, output_hidden_states=True)\n",
    "        cls_output = outputs.pooler_output\n",
    "        return self._classifier(cls_output)\n",
    "\n",
    "    def compute_probits(self, logits: torch.Tensor):\n",
    "        probits: torch.Tensor = self.probits_proj(logits)\n",
    "        if len(probits.shape) > 1 and self.n_classes == 1:\n",
    "            probits = probits.squeeze(1)\n",
    "        return probits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(model_name: str, n_classes: int = 1, device: str = None):\n",
    "    backbone = AutoModel.from_pretrained(\n",
    "        model_name,\n",
    "        torch_dtype=torch.bfloat16\n",
    "    )\n",
    "    peft_config = LoraConfig(\n",
    "        task_type=TaskType.FEATURE_EXTRACTION, \n",
    "        inference_mode=False, \n",
    "        r=8, \n",
    "        lora_alpha=32, \n",
    "        lora_dropout=0.1,\n",
    "        # target_modules=[\"query\", \"value\"]\n",
    "    )\n",
    "    backbone = get_peft_model(backbone, peft_config)\n",
    "\n",
    "    model = FeedbackClassifier(backbone, n_classes=n_classes)\n",
    "    if device:\n",
    "        model = model.to(device)\n",
    "    model = model.to(torch.bfloat16)\n",
    "    return model\n",
    "\n",
    "def download_checkpoint():\n",
    "    checkpoint_path = './checkpoint.ckpt'\n",
    "    if not os.path.exists(checkpoint_path):\n",
    "        raise ValueError(\"\"\"1) Download the model checkpoint and 2) move it to ./checkpoint.ckpt\n",
    "                         https://drive.google.com/file/d/1dmQqr7K3OL8TVNYPj8yF9OnhNtzroOO0/view?usp=drive_link\"\"\")\n",
    "    return checkpoint_path\n",
    "\n",
    "def load_model_from_checkpoint(base_model_name: str):\n",
    "    model = create_model(base_model_name, device=device)\n",
    "    checkpoint_path = download_checkpoint()\n",
    "    state_dict = torch.load(checkpoint_path, map_location=torch.device(device))\n",
    "    state_dict = {k[6:]: v for k, v in state_dict['state_dict'].items()}\n",
    "    model.load_state_dict(state_dict)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model_name: str = \"roberta-base\"\n",
    "model = load_model_from_checkpoint(base_model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from typing import Mapping, Optional\n",
    "from transformers import AutoTokenizer, PreTrainedTokenizer, PreTrainedTokenizerFast\n",
    "\n",
    "def tokenize_batch(\n",
    "    batch, \n",
    "    tokenizer: PreTrainedTokenizer | PreTrainedTokenizerFast, \n",
    "    max_length: Optional[int] = None,\n",
    "    pad_seq: bool = True,\n",
    "    truncate_seq: bool = True,\n",
    "    return_type: str = \"pt\"\n",
    ") -> Mapping[str, torch.Tensor]:\n",
    "    tokenizer_kwargs = {\n",
    "        \"return_tensors\": return_type, \n",
    "        \"padding\": \"max_length\" if pad_seq else \"do_not_pad\", \n",
    "        \"truncation\": truncate_seq, \n",
    "        \"max_length\": max_length\n",
    "    }\n",
    "    texts = batch['text']\n",
    "    return tokenizer(\n",
    "        texts, \n",
    "        **tokenizer_kwargs\n",
    "    )\n",
    "\n",
    "def combine_premise_hypothesis(batch, sep_token: str):\n",
    "    premises = batch['premise']\n",
    "    hypotheses = batch['hypothesis']    \n",
    "    return {\"text\": [f\"{premise}{sep_token}{hypothesis}\" for premise, hypothesis in zip(premises, hypotheses)]}\n",
    "\n",
    "def collate_batch(record_batch):\n",
    "    assert isinstance(record_batch, dict)\n",
    "    for k, v in record_batch.items():\n",
    "        if not isinstance(v, list):\n",
    "            assert isinstance(v, str)\n",
    "            record_batch[k] = [v]\n",
    "    return record_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Provider"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "from trulens_eval import Provider, Feedback\n",
    "\n",
    "class TruEraDistill(Provider):\n",
    "    models = {}\n",
    "    base_models = {}\n",
    "\n",
    "    def __init__(self, context_relevance_model: nn.Module, context_base_model: str):\n",
    "        super().__init__(\"TruEraDistill\")\n",
    "        self.models = {\n",
    "            \"context_relevance\": context_relevance_model\n",
    "        }\n",
    "        self.base_models = {\n",
    "            \"context_relevance\": context_base_model\n",
    "        }\n",
    "        \n",
    "    def _prepare_inputs(self, premise: str, hypothesis: str, objective: str) -> dict:\n",
    "        model_name = self.base_models[objective]\n",
    "        \n",
    "        record_batch = collate_batch({\"premise\": premise, \"hypothesis\": hypothesis})\n",
    "\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        # Combine premise and hypothesis into a single text field\n",
    "        record_batch = combine_premise_hypothesis(record_batch, sep_token=tokenizer.sep_token)\n",
    "\n",
    "        return tokenize_batch(\n",
    "            record_batch, \n",
    "            tokenizer=tokenizer,\n",
    "            max_length=tokenizer.model_max_length,\n",
    "            pad_seq=False,\n",
    "            truncate_seq=True,\n",
    "        )\n",
    "\n",
    "    def _call_model(self, inputs, objective: str):\n",
    "        model = self.models[objective]\n",
    "        logits = model(inputs['input_ids'].to(device), attention_mask=inputs['attention_mask'].to(device))\n",
    "        probits = F.sigmoid(logits)\n",
    "        return {\n",
    "            \"probits\": probits.detach().cpu().float().numpy(),\n",
    "            \"logits\": logits.detach().cpu().float().numpy()\n",
    "        }\n",
    "\n",
    "    def context_relevance(self, instruction: str, context: str) -> float:\n",
    "        objective = \"context_relevance\"\n",
    "        inputs = self._prepare_inputs(instruction, context, objective=objective)\n",
    "        return float(self._call_model(inputs, objective)['probits'].squeeze())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "truera_distill = TruEraDistill(model, base_model_name)\n",
    "\n",
    "f_ctx_relevance = Feedback(truera_distill.context_relevance).on_input().on(\n",
    "    TruLlama.select_source_nodes().node.text\n",
    "    ).aggregate(np.mean)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instrument chain for logging with TruLens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_evalautions = [\n",
    "    \"What did the author do growing up?\",\n",
    "    \"Where did the author work?\",\n",
    "    \"What notable achievements did the author have?\",\n",
    "    \"What did the author do after graduating?\",\n",
    "    # irrelevant questions\n",
    "    \"Where can I order a Big Mac?\",\n",
    "    \"Who was the first President of PepsiCo?\",\n",
    "    \"How many children did Napoleon have?\",\n",
    "    \"What is the capital of California?\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tru_query_engine_recorder = TruLlama(query_engine,\n",
    "    app_id='LlamaIndex_App1',\n",
    "    feedbacks=[f_ctx_relevance])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tru_query_engine_recorder as recording:\n",
    "    for query in test_evalautions:\n",
    "        query_engine.query(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieve records and feedback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The record of the ap invocation can be retrieved from the `recording`:\n",
    "\n",
    "rec = recording.records # use .get if only one record\n",
    "# recs = recording.records # use .records if multiple\n",
    "\n",
    "display(rec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tru.run_dashboard() # open a local streamlit app to explore\n",
    "\n",
    "# tru.stop_dashboard() # stop if needed"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Or view results directly in your notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tru.get_records_and_feedback(app_ids=[])[0] # pass an empty list of app_ids to get all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "d5737f6101ac92451320b0e41890107145710b89f85909f3780d702e7818f973"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
