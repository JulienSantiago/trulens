{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install trulens_eval==0.15.3 langchain>=0.0.263 unstructured>=0.0.1 chromadb>=0.0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain.document_loaders import WebBaseLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import  RetrievalQA\n",
    "\n",
    "from langchain import OpenAI\n",
    "\n",
    "from langchain.agents import Tool\n",
    "from langchain.agents import initialize_agent\n",
    "from langchain.memory import ConversationSummaryBufferMemory\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "import openai\n",
    "\n",
    "\n",
    "from trulens_eval import TruChain, Feedback, Tru\n",
    "from trulens_eval.feedback import OpenAI as fOpenAI\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"...\"\n",
    "tru = Tru()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define custom class that loads dcouments into local vector store (Chroma instance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class VectorstoreManager:\n",
    "    def __init__(self):\n",
    "        self.vectorstore = None  # Vectorstore for the current conversation\n",
    "        self.all_document_splits = []  # List to hold all document splits added during a conversation\n",
    "\n",
    "    def initialize_vectorstore(self):\n",
    "        \"\"\"Initialize an empty vectorstore for the current conversation.\"\"\"\n",
    "        self.vectorstore = Chroma(\n",
    "            embedding_function=OpenAIEmbeddings(), \n",
    "        )\n",
    "        self.all_document_splits = []  # Reset the documents list for the new conversation\n",
    "        return self.vectorstore\n",
    "\n",
    "    def add_documents_to_vectorstore(self, url_lst: list):\n",
    "        \"\"\"Example assumes loading new documents from websites to the vectorstore during a conversation.\"\"\"\n",
    "        for doc_url in url_lst:\n",
    "            document_splits = self.load_and_split_document(doc_url)\n",
    "            self.all_document_splits.extend(document_splits)\n",
    "        \n",
    "        # Create a new Chroma instance with all the documents\n",
    "        self.vectorstore = Chroma.from_documents(\n",
    "            documents=self.all_document_splits, \n",
    "            embedding=OpenAIEmbeddings(), \n",
    "        )\n",
    "\n",
    "        return self.vectorstore\n",
    "\n",
    "    def get_vectorstore(self):\n",
    "        \"\"\"Provide the initialized vectorstore for the current conversation. If not initialized, do it first.\"\"\"\n",
    "        if self.vectorstore is None:\n",
    "            raise ValueError(\"Vectorstore is not initialized. Please initialize it first.\")\n",
    "        return self.vectorstore\n",
    "\n",
    "    @staticmethod\n",
    "    def load_and_split_document(url: str, chunk_size=1000, chunk_overlap=0):  \n",
    "        \"\"\"Load and split a document into chunks.\"\"\"\n",
    "        loader = WebBaseLoader(url)\n",
    "        splits = loader.load_and_split(RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap))\n",
    "        return splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "DOC_URL = \"http://paulgraham.com/worked.html\"\n",
    "\n",
    "vectorstore_manager = VectorstoreManager()\n",
    "vec_store = vectorstore_manager.add_documents_to_vectorstore([DOC_URL])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore_manager.all_document_splits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up multiple agents for document retrieval and summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(\n",
    "        model_name='gpt-3.5-turbo-16k',\n",
    "        temperature=0.0    \n",
    "    )\n",
    "\n",
    "\n",
    "conversational_memory = ConversationSummaryBufferMemory(\n",
    "    k=4,\n",
    "    max_token_limit=64,\n",
    "    llm=llm,\n",
    "    memory_key = \"chat_history\",\n",
    "    return_messages=True\n",
    ")\n",
    "\n",
    "\n",
    "retrieval_summarization_template = \"\"\"\n",
    "System: Follow these instructions below in all your responses:\n",
    "System: always try to retrieve documents as knowledge base or external data source from retriever (vector DB). \n",
    "System: If performing summarization, you will try to be as accurate and informational as possible.\n",
    "System: If providing a summary/key takeaways/highlights, make sure the output is numbered as bullet points.\n",
    "If you don't understand the source document or cannot find sufficient relevant context, be sure to ask me for more context information.\n",
    "{context}\n",
    "Question: {question}\n",
    "Action:\n",
    "\"\"\"\n",
    "question_generation_template = \"\"\"\n",
    "System: Based on the summarized context, you are expected to generate a specified number of multiple choice questions from the context to ensure understanding. Each question, unless specified otherwise, is expected to have 4 options and only correct answer.\n",
    "System: Questions should be in the format of numbered list.\n",
    "{context}\n",
    "Question: {question}\n",
    "Action:\n",
    "\"\"\"\n",
    "\n",
    "answer_generation_template = \"\"\"\n",
    "System: You are going to give the correct answers to these 10 multiple choice questions from the {context}, 1 single correct answer for each question, with detailed explanation. \n",
    "System: If you are referring back to the multiple choice questions, do not rephrase the question. Just retrieve and present the question verbatim. c\n",
    "System: Answers to questions should be in the format of numbered list.\n",
    "{context}\n",
    "Question: {question}\n",
    "Action:\n",
    "\"\"\"\n",
    "\n",
    "summarization_prompt = PromptTemplate(template=retrieval_summarization_template, input_variables=[\"question\", \"context\"])\n",
    "answer_generator_prompt = PromptTemplate(template=answer_generation_template, input_variables=[\"question\", \"context\"])\n",
    "question_generator_prompt = PromptTemplate(template=question_generation_template, input_variables=[\"question\", \"context\"])\n",
    "\n",
    "\n",
    "\n",
    "# retrieval qa chain\n",
    "summarization_chain = RetrievalQA.from_chain_type(\n",
    "        llm=llm,\n",
    "        chain_type=\"stuff\",\n",
    "        retriever=vec_store.as_retriever(),\n",
    "        chain_type_kwargs={'prompt': summarization_prompt}\n",
    ")\n",
    "\n",
    "question_chain = RetrievalQA.from_chain_type(llm=llm,\n",
    "        chain_type=\"stuff\",\n",
    "        retriever=vec_store.as_retriever(),\n",
    "        chain_type_kwargs={'prompt': question_generator_prompt}\n",
    "    )\n",
    "\n",
    "answer_chain = RetrievalQA.from_chain_type(llm=llm,\n",
    "        chain_type=\"stuff\",\n",
    "        retriever=vec_store.as_retriever(),\n",
    "        chain_type_kwargs={'prompt': answer_generator_prompt}\n",
    "    )\n",
    "tools = [\n",
    "        Tool(\n",
    "            name=\"Knowledge Base / retrieval from documents\",\n",
    "            func=summarization_chain.run,\n",
    "            description=\"useful for when you need to answer questions about the source document(s).\",\n",
    "        ),\n",
    "    \n",
    "        Tool(\n",
    "            name=\"Conversational agent to generate multiple choice questions about the summary of the source document(s)\",\n",
    "            func=question_chain.run,\n",
    "            description=\"useful for when you need to have a conversation with a human and hold the memory of the current / previous conversation.\",\n",
    "        ),\n",
    "\n",
    "        Tool(\n",
    "            name=\"Conversational agent to give answers about the summary of the source document(s)\",\n",
    "            func=answer_chain.run,\n",
    "            description=\"useful for when you need to have a conversation with a human and hold the memory of the current / previous conversation.\",\n",
    "        )\n",
    "    ]\n",
    "agent = initialize_agent(\n",
    "        agent='chat-conversational-react-description',\n",
    "        tools=tools,\n",
    "        llm=llm,\n",
    "        memory=conversational_memory\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(tools[0]).__bases__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent(\"Please summarize the document into 5 to 10 highlight points\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OpenAI_custom(fOpenAI):\n",
    "    def no_answer_feedback(self, question: str, response: str) -> float:\n",
    "        return float(openai.ChatCompletion.create(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    messages=[\n",
    "            {\"role\": \"system\", \"content\": \"Does the RESPONSE provide an answer to the QUESTION? Rate on a scale of 1 to 10. Respond with the number only.\"},\n",
    "            {\"role\": \"user\", \"content\": f\"QUESTION: {question}; RESPONSE: {response}\"}\n",
    "        ]\n",
    "    )[\"choices\"][0][\"message\"][\"content\"]) / 10\n",
    "\n",
    "custom = OpenAI_custom()\n",
    "\n",
    "# No answer feedback (custom)\n",
    "f_no_answer = Feedback(custom.no_answer_feedback).on_input_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tru_agent = TruChain(agent, app_id = \"Conversational_Retrieval_Agent\", feedbacks = [f_no_answer])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_prompts = [\n",
    "    \"Please summarize the document to a short summary under 100 words\"\n",
    "]\n",
    "\n",
    "with tru_agent as recording:\n",
    "    for prompt in user_prompts:\n",
    "        agent(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai_vtuber",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
