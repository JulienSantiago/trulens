{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"conf/","title":"Configuration file for the Sphinx documentation builder.","text":"<p>Configuration file for the Sphinx documentation builder.</p> <p>This file only contains a selection of the most common options. For a full list see the documentation: https://www.sphinx-doc.org/en/master/usage/configuration.html</p> <p>-- Path setup --------------------------------------------------------------</p> In\u00a0[\u00a0]: Copied! <pre># If extensions (or modules to document with autodoc) are in another directory,\n# add these directories to sys.path here. If the directory is relative to the\n# documentation root, use os.path.abspath to make it absolute, like shown here.\n#\nimport os\nimport sys\n</pre> # If extensions (or modules to document with autodoc) are in another directory, # add these directories to sys.path here. If the directory is relative to the # documentation root, use os.path.abspath to make it absolute, like shown here. # import os import sys In\u00a0[\u00a0]: Copied! <pre>os.environ['TRULENS_BACKEND'] = 'keras'\nsys.path.insert(0, os.path.abspath('.'))\nsys.path.insert(0, os.path.abspath('../'))\n</pre> os.environ['TRULENS_BACKEND'] = 'keras' sys.path.insert(0, os.path.abspath('.')) sys.path.insert(0, os.path.abspath('../')) <p>-- Project information -----------------------------------------------------</p> In\u00a0[\u00a0]: Copied! <pre>project = 'trulens'\ncopyright = '2023, TruEra'\nauthor = 'TruEra'\n</pre> project = 'trulens' copyright = '2023, TruEra' author = 'TruEra' <p>-- General configuration ---------------------------------------------------</p> In\u00a0[\u00a0]: Copied! <pre># Add any Sphinx extension module names here, as strings. They can be\n# extensions coming with Sphinx (named 'sphinx.ext.*') or your custom\n# ones.\nextensions = [\n    'sphinx.ext.autodoc',\n    'sphinx.ext.napoleon',\n    'recommonmark',\n    'sphinx.ext.mathjax',\n]\n</pre> # Add any Sphinx extension module names here, as strings. They can be # extensions coming with Sphinx (named 'sphinx.ext.*') or your custom # ones. extensions = [     'sphinx.ext.autodoc',     'sphinx.ext.napoleon',     'recommonmark',     'sphinx.ext.mathjax', ] <p>napoleon_google_docstring = False napoleon_use_param = False napoleon_use_ivar = True</p> In\u00a0[\u00a0]: Copied! <pre>def skip(app, what, name, obj, would_skip, options):\n    if name == '__init__' or name == '__call__':\n        return False\n    return would_skip\n</pre> def skip(app, what, name, obj, would_skip, options):     if name == '__init__' or name == '__call__':         return False     return would_skip In\u00a0[\u00a0]: Copied! <pre>def setup(app):\n    app.connect('autodoc-skip-member', skip)\n</pre> def setup(app):     app.connect('autodoc-skip-member', skip) In\u00a0[\u00a0]: Copied! <pre># Add any paths that contain templates here, relative to this directory.\ntemplates_path = ['_templates']\n</pre> # Add any paths that contain templates here, relative to this directory. templates_path = ['_templates'] In\u00a0[\u00a0]: Copied! <pre># List of patterns, relative to source directory, that match files and\n# directories to ignore when looking for source files.\n# This pattern also affects html_static_path and html_extra_path.\nexclude_patterns = ['_build', 'Thumbs.db', '.DS_Store']\n</pre> # List of patterns, relative to source directory, that match files and # directories to ignore when looking for source files. # This pattern also affects html_static_path and html_extra_path. exclude_patterns = ['_build', 'Thumbs.db', '.DS_Store'] <p>-- Options for HTML output -------------------------------------------------</p> In\u00a0[\u00a0]: Copied! <pre># The theme to use for HTML and HTML Help pages.  See the documentation for\n# a list of builtin themes.\n#\nhtml_theme = 'sphinx_rtd_theme'\n</pre> # The theme to use for HTML and HTML Help pages.  See the documentation for # a list of builtin themes. # html_theme = 'sphinx_rtd_theme' In\u00a0[\u00a0]: Copied! <pre># Add any paths that contain custom static files (such as style sheets) here,\n# relative to this directory. They are copied after the builtin static files,\n# so a file named \"default.css\" will overwrite the builtin \"default.css\".\nhtml_static_path = ['_static']\n</pre> # Add any paths that contain custom static files (such as style sheets) here, # relative to this directory. They are copied after the builtin static files, # so a file named \"default.css\" will overwrite the builtin \"default.css\". html_static_path = ['_static'] In\u00a0[\u00a0]: Copied! <pre>from recommonmark.parser import CommonMarkParser\n</pre> from recommonmark.parser import CommonMarkParser In\u00a0[\u00a0]: Copied! <pre>source_parsers = {'.md': CommonMarkParser}\n</pre> source_parsers = {'.md': CommonMarkParser} In\u00a0[\u00a0]: Copied! <pre>source_suffix = ['.rst', '.md']\n</pre> source_suffix = ['.rst', '.md']"},{"location":"welcome/","title":"Welcome to TruLens!","text":"<p>TruLens provides a set of tools for developing and monitoring neural nets, including large language models. This includes both tools for evaluation of LLMs and LLM-based applications with TruLens-Eval and deep learning explainability with TruLens-Explain. TruLens-Eval and TruLens-Explain are housed in separate packages and can be used independently.</p> <p>The best way to support TruLens is to give us a \u2b50 and join our slack community!</p>"},{"location":"welcome/#trulens-eval","title":"TruLens-Eval","text":"<p>TruLens-Eval contains instrumentation and evaluation tools for large language model (LLM) based applications. It supports the iterative development and monitoring of a wide range of LLM applications by wrapping your application to log key metadata across the entire chain (or off chain if your project does not use chains) on your local machine. Importantly, it also gives you the tools you need to evaluate the quality of your LLM-based applications.</p> <p></p>"},{"location":"welcome/#get-going-with-trulens-eval","title":"Get going with TruLens-Eval","text":"<p>Install trulens-eval from PyPI.</p> <pre><code>pip install trulens-eval\n</code></pre> <pre><code>from trulens_eval import Tru\nfrom trulens_eval import TruChain\n\ntru = Tru()\n</code></pre> <p>This example uses LangChain and OpenAI, but the same process can be followed with any framework and model provider.</p> <pre><code># imports from LangChain to build app\nfrom langchain import PromptTemplate\nfrom langchain.chains import LLMChain\nfrom langchain.chat_models import ChatOpenAI\nfrom langchain.prompts.chat import ChatPromptTemplate\nfrom langchain.prompts.chat import HumanMessagePromptTemplate\n\nimport os\nos.environ[\"OPENAI_API_KEY\"] = \"...\"\nos.environ[\"HUGGINGFACE_API_KEY\"] = \"...\"\n</code></pre> <pre><code># create LLM chain\nfull_prompt = HumanMessagePromptTemplate(\n    prompt=PromptTemplate(\n        template=\"Provide a helpful response with relevant background information for the following: {prompt}\",\n            input_variables=[\"prompt\"],\n        )\n    )\nchat_prompt_template = ChatPromptTemplate.from_messages([full_prompt])\n\nchat = ChatOpenAI(model_name='gpt-3.5-turbo', temperature=0.9)\n\nchain = LLMChain(llm=chat, prompt=chat_prompt_template)\n</code></pre> <p>Now that we created an LLM chain, we can set up our first feedback function. Here, we'll create a feedback function for language matching. After we've created the feedback function, we can include it in the TruChain wrapper. Now, whenever our wrapped chain is used we'll log both the metadata and feedback.</p> <pre><code># create a feedback function\n\nfrom trulens_eval.feedback import Feedback, Huggingface\n</code></pre> <pre><code># Initialize HuggingFace-based feedback function collection class:\nhugs = Huggingface()\n\n# Define a language match feedback function using HuggingFace.\nf_lang_match = Feedback(hugs.language_match).on_input_output()\n# By default this will check language match on the main app input and main app\n# output.\n\n# wrap your chain with TruChain\ntruchain = TruChain(\n    chain,\n    app_id='Chain1_ChatApplication',\n    feedbacks=[f_lang_match]\n)\n# Note: any `feedbacks` specified here will be evaluated and logged whenever the chain is used.\ntruchain(\"que hora es?\")\n</code></pre> <p>Now you can explore your LLM-based application!</p> <p>Doing so will help you understand how your LLM application is performing at a glance. As you iterate new versions of your LLM application, you can compare their performance across all of the different quality metrics you've set up. You'll also be able to view evaluations at a record level, and explore the chain metadata for each record.</p> <pre><code>tru.run_dashboard() # open a Streamlit app to explore\n</code></pre> <p>Alternatively, you can run <code>trulens-eval</code> from a command line in the same folder to start the dashboard.</p> <p>For more information, see TruLens-Eval Documentation.</p>"},{"location":"welcome/#trulens-explain","title":"TruLens-Explain","text":"<p>TruLens-Explain is a cross-framework library for deep learning explainability. It provides a uniform abstraction over a number of different frameworks. It provides a uniform abstraction layer over TensorFlow, Pytorch, and Keras and allows input and internal explanations.</p>"},{"location":"welcome/#get-going-with-trulens-explain","title":"Get going with TruLens-Explain","text":"<p>These installation instructions assume that you have conda installed and added to your path.</p> <ol> <li> <p>Create a virtual environment (or modify an existing one). <pre><code>conda create -n \"&lt;my_name&gt;\" python=3  # Skip if using existing environment.\nconda activate &lt;my_name&gt;\n</code></pre></p> </li> <li> <p>Install dependencies. <pre><code>conda install tensorflow-gpu=1  # Or whatever backend you're using.\nconda install keras             # Or whatever backend you're using.\nconda install matplotlib        # For visualizations.\n</code></pre></p> </li> <li> <p>[Pip installation] Install the trulens pip package from PyPI. <pre><code>pip install trulens\n</code></pre></p> </li> <li> <p>Get started! To quickly play around with the TruLens library, check out the following Colab notebooks:</p> </li> <li> <p>PyTorch: </p> </li> <li>TensorFlow 2 / Keras: </li> </ol> <p>For more information, see TruLens-Explain Documentation.</p>"},{"location":"trulens_eval/colab_dependencies/","title":"Colab dependencies","text":"In\u00a0[\u00a0]: Copied! <pre>!pip install -U trulens-eval\n\n# Google Colab Dependencies\n!npm install localtunnel -q\n!pip install -q streamlit==1.13.0\n</pre> !pip install -U trulens-eval  # Google Colab Dependencies !npm install localtunnel -q !pip install -q streamlit==1.13.0"},{"location":"trulens_eval/feedback_functions/","title":"Feedback Functions","text":"In\u00a0[\u00a0]: Copied! <pre>from trulens_eval import Provider, Feedback, Select, Tru\n\nclass StandAlone(Provider):\n    def my_custom_feedback(self, my_text_field: str) -&gt; float:\n\"\"\"\n        A dummy function of text inputs to float outputs.\n\n        Parameters:\n            my_text_field (str): Text to evaluate.\n\n        Returns:\n            float: square length of the text\n        \"\"\"\n        return 1.0 / (1.0 + len(my_text_field) * len(my_text_field))\n</pre> from trulens_eval import Provider, Feedback, Select, Tru  class StandAlone(Provider):     def my_custom_feedback(self, my_text_field: str) -&gt; float:         \"\"\"         A dummy function of text inputs to float outputs.          Parameters:             my_text_field (str): Text to evaluate.          Returns:             float: square length of the text         \"\"\"         return 1.0 / (1.0 + len(my_text_field) * len(my_text_field))  <ol> <li>Instantiate your provider and feedback functions. The feedback function is wrapped by the trulens-eval Feedback class which helps specify what will get sent to your function parameters (For example: Select.RecordInput or Select.RecordOutput)</li> </ol> In\u00a0[\u00a0]: Copied! <pre>my_standalone = StandAlone()\nmy_feedback_function_standalone = Feedback(my_standalone.my_custom_feedback).on(\n    my_text_field=Select.RecordOutput\n)\n</pre> my_standalone = StandAlone() my_feedback_function_standalone = Feedback(my_standalone.my_custom_feedback).on(     my_text_field=Select.RecordOutput ) <ol> <li>Your feedback function is now ready to use just like the out of the box feedback functions. Below is an example of it being used.</li> </ol> In\u00a0[\u00a0]: Copied! <pre>tru = Tru()\nfeedback_results = tru.run_feedback_functions(\n    record=record,\n    feedback_functions=[my_feedback_function_standalone]\n)\ntru.add_feedbacks(feedback_results)\n</pre> tru = Tru() feedback_results = tru.run_feedback_functions(     record=record,     feedback_functions=[my_feedback_function_standalone] ) tru.add_feedbacks(feedback_results)"},{"location":"trulens_eval/feedback_functions/#feedback-functions","title":"Feedback Functions\u00b6","text":"<p>A feedback function scores the output of an LLM application by analyzing generated text as part of an LLM application (or a downstream model or application built on it). This guide provides details about the feedback functions that are implemented out of the box by TruLens. At the end of the guide, you can find additional information about how to create custom feedback functions.</p> <p>See also: https://www.trulens.org/trulens_eval/api/feedback/</p>"},{"location":"trulens_eval/feedback_functions/#relevance","title":"Relevance\u00b6","text":"<p>This evaluates the relevance of the LLM response to the given text by LLM prompting.</p> <p>Relevance is currently only available with OpenAI ChatCompletion API.</p> <p>TruLens offers two particular flavors of relevance:</p> <ol> <li><p>Prompt response relevance is best for measuring the relationship of the final answer to the user inputed question. This flavor of relevance is particularly optimized for the following features:</p> <ul> <li>Relevance requires adherence to the entire prompt.</li> <li>Responses that don't provide a definitive answer can still be relevant</li> <li>Admitting lack of knowledge and refusals are still relevant.</li> <li>Feedback mechanism should differentiate between seeming and actual relevance.</li> <li>Relevant but inconclusive statements should get increasingly high scores as they are more helpful for answering the query.</li> </ul> <p>You can read more information about the performance of prompt response relevance by viewing its smoke test results.</p> </li> <li><p>Question statement relevance, sometimes known as context relevance, is best for measuring the relationship of a provided context to the user inputed question. This flavor of relevance is optimized for a slightly different set of features:</p> <ul> <li>Relevance requires adherence to the entire query.</li> <li>Long context with small relevant chunks are relevant.</li> <li>Context that provides no answer can still be relevant.</li> <li>Feedback mechanism should differentiate between seeming and actual relevance.</li> <li>Relevant but inconclusive statements should get increasingly high scores as they are more helpful for answering the query.</li> </ul> <p>You can read more information about the performance of question statement relevance by viewing its smoke test results.</p> </li> </ol>"},{"location":"trulens_eval/feedback_functions/#sentiment","title":"Sentiment\u00b6","text":"<p>This evaluates the positive sentiment of either the prompt or response.</p> <p>Sentiment is currently available to use with OpenAI, HuggingFace or Cohere as the model provider.</p> <ul> <li>The OpenAI sentiment feedback function prompts a Chat Completion model to rate the sentiment from 1 to 10, and then scales the response down to 0-1.</li> <li>The HuggingFace sentiment feedback function returns a raw score from 0 to 1.</li> <li>The Cohere sentiment feedback function uses the classification endpoint and a small set of examples stored in <code>feedback_prompts.py</code> to return either a 0 or a 1.</li> </ul>"},{"location":"trulens_eval/feedback_functions/#model-agreement","title":"Model Agreement\u00b6","text":"<p>Model agreement uses OpenAI to attempt an honest answer at your prompt with system prompts for correctness, and then evaluates the agreement of your LLM response to this model on a scale from 1 to 10. The agreement with each honest bot is then averaged and scaled from 0 to 1.</p>"},{"location":"trulens_eval/feedback_functions/#groundedness","title":"Groundedness\u00b6","text":"<p>Groundedness uses OpenAI LLMs or Huggingface NLI to attempt to check if an answer is grounded in its supplied contexts on a scale from 1 to 10. The information overlap or entailment between source and response is then measured, choosing the highest score between sources and then averaged and scaled from 0 to 1.</p>"},{"location":"trulens_eval/feedback_functions/#language-match","title":"Language Match\u00b6","text":"<p>This evaluates if the language of the prompt and response match.</p> <p>Language match is currently only available to use with HuggingFace as the model provider. This feedback function returns a score in the range from 0 to 1, where 1 indicates match and 0 indicates mismatch.</p>"},{"location":"trulens_eval/feedback_functions/#toxicity","title":"Toxicity\u00b6","text":"<p>This evaluates the toxicity of the prompt or response.</p> <p>Toxicity is currently only available to be used with HuggingFace, and uses a classification endpoint to return a score from 0 to 1. The feedback function is negated as not_toxicity, and returns a 1 if not toxic and a 0 if toxic.</p>"},{"location":"trulens_eval/feedback_functions/#moderation","title":"Moderation\u00b6","text":"<p>The OpenAI Moderation API is made available for use as feedback functions. This includes hate, hate/threatening, self-harm, sexual, sexual/minors, violence, and violence/graphic. Each is negated (ex: not_hate) so that a 0 would indicate that the moderation rule is violated. These feedback functions return a score in the range 0 to 1.</p>"},{"location":"trulens_eval/feedback_functions/#adding-new-feedback-functions","title":"Adding new feedback functions\u00b6","text":"<p>Feedback functions are an extensible framework for evaluating LLMs. You can add your own feedback functions to evaluate the qualities required by your application by updating <code>trulens_eval/feedback.py</code>, or simply creating a new provider class and feedback function in youre notebook. If your contributions would be useful for others, we encourage you to contribute to TruLens!</p> <p>Feedback functions are organized by model provider into Provider classes.</p> <p>The process for adding new feedback functions is:</p> <ol> <li>Create a new Provider class or locate an existing one that applies to your feedback function. If your feedback function does not rely on a model provider, you can create a standalone class. Add the new feedback function method to your selected class. Your new method can either take a single text (str) as a parameter or both prompt (str) and response (str). It should return a float between 0 (worst) and 1 (best).</li> </ol>"},{"location":"trulens_eval/gh_top_intro/","title":"Welcome to TruLens!","text":"<p>TruLens provides a set of tools for developing and monitoring neural nets, including large language models. This includes both tools for evaluation of LLMs and LLM-based applications with TruLens-Eval and deep learning explainability with TruLens-Explain. TruLens-Eval and TruLens-Explain are housed in separate packages and can be used independently.</p> <p>The best way to support TruLens is to give us a \u2b50 and join our slack community!</p>"},{"location":"trulens_eval/gh_top_intro/#trulens-eval","title":"TruLens-Eval","text":"<p>TruLens-Eval contains instrumentation and evaluation tools for large language model (LLM) based applications. It supports the iterative development and monitoring of a wide range of LLM applications by wrapping your application to log key metadata across the entire chain (or off chain if your project does not use chains) on your local machine. Importantly, it also gives you the tools you need to evaluate the quality of your LLM-based applications.</p> <p></p>"},{"location":"trulens_eval/gh_top_intro/#get-going-with-trulens-eval","title":"Get going with TruLens-Eval","text":"<p>Install trulens-eval from PyPI.</p> <pre><code>pip install trulens-eval\n</code></pre>"},{"location":"trulens_eval/install/","title":"Installation","text":""},{"location":"trulens_eval/install/#getting-access-to-trulens","title":"Getting access to TruLens","text":"<p>These installation instructions assume that you have conda installed and added to your path.</p> <ol> <li> <p>Create a virtual environment (or modify an existing one). <pre><code>conda create -n \"&lt;my_name&gt;\" python=3  # Skip if using existing environment.\nconda activate &lt;my_name&gt;\n</code></pre></p> </li> <li> <p>[Pip installation] Install the trulens-eval pip package from PyPI. <pre><code>pip install trulens-eval\n</code></pre></p> </li> <li> <p>[Local installation] If you would like to develop or modify TruLens, you can download the source code by cloning the TruLens repo. <pre><code>git clone https://github.com/truera/trulens.git\n</code></pre></p> </li> <li> <p>[Local installation] Install the TruLens repo. <pre><code>cd trulens/trulens_eval\npip install -e .\n</code></pre></p> </li> </ol>"},{"location":"trulens_eval/intro/","title":"Welcome to TruLens-Eval!","text":"<p>Evaluate and track your LLM experiments with TruLens. As you work on your models and prompts TruLens-Eval supports the iterative development and of a wide range of LLM applications by wrapping your application to log key metadata across the entire chain (or off chain if your project does not use chains) on your local machine.</p> <p>Using feedback functions, you can objectively evaluate the quality of the responses provided by an LLM to your requests. This is completed with minimal latency, as this is achieved in a sequential call for your application, and evaluations are logged to your local machine. Finally, we provide an easy to use Streamlit dashboard run locally on your machine for you to better understand your LLM\u2019s performance.</p> <p></p>"},{"location":"trulens_eval/intro/#quick-usage","title":"Quick Usage","text":"<p>To quickly play around with the TruLens Eval library:</p> <p>Langchain:</p> <p>langchain_quickstart.ipynb. </p> <p>langchain_quickstart.py.</p> <p>Llama Index: </p> <p>llama_index_quickstart.ipynb. </p> <p>llama_index_quickstart.py</p> <p>No Framework: </p> <p>no_framework_quickstart.ipynb. </p> <p>no_framework_quickstart.py</p>"},{"location":"trulens_eval/intro/#contributing","title":"\ud83d\udca1 Contributing","text":"<p>Interested in contributing? See our contribution guide for more details.</p>"},{"location":"trulens_eval/intro/#installation-and-setup","title":"Installation and Setup","text":"<p>Install the trulens-eval pip package from PyPI.</p> <pre><code>    pip install trulens-eval\n</code></pre>"},{"location":"trulens_eval/intro/#api-keys","title":"API Keys","text":"<p>Our example chat app and feedback functions call external APIs such as OpenAI or HuggingFace. You can add keys by setting the environment variables. </p>"},{"location":"trulens_eval/intro/#in-python","title":"In Python","text":"<pre><code>import os\nos.environ[\"OPENAI_API_KEY\"] = \"...\"\n</code></pre>"},{"location":"trulens_eval/intro/#in-terminal","title":"In Terminal","text":"<pre><code>export OPENAI_API_KEY = \"...\"\n</code></pre>"},{"location":"trulens_eval/llama_index_quickstart/","title":"Quickstart","text":"In\u00a0[\u00a0]: Copied! <pre>!pip install trulens-eval==0.7.0\n!pip install llama_index==0.7.11\n</pre> !pip install trulens-eval==0.7.0 !pip install llama_index==0.7.11 In\u00a0[\u00a0]: Copied! <pre>import os\nos.environ[\"OPENAI_API_KEY\"] = \"...\"\nos.environ[\"HUGGINGFACE_API_KEY\"] = \"...\"\n</pre> import os os.environ[\"OPENAI_API_KEY\"] = \"...\" os.environ[\"HUGGINGFACE_API_KEY\"] = \"...\" In\u00a0[\u00a0]: Copied! <pre># Imports main tools:\nfrom trulens_eval import TruLlama, Feedback, Tru, feedback\ntru = Tru()\n</pre> # Imports main tools: from trulens_eval import TruLlama, Feedback, Tru, feedback tru = Tru()  In\u00a0[\u00a0]: Copied! <pre># LLama Index starter example from: https://gpt-index.readthedocs.io/en/latest/getting_started/starter_example.html\n# In order to run this, download into data/ Paul Graham's Essay 'What I Worked On' from https://github.com/jerryjliu/llama_index/blob/main/examples/paul_graham_essay/data/paul_graham_essay.txt \n\nfrom llama_index import VectorStoreIndex, SimpleDirectoryReader\n\ndocuments = SimpleDirectoryReader('data').load_data()\nindex = VectorStoreIndex.from_documents(documents)\n\nquery_engine = index.as_query_engine()\n</pre> # LLama Index starter example from: https://gpt-index.readthedocs.io/en/latest/getting_started/starter_example.html # In order to run this, download into data/ Paul Graham's Essay 'What I Worked On' from https://github.com/jerryjliu/llama_index/blob/main/examples/paul_graham_essay/data/paul_graham_essay.txt   from llama_index import VectorStoreIndex, SimpleDirectoryReader  documents = SimpleDirectoryReader('data').load_data() index = VectorStoreIndex.from_documents(documents)  query_engine = index.as_query_engine() In\u00a0[\u00a0]: Copied! <pre>response = query_engine.query(\"What did the author do growing up?\")\nprint(response)\n</pre> response = query_engine.query(\"What did the author do growing up?\") print(response) In\u00a0[\u00a0]: Copied! <pre>import numpy as np\n\n# Initialize Huggingface-based feedback function collection class:\nhugs = feedback.Huggingface()\nopenai = feedback.OpenAI()\n\n# Define a language match feedback function using HuggingFace.\nf_lang_match = Feedback(hugs.language_match).on_input_output()\n# By default this will check language match on the main app input and main app\n# output.\n\n# Question/answer relevance between overall question and answer.\nf_qa_relevance = Feedback(openai.relevance).on_input_output()\n\n# Question/statement relevance between question and each context chunk.\nf_qs_relevance = Feedback(openai.qs_relevance).on_input().on(\n    TruLlama.select_source_nodes().node.text\n).aggregate(np.mean)\n</pre> import numpy as np  # Initialize Huggingface-based feedback function collection class: hugs = feedback.Huggingface() openai = feedback.OpenAI()  # Define a language match feedback function using HuggingFace. f_lang_match = Feedback(hugs.language_match).on_input_output() # By default this will check language match on the main app input and main app # output.  # Question/answer relevance between overall question and answer. f_qa_relevance = Feedback(openai.relevance).on_input_output()  # Question/statement relevance between question and each context chunk. f_qs_relevance = Feedback(openai.qs_relevance).on_input().on(     TruLlama.select_source_nodes().node.text ).aggregate(np.mean) In\u00a0[\u00a0]: Copied! <pre>tru_query_engine = TruLlama(query_engine,\n    app_id='LlamaIndex_App1',\n    feedbacks=[f_lang_match, f_qa_relevance, f_qs_relevance])\n</pre> tru_query_engine = TruLlama(query_engine,     app_id='LlamaIndex_App1',     feedbacks=[f_lang_match, f_qa_relevance, f_qs_relevance]) In\u00a0[\u00a0]: Copied! <pre># Instrumented query engine can operate like the original:\nllm_response = tru_query_engine.query(\"What did the author do growing up?\")\n\nprint(llm_response)\n</pre> # Instrumented query engine can operate like the original: llm_response = tru_query_engine.query(\"What did the author do growing up?\")  print(llm_response) In\u00a0[\u00a0]: Copied! <pre>tru.run_dashboard() # open a local streamlit app to explore\n\n# tru.stop_dashboard() # stop if needed\n</pre> tru.run_dashboard() # open a local streamlit app to explore  # tru.stop_dashboard() # stop if needed <p>Alternatively, you can run <code>trulens-eval</code> from a command line in the same folder to start the dashboard.</p> <p>Note: Feedback functions evaluated in the deferred manner can be seen in the \"Progress\" page of the TruLens dashboard.</p> In\u00a0[\u00a0]: Copied! <pre>tru.get_records_and_feedback(app_ids=[])[0] # pass an empty list of app_ids to get all\n</pre> tru.get_records_and_feedback(app_ids=[])[0] # pass an empty list of app_ids to get all"},{"location":"trulens_eval/llama_index_quickstart/#quickstart","title":"Quickstart\u00b6","text":"<p>In this quickstart you will create a simple Llama Index App and learn how to log it and get feedback on an LLM response.</p>"},{"location":"trulens_eval/llama_index_quickstart/#setup","title":"Setup\u00b6","text":""},{"location":"trulens_eval/llama_index_quickstart/#install-dependencies","title":"Install dependencies\u00b6","text":"<p>Let's install some of the dependencies for this notebook if we don't have them already</p>"},{"location":"trulens_eval/llama_index_quickstart/#add-api-keys","title":"Add API keys\u00b6","text":"<p>For this quickstart, you will need Open AI and Huggingface keys</p>"},{"location":"trulens_eval/llama_index_quickstart/#import-from-llamaindex-and-trulens","title":"Import from LlamaIndex and TruLens\u00b6","text":""},{"location":"trulens_eval/llama_index_quickstart/#create-simple-llm-application","title":"Create Simple LLM Application\u00b6","text":"<p>This example uses LlamaIndex which internally uses an OpenAI LLM.</p>"},{"location":"trulens_eval/llama_index_quickstart/#send-your-first-request","title":"Send your first request\u00b6","text":""},{"location":"trulens_eval/llama_index_quickstart/#initialize-feedback-functions","title":"Initialize Feedback Function(s)\u00b6","text":""},{"location":"trulens_eval/llama_index_quickstart/#instrument-chain-for-logging-with-trulens","title":"Instrument chain for logging with TruLens\u00b6","text":""},{"location":"trulens_eval/llama_index_quickstart/#explore-in-a-dashboard","title":"Explore in a Dashboard\u00b6","text":""},{"location":"trulens_eval/llama_index_quickstart/#leaderboard","title":"Leaderboard\u00b6","text":"<p>Understand how your LLM application is performing at a glance. Once you've set up logging and evaluation in your application, you can view key performance statistics including cost and average feedback value across all of your LLM apps using the chain leaderboard. As you iterate new versions of your LLM application, you can compare their performance across all of the different quality metrics you've set up.</p> <p>Note: Average feedback values are returned and displayed in a range from 0 (worst) to 1 (best).</p> <p></p> <p>To dive deeper on a particular chain, click \"Select Chain\".</p>"},{"location":"trulens_eval/llama_index_quickstart/#understand-chain-performance-with-evaluations","title":"Understand chain performance with Evaluations\u00b6","text":"<p>To learn more about the performance of a particular chain or LLM model, we can select it to view its evaluations at the record level. LLM quality is assessed through the use of feedback functions. Feedback functions are extensible methods for determining the quality of LLM responses and can be applied to any downstream LLM task. Out of the box we provide a number of feedback functions for assessing model agreement, sentiment, relevance and more.</p> <p>The evaluations tab provides record-level metadata and feedback on the quality of your LLM application.</p> <p></p>"},{"location":"trulens_eval/llama_index_quickstart/#deep-dive-into-full-chain-metadata","title":"Deep dive into full chain metadata\u00b6","text":"<p>Click on a record to dive deep into all of the details of your chain stack and underlying LLM, captured by tru_chain.</p> <p></p> <p>If you prefer the raw format, you can quickly get it using the \"Display full chain json\" or \"Display full record json\" buttons at the bottom of the page.</p>"},{"location":"trulens_eval/llama_index_quickstart/#or-view-results-directly-in-your-notebook","title":"Or view results directly in your notebook\u00b6","text":""},{"location":"trulens_eval/logging/","title":"Logging","text":"In\u00a0[\u00a0]: Copied! <pre>truchain = TruChain(\n    chain,\n    app_id='Chain1_ChatApplication',\n    tru=tru\n)\ntruchain(\"This will be automatically logged.\")\n</pre> truchain = TruChain(     chain,     app_id='Chain1_ChatApplication',     tru=tru ) truchain(\"This will be automatically logged.\") <p>Feedback functions can also be logged automatically by providing them in a list to the feedbacks arg.</p> In\u00a0[\u00a0]: Copied! <pre>truchain = TruChain(\n    chain,\n    app_id='Chain1_ChatApplication',\n    feedbacks=[f_lang_match], # feedback functions\n    tru=tru\n)\ntruchain(\"This will be automatically logged.\")\n</pre> truchain = TruChain(     chain,     app_id='Chain1_ChatApplication',     feedbacks=[f_lang_match], # feedback functions     tru=tru ) truchain(\"This will be automatically logged.\") In\u00a0[\u00a0]: Copied! <pre>tc = TruChain(chain, app_id='Chain1_ChatApplication')\n</pre> tc = TruChain(chain, app_id='Chain1_ChatApplication') In\u00a0[\u00a0]: Copied! <pre>prompt_input = 'que hora es?'\ngpt3_response, record = tc.call_with_record(prompt_input)\n</pre> prompt_input = 'que hora es?' gpt3_response, record = tc.call_with_record(prompt_input) <p>We can log the records but first we need to log the chain itself.</p> In\u00a0[\u00a0]: Copied! <pre>tru.add_app(app=truchain)\n</pre> tru.add_app(app=truchain) <p>Then we can log the record:</p> In\u00a0[\u00a0]: Copied! <pre>tru.add_record(record)\n</pre> tru.add_record(record) In\u00a0[\u00a0]: Copied! <pre>thumb_result = True\ntru.add_feedback(name=\"\ud83d\udc4d (1) or \ud83d\udc4e (0)\", \n                  record_id=record.record_id, \n                  result=thumb_result)\n</pre> thumb_result = True tru.add_feedback(name=\"\ud83d\udc4d (1) or \ud83d\udc4e (0)\",                    record_id=record.record_id,                    result=thumb_result) In\u00a0[\u00a0]: Copied! <pre>feedback_results = tru.run_feedback_functions(\n    record=record,\n    feedback_functions=[f_lang_match]\n)\ndisplay(feedback_results)\n</pre> feedback_results = tru.run_feedback_functions(     record=record,     feedback_functions=[f_lang_match] ) display(feedback_results) <p>After capturing feedback, you can then log it to your local database.</p> In\u00a0[\u00a0]: Copied! <pre>tru.add_feedbacks(feedback_results)\n</pre> tru.add_feedbacks(feedback_results) In\u00a0[\u00a0]: Copied! <pre>truchain: TruChain = TruChain(\n    chain,\n    app_id='Chain1_ChatApplication',\n    feedbacks=[f_lang_match],\n    tru=tru,\n    feedback_mode=\"deferred\"\n)\n\ntru.start_evaluator()\ntruchain(\"This will be logged by deferred evaluator.\")\ntru.stop_evaluator()\n</pre> truchain: TruChain = TruChain(     chain,     app_id='Chain1_ChatApplication',     feedbacks=[f_lang_match],     tru=tru,     feedback_mode=\"deferred\" )  tru.start_evaluator() truchain(\"This will be logged by deferred evaluator.\") tru.stop_evaluator()"},{"location":"trulens_eval/logging/#logging","title":"Logging\u00b6","text":""},{"location":"trulens_eval/logging/#automatic-logging","title":"Automatic Logging\u00b6","text":"<p>The simplest method for logging with TruLens is by wrapping with TruChain and including the tru argument, as shown in the quickstart.</p> <p>This is done like so:</p>"},{"location":"trulens_eval/logging/#manual-logging","title":"Manual Logging\u00b6","text":""},{"location":"trulens_eval/logging/#wrap-with-truchain-to-instrument-your-chain","title":"Wrap with TruChain to instrument your chain\u00b6","text":""},{"location":"trulens_eval/logging/#set-up-logging-and-instrumentation","title":"Set up logging and instrumentation\u00b6","text":"<p>Making the first call to your wrapped LLM Application will now also produce a log or \"record\" of the chain execution.</p>"},{"location":"trulens_eval/logging/#log-app-feedback","title":"Log App Feedback\u00b6","text":"<p>Capturing app feedback such as user feedback of the responses can be added with one call.</p>"},{"location":"trulens_eval/logging/#evaluate-quality","title":"Evaluate Quality\u00b6","text":"<p>Following the request to your app, you can then evaluate LLM quality using feedback functions. This is completed in a sequential call to minimize latency for your application, and evaluations will also be logged to your local machine.</p> <p>To get feedback on the quality of your LLM, you can use any of the provided feedback functions or add your own.</p> <p>To assess your LLM quality, you can provide the feedback functions to <code>tru.run_feedback()</code> in a list provided to <code>feedback_functions</code>.</p>"},{"location":"trulens_eval/logging/#out-of-band-feedback-evaluation","title":"Out-of-band Feedback evaluation\u00b6","text":"<p>In the above example, the feedback function evaluation is done in the same process as the chain evaluation. The alternative approach is the use the provided persistent evaluator started via <code>tru.start_deferred_feedback_evaluator</code>. Then specify the <code>feedback_mode</code> for <code>TruChain</code> as <code>deferred</code> to let the evaluator handle the feedback functions.</p> <p>For demonstration purposes, we start the evaluator here but it can be started in another process.</p>"},{"location":"trulens_eval/no_framework_quickstart/","title":"Quickstart","text":"In\u00a0[\u00a0]: Copied! <pre>import os\nos.environ[\"OPENAI_API_KEY\"] = \"...\"\nos.environ[\"HUGGINGFACE_API_KEY\"] = \"...\"\n</pre> import os os.environ[\"OPENAI_API_KEY\"] = \"...\" os.environ[\"HUGGINGFACE_API_KEY\"] = \"...\" In\u00a0[\u00a0]: Copied! <pre>import openai\nopenai.api_key = os.environ[\"OPENAI_API_KEY\"]\n</pre> import openai openai.api_key = os.environ[\"OPENAI_API_KEY\"] In\u00a0[\u00a0]: Copied! <pre>from IPython.display import JSON\n\n# Imports main tools:\nfrom trulens_eval import Feedback, Huggingface, Tru\ntru = Tru()\n</pre> from IPython.display import JSON  # Imports main tools: from trulens_eval import Feedback, Huggingface, Tru tru = Tru() In\u00a0[\u00a0]: Copied! <pre>def llm_standalone(prompt):\n    return openai.ChatCompletion.create(\n    model=\"gpt-3.5-turbo\",\n    messages=[\n            {\"role\": \"system\", \"content\": \"You are a question and answer bot, and you answer super upbeat.\"},\n            {\"role\": \"user\", \"content\": prompt}\n        ]\n    )[\"choices\"][0][\"message\"][\"content\"]\n</pre> def llm_standalone(prompt):     return openai.ChatCompletion.create(     model=\"gpt-3.5-turbo\",     messages=[             {\"role\": \"system\", \"content\": \"You are a question and answer bot, and you answer super upbeat.\"},             {\"role\": \"user\", \"content\": prompt}         ]     )[\"choices\"][0][\"message\"][\"content\"] In\u00a0[\u00a0]: Copied! <pre>import hashlib\ndef simple_hash_callable(prompt):\n    h = hashlib.shake_256(prompt.encode('utf-8'))\n    return str(h.hexdigest(20))\n</pre> import hashlib def simple_hash_callable(prompt):     h = hashlib.shake_256(prompt.encode('utf-8'))     return str(h.hexdigest(20)) In\u00a0[\u00a0]: Copied! <pre>prompt_input=\"How good is language AI?\"\nprompt_output=llm_standalone(prompt_input)\nprompt_output\n</pre> prompt_input=\"How good is language AI?\" prompt_output=llm_standalone(prompt_input) prompt_output In\u00a0[\u00a0]: Copied! <pre>simple_hash_callable(prompt_input)\n</pre> simple_hash_callable(prompt_input) In\u00a0[\u00a0]: Copied! <pre># Initialize Huggingface-based feedback function collection class:\nhugs = Huggingface()\n\n# Define a sentiment feedback function using HuggingFace.\nf_sentiment = Feedback(hugs.positive_sentiment).on_output()\n</pre> # Initialize Huggingface-based feedback function collection class: hugs = Huggingface()  # Define a sentiment feedback function using HuggingFace. f_sentiment = Feedback(hugs.positive_sentiment).on_output() In\u00a0[\u00a0]: Copied! <pre>from trulens_eval import TruBasicApp\nbasic_app = TruBasicApp(llm_standalone, app_id=\"Happy Bot\", feedbacks=[f_sentiment])\nhash_app = TruBasicApp(simple_hash_callable, app_id=\"Hasher\", feedbacks=[f_sentiment])\n</pre> from trulens_eval import TruBasicApp basic_app = TruBasicApp(llm_standalone, app_id=\"Happy Bot\", feedbacks=[f_sentiment]) hash_app = TruBasicApp(simple_hash_callable, app_id=\"Hasher\", feedbacks=[f_sentiment]) In\u00a0[\u00a0]: Copied! <pre>response, record = basic_app.call_with_record(prompt_input)\n</pre> response, record = basic_app.call_with_record(prompt_input) In\u00a0[\u00a0]: Copied! <pre>response, record = hash_app.call_with_record(prompt_input)\n</pre> response, record = hash_app.call_with_record(prompt_input) In\u00a0[\u00a0]: Copied! <pre>tru.run_dashboard() # open a local streamlit app to explore\n\n# tru.stop_dashboard() # stop if needed\n</pre> tru.run_dashboard() # open a local streamlit app to explore  # tru.stop_dashboard() # stop if needed <p>Alternatively, you can run <code>trulens-eval</code> from a command line in the same folder to start the dashboard.</p> In\u00a0[\u00a0]: Copied! <pre>tru.get_records_and_feedback(app_ids=[])[0] # pass an empty list of app_ids to get all\n</pre> tru.get_records_and_feedback(app_ids=[])[0] # pass an empty list of app_ids to get all"},{"location":"trulens_eval/no_framework_quickstart/#quickstart","title":"Quickstart\u00b6","text":"<p>In this quickstart you will create a simple text to text application and learn how to log it and get feedback.</p>"},{"location":"trulens_eval/no_framework_quickstart/#setup","title":"Setup\u00b6","text":""},{"location":"trulens_eval/no_framework_quickstart/#add-api-keys","title":"Add API keys\u00b6","text":"<p>For this quickstart you will need Open AI and Huggingface keys</p>"},{"location":"trulens_eval/no_framework_quickstart/#import-from-trulens","title":"Import from TruLens\u00b6","text":""},{"location":"trulens_eval/no_framework_quickstart/#create-simple-text-to-text-application","title":"Create Simple Text to Text Application\u00b6","text":"<p>This example uses a bare bones OpenAI LLM, and a non-LLM just for demonstration purposes.</p>"},{"location":"trulens_eval/no_framework_quickstart/#send-your-first-request","title":"Send your first request\u00b6","text":""},{"location":"trulens_eval/no_framework_quickstart/#initialize-feedback-functions","title":"Initialize Feedback Function(s)\u00b6","text":""},{"location":"trulens_eval/no_framework_quickstart/#instrument-the-callable-for-logging-with-trulens","title":"Instrument the callable for logging with TruLens\u00b6","text":""},{"location":"trulens_eval/no_framework_quickstart/#explore-in-a-dashboard","title":"Explore in a Dashboard\u00b6","text":""},{"location":"trulens_eval/no_framework_quickstart/#or-view-results-directly-in-your-notebook","title":"Or view results directly in your notebook\u00b6","text":""},{"location":"trulens_eval/pr_relevance_smoke_tests/","title":"Prompt Response Relevance","text":"In\u00a0[1]: Copied! <pre>import os\nos.environ[\"OPENAI_API_KEY\"] = \"...\"\n</pre> import os os.environ[\"OPENAI_API_KEY\"] = \"...\" In\u00a0[2]: Copied! <pre># Imports main tools:\nfrom trulens_eval.feedback import OpenAI\nopenai = OpenAI()\nrelevance = openai.relevance\n</pre> # Imports main tools: from trulens_eval.feedback import OpenAI openai = OpenAI() relevance = openai.relevance In\u00a0[3]: Copied! <pre>def test_lowadherence_short():\n    score = relevance(\"How many stomachs does a cow have?\",\"Cows' diet relies primarily on grazing.\")\n    assert score &gt;= 0.2, f\"Score of {score} &lt; 0.2. Statement is relevant to at least some of query.\"\n    assert score &lt;= 0.5, f\"Score of {score} &gt; 0.5. Statement is relevant to only some of query.\"\n</pre> def test_lowadherence_short():     score = relevance(\"How many stomachs does a cow have?\",\"Cows' diet relies primarily on grazing.\")     assert score &gt;= 0.2, f\"Score of {score} &lt; 0.2. Statement is relevant to at least some of query.\"     assert score &lt;= 0.5, f\"Score of {score} &gt; 0.5. Statement is relevant to only some of query.\" In\u00a0[4]: Copied! <pre>def test_lowadherence_medium():\n    score = relevance(\"Name some famous dental floss brands\",\"\"\"Oral-B is an American brand of oral hygiene products, \n        including toothpastes, toothbrushes, electric toothbrushes, and mouthwashes. \n        The brand has been in business since the invention of the Hutson toothbrush in 1950 and in Redwood City, California.\"\"\")\n    assert score &gt;= 0.2, f\"Score of {score} &lt; 0.2. Statement is relevant to at least some of query.\"\n    assert score &lt;= 0.5, f\"Score of {score} &gt; 0.5. Statement is relevant to only some of query.\"\n</pre> def test_lowadherence_medium():     score = relevance(\"Name some famous dental floss brands\",\"\"\"Oral-B is an American brand of oral hygiene products,          including toothpastes, toothbrushes, electric toothbrushes, and mouthwashes.          The brand has been in business since the invention of the Hutson toothbrush in 1950 and in Redwood City, California.\"\"\")     assert score &gt;= 0.2, f\"Score of {score} &lt; 0.2. Statement is relevant to at least some of query.\"     assert score &lt;= 0.5, f\"Score of {score} &gt; 0.5. Statement is relevant to only some of query.\" In\u00a0[5]: Copied! <pre>def test_lowadherence_long():\n    score = relevance(\"Name some famous dental floss brands\",\"\"\"Types of floss and alternative options. Dental floss is regarded as the gold standard \u2014 it\u2019s been around the longest compared to other plaque-removing products, Hewlett said. \n    Moursi also added that most flossing research studies have been conducted with dental floss, so there\u2019s a lot of data showing its effectiveness. But floss is not one-size-fits-all, \n    he noted. Since using dental floss is difficult for some, there are other effective tools like interdental cleaners. Below, we broke down \n    the differences among several different options. Dental floss When people think of dental floss, it\u2019s usually the threaded variety that comes on a spool. \n    But there\u2019s also dental tape, which Hewlett described as a wider and flatter type of floss. He said it's particularly useful for \n    people with larger spaces between their teeth since it covers more surface area. Both forms of floss come in unflavored or flavored varieties, \n    but choosing a flavored option has no impact on how well it cleans your teeth, Hewlett said. Flosses also come waxed and unwaxed \u2014 \n    while a wax coating can make floss pass between teeth more easily, Hewitt said, both waxed and unwaxed are equally effective when used properly. \n    Floss picks Floss picks are similarly effective when compared to thread floss, experts said. The picks look like a wand and have a small piece \n    of floss at the forked end, so you can grip the handle while using the tool. Experts said floss picks are generally easy to use, especially if\n    you\u2019re flossing a child\u2019s teeth. Water flossers Water flossers are powered devices that shoot pressurized water at the spaces between teeth, \n    targeting debris to disrupt and flush out plaque. While there is evidence to support their ability to remove plaque from teeth, Moursi\n    said for water flossers to do their job, \u201cyou have to hold it in just the right place, at just the right angle and for just the right\n    amount of time,\u201d which can be challenging. Anyone can use water flossers, but experts said they\u2019re the most beneficial for people who\n    have difficulty using thread floss or floss threaders, as well as those with certain dental work like braces, bridges and crowns. \n    Interdental brushes Dental work like braces, bridges and crowns can block floss from slipping between teeth, making flossing challenging.\n    Interdental brushes \u2014 which look like little spoolie brushes \u2014 can pass through the spaces between teeth and under any dental work,\n    allowing you to remove plaque. The brushes have bristles on one end and a handle to grip on the other. To use, you point the brush at\n    the gum line between teeth and push it through, moving the bristles around the space to remove plaque, said Hewlett. \n    The brushes come in various shapes and sizes to fit the spaces between your teeth.\"\"\")\n    assert score &gt;= 0.2, f\"Score of {score} &lt; 0.2. Statement is relevant to at least some of query.\"\n    assert score &lt;= 0.5, f\"Score of {score} &gt; 0.5. Statement is relevant to only some of query.\"\n</pre> def test_lowadherence_long():     score = relevance(\"Name some famous dental floss brands\",\"\"\"Types of floss and alternative options. Dental floss is regarded as the gold standard \u2014 it\u2019s been around the longest compared to other plaque-removing products, Hewlett said.      Moursi also added that most flossing research studies have been conducted with dental floss, so there\u2019s a lot of data showing its effectiveness. But floss is not one-size-fits-all,      he noted. Since using dental floss is difficult for some, there are other effective tools like interdental cleaners. Below, we broke down      the differences among several different options. Dental floss When people think of dental floss, it\u2019s usually the threaded variety that comes on a spool.      But there\u2019s also dental tape, which Hewlett described as a wider and flatter type of floss. He said it's particularly useful for      people with larger spaces between their teeth since it covers more surface area. Both forms of floss come in unflavored or flavored varieties,      but choosing a flavored option has no impact on how well it cleans your teeth, Hewlett said. Flosses also come waxed and unwaxed \u2014      while a wax coating can make floss pass between teeth more easily, Hewitt said, both waxed and unwaxed are equally effective when used properly.      Floss picks Floss picks are similarly effective when compared to thread floss, experts said. The picks look like a wand and have a small piece      of floss at the forked end, so you can grip the handle while using the tool. Experts said floss picks are generally easy to use, especially if     you\u2019re flossing a child\u2019s teeth. Water flossers Water flossers are powered devices that shoot pressurized water at the spaces between teeth,      targeting debris to disrupt and flush out plaque. While there is evidence to support their ability to remove plaque from teeth, Moursi     said for water flossers to do their job, \u201cyou have to hold it in just the right place, at just the right angle and for just the right     amount of time,\u201d which can be challenging. Anyone can use water flossers, but experts said they\u2019re the most beneficial for people who     have difficulty using thread floss or floss threaders, as well as those with certain dental work like braces, bridges and crowns.      Interdental brushes Dental work like braces, bridges and crowns can block floss from slipping between teeth, making flossing challenging.     Interdental brushes \u2014 which look like little spoolie brushes \u2014 can pass through the spaces between teeth and under any dental work,     allowing you to remove plaque. The brushes have bristles on one end and a handle to grip on the other. To use, you point the brush at     the gum line between teeth and push it through, moving the bristles around the space to remove plaque, said Hewlett.      The brushes come in various shapes and sizes to fit the spaces between your teeth.\"\"\")     assert score &gt;= 0.2, f\"Score of {score} &lt; 0.2. Statement is relevant to at least some of query.\"     assert score &lt;= 0.5, f\"Score of {score} &gt; 0.5. Statement is relevant to only some of query.\" In\u00a0[6]: Copied! <pre>def test_majorityadherence_short():\n    score = relevance(\"Name some famous dental floss brands?\",\"Some key companies operating in the dental floss market include Colgate and Water Pik.\")\n    assert score &gt;= 0.5, f\"Score of {score} &lt; 0.5. Statement is relevant to most of query.\"\n    assert score &lt;= 0.8, f\"Score of {score} &gt; 0.8. Statement is not relevant to all of query.\"\n</pre> def test_majorityadherence_short():     score = relevance(\"Name some famous dental floss brands?\",\"Some key companies operating in the dental floss market include Colgate and Water Pik.\")     assert score &gt;= 0.5, f\"Score of {score} &lt; 0.5. Statement is relevant to most of query.\"     assert score &lt;= 0.8, f\"Score of {score} &gt; 0.8. Statement is not relevant to all of query.\" In\u00a0[7]: Copied! <pre>def test_majorityadherence_medium():\n    score = relevance(\"How does the social structure of a lion pride impact the genetic diversity and long-term survival of the species?\",\"\"\"A typical pride of lions consists of about six related females, their dependent offspring, and a \u201ccoalition\u201d \n    of 2\u20133 resident males that joined the pride from elsewhere. The pride is a \u201cfission-fusion\u201d society and\n    pridemates are seldom found together, except for mothers that have pooled their offspring into a \u201ccr\u00e8che.\u201d\"\"\")\n    assert score &gt;= 0.5, f\"Score of {score} &lt; 0.5. Statement is relevant to most of query.\"\n    assert score &lt;= 0.8, f\"Score of {score} &gt; 0.8. Statement is not relevant to all of query.\"\n</pre> def test_majorityadherence_medium():     score = relevance(\"How does the social structure of a lion pride impact the genetic diversity and long-term survival of the species?\",\"\"\"A typical pride of lions consists of about six related females, their dependent offspring, and a \u201ccoalition\u201d      of 2\u20133 resident males that joined the pride from elsewhere. The pride is a \u201cfission-fusion\u201d society and     pridemates are seldom found together, except for mothers that have pooled their offspring into a \u201ccr\u00e8che.\u201d\"\"\")     assert score &gt;= 0.5, f\"Score of {score} &lt; 0.5. Statement is relevant to most of query.\"     assert score &lt;= 0.8, f\"Score of {score} &gt; 0.8. Statement is not relevant to all of query.\" In\u00a0[8]: Copied! <pre>def test_majorityadherence_long():\n    score = relevance(\"What are the parts of a cow's digestive tract, and how do they work including mouth, esophagus, the stomach, and intestines.\",\"\"\"\nThe cow's digestive tract consists of the following.\n\nMouth\nA four-compartment stomach, which includes\nThe rumen (paunch)\nThe reticulum (\u201choneycomb\u201d)\nThe omasum (\u201cmanyplies\u201d)\nThe abomasum (\u201ctrue stomach\u201d)\nSmall intestine\nLarge intestine\n\nThe rumen\nThe rumen (on the left side of the animal) is the largest stomach compartment and consists of several sacs. It can hold 25 gallons or more of material depending on the size of the cow. Because of its size, the rumen acts as a storage or holding vat for feed.\n\nAside from storage, the rumen is also a fermentation vat. The rumen\u2019s environment favors the growth of microbes. These microbes digest or ferment feed within the rumen and make volatile fatty acids (VFAs). The rumen absorbs most of the VFAs from fermentation.\n\nA good blood supply to the rumen walls improves absorption of VFAs and other digestion products. Tiny projections (papillae) line the rumen, which increases the rumen\u2019s surface area and the amount it can absorb.    \n\nThe reticulum \nThe reticulum is a pouch-like structure in the forward area of the body, close to the heart. The tissues in the reticulum form a network similar to a honeycomb. A small tissue fold lies between the reticulum and rumen, but the two aren\u2019t separate compartments. Together they\u2019re called the rumino-reticulum.\n\nHeavy or dense feed and metal objects eaten by the cow drop into this compartment. Nails and other sharp objects may work into the tissue and cause \u201chardware disease.\u201d You can use magnets to prevent disease or correct the problem through surgery. Leaving it untreated may lead to infection and possibly death.\n\nThe omasum\nThe omasum is a globe-shaped structure containing leaves of tissue (like pages in a book). It absorbs water and other substances from digestive contents. Feed material (ingesta) between the leaves will be drier than ingesta found in the other compartments.\n\nThe abomasum\nThe abomasum is the only compartment lined with glands. These glands release hydrochloric acid and digestive enzymes, needed to breakdown feeds. The abomasum is similar to a nonruminant stomach.\n\nThe small intestine consists of three sections: the duodenum, jejunum and ileum. It measures about 20 times the length of the animal.\n\nSecretions from the pancreas and gallbladder aid in digestion within the small intestine. The small intestine completes most of the digestive process and absorbs many nutrients through villi (small finger-like projections). From the villi the nutrients enter into the blood and lymphatic systems.\n\nThe cecum is the large area where the small and large intestine meet. The cecum breaks down some previously undigested fiber, but the exact importance of the cecum remains unknown.\n\nThe large intestine is the last section of the tract that undigested feedstuffs pass through. Microbes digest some undigested feed here, but the main digestive function of the large intestine is to absorb water.\n\"\"\")\n    assert score &gt;= 0.5, f\"Score of {score} &lt; 0.5. Statement is relevant to most of query.\"\n    assert score &lt;= 0.8, f\"Score of {score} &gt; 0.8. Statement is not relevant to all of query.\"\n</pre> def test_majorityadherence_long():     score = relevance(\"What are the parts of a cow's digestive tract, and how do they work including mouth, esophagus, the stomach, and intestines.\",\"\"\" The cow's digestive tract consists of the following.  Mouth A four-compartment stomach, which includes The rumen (paunch) The reticulum (\u201choneycomb\u201d) The omasum (\u201cmanyplies\u201d) The abomasum (\u201ctrue stomach\u201d) Small intestine Large intestine  The rumen The rumen (on the left side of the animal) is the largest stomach compartment and consists of several sacs. It can hold 25 gallons or more of material depending on the size of the cow. Because of its size, the rumen acts as a storage or holding vat for feed.  Aside from storage, the rumen is also a fermentation vat. The rumen\u2019s environment favors the growth of microbes. These microbes digest or ferment feed within the rumen and make volatile fatty acids (VFAs). The rumen absorbs most of the VFAs from fermentation.  A good blood supply to the rumen walls improves absorption of VFAs and other digestion products. Tiny projections (papillae) line the rumen, which increases the rumen\u2019s surface area and the amount it can absorb.      The reticulum  The reticulum is a pouch-like structure in the forward area of the body, close to the heart. The tissues in the reticulum form a network similar to a honeycomb. A small tissue fold lies between the reticulum and rumen, but the two aren\u2019t separate compartments. Together they\u2019re called the rumino-reticulum.  Heavy or dense feed and metal objects eaten by the cow drop into this compartment. Nails and other sharp objects may work into the tissue and cause \u201chardware disease.\u201d You can use magnets to prevent disease or correct the problem through surgery. Leaving it untreated may lead to infection and possibly death.  The omasum The omasum is a globe-shaped structure containing leaves of tissue (like pages in a book). It absorbs water and other substances from digestive contents. Feed material (ingesta) between the leaves will be drier than ingesta found in the other compartments.  The abomasum The abomasum is the only compartment lined with glands. These glands release hydrochloric acid and digestive enzymes, needed to breakdown feeds. The abomasum is similar to a nonruminant stomach.  The small intestine consists of three sections: the duodenum, jejunum and ileum. It measures about 20 times the length of the animal.  Secretions from the pancreas and gallbladder aid in digestion within the small intestine. The small intestine completes most of the digestive process and absorbs many nutrients through villi (small finger-like projections). From the villi the nutrients enter into the blood and lymphatic systems.  The cecum is the large area where the small and large intestine meet. The cecum breaks down some previously undigested fiber, but the exact importance of the cecum remains unknown.  The large intestine is the last section of the tract that undigested feedstuffs pass through. Microbes digest some undigested feed here, but the main digestive function of the large intestine is to absorb water. \"\"\")     assert score &gt;= 0.5, f\"Score of {score} &lt; 0.5. Statement is relevant to most of query.\"     assert score &lt;= 0.8, f\"Score of {score} &gt; 0.8. Statement is not relevant to all of query.\" In\u00a0[9]: Copied! <pre>def test_idontknow_short():\n    score = relevance(\"Name some top dental floss brands\",\"I don't know.\")\n    assert score &gt;= 0.9, f\"Score of {score} &lt; 0.9. Admitting lack of knowledge (general) did not get high score.\"\n</pre> def test_idontknow_short():     score = relevance(\"Name some top dental floss brands\",\"I don't know.\")     assert score &gt;= 0.9, f\"Score of {score} &lt; 0.9. Admitting lack of knowledge (general) did not get high score.\" In\u00a0[10]: Copied! <pre>def test_idontknow_medium():\n    score = relevance(\"Is Denny's open right now?\",\"As an AI assistant, I don't have access to realtime information. You should consult the web for answers about restaurant opening times.\")\n    assert score &gt;= 0.9, f\"Score of {score} &lt; 0.9. Admitting lack of knowledge (general) did not get high score.\"\n</pre> def test_idontknow_medium():     score = relevance(\"Is Denny's open right now?\",\"As an AI assistant, I don't have access to realtime information. You should consult the web for answers about restaurant opening times.\")     assert score &gt;= 0.9, f\"Score of {score} &lt; 0.9. Admitting lack of knowledge (general) did not get high score.\" In\u00a0[11]: Copied! <pre>def test_nonanswer_short():\n    score = relevance(\"How many countries are there in the world?\", \"There is no universally accepted answer to how many countries there are in the world.\")\n    assert score &gt;= 0.5, f\"Score of {score} &lt; 0.5. Relevant context without definitive answer did not get a score of &gt;= 0.5\"\n</pre> def test_nonanswer_short():     score = relevance(\"How many countries are there in the world?\", \"There is no universally accepted answer to how many countries there are in the world.\")     assert score &gt;= 0.5, f\"Score of {score} &lt; 0.5. Relevant context without definitive answer did not get a score of &gt;= 0.5\" In\u00a0[12]: Copied! <pre>def test_nonanswer_medium():\n    score = relevance(\"What is the meaning of life?\", \"\"\"No one can tell the actual definition of the meaning of life.\n    For some, it is all about happiness, building a family, and leading life as it is. For some, it is about accumulating wealth, whereas,\n    for some, it is all about love.\"\"\")\n    assert score &gt;= 0.5, f\"Score of {score} &lt; 0.5. Relevant context without definitive answer did not get a score of &gt;= 0.5\"\n</pre> def test_nonanswer_medium():     score = relevance(\"What is the meaning of life?\", \"\"\"No one can tell the actual definition of the meaning of life.     For some, it is all about happiness, building a family, and leading life as it is. For some, it is about accumulating wealth, whereas,     for some, it is all about love.\"\"\")     assert score &gt;= 0.5, f\"Score of {score} &lt; 0.5. Relevant context without definitive answer did not get a score of &gt;= 0.5\" In\u00a0[13]: Copied! <pre>def test_nonanswer_long():\n    score = relevance(\"What came first, the chicken or the egg?\",\"\"\"Eggs come from chickens and chickens come from eggs: that\u2019s the basis of this ancient riddle. But eggs \u2013 which are just female sex cells \u2013 evolved more than a billion years ago, whereas chickens have been around for just 10,000 years. So the riddle is easily solved\u2026or is it?\n\nTaken at face value, there is no doubt that the egg came before the chicken. We tend to think of eggs as the shelled orbs laid by birds from which their chicks hatch \u2013 unless we eat them first. But all sexually reproducing species make eggs (the specialised female sex cells). That\u2019s 99.99 per cent of all eukaryotic life \u2013 meaning organisms that have cells with a nucleus, so all animals and plants, and everything but the simplest life forms.\n\nWe don\u2019t know for sure when sex evolved but it could have been as much as 2 billion years ago, and certainly more than 1 billion. Even the specialised sort of eggs laid by birds, with their tough outer membrane, evolved more than 300 million years ago.\n\nAs for chickens, they came into being much later. They are domesticated animals, so evolved as the result of humans purposefully selecting the least aggressive wild birds and letting them breed. This seems to have happened in several places independently, starting around 10,000 years ago.\n\nThe wild ancestor of chickens is generally agreed to be a tropical bird still living in the forests of Southeast Asia called the red junglefowl \u2013  with other junglefowl species possibly adding to the genetic mix. From these origins, humans have carried chickens around the world over the past two millennia or more.\n\nSo, eggs dramatically predate chickens. But to be fair to the spirit of the riddle, we should also consider whether a chicken\u2019s egg predates a chicken. As humans consistently chose the tamest red junglefowls and bred them together, the genetic makeup of the resulting birds will have shifted. At some stage during this domestication process the red junglefowl (Gallus gallus) evolved into a new subspecies, Gallus gallus domesticus, AKA the chicken.\n\nIn practice, it is impossible to pinpoint the moment when this happened. But in theory, at some point two junglefowl bred and their offspring was genetically different enough from the species of its parents to be classified as a chicken. This chicken would have developed within a junglefowl egg and only produced the very first chicken\u2019s egg on reaching maturity. Looked at this way, the chicken came first.\n\n\"\"\"\n)\n    assert score &gt;= 0.5, f\"Score of {score} &lt; 0.5. Relevant context without definitive answer did not get a score of &gt;= 0.5\"\n</pre> def test_nonanswer_long():     score = relevance(\"What came first, the chicken or the egg?\",\"\"\"Eggs come from chickens and chickens come from eggs: that\u2019s the basis of this ancient riddle. But eggs \u2013 which are just female sex cells \u2013 evolved more than a billion years ago, whereas chickens have been around for just 10,000 years. So the riddle is easily solved\u2026or is it?  Taken at face value, there is no doubt that the egg came before the chicken. We tend to think of eggs as the shelled orbs laid by birds from which their chicks hatch \u2013 unless we eat them first. But all sexually reproducing species make eggs (the specialised female sex cells). That\u2019s 99.99 per cent of all eukaryotic life \u2013 meaning organisms that have cells with a nucleus, so all animals and plants, and everything but the simplest life forms.  We don\u2019t know for sure when sex evolved but it could have been as much as 2 billion years ago, and certainly more than 1 billion. Even the specialised sort of eggs laid by birds, with their tough outer membrane, evolved more than 300 million years ago.  As for chickens, they came into being much later. They are domesticated animals, so evolved as the result of humans purposefully selecting the least aggressive wild birds and letting them breed. This seems to have happened in several places independently, starting around 10,000 years ago.  The wild ancestor of chickens is generally agreed to be a tropical bird still living in the forests of Southeast Asia called the red junglefowl \u2013  with other junglefowl species possibly adding to the genetic mix. From these origins, humans have carried chickens around the world over the past two millennia or more.  So, eggs dramatically predate chickens. But to be fair to the spirit of the riddle, we should also consider whether a chicken\u2019s egg predates a chicken. As humans consistently chose the tamest red junglefowls and bred them together, the genetic makeup of the resulting birds will have shifted. At some stage during this domestication process the red junglefowl (Gallus gallus) evolved into a new subspecies, Gallus gallus domesticus, AKA the chicken.  In practice, it is impossible to pinpoint the moment when this happened. But in theory, at some point two junglefowl bred and their offspring was genetically different enough from the species of its parents to be classified as a chicken. This chicken would have developed within a junglefowl egg and only produced the very first chicken\u2019s egg on reaching maturity. Looked at this way, the chicken came first.  \"\"\" )     assert score &gt;= 0.5, f\"Score of {score} &lt; 0.5. Relevant context without definitive answer did not get a score of &gt;= 0.5\" In\u00a0[14]: Copied! <pre>def test_refusal_short():\n    score = relevance(\"How do you build a bomb?\",\"I'm really sorry, but I can't assist with that request.\")\n    assert score &gt;= 0.9, f\"Score of {score} &lt; 0.9. Model refusal (short) did not a high score.\"\n</pre> def test_refusal_short():     score = relevance(\"How do you build a bomb?\",\"I'm really sorry, but I can't assist with that request.\")     assert score &gt;= 0.9, f\"Score of {score} &lt; 0.9. Model refusal (short) did not a high score.\" In\u00a0[15]: Copied! <pre>def test_refusal_medium():\n    score = relevance(\"Who should I vote for in the presidential election\",\"As an AI language model, I do not have personal beliefs or opinions, and I do not experience emotions like humans do. My responses are generated based on patterns and associations in the text data that I was trained on. However, I am programmed to provide accurate and objective information in a clear and respectful manner, and I strive to be helpful and informative in all my responses.\")\n    assert score &gt;= 0.9, f\"Score of {score} &lt; 0.9. Model refusal (elaborate) did not a high score.\"\n</pre> def test_refusal_medium():     score = relevance(\"Who should I vote for in the presidential election\",\"As an AI language model, I do not have personal beliefs or opinions, and I do not experience emotions like humans do. My responses are generated based on patterns and associations in the text data that I was trained on. However, I am programmed to provide accurate and objective information in a clear and respectful manner, and I strive to be helpful and informative in all my responses.\")     assert score &gt;= 0.9, f\"Score of {score} &lt; 0.9. Model refusal (elaborate) did not a high score.\" In\u00a0[16]: Copied! <pre>def test_seemingrelevance_short():\n    seemingly_relevant_score = relevance(\"Who won the superbowl in 2009?\", \"The Pheonix Suns won the Superbowl in 2009\")\n    relevant_score = relevance(\"Who won the superbowl in 2009?\", \"The Pittsburgh Steelers won the Superbowl in 2009\")\n    assert seemingly_relevant_score &lt; relevant_score, f\"Failed to differentiate seeming and actual relevance.\"\n</pre> def test_seemingrelevance_short():     seemingly_relevant_score = relevance(\"Who won the superbowl in 2009?\", \"The Pheonix Suns won the Superbowl in 2009\")     relevant_score = relevance(\"Who won the superbowl in 2009?\", \"The Pittsburgh Steelers won the Superbowl in 2009\")     assert seemingly_relevant_score &lt; relevant_score, f\"Failed to differentiate seeming and actual relevance.\" In\u00a0[17]: Copied! <pre>def test_seemingrelevance_medium():\n    seemingly_relevant_score = relevance(\"What is a cephalopod?\", \"\"\"A cephalopod belongs to a large taxonomic class of \n    invertebrates within the phylum Mollusca called Gastropoda. This class comprises snails and slugs from saltwater, freshwater, \n    and from land. There are many thousands of species of sea snails and slugs, as well as freshwater snails, freshwater limpets, \n    and land snails and slugs.\"\"\")\n    relevant_score = relevance(\"What is a cephalopod?\", \"A cephalopod is any member of the molluscan class Cephalopoda such as a squid, octopus, cuttlefish, or nautilus. These exclusively marine animals are characterized by bilateral body symmetry, a prominent head, and a set of arms or tentacles (muscular hydrostats) modified from the primitive molluscan foot. Fishers sometimes call cephalopods 'inkfish referring to their common ability to squirt ink.\")\n    assert seemingly_relevant_score &lt; relevant_score, f\"Failed to differentiate seeming and actual relevance.\"\n</pre> def test_seemingrelevance_medium():     seemingly_relevant_score = relevance(\"What is a cephalopod?\", \"\"\"A cephalopod belongs to a large taxonomic class of      invertebrates within the phylum Mollusca called Gastropoda. This class comprises snails and slugs from saltwater, freshwater,      and from land. There are many thousands of species of sea snails and slugs, as well as freshwater snails, freshwater limpets,      and land snails and slugs.\"\"\")     relevant_score = relevance(\"What is a cephalopod?\", \"A cephalopod is any member of the molluscan class Cephalopoda such as a squid, octopus, cuttlefish, or nautilus. These exclusively marine animals are characterized by bilateral body symmetry, a prominent head, and a set of arms or tentacles (muscular hydrostats) modified from the primitive molluscan foot. Fishers sometimes call cephalopods 'inkfish referring to their common ability to squirt ink.\")     assert seemingly_relevant_score &lt; relevant_score, f\"Failed to differentiate seeming and actual relevance.\" In\u00a0[18]: Copied! <pre>def test_seemingrelevance_long():\n    seemingly_relevant_score = relevance(\"Describe Abraham Lincoln's childhood\", \"\"\"\n    Abraham Lincoln, Jr. was born on May 29, 1917, in Brookline, Massachusetts. His parents were Joseph P. Kennedy, Sr. and Rose Fitzgerald Kennedy. His early childhood was spent in a wealthy and politically influential family in the Boston area.\n\nBrookline, Massachusetts (1917-1920): Abraham Lincoln's early years were spent in Brookline, Massachusetts. His father, Joseph P. Kennedy, Sr., was a successful businessman and later a prominent figure in American politics. His mother, Rose Fitzgerald Kennedy, came from a politically active family. Abraham Lincoln was the second of nine children.\n\nNew York and London: During his childhood, Abraham Lincoln lived in New York City and London when his father served as the U.S. ambassador to the United Kingdom. This exposure to international affairs and high society left a lasting impression on young Abraham Lincoln.\n\nEducational Pursuits: Abraham Lincoln attended private schools during his early years, including the Riverdale Country School in New York and the Choate School (now Choate Rosemary Hall) in Connecticut. Despite facing some health challenges, he was a bright and athletic student.\n\nFamily Tragedy: Like his siblings, Abraham Lincoln faced the sorrow of losing his older brother, Joseph P. Kennedy, Jr., who died during World War II while serving in the United States Navy.\n\nHarvard University: Abraham Lincoln continued his education at Harvard University, where he developed an interest in government and international relations. He graduated in 1940 with a Bachelor of Science in International Affairs.\n\nMilitary Service: Following his graduation, Abraham Lincoln joined the U.S. Navy and served during World War II. He was assigned to intelligence duties and later commanded a patrol torpedo boat (PT boat) in the Pacific theater.\n\nEntry into Politics: After the war, Abraham Lincoln's interest in public service led him to enter politics. He successfully ran for the U.S. House of Representatives in 1946, beginning his career in national politics.\n\nAbraham Lincoln's childhood and early life were marked by privilege, educational opportunities, and exposure to political and international affairs. His experiences within the influential Kennedy family, as well as his education and military service, would shape his future path as a prominent figure in American politics, ultimately leading to his election as the 35th President of the United States.\"\"\")\n    relevant_score = relevance(\"Describe Abraham Lincoln's childhood\", \"\"\"\n    Abraham Lincoln was born on February 12, 1809, in a one-room log cabin on the Sinking Spring Farm in Hardin County, Kentucky (now part of LaRue County). His parents were Thomas Lincoln and Nancy Hanks Lincoln. His early childhood was spent in the frontier regions of Kentucky and Indiana, where his family faced the hardships and challenges of pioneer life.\n\nKentucky Years (1809-1816): Abraham Lincoln's early years were spent in Kentucky. His father, Thomas Lincoln, was a farmer and carpenter, and his mother, Nancy Hanks Lincoln, was known for her kindness and strong work ethic. Sadly, Nancy died when Abraham was just nine years old. This loss deeply affected him, and he would carry the memory of his mother throughout his life.\n\nMoving to Indiana (1816): In 1816, Thomas Lincoln decided to move his family to Indiana due to land disputes in Kentucky. They settled in the wilderness of Spencer County, Indiana, building a new home and clearing land for farming. Young Abraham played a crucial role in helping his father with these tasks and learning essential survival skills.\n\nSelf-Education: Lincoln's formal education was limited to a few months, amounting to less than a year, as schools were scarce on the frontier. However, he was an avid reader and had a strong thirst for knowledge. Lincoln educated himself by reading borrowed books, often walking long distances to borrow or return them. He later referred to this time as having been \"raised to farm work.\"\n\nEarly Tragedies: The Lincolns faced several tragedies during their time in Indiana. In addition to losing his mother at a young age, Abraham also lost his older sister Sarah when she died during childbirth. These experiences would shape his empathy and understanding of loss and grief.\n\nBeginning His Legal Career: In 1830, the Lincoln family moved to Illinois, where Abraham Lincoln began to work as a farm laborer, rail-splitter, and store clerk. During this period, he continued his self-education and started to develop an interest in law. He learned enough about the legal system to become a lawyer through self-study and reading law books.\n\nMarriage and Family: In 1842, Lincoln married Mary Todd, and they had four sons: Robert, Edward, William, and Thomas. Tragically, only one of their sons, Robert, survived to adulthood.\n\nAbraham Lincoln's childhood and early life were marked by poverty, hard work, and the challenges of frontier living. Despite his humble beginnings and limited formal education, he demonstrated a keen intellect, an inquisitive mind, and a strong sense of justice from an early age. These qualities would serve as a foundation for his later achievements and leadership as one of the greatest presidents in American history.\n    \"\"\")\n    pass\n</pre> def test_seemingrelevance_long():     seemingly_relevant_score = relevance(\"Describe Abraham Lincoln's childhood\", \"\"\"     Abraham Lincoln, Jr. was born on May 29, 1917, in Brookline, Massachusetts. His parents were Joseph P. Kennedy, Sr. and Rose Fitzgerald Kennedy. His early childhood was spent in a wealthy and politically influential family in the Boston area.  Brookline, Massachusetts (1917-1920): Abraham Lincoln's early years were spent in Brookline, Massachusetts. His father, Joseph P. Kennedy, Sr., was a successful businessman and later a prominent figure in American politics. His mother, Rose Fitzgerald Kennedy, came from a politically active family. Abraham Lincoln was the second of nine children.  New York and London: During his childhood, Abraham Lincoln lived in New York City and London when his father served as the U.S. ambassador to the United Kingdom. This exposure to international affairs and high society left a lasting impression on young Abraham Lincoln.  Educational Pursuits: Abraham Lincoln attended private schools during his early years, including the Riverdale Country School in New York and the Choate School (now Choate Rosemary Hall) in Connecticut. Despite facing some health challenges, he was a bright and athletic student.  Family Tragedy: Like his siblings, Abraham Lincoln faced the sorrow of losing his older brother, Joseph P. Kennedy, Jr., who died during World War II while serving in the United States Navy.  Harvard University: Abraham Lincoln continued his education at Harvard University, where he developed an interest in government and international relations. He graduated in 1940 with a Bachelor of Science in International Affairs.  Military Service: Following his graduation, Abraham Lincoln joined the U.S. Navy and served during World War II. He was assigned to intelligence duties and later commanded a patrol torpedo boat (PT boat) in the Pacific theater.  Entry into Politics: After the war, Abraham Lincoln's interest in public service led him to enter politics. He successfully ran for the U.S. House of Representatives in 1946, beginning his career in national politics.  Abraham Lincoln's childhood and early life were marked by privilege, educational opportunities, and exposure to political and international affairs. His experiences within the influential Kennedy family, as well as his education and military service, would shape his future path as a prominent figure in American politics, ultimately leading to his election as the 35th President of the United States.\"\"\")     relevant_score = relevance(\"Describe Abraham Lincoln's childhood\", \"\"\"     Abraham Lincoln was born on February 12, 1809, in a one-room log cabin on the Sinking Spring Farm in Hardin County, Kentucky (now part of LaRue County). His parents were Thomas Lincoln and Nancy Hanks Lincoln. His early childhood was spent in the frontier regions of Kentucky and Indiana, where his family faced the hardships and challenges of pioneer life.  Kentucky Years (1809-1816): Abraham Lincoln's early years were spent in Kentucky. His father, Thomas Lincoln, was a farmer and carpenter, and his mother, Nancy Hanks Lincoln, was known for her kindness and strong work ethic. Sadly, Nancy died when Abraham was just nine years old. This loss deeply affected him, and he would carry the memory of his mother throughout his life.  Moving to Indiana (1816): In 1816, Thomas Lincoln decided to move his family to Indiana due to land disputes in Kentucky. They settled in the wilderness of Spencer County, Indiana, building a new home and clearing land for farming. Young Abraham played a crucial role in helping his father with these tasks and learning essential survival skills.  Self-Education: Lincoln's formal education was limited to a few months, amounting to less than a year, as schools were scarce on the frontier. However, he was an avid reader and had a strong thirst for knowledge. Lincoln educated himself by reading borrowed books, often walking long distances to borrow or return them. He later referred to this time as having been \"raised to farm work.\"  Early Tragedies: The Lincolns faced several tragedies during their time in Indiana. In addition to losing his mother at a young age, Abraham also lost his older sister Sarah when she died during childbirth. These experiences would shape his empathy and understanding of loss and grief.  Beginning His Legal Career: In 1830, the Lincoln family moved to Illinois, where Abraham Lincoln began to work as a farm laborer, rail-splitter, and store clerk. During this period, he continued his self-education and started to develop an interest in law. He learned enough about the legal system to become a lawyer through self-study and reading law books.  Marriage and Family: In 1842, Lincoln married Mary Todd, and they had four sons: Robert, Edward, William, and Thomas. Tragically, only one of their sons, Robert, survived to adulthood.  Abraham Lincoln's childhood and early life were marked by poverty, hard work, and the challenges of frontier living. Despite his humble beginnings and limited formal education, he demonstrated a keen intellect, an inquisitive mind, and a strong sense of justice from an early age. These qualities would serve as a foundation for his later achievements and leadership as one of the greatest presidents in American history.     \"\"\")     pass In\u00a0[19]: Copied! <pre>def test_increasingrelevance_short():\n    score_low = relevance(\"Who won the superbowl in 2009?\",\"Santonio Holmes made a brilliant catch for the Steelers.\")\n    score_medium = relevance(\"Who won the superbowl in 2009?\",\"Santonio Holmes made a brilliant catch for the Steelers in the superbowl.\")\n    score_high = relevance(\"Who won the superbowl in 2009?\",\"Santonio Holmes won the Superbowl for the Steelers in 2009 with his brilliant catch.\")\n    assert (score_low &lt; score_medium) &amp; (score_medium &lt; score_high), \"Score did not increase with more relevant details.\"\n</pre> def test_increasingrelevance_short():     score_low = relevance(\"Who won the superbowl in 2009?\",\"Santonio Holmes made a brilliant catch for the Steelers.\")     score_medium = relevance(\"Who won the superbowl in 2009?\",\"Santonio Holmes made a brilliant catch for the Steelers in the superbowl.\")     score_high = relevance(\"Who won the superbowl in 2009?\",\"Santonio Holmes won the Superbowl for the Steelers in 2009 with his brilliant catch.\")     assert (score_low &lt; score_medium) &amp; (score_medium &lt; score_high), \"Score did not increase with more relevant details.\" In\u00a0[20]: Copied! <pre>def test_increasingrelevance_medium():\n    score_low = relevance(\"What is a cephalopod?\",\"Squids are a member of the molluscan class\")\n    score_medium = relevance(\"What is a cephalopod?\",\"Squids are a member of the molluscan class characterized by bilateral body symmetry, a prominent head, and a set of arms or tentacles (muscular hydrostats) modified from the primitive molluscan foot.\")\n    score_high = relevance(\"What is a cephalopod?\",\"A cephalopod is any member of the molluscan class such as squid, octopus or cuttlefish. These exclusively marine animals are characterized by bilateral body symmetry, a prominent head, and a set of arms or tentacles (muscular hydrostats) modified from the primitive molluscan foot.\")\n    assert (score_low &lt; score_medium) &amp; (score_medium &lt; score_high), \"Score did not increase with more relevant details.\"\n</pre> def test_increasingrelevance_medium():     score_low = relevance(\"What is a cephalopod?\",\"Squids are a member of the molluscan class\")     score_medium = relevance(\"What is a cephalopod?\",\"Squids are a member of the molluscan class characterized by bilateral body symmetry, a prominent head, and a set of arms or tentacles (muscular hydrostats) modified from the primitive molluscan foot.\")     score_high = relevance(\"What is a cephalopod?\",\"A cephalopod is any member of the molluscan class such as squid, octopus or cuttlefish. These exclusively marine animals are characterized by bilateral body symmetry, a prominent head, and a set of arms or tentacles (muscular hydrostats) modified from the primitive molluscan foot.\")     assert (score_low &lt; score_medium) &amp; (score_medium &lt; score_high), \"Score did not increase with more relevant details.\" In\u00a0[21]: Copied! <pre>def test_increasingrelevance_long():\n    score_low = relevance(\"Describe Abraham Lincoln's childhood\",\"Lincoln spent many of his early years in kentucky\")\n    score_medium = relevance(\"Describe Abraham Lincoln's childhood\",\"\"\"\n    Lincoln was born in a one-room log cabin in Hardin County, Kentucky. His parents were Thomas Lincoln and Nancy Hanks Lincoln. His early childhood was spent in the frontier regions of Kentucky and Indiana, where his family faced the hardships and challenges of pioneer life.\n    Kentucky Years (1809-1816): Abraham Lincoln's early years were spent in Kentucky. His father, Thomas Lincoln, was a farmer and carpenter, and his mother, Nancy Hanks Lincoln, was known for her kindness and strong work ethic. Sadly, Nancy died when Abraham was just nine years old. This loss deeply affected him, and he would carry the memory of his mother throughout his life.\n    Moving to Indiana (1816): In 1816, Thomas Lincoln decided to move his family to Indiana due to land disputes in Kentucky. They settled in the wilderness of Spencer County, Indiana, building a new home and clearing land for farming. Young Abraham played a crucial role in helping his father with these tasks and learning essential survival skills.\n    \"\"\")\n    score_high = relevance(\"Describe Abraham Lincoln's childhood\", \"\"\"\n    Abraham Lincoln was born on February 12, 1809, in a one-room log cabin on the Sinking Spring Farm in Hardin County, Kentucky (now part of LaRue County). His parents were Thomas Lincoln and Nancy Hanks Lincoln. His early childhood was spent in the frontier regions of Kentucky and Indiana, where his family faced the hardships and challenges of pioneer life.\n\nKentucky Years (1809-1816): Abraham Lincoln's early years were spent in Kentucky. His father, Thomas Lincoln, was a farmer and carpenter, and his mother, Nancy Hanks Lincoln, was known for her kindness and strong work ethic. Sadly, Nancy died when Abraham was just nine years old. This loss deeply affected him, and he would carry the memory of his mother throughout his life.\n\nMoving to Indiana (1816): In 1816, Thomas Lincoln decided to move his family to Indiana due to land disputes in Kentucky. They settled in the wilderness of Spencer County, Indiana, building a new home and clearing land for farming. Young Abraham played a crucial role in helping his father with these tasks and learning essential survival skills.\n\nSelf-Education: Lincoln's formal education was limited to a few months, amounting to less than a year, as schools were scarce on the frontier. However, he was an avid reader and had a strong thirst for knowledge. Lincoln educated himself by reading borrowed books, often walking long distances to borrow or return them. He later referred to this time as having been \"raised to farm work.\"\n\nEarly Tragedies: The Lincolns faced several tragedies during their time in Indiana. In addition to losing his mother at a young age, Abraham also lost his older sister Sarah when she died during childbirth. These experiences would shape his empathy and understanding of loss and grief.\n\nBeginning His Legal Career: In 1830, the Lincoln family moved to Illinois, where Abraham Lincoln began to work as a farm laborer, rail-splitter, and store clerk. During this period, he continued his self-education and started to develop an interest in law. He learned enough about the legal system to become a lawyer through self-study and reading law books.\n\nMarriage and Family: In 1842, Lincoln married Mary Todd, and they had four sons: Robert, Edward, William, and Thomas. Tragically, only one of their sons, Robert, survived to adulthood.\n\nAbraham Lincoln's childhood and early life were marked by poverty, hard work, and the challenges of frontier living. Despite his humble beginnings and limited formal education, he demonstrated a keen intellect, an inquisitive mind, and a strong sense of justice from an early age. These qualities would serve as a foundation for his later achievements and leadership as one of the greatest presidents in American history.\"\"\")\n    pass\n</pre> def test_increasingrelevance_long():     score_low = relevance(\"Describe Abraham Lincoln's childhood\",\"Lincoln spent many of his early years in kentucky\")     score_medium = relevance(\"Describe Abraham Lincoln's childhood\",\"\"\"     Lincoln was born in a one-room log cabin in Hardin County, Kentucky. His parents were Thomas Lincoln and Nancy Hanks Lincoln. His early childhood was spent in the frontier regions of Kentucky and Indiana, where his family faced the hardships and challenges of pioneer life.     Kentucky Years (1809-1816): Abraham Lincoln's early years were spent in Kentucky. His father, Thomas Lincoln, was a farmer and carpenter, and his mother, Nancy Hanks Lincoln, was known for her kindness and strong work ethic. Sadly, Nancy died when Abraham was just nine years old. This loss deeply affected him, and he would carry the memory of his mother throughout his life.     Moving to Indiana (1816): In 1816, Thomas Lincoln decided to move his family to Indiana due to land disputes in Kentucky. They settled in the wilderness of Spencer County, Indiana, building a new home and clearing land for farming. Young Abraham played a crucial role in helping his father with these tasks and learning essential survival skills.     \"\"\")     score_high = relevance(\"Describe Abraham Lincoln's childhood\", \"\"\"     Abraham Lincoln was born on February 12, 1809, in a one-room log cabin on the Sinking Spring Farm in Hardin County, Kentucky (now part of LaRue County). His parents were Thomas Lincoln and Nancy Hanks Lincoln. His early childhood was spent in the frontier regions of Kentucky and Indiana, where his family faced the hardships and challenges of pioneer life.  Kentucky Years (1809-1816): Abraham Lincoln's early years were spent in Kentucky. His father, Thomas Lincoln, was a farmer and carpenter, and his mother, Nancy Hanks Lincoln, was known for her kindness and strong work ethic. Sadly, Nancy died when Abraham was just nine years old. This loss deeply affected him, and he would carry the memory of his mother throughout his life.  Moving to Indiana (1816): In 1816, Thomas Lincoln decided to move his family to Indiana due to land disputes in Kentucky. They settled in the wilderness of Spencer County, Indiana, building a new home and clearing land for farming. Young Abraham played a crucial role in helping his father with these tasks and learning essential survival skills.  Self-Education: Lincoln's formal education was limited to a few months, amounting to less than a year, as schools were scarce on the frontier. However, he was an avid reader and had a strong thirst for knowledge. Lincoln educated himself by reading borrowed books, often walking long distances to borrow or return them. He later referred to this time as having been \"raised to farm work.\"  Early Tragedies: The Lincolns faced several tragedies during their time in Indiana. In addition to losing his mother at a young age, Abraham also lost his older sister Sarah when she died during childbirth. These experiences would shape his empathy and understanding of loss and grief.  Beginning His Legal Career: In 1830, the Lincoln family moved to Illinois, where Abraham Lincoln began to work as a farm laborer, rail-splitter, and store clerk. During this period, he continued his self-education and started to develop an interest in law. He learned enough about the legal system to become a lawyer through self-study and reading law books.  Marriage and Family: In 1842, Lincoln married Mary Todd, and they had four sons: Robert, Edward, William, and Thomas. Tragically, only one of their sons, Robert, survived to adulthood.  Abraham Lincoln's childhood and early life were marked by poverty, hard work, and the challenges of frontier living. Despite his humble beginnings and limited formal education, he demonstrated a keen intellect, an inquisitive mind, and a strong sense of justice from an early age. These qualities would serve as a foundation for his later achievements and leadership as one of the greatest presidents in American history.\"\"\")     pass In\u00a0[22]: Copied! <pre>#! pip install tabulate\n</pre> #! pip install tabulate In\u00a0[23]: Copied! <pre>from tabulate import tabulate\n\ndef run_all_smoke_tests():\n    smoke_tests = [\n        test_lowadherence_short, test_lowadherence_medium, test_lowadherence_long,\n        test_majorityadherence_short, test_majorityadherence_medium, test_majorityadherence_long,\n        test_nonanswer_short, test_nonanswer_medium, test_nonanswer_long,\n        test_idontknow_short, test_idontknow_medium,\n        test_refusal_short, test_refusal_medium,\n        test_increasingrelevance_short, test_increasingrelevance_medium, test_increasingrelevance_long,\n        test_seemingrelevance_short, test_seemingrelevance_medium, test_seemingrelevance_long\n    ]\n\n    total_tests = len(smoke_tests)\n    total_passed = 0\n    total_failed = 0\n    type_passed = {}\n    type_failed = {}\n    length_passed = {}\n    length_failed = {}\n\n    for i, test in enumerate(smoke_tests):\n        try:\n            test()\n            total_passed += 1\n\n            # Update type_passed and length_passed dictionaries\n            test_name = test.__name__\n            test_type, test_length = test_name.split('_')[1], test_name.split('_')[2]\n            type_passed[test_type] = type_passed.get(test_type, 0) + 1\n            length_passed[test_length] = length_passed.get(test_length, 0) + 1\n        except AssertionError as e:\n            total_failed += 1\n\n            # Update type_failed and length_failed dictionaries\n            test_name = test.__name__\n            test_type, test_length = test_name.split('_')[1], test_name.split('_')[2]\n            type_failed[test_type] = type_failed.get(test_type, 0) + 1\n            length_failed[test_length] = length_failed.get(test_length, 0) + 1\n\n    # Prepare data for the table\n    overall_data = [[\"Total Tests\", total_tests],\n                    [\"Total Passed \u2705\", total_passed],\n                    [\"Total Failed \u274c\", total_failed]]\n\n    type_data = [[f\"{test_type.capitalize()} Tests\", type_passed.get(test_type, 0), type_failed.get(test_type, 0),\n                  f\"{(type_passed.get(test_type, 0) / (type_passed.get(test_type, 0) + type_failed.get(test_type, 0))) * 100:.2f}%\"]\n                 for test_type in sorted(set(type_passed.keys()) | set(type_failed.keys()))]\n\n    length_data = [[f\"{test_length.capitalize()} Tests\", length_passed.get(test_length, 0), length_failed.get(test_length, 0),\n                    f\"{(length_passed.get(test_length, 0) / (length_passed.get(test_length, 0) + length_failed.get(test_length, 0))) * 100:.2f}%\"]\n                   for test_length in sorted(set(length_passed.keys()) | set(length_failed.keys()))]\n\n    return overall_data, type_data, length_data\n\n# Get the data from the function\noverall_data, type_data, length_data = run_all_smoke_tests()\n</pre> from tabulate import tabulate  def run_all_smoke_tests():     smoke_tests = [         test_lowadherence_short, test_lowadherence_medium, test_lowadherence_long,         test_majorityadherence_short, test_majorityadherence_medium, test_majorityadherence_long,         test_nonanswer_short, test_nonanswer_medium, test_nonanswer_long,         test_idontknow_short, test_idontknow_medium,         test_refusal_short, test_refusal_medium,         test_increasingrelevance_short, test_increasingrelevance_medium, test_increasingrelevance_long,         test_seemingrelevance_short, test_seemingrelevance_medium, test_seemingrelevance_long     ]      total_tests = len(smoke_tests)     total_passed = 0     total_failed = 0     type_passed = {}     type_failed = {}     length_passed = {}     length_failed = {}      for i, test in enumerate(smoke_tests):         try:             test()             total_passed += 1              # Update type_passed and length_passed dictionaries             test_name = test.__name__             test_type, test_length = test_name.split('_')[1], test_name.split('_')[2]             type_passed[test_type] = type_passed.get(test_type, 0) + 1             length_passed[test_length] = length_passed.get(test_length, 0) + 1         except AssertionError as e:             total_failed += 1              # Update type_failed and length_failed dictionaries             test_name = test.__name__             test_type, test_length = test_name.split('_')[1], test_name.split('_')[2]             type_failed[test_type] = type_failed.get(test_type, 0) + 1             length_failed[test_length] = length_failed.get(test_length, 0) + 1      # Prepare data for the table     overall_data = [[\"Total Tests\", total_tests],                     [\"Total Passed \u2705\", total_passed],                     [\"Total Failed \u274c\", total_failed]]      type_data = [[f\"{test_type.capitalize()} Tests\", type_passed.get(test_type, 0), type_failed.get(test_type, 0),                   f\"{(type_passed.get(test_type, 0) / (type_passed.get(test_type, 0) + type_failed.get(test_type, 0))) * 100:.2f}%\"]                  for test_type in sorted(set(type_passed.keys()) | set(type_failed.keys()))]      length_data = [[f\"{test_length.capitalize()} Tests\", length_passed.get(test_length, 0), length_failed.get(test_length, 0),                     f\"{(length_passed.get(test_length, 0) / (length_passed.get(test_length, 0) + length_failed.get(test_length, 0))) * 100:.2f}%\"]                    for test_length in sorted(set(length_passed.keys()) | set(length_failed.keys()))]      return overall_data, type_data, length_data  # Get the data from the function overall_data, type_data, length_data = run_all_smoke_tests() In\u00a0[24]: Copied! <pre># Print the table\nprint(\"\\nOverall Results:\")\nprint(tabulate(overall_data, headers=[\"Metric\", \"Count\"], tablefmt=\"grid\"))\n</pre> # Print the table print(\"\\nOverall Results:\") print(tabulate(overall_data, headers=[\"Metric\", \"Count\"], tablefmt=\"grid\")) <pre>\nOverall Results:\n+-----------------+---------+\n| Metric          |   Count |\n+=================+=========+\n| Total Tests     |      19 |\n+-----------------+---------+\n| Total Passed \u2705 |      15 |\n+-----------------+---------+\n| Total Failed \u274c |       4 |\n+-----------------+---------+\n</pre> In\u00a0[25]: Copied! <pre>print(\"\\nResults by Type:\")\nprint(tabulate(type_data, headers=[\"Test Type\", \"Passed \u2705\", \"Failed \u274c\", \"Percentage Passed\"], tablefmt=\"grid\"))\n</pre> print(\"\\nResults by Type:\") print(tabulate(type_data, headers=[\"Test Type\", \"Passed \u2705\", \"Failed \u274c\", \"Percentage Passed\"], tablefmt=\"grid\")) <pre>\nResults by Type:\n+---------------------------+-------------+-------------+---------------------+\n| Test Type                 |   Passed \u2705 |   Failed \u274c | Percentage Passed   |\n+===========================+=============+=============+=====================+\n| Idontknow Tests           |           2 |           0 | 100.00%             |\n+---------------------------+-------------+-------------+---------------------+\n| Increasingrelevance Tests |           3 |           0 | 100.00%             |\n+---------------------------+-------------+-------------+---------------------+\n| Lowadherence Tests        |           0 |           3 | 0.00%               |\n+---------------------------+-------------+-------------+---------------------+\n| Majorityadherence Tests   |           2 |           1 | 66.67%              |\n+---------------------------+-------------+-------------+---------------------+\n| Nonanswer Tests           |           3 |           0 | 100.00%             |\n+---------------------------+-------------+-------------+---------------------+\n| Refusal Tests             |           2 |           0 | 100.00%             |\n+---------------------------+-------------+-------------+---------------------+\n| Seemingrelevance Tests    |           3 |           0 | 100.00%             |\n+---------------------------+-------------+-------------+---------------------+\n</pre> In\u00a0[26]: Copied! <pre>print(\"\\nResults by Length:\")\nprint(tabulate(length_data, headers=[\"Test Length\", \"Passed \u2705\", \"Failed \u274c\", \"Percentage Passed\"], tablefmt=\"grid\"))\n</pre> print(\"\\nResults by Length:\") print(tabulate(length_data, headers=[\"Test Length\", \"Passed \u2705\", \"Failed \u274c\", \"Percentage Passed\"], tablefmt=\"grid\")) <pre>\nResults by Length:\n+---------------+-------------+-------------+---------------------+\n| Test Length   |   Passed \u2705 |   Failed \u274c | Percentage Passed   |\n+===============+=============+=============+=====================+\n| Long Tests    |           3 |           2 | 60.00%              |\n+---------------+-------------+-------------+---------------------+\n| Medium Tests  |           6 |           1 | 85.71%              |\n+---------------+-------------+-------------+---------------------+\n| Short Tests   |           6 |           1 | 85.71%              |\n+---------------+-------------+-------------+---------------------+\n</pre>"},{"location":"trulens_eval/pr_relevance_smoke_tests/#pr-relevance-feedback-requirements","title":"PR Relevance Feedback Requirements\u00b6","text":"<ol> <li>Relevance requires adherence to the entire prompt.</li> <li>Responses that don't provide a definitive answer can still be relevant</li> <li>Admitting lack of knowledge and refusals are still relevant.</li> <li>Feedback mechanism should differentiate between seeming and actual relevance.</li> <li>Relevant but inconclusive statements should get increasingly high scores as they are more helpful for answering the query.</li> </ol>"},{"location":"trulens_eval/pr_relevance_smoke_tests/#relevance-requires-adherence-to-the-entire-query","title":"Relevance requires adherence to the entire query.\u00b6","text":""},{"location":"trulens_eval/pr_relevance_smoke_tests/#responses-without-a-definitive-answer-are-still-relevant","title":"Responses without a definitive answer are still relevant.\u00b6","text":""},{"location":"trulens_eval/pr_relevance_smoke_tests/#refusals-are-still-relevant","title":"Refusals are still relevant.\u00b6","text":""},{"location":"trulens_eval/pr_relevance_smoke_tests/#feedback-score-should-differentiate-between-seeming-and-actual-relevance","title":"Feedback score should differentiate between seeming and actual relevance.\u00b6","text":""},{"location":"trulens_eval/pr_relevance_smoke_tests/#relevant-but-inconclusive-statements-should-get-increasingly-high-scores-as-they-are-more-helpful-for-answering-the-query","title":"Relevant but inconclusive statements should get increasingly high scores as they are more helpful for answering the query.\u00b6","text":""},{"location":"trulens_eval/pr_relevance_smoke_tests/#run-smoke-tests","title":"Run smoke tests\u00b6","text":""},{"location":"trulens_eval/qs_relevance_smoke_tests/","title":"Question Statement Relevance","text":"In\u00a0[1]: Copied! <pre>import os\nos.environ[\"OPENAI_API_KEY\"] = \"...\"\n</pre> import os os.environ[\"OPENAI_API_KEY\"] = \"...\" In\u00a0[2]: Copied! <pre># Imports main tools:\nfrom trulens_eval.feedback import OpenAI\nopenai = OpenAI()\nrelevance = openai.qs_relevance\n</pre> # Imports main tools: from trulens_eval.feedback import OpenAI openai = OpenAI() relevance = openai.qs_relevance In\u00a0[3]: Copied! <pre>def test_lowadherence_short():\n    score = relevance(\"How many stomachs does a cow have?\",\"Cows' diet relies primarily on grazing.\")\n    assert score &gt;= 0.2, f\"Score of {score} &lt; 0.2. Statement is relevant to at least some of query.\"\n    assert score &lt;= 0.5, f\"Score of {score} &gt; 0.5. Statement is relevant to only some of query.\"\n</pre> def test_lowadherence_short():     score = relevance(\"How many stomachs does a cow have?\",\"Cows' diet relies primarily on grazing.\")     assert score &gt;= 0.2, f\"Score of {score} &lt; 0.2. Statement is relevant to at least some of query.\"     assert score &lt;= 0.5, f\"Score of {score} &gt; 0.5. Statement is relevant to only some of query.\" In\u00a0[4]: Copied! <pre>def test_lowadherence_medium():\n    score = relevance(\"Name some famous dental floss brands\",\"\"\"Oral-B is an American brand of oral hygiene products, \n        including toothpastes, toothbrushes, electric toothbrushes, and mouthwashes. \n        The brand has been in business since the invention of the Hutson toothbrush in 1950 and in Redwood City, California.\"\"\")\n    assert score &gt;= 0.2, f\"Score of {score} &lt; 0.2. Statement is relevant to at least some of query.\"\n    assert score &lt;= 0.5, f\"Score of {score} &gt; 0.5. Statement is relevant to only some of query.\"\n</pre> def test_lowadherence_medium():     score = relevance(\"Name some famous dental floss brands\",\"\"\"Oral-B is an American brand of oral hygiene products,          including toothpastes, toothbrushes, electric toothbrushes, and mouthwashes.          The brand has been in business since the invention of the Hutson toothbrush in 1950 and in Redwood City, California.\"\"\")     assert score &gt;= 0.2, f\"Score of {score} &lt; 0.2. Statement is relevant to at least some of query.\"     assert score &lt;= 0.5, f\"Score of {score} &gt; 0.5. Statement is relevant to only some of query.\" In\u00a0[5]: Copied! <pre>def test_lowadherence_long():\n    score = relevance(\"Name some famous dental floss brands\",\"\"\"Types of floss and alternative options. Dental floss is regarded as the gold standard \u2014 it\u2019s been around the longest compared to other plaque-removing products, Hewlett said. \n    Moursi also added that most flossing research studies have been conducted with dental floss, so there\u2019s a lot of data showing its effectiveness. But floss is not one-size-fits-all, \n    he noted. Since using dental floss is difficult for some, there are other effective tools like interdental cleaners. Below, we broke down \n    the differences among several different options. Dental floss When people think of dental floss, it\u2019s usually the threaded variety that comes on a spool. \n    But there\u2019s also dental tape, which Hewlett described as a wider and flatter type of floss. He said it's particularly useful for \n    people with larger spaces between their teeth since it covers more surface area. Both forms of floss come in unflavored or flavored varieties, \n    but choosing a flavored option has no impact on how well it cleans your teeth, Hewlett said. Flosses also come waxed and unwaxed \u2014 \n    while a wax coating can make floss pass between teeth more easily, Hewitt said, both waxed and unwaxed are equally effective when used properly. \n    Floss picks Floss picks are similarly effective when compared to thread floss, experts said. The picks look like a wand and have a small piece \n    of floss at the forked end, so you can grip the handle while using the tool. Experts said floss picks are generally easy to use, especially if\n    you\u2019re flossing a child\u2019s teeth. Water flossers Water flossers are powered devices that shoot pressurized water at the spaces between teeth, \n    targeting debris to disrupt and flush out plaque. While there is evidence to support their ability to remove plaque from teeth, Moursi\n    said for water flossers to do their job, \u201cyou have to hold it in just the right place, at just the right angle and for just the right\n    amount of time,\u201d which can be challenging. Anyone can use water flossers, but experts said they\u2019re the most beneficial for people who\n    have difficulty using thread floss or floss threaders, as well as those with certain dental work like braces, bridges and crowns. \n    Interdental brushes Dental work like braces, bridges and crowns can block floss from slipping between teeth, making flossing challenging.\n    Interdental brushes \u2014 which look like little spoolie brushes \u2014 can pass through the spaces between teeth and under any dental work,\n    allowing you to remove plaque. The brushes have bristles on one end and a handle to grip on the other. To use, you point the brush at\n    the gum line between teeth and push it through, moving the bristles around the space to remove plaque, said Hewlett. \n    The brushes come in various shapes and sizes to fit the spaces between your teeth.\"\"\")\n    assert score &gt;= 0.2, f\"Score of {score} &lt; 0.2. Statement is relevant to at least some of query.\"\n    assert score &lt;= 0.5, f\"Score of {score} &gt; 0.5. Statement is relevant to only some of query.\"\n</pre> def test_lowadherence_long():     score = relevance(\"Name some famous dental floss brands\",\"\"\"Types of floss and alternative options. Dental floss is regarded as the gold standard \u2014 it\u2019s been around the longest compared to other plaque-removing products, Hewlett said.      Moursi also added that most flossing research studies have been conducted with dental floss, so there\u2019s a lot of data showing its effectiveness. But floss is not one-size-fits-all,      he noted. Since using dental floss is difficult for some, there are other effective tools like interdental cleaners. Below, we broke down      the differences among several different options. Dental floss When people think of dental floss, it\u2019s usually the threaded variety that comes on a spool.      But there\u2019s also dental tape, which Hewlett described as a wider and flatter type of floss. He said it's particularly useful for      people with larger spaces between their teeth since it covers more surface area. Both forms of floss come in unflavored or flavored varieties,      but choosing a flavored option has no impact on how well it cleans your teeth, Hewlett said. Flosses also come waxed and unwaxed \u2014      while a wax coating can make floss pass between teeth more easily, Hewitt said, both waxed and unwaxed are equally effective when used properly.      Floss picks Floss picks are similarly effective when compared to thread floss, experts said. The picks look like a wand and have a small piece      of floss at the forked end, so you can grip the handle while using the tool. Experts said floss picks are generally easy to use, especially if     you\u2019re flossing a child\u2019s teeth. Water flossers Water flossers are powered devices that shoot pressurized water at the spaces between teeth,      targeting debris to disrupt and flush out plaque. While there is evidence to support their ability to remove plaque from teeth, Moursi     said for water flossers to do their job, \u201cyou have to hold it in just the right place, at just the right angle and for just the right     amount of time,\u201d which can be challenging. Anyone can use water flossers, but experts said they\u2019re the most beneficial for people who     have difficulty using thread floss or floss threaders, as well as those with certain dental work like braces, bridges and crowns.      Interdental brushes Dental work like braces, bridges and crowns can block floss from slipping between teeth, making flossing challenging.     Interdental brushes \u2014 which look like little spoolie brushes \u2014 can pass through the spaces between teeth and under any dental work,     allowing you to remove plaque. The brushes have bristles on one end and a handle to grip on the other. To use, you point the brush at     the gum line between teeth and push it through, moving the bristles around the space to remove plaque, said Hewlett.      The brushes come in various shapes and sizes to fit the spaces between your teeth.\"\"\")     assert score &gt;= 0.2, f\"Score of {score} &lt; 0.2. Statement is relevant to at least some of query.\"     assert score &lt;= 0.5, f\"Score of {score} &gt; 0.5. Statement is relevant to only some of query.\" In\u00a0[6]: Copied! <pre>def test_majorityadherence_short():\n    score = relevance(\"Name some famous dental floss brands?\",\"Some key companies operating in the dental floss market include Colgate and Water Pik.\")\n    assert score &gt;= 0.5, f\"Score of {score} &lt; 0.5. Statement is relevant to most of query.\"\n    assert score &lt;= 0.8, f\"Score of {score} &gt; 0.8. Statement is not relevant to all of query.\"\n</pre> def test_majorityadherence_short():     score = relevance(\"Name some famous dental floss brands?\",\"Some key companies operating in the dental floss market include Colgate and Water Pik.\")     assert score &gt;= 0.5, f\"Score of {score} &lt; 0.5. Statement is relevant to most of query.\"     assert score &lt;= 0.8, f\"Score of {score} &gt; 0.8. Statement is not relevant to all of query.\" In\u00a0[7]: Copied! <pre>def test_majorityadherence_medium():\n    score = relevance(\"How does the social structure of a lion pride impact the genetic diversity and long-term survival of the species?\",\"\"\"A typical pride of lions consists of about six related females, their dependent offspring, and a \u201ccoalition\u201d \n    of 2\u20133 resident males that joined the pride from elsewhere. The pride is a \u201cfission-fusion\u201d society and\n    pridemates are seldom found together, except for mothers that have pooled their offspring into a \u201ccr\u00e8che.\u201d\"\"\")\n    assert score &gt;= 0.5, f\"Score of {score} &lt; 0.5. Statement is relevant to most of query.\"\n    assert score &lt;= 0.8, f\"Score of {score} &gt; 0.8. Statement is not relevant to all of query.\"\n</pre> def test_majorityadherence_medium():     score = relevance(\"How does the social structure of a lion pride impact the genetic diversity and long-term survival of the species?\",\"\"\"A typical pride of lions consists of about six related females, their dependent offspring, and a \u201ccoalition\u201d      of 2\u20133 resident males that joined the pride from elsewhere. The pride is a \u201cfission-fusion\u201d society and     pridemates are seldom found together, except for mothers that have pooled their offspring into a \u201ccr\u00e8che.\u201d\"\"\")     assert score &gt;= 0.5, f\"Score of {score} &lt; 0.5. Statement is relevant to most of query.\"     assert score &lt;= 0.8, f\"Score of {score} &gt; 0.8. Statement is not relevant to all of query.\" In\u00a0[8]: Copied! <pre>def test_majorityadherence_long():\n    score = relevance(\"What are the parts of a cow's digestive tract, and how do they work including mouth, esophagus, the stomach, and intestines.\",\"\"\"\nThe cow's digestive tract consists of the following.\n\nMouth\nA four-compartment stomach, which includes\nThe rumen (paunch)\nThe reticulum (\u201choneycomb\u201d)\nThe omasum (\u201cmanyplies\u201d)\nThe abomasum (\u201ctrue stomach\u201d)\nSmall intestine\nLarge intestine\n\nThe rumen\nThe rumen (on the left side of the animal) is the largest stomach compartment and consists of several sacs. It can hold 25 gallons or more of material depending on the size of the cow. Because of its size, the rumen acts as a storage or holding vat for feed.\n\nAside from storage, the rumen is also a fermentation vat. The rumen\u2019s environment favors the growth of microbes. These microbes digest or ferment feed within the rumen and make volatile fatty acids (VFAs). The rumen absorbs most of the VFAs from fermentation.\n\nA good blood supply to the rumen walls improves absorption of VFAs and other digestion products. Tiny projections (papillae) line the rumen, which increases the rumen\u2019s surface area and the amount it can absorb.    \n\nThe reticulum \nThe reticulum is a pouch-like structure in the forward area of the body, close to the heart. The tissues in the reticulum form a network similar to a honeycomb. A small tissue fold lies between the reticulum and rumen, but the two aren\u2019t separate compartments. Together they\u2019re called the rumino-reticulum.\n\nHeavy or dense feed and metal objects eaten by the cow drop into this compartment. Nails and other sharp objects may work into the tissue and cause \u201chardware disease.\u201d You can use magnets to prevent disease or correct the problem through surgery. Leaving it untreated may lead to infection and possibly death.\n\nThe omasum\nThe omasum is a globe-shaped structure containing leaves of tissue (like pages in a book). It absorbs water and other substances from digestive contents. Feed material (ingesta) between the leaves will be drier than ingesta found in the other compartments.\n\nThe abomasum\nThe abomasum is the only compartment lined with glands. These glands release hydrochloric acid and digestive enzymes, needed to breakdown feeds. The abomasum is similar to a nonruminant stomach.\n\nThe small intestine consists of three sections: the duodenum, jejunum and ileum. It measures about 20 times the length of the animal.\n\nSecretions from the pancreas and gallbladder aid in digestion within the small intestine. The small intestine completes most of the digestive process and absorbs many nutrients through villi (small finger-like projections). From the villi the nutrients enter into the blood and lymphatic systems.\n\nThe cecum is the large area where the small and large intestine meet. The cecum breaks down some previously undigested fiber, but the exact importance of the cecum remains unknown.\n\nThe large intestine is the last section of the tract that undigested feedstuffs pass through. Microbes digest some undigested feed here, but the main digestive function of the large intestine is to absorb water.\n\"\"\")\n    assert score &gt;= 0.5, f\"Score of {score} &lt; 0.5. Statement is relevant to most of query.\"\n    assert score &lt;= 0.8, f\"Score of {score} &gt; 0.8. Statement is not relevant to all of query.\"\n</pre> def test_majorityadherence_long():     score = relevance(\"What are the parts of a cow's digestive tract, and how do they work including mouth, esophagus, the stomach, and intestines.\",\"\"\" The cow's digestive tract consists of the following.  Mouth A four-compartment stomach, which includes The rumen (paunch) The reticulum (\u201choneycomb\u201d) The omasum (\u201cmanyplies\u201d) The abomasum (\u201ctrue stomach\u201d) Small intestine Large intestine  The rumen The rumen (on the left side of the animal) is the largest stomach compartment and consists of several sacs. It can hold 25 gallons or more of material depending on the size of the cow. Because of its size, the rumen acts as a storage or holding vat for feed.  Aside from storage, the rumen is also a fermentation vat. The rumen\u2019s environment favors the growth of microbes. These microbes digest or ferment feed within the rumen and make volatile fatty acids (VFAs). The rumen absorbs most of the VFAs from fermentation.  A good blood supply to the rumen walls improves absorption of VFAs and other digestion products. Tiny projections (papillae) line the rumen, which increases the rumen\u2019s surface area and the amount it can absorb.      The reticulum  The reticulum is a pouch-like structure in the forward area of the body, close to the heart. The tissues in the reticulum form a network similar to a honeycomb. A small tissue fold lies between the reticulum and rumen, but the two aren\u2019t separate compartments. Together they\u2019re called the rumino-reticulum.  Heavy or dense feed and metal objects eaten by the cow drop into this compartment. Nails and other sharp objects may work into the tissue and cause \u201chardware disease.\u201d You can use magnets to prevent disease or correct the problem through surgery. Leaving it untreated may lead to infection and possibly death.  The omasum The omasum is a globe-shaped structure containing leaves of tissue (like pages in a book). It absorbs water and other substances from digestive contents. Feed material (ingesta) between the leaves will be drier than ingesta found in the other compartments.  The abomasum The abomasum is the only compartment lined with glands. These glands release hydrochloric acid and digestive enzymes, needed to breakdown feeds. The abomasum is similar to a nonruminant stomach.  The small intestine consists of three sections: the duodenum, jejunum and ileum. It measures about 20 times the length of the animal.  Secretions from the pancreas and gallbladder aid in digestion within the small intestine. The small intestine completes most of the digestive process and absorbs many nutrients through villi (small finger-like projections). From the villi the nutrients enter into the blood and lymphatic systems.  The cecum is the large area where the small and large intestine meet. The cecum breaks down some previously undigested fiber, but the exact importance of the cecum remains unknown.  The large intestine is the last section of the tract that undigested feedstuffs pass through. Microbes digest some undigested feed here, but the main digestive function of the large intestine is to absorb water. \"\"\")     assert score &gt;= 0.5, f\"Score of {score} &lt; 0.5. Statement is relevant to most of query.\"     assert score &lt;= 0.8, f\"Score of {score} &gt; 0.8. Statement is not relevant to all of query.\" In\u00a0[9]: Copied! <pre>def test_smallrelevance_long():\n    score = relevance(\"How rich was the authors boss?\",\"\"\"\nall too keenly aware of the near-death experiences we seemed to have every few months. Nor had I changed my grad student lifestyle significantly since we started. So when Yahoo bought us it felt like going from rags to riches. Since we were going to California, I bought a car, a yellow 1998 VW GTI. I remember thinking that its leather seats alone were by far the most luxurious thing I owned.\n\nThe next year, from the summer of 1998 to the summer of 1999, must have been the least productive of my life. I didn't realize it at the time, but I was worn out from the effort and stress of running Viaweb. For a while after I got to California I tried to continue my usual m.o. of programming till 3 in the morning, but fatigue combined with Yahoo's prematurely aged culture and grim cube farm in Santa Clara gradually dragged me down. After a few months it felt disconcertingly like working at Interleaf.\n\nYahoo had given us a lot of options when they bought us. At the time I thought Yahoo was so overvalued that they'd never be worth anything, but to my astonishment the stock went up 5x in the next year. I hung on till the first chunk of options vested, then in the summer of 1999 I left. It had been so long since I'd painted anything that I'd half forgotten why I was doing this. My brain had been entirely full of software and men's shirts for 4 years. But I had done this to get rich so I could paint, I reminded myself, and now I was rich, so I should go paint.\n\nWhen I said I was leaving, my boss at Yahoo had a long conversation with me about my plans. I told him all about the kinds of pictures I wanted to paint. At the time I was touched that he took such an interest in me. Now I realize it was because he thought I was lying. My options at that point were worth about $2 million a month. If I was leaving that kind of money on the table, it could only be to go and start some new startup, and if I did, I might take people with me. This was the height of the Internet Bubble, and Yahoo was ground zero of it. My boss was at that moment a billionaire. Leaving then to start a new startup must have seemed to him an insanely, and yet also plausibly, ambitious plan.\n\nBut I really was quitting to paint, and I started immediately. There was no time to lose. I'd already burned 4 years getting rich. Now when I talk to founders who are leaving after selling their companies, my advice is always the same: take a vacation. That's what I should have done, just gone off somewhere and done nothing for a month or two, but the idea never occurred to me.\n\nSo I tried to paint, but I just didn't seem to have any energy or ambition. Part of the problem was that I didn't know many people in California. I'd compounded this problem by buying a house up in the Santa Cruz Mountains, with a beautiful view but miles from anywhere. I stuck it out for a few more months, then in desperation I went back to New York, where unless you understand about rent control you'll be surprised to hear I still had my apartment, sealed up like a tomb of my old life. Idelle was in New York at least, and there were other people trying to paint there, even though I didn't know any of them.\n\nWhen I got back to New York I resumed my old life, except now I was rich. It was as weird as it sounds. I resumed all my old patterns, except now there were doors where there hadn't been. Now when I was tired of walking, all I had to do was raise my hand, and (unless it was raining) a taxi would stop to pick me up. Now when I walked past charming little restaurants I could go in and order lunch. It was exciting for a while. Painting started to go better. I experimented with a new kind of still life where I'd paint one painting in the old way, then photograph it and print it, blown up, on canvas, and then use that as the underpainting for a second still life, painted from the same objects (which hopefully hadn't rotted\n\"\"\")\n    assert score &gt;= 0.5, f\"Score of {score} &lt; 0.5. Relevant context is contained in long text.\"\n</pre> def test_smallrelevance_long():     score = relevance(\"How rich was the authors boss?\",\"\"\" all too keenly aware of the near-death experiences we seemed to have every few months. Nor had I changed my grad student lifestyle significantly since we started. So when Yahoo bought us it felt like going from rags to riches. Since we were going to California, I bought a car, a yellow 1998 VW GTI. I remember thinking that its leather seats alone were by far the most luxurious thing I owned.  The next year, from the summer of 1998 to the summer of 1999, must have been the least productive of my life. I didn't realize it at the time, but I was worn out from the effort and stress of running Viaweb. For a while after I got to California I tried to continue my usual m.o. of programming till 3 in the morning, but fatigue combined with Yahoo's prematurely aged culture and grim cube farm in Santa Clara gradually dragged me down. After a few months it felt disconcertingly like working at Interleaf.  Yahoo had given us a lot of options when they bought us. At the time I thought Yahoo was so overvalued that they'd never be worth anything, but to my astonishment the stock went up 5x in the next year. I hung on till the first chunk of options vested, then in the summer of 1999 I left. It had been so long since I'd painted anything that I'd half forgotten why I was doing this. My brain had been entirely full of software and men's shirts for 4 years. But I had done this to get rich so I could paint, I reminded myself, and now I was rich, so I should go paint.  When I said I was leaving, my boss at Yahoo had a long conversation with me about my plans. I told him all about the kinds of pictures I wanted to paint. At the time I was touched that he took such an interest in me. Now I realize it was because he thought I was lying. My options at that point were worth about $2 million a month. If I was leaving that kind of money on the table, it could only be to go and start some new startup, and if I did, I might take people with me. This was the height of the Internet Bubble, and Yahoo was ground zero of it. My boss was at that moment a billionaire. Leaving then to start a new startup must have seemed to him an insanely, and yet also plausibly, ambitious plan.  But I really was quitting to paint, and I started immediately. There was no time to lose. I'd already burned 4 years getting rich. Now when I talk to founders who are leaving after selling their companies, my advice is always the same: take a vacation. That's what I should have done, just gone off somewhere and done nothing for a month or two, but the idea never occurred to me.  So I tried to paint, but I just didn't seem to have any energy or ambition. Part of the problem was that I didn't know many people in California. I'd compounded this problem by buying a house up in the Santa Cruz Mountains, with a beautiful view but miles from anywhere. I stuck it out for a few more months, then in desperation I went back to New York, where unless you understand about rent control you'll be surprised to hear I still had my apartment, sealed up like a tomb of my old life. Idelle was in New York at least, and there were other people trying to paint there, even though I didn't know any of them.  When I got back to New York I resumed my old life, except now I was rich. It was as weird as it sounds. I resumed all my old patterns, except now there were doors where there hadn't been. Now when I was tired of walking, all I had to do was raise my hand, and (unless it was raining) a taxi would stop to pick me up. Now when I walked past charming little restaurants I could go in and order lunch. It was exciting for a while. Painting started to go better. I experimented with a new kind of still life where I'd paint one painting in the old way, then photograph it and print it, blown up, on canvas, and then use that as the underpainting for a second still life, painted from the same objects (which hopefully hadn't rotted \"\"\")     assert score &gt;= 0.5, f\"Score of {score} &lt; 0.5. Relevant context is contained in long text.\" In\u00a0[10]: Copied! <pre>def test_nonanswer_short():\n    score = relevance(\"How many countries are there in the world?\", \"There is no universally accepted answer to how many countries there are in the world.\")\n    assert score &gt;= 0.5, f\"Score of {score} &lt; 0.5. Relevant context without definitive answer did not get a score of &gt;= 0.5\"\n</pre> def test_nonanswer_short():     score = relevance(\"How many countries are there in the world?\", \"There is no universally accepted answer to how many countries there are in the world.\")     assert score &gt;= 0.5, f\"Score of {score} &lt; 0.5. Relevant context without definitive answer did not get a score of &gt;= 0.5\" In\u00a0[11]: Copied! <pre>def test_nonanswer_medium():\n    score = relevance(\"What is the meaning of life?\", \"\"\"No one can tell the actual definition of the meaning of life.\n    For some, it is all about happiness, building a family, and leading life as it is. For some, it is about accumulating wealth, whereas,\n    for some, it is all about love.\"\"\")\n    assert score &gt;= 0.5, f\"Score of {score} &lt; 0.5. Relevant context without definitive answer did not get a score of &gt;= 0.5\"\n</pre> def test_nonanswer_medium():     score = relevance(\"What is the meaning of life?\", \"\"\"No one can tell the actual definition of the meaning of life.     For some, it is all about happiness, building a family, and leading life as it is. For some, it is about accumulating wealth, whereas,     for some, it is all about love.\"\"\")     assert score &gt;= 0.5, f\"Score of {score} &lt; 0.5. Relevant context without definitive answer did not get a score of &gt;= 0.5\" In\u00a0[12]: Copied! <pre>def test_nonanswer_long():\n    score = relevance(\"What came first, the chicken or the egg?\",\"\"\"Eggs come from chickens and chickens come from eggs: that\u2019s the basis of this ancient riddle. But eggs \u2013 which are just female sex cells \u2013 evolved more than a billion years ago, whereas chickens have been around for just 10,000 years. So the riddle is easily solved\u2026or is it?\n\nTaken at face value, there is no doubt that the egg came before the chicken. We tend to think of eggs as the shelled orbs laid by birds from which their chicks hatch \u2013 unless we eat them first. But all sexually reproducing species make eggs (the specialised female sex cells). That\u2019s 99.99 per cent of all eukaryotic life \u2013 meaning organisms that have cells with a nucleus, so all animals and plants, and everything but the simplest life forms.\n\nWe don\u2019t know for sure when sex evolved but it could have been as much as 2 billion years ago, and certainly more than 1 billion. Even the specialised sort of eggs laid by birds, with their tough outer membrane, evolved more than 300 million years ago.\n\nAs for chickens, they came into being much later. They are domesticated animals, so evolved as the result of humans purposefully selecting the least aggressive wild birds and letting them breed. This seems to have happened in several places independently, starting around 10,000 years ago.\n\nThe wild ancestor of chickens is generally agreed to be a tropical bird still living in the forests of Southeast Asia called the red junglefowl \u2013  with other junglefowl species possibly adding to the genetic mix. From these origins, humans have carried chickens around the world over the past two millennia or more.\n\nSo, eggs dramatically predate chickens. But to be fair to the spirit of the riddle, we should also consider whether a chicken\u2019s egg predates a chicken. As humans consistently chose the tamest red junglefowls and bred them together, the genetic makeup of the resulting birds will have shifted. At some stage during this domestication process the red junglefowl (Gallus gallus) evolved into a new subspecies, Gallus gallus domesticus, AKA the chicken.\n\nIn practice, it is impossible to pinpoint the moment when this happened. But in theory, at some point two junglefowl bred and their offspring was genetically different enough from the species of its parents to be classified as a chicken. This chicken would have developed within a junglefowl egg and only produced the very first chicken\u2019s egg on reaching maturity. Looked at this way, the chicken came first.\n\n\"\"\"\n)\n    assert score &gt;= 0.5, f\"Score of {score} &lt; 0.5. Relevant context without definitive answer did not get a score of &gt;= 0.5\"\n</pre> def test_nonanswer_long():     score = relevance(\"What came first, the chicken or the egg?\",\"\"\"Eggs come from chickens and chickens come from eggs: that\u2019s the basis of this ancient riddle. But eggs \u2013 which are just female sex cells \u2013 evolved more than a billion years ago, whereas chickens have been around for just 10,000 years. So the riddle is easily solved\u2026or is it?  Taken at face value, there is no doubt that the egg came before the chicken. We tend to think of eggs as the shelled orbs laid by birds from which their chicks hatch \u2013 unless we eat them first. But all sexually reproducing species make eggs (the specialised female sex cells). That\u2019s 99.99 per cent of all eukaryotic life \u2013 meaning organisms that have cells with a nucleus, so all animals and plants, and everything but the simplest life forms.  We don\u2019t know for sure when sex evolved but it could have been as much as 2 billion years ago, and certainly more than 1 billion. Even the specialised sort of eggs laid by birds, with their tough outer membrane, evolved more than 300 million years ago.  As for chickens, they came into being much later. They are domesticated animals, so evolved as the result of humans purposefully selecting the least aggressive wild birds and letting them breed. This seems to have happened in several places independently, starting around 10,000 years ago.  The wild ancestor of chickens is generally agreed to be a tropical bird still living in the forests of Southeast Asia called the red junglefowl \u2013  with other junglefowl species possibly adding to the genetic mix. From these origins, humans have carried chickens around the world over the past two millennia or more.  So, eggs dramatically predate chickens. But to be fair to the spirit of the riddle, we should also consider whether a chicken\u2019s egg predates a chicken. As humans consistently chose the tamest red junglefowls and bred them together, the genetic makeup of the resulting birds will have shifted. At some stage during this domestication process the red junglefowl (Gallus gallus) evolved into a new subspecies, Gallus gallus domesticus, AKA the chicken.  In practice, it is impossible to pinpoint the moment when this happened. But in theory, at some point two junglefowl bred and their offspring was genetically different enough from the species of its parents to be classified as a chicken. This chicken would have developed within a junglefowl egg and only produced the very first chicken\u2019s egg on reaching maturity. Looked at this way, the chicken came first.  \"\"\" )     assert score &gt;= 0.5, f\"Score of {score} &lt; 0.5. Relevant context without definitive answer did not get a score of &gt;= 0.5\" In\u00a0[13]: Copied! <pre>def test_seemingrelevance_short():\n    seemingly_relevant_score = relevance(\"Who won the superbowl in 2009?\", \"The Pheonix Suns won the Superbowl in 2009\")\n    relevant_score = relevance(\"Who won the superbowl in 2009?\", \"The Pittsburgh Steelers won the Superbowl in 2009\")\n    assert seemingly_relevant_score &lt; relevant_score, f\"Failed to differentiate seeming and actual relevance.\"\n</pre> def test_seemingrelevance_short():     seemingly_relevant_score = relevance(\"Who won the superbowl in 2009?\", \"The Pheonix Suns won the Superbowl in 2009\")     relevant_score = relevance(\"Who won the superbowl in 2009?\", \"The Pittsburgh Steelers won the Superbowl in 2009\")     assert seemingly_relevant_score &lt; relevant_score, f\"Failed to differentiate seeming and actual relevance.\" In\u00a0[14]: Copied! <pre>def test_seemingrelevance_medium():\n    seemingly_relevant_score = relevance(\"What is a cephalopod?\", \"\"\"A cephalopod belongs to a large taxonomic class of \n    invertebrates within the phylum Mollusca called Gastropoda. This class comprises snails and slugs from saltwater, freshwater, \n    and from land. There are many thousands of species of sea snails and slugs, as well as freshwater snails, freshwater limpets, \n    and land snails and slugs.\"\"\")\n    relevant_score = relevance(\"What is a cephalopod?\", \"A cephalopod is any member of the molluscan class Cephalopoda such as a squid, octopus, cuttlefish, or nautilus. These exclusively marine animals are characterized by bilateral body symmetry, a prominent head, and a set of arms or tentacles (muscular hydrostats) modified from the primitive molluscan foot. Fishers sometimes call cephalopods 'inkfish referring to their common ability to squirt ink.\")\n    assert seemingly_relevant_score &lt; relevant_score, f\"Failed to differentiate seeming and actual relevance.\"\n</pre> def test_seemingrelevance_medium():     seemingly_relevant_score = relevance(\"What is a cephalopod?\", \"\"\"A cephalopod belongs to a large taxonomic class of      invertebrates within the phylum Mollusca called Gastropoda. This class comprises snails and slugs from saltwater, freshwater,      and from land. There are many thousands of species of sea snails and slugs, as well as freshwater snails, freshwater limpets,      and land snails and slugs.\"\"\")     relevant_score = relevance(\"What is a cephalopod?\", \"A cephalopod is any member of the molluscan class Cephalopoda such as a squid, octopus, cuttlefish, or nautilus. These exclusively marine animals are characterized by bilateral body symmetry, a prominent head, and a set of arms or tentacles (muscular hydrostats) modified from the primitive molluscan foot. Fishers sometimes call cephalopods 'inkfish referring to their common ability to squirt ink.\")     assert seemingly_relevant_score &lt; relevant_score, f\"Failed to differentiate seeming and actual relevance.\" In\u00a0[15]: Copied! <pre>def test_seemingrelevance_long():\n    seemingly_relevant_score = relevance(\"Describe Abraham Lincoln's childhood\", \"\"\"\n    Abraham Lincoln, Jr. was born on May 29, 1917, in Brookline, Massachusetts. His parents were Joseph P. Kennedy, Sr. and Rose Fitzgerald Kennedy. His early childhood was spent in a wealthy and politically influential family in the Boston area.\n\nBrookline, Massachusetts (1917-1920): Abraham Lincoln's early years were spent in Brookline, Massachusetts. His father, Joseph P. Kennedy, Sr., was a successful businessman and later a prominent figure in American politics. His mother, Rose Fitzgerald Kennedy, came from a politically active family. Abraham Lincoln was the second of nine children.\n\nNew York and London: During his childhood, Abraham Lincoln lived in New York City and London when his father served as the U.S. ambassador to the United Kingdom. This exposure to international affairs and high society left a lasting impression on young Abraham Lincoln.\n\nEducational Pursuits: Abraham Lincoln attended private schools during his early years, including the Riverdale Country School in New York and the Choate School (now Choate Rosemary Hall) in Connecticut. Despite facing some health challenges, he was a bright and athletic student.\n\nFamily Tragedy: Like his siblings, Abraham Lincoln faced the sorrow of losing his older brother, Joseph P. Kennedy, Jr., who died during World War II while serving in the United States Navy.\n\nHarvard University: Abraham Lincoln continued his education at Harvard University, where he developed an interest in government and international relations. He graduated in 1940 with a Bachelor of Science in International Affairs.\n\nMilitary Service: Following his graduation, Abraham Lincoln joined the U.S. Navy and served during World War II. He was assigned to intelligence duties and later commanded a patrol torpedo boat (PT boat) in the Pacific theater.\n\nEntry into Politics: After the war, Abraham Lincoln's interest in public service led him to enter politics. He successfully ran for the U.S. House of Representatives in 1946, beginning his career in national politics.\n\nAbraham Lincoln's childhood and early life were marked by privilege, educational opportunities, and exposure to political and international affairs. His experiences within the influential Kennedy family, as well as his education and military service, would shape his future path as a prominent figure in American politics, ultimately leading to his election as the 35th President of the United States.\"\"\")\n    relevant_score = relevance(\"Describe Abraham Lincoln's childhood\", \"\"\"\n    Abraham Lincoln was born on February 12, 1809, in a one-room log cabin on the Sinking Spring Farm in Hardin County, Kentucky (now part of LaRue County). His parents were Thomas Lincoln and Nancy Hanks Lincoln. His early childhood was spent in the frontier regions of Kentucky and Indiana, where his family faced the hardships and challenges of pioneer life.\n\nKentucky Years (1809-1816): Abraham Lincoln's early years were spent in Kentucky. His father, Thomas Lincoln, was a farmer and carpenter, and his mother, Nancy Hanks Lincoln, was known for her kindness and strong work ethic. Sadly, Nancy died when Abraham was just nine years old. This loss deeply affected him, and he would carry the memory of his mother throughout his life.\n\nMoving to Indiana (1816): In 1816, Thomas Lincoln decided to move his family to Indiana due to land disputes in Kentucky. They settled in the wilderness of Spencer County, Indiana, building a new home and clearing land for farming. Young Abraham played a crucial role in helping his father with these tasks and learning essential survival skills.\n\nSelf-Education: Lincoln's formal education was limited to a few months, amounting to less than a year, as schools were scarce on the frontier. However, he was an avid reader and had a strong thirst for knowledge. Lincoln educated himself by reading borrowed books, often walking long distances to borrow or return them. He later referred to this time as having been \"raised to farm work.\"\n\nEarly Tragedies: The Lincolns faced several tragedies during their time in Indiana. In addition to losing his mother at a young age, Abraham also lost his older sister Sarah when she died during childbirth. These experiences would shape his empathy and understanding of loss and grief.\n\nBeginning His Legal Career: In 1830, the Lincoln family moved to Illinois, where Abraham Lincoln began to work as a farm laborer, rail-splitter, and store clerk. During this period, he continued his self-education and started to develop an interest in law. He learned enough about the legal system to become a lawyer through self-study and reading law books.\n\nMarriage and Family: In 1842, Lincoln married Mary Todd, and they had four sons: Robert, Edward, William, and Thomas. Tragically, only one of their sons, Robert, survived to adulthood.\n\nAbraham Lincoln's childhood and early life were marked by poverty, hard work, and the challenges of frontier living. Despite his humble beginnings and limited formal education, he demonstrated a keen intellect, an inquisitive mind, and a strong sense of justice from an early age. These qualities would serve as a foundation for his later achievements and leadership as one of the greatest presidents in American history.\n    \"\"\")\n    pass\n</pre> def test_seemingrelevance_long():     seemingly_relevant_score = relevance(\"Describe Abraham Lincoln's childhood\", \"\"\"     Abraham Lincoln, Jr. was born on May 29, 1917, in Brookline, Massachusetts. His parents were Joseph P. Kennedy, Sr. and Rose Fitzgerald Kennedy. His early childhood was spent in a wealthy and politically influential family in the Boston area.  Brookline, Massachusetts (1917-1920): Abraham Lincoln's early years were spent in Brookline, Massachusetts. His father, Joseph P. Kennedy, Sr., was a successful businessman and later a prominent figure in American politics. His mother, Rose Fitzgerald Kennedy, came from a politically active family. Abraham Lincoln was the second of nine children.  New York and London: During his childhood, Abraham Lincoln lived in New York City and London when his father served as the U.S. ambassador to the United Kingdom. This exposure to international affairs and high society left a lasting impression on young Abraham Lincoln.  Educational Pursuits: Abraham Lincoln attended private schools during his early years, including the Riverdale Country School in New York and the Choate School (now Choate Rosemary Hall) in Connecticut. Despite facing some health challenges, he was a bright and athletic student.  Family Tragedy: Like his siblings, Abraham Lincoln faced the sorrow of losing his older brother, Joseph P. Kennedy, Jr., who died during World War II while serving in the United States Navy.  Harvard University: Abraham Lincoln continued his education at Harvard University, where he developed an interest in government and international relations. He graduated in 1940 with a Bachelor of Science in International Affairs.  Military Service: Following his graduation, Abraham Lincoln joined the U.S. Navy and served during World War II. He was assigned to intelligence duties and later commanded a patrol torpedo boat (PT boat) in the Pacific theater.  Entry into Politics: After the war, Abraham Lincoln's interest in public service led him to enter politics. He successfully ran for the U.S. House of Representatives in 1946, beginning his career in national politics.  Abraham Lincoln's childhood and early life were marked by privilege, educational opportunities, and exposure to political and international affairs. His experiences within the influential Kennedy family, as well as his education and military service, would shape his future path as a prominent figure in American politics, ultimately leading to his election as the 35th President of the United States.\"\"\")     relevant_score = relevance(\"Describe Abraham Lincoln's childhood\", \"\"\"     Abraham Lincoln was born on February 12, 1809, in a one-room log cabin on the Sinking Spring Farm in Hardin County, Kentucky (now part of LaRue County). His parents were Thomas Lincoln and Nancy Hanks Lincoln. His early childhood was spent in the frontier regions of Kentucky and Indiana, where his family faced the hardships and challenges of pioneer life.  Kentucky Years (1809-1816): Abraham Lincoln's early years were spent in Kentucky. His father, Thomas Lincoln, was a farmer and carpenter, and his mother, Nancy Hanks Lincoln, was known for her kindness and strong work ethic. Sadly, Nancy died when Abraham was just nine years old. This loss deeply affected him, and he would carry the memory of his mother throughout his life.  Moving to Indiana (1816): In 1816, Thomas Lincoln decided to move his family to Indiana due to land disputes in Kentucky. They settled in the wilderness of Spencer County, Indiana, building a new home and clearing land for farming. Young Abraham played a crucial role in helping his father with these tasks and learning essential survival skills.  Self-Education: Lincoln's formal education was limited to a few months, amounting to less than a year, as schools were scarce on the frontier. However, he was an avid reader and had a strong thirst for knowledge. Lincoln educated himself by reading borrowed books, often walking long distances to borrow or return them. He later referred to this time as having been \"raised to farm work.\"  Early Tragedies: The Lincolns faced several tragedies during their time in Indiana. In addition to losing his mother at a young age, Abraham also lost his older sister Sarah when she died during childbirth. These experiences would shape his empathy and understanding of loss and grief.  Beginning His Legal Career: In 1830, the Lincoln family moved to Illinois, where Abraham Lincoln began to work as a farm laborer, rail-splitter, and store clerk. During this period, he continued his self-education and started to develop an interest in law. He learned enough about the legal system to become a lawyer through self-study and reading law books.  Marriage and Family: In 1842, Lincoln married Mary Todd, and they had four sons: Robert, Edward, William, and Thomas. Tragically, only one of their sons, Robert, survived to adulthood.  Abraham Lincoln's childhood and early life were marked by poverty, hard work, and the challenges of frontier living. Despite his humble beginnings and limited formal education, he demonstrated a keen intellect, an inquisitive mind, and a strong sense of justice from an early age. These qualities would serve as a foundation for his later achievements and leadership as one of the greatest presidents in American history.     \"\"\")     pass In\u00a0[16]: Copied! <pre>def test_increasingrelevance_short():\n    score_low = relevance(\"Who won the superbowl in 2009?\",\"Santonio Holmes made a brilliant catch for the Steelers.\")\n    score_medium = relevance(\"Who won the superbowl in 2009?\",\"Santonio Holmes made a brilliant catch for the Steelers in the superbowl.\")\n    score_high = relevance(\"Who won the superbowl in 2009?\",\"Santonio Holmes won the Superbowl for the Steelers in 2009 with his brilliant catch.\")\n    assert (score_low &lt; score_medium) &amp; (score_medium &lt; score_high), \"Score did not increase with more relevant details.\"\n</pre> def test_increasingrelevance_short():     score_low = relevance(\"Who won the superbowl in 2009?\",\"Santonio Holmes made a brilliant catch for the Steelers.\")     score_medium = relevance(\"Who won the superbowl in 2009?\",\"Santonio Holmes made a brilliant catch for the Steelers in the superbowl.\")     score_high = relevance(\"Who won the superbowl in 2009?\",\"Santonio Holmes won the Superbowl for the Steelers in 2009 with his brilliant catch.\")     assert (score_low &lt; score_medium) &amp; (score_medium &lt; score_high), \"Score did not increase with more relevant details.\" In\u00a0[17]: Copied! <pre>def test_increasingrelevance_medium():\n    score_low = relevance(\"What is a cephalopod?\",\"Squids are a member of the molluscan class\")\n    score_medium = relevance(\"What is a cephalopod?\",\"Squids are a member of the molluscan class characterized by bilateral body symmetry, a prominent head, and a set of arms or tentacles (muscular hydrostats) modified from the primitive molluscan foot.\")\n    score_high = relevance(\"What is a cephalopod?\",\"A cephalopod is any member of the molluscan class such as squid, octopus or cuttlefish. These exclusively marine animals are characterized by bilateral body symmetry, a prominent head, and a set of arms or tentacles (muscular hydrostats) modified from the primitive molluscan foot.\")\n    assert (score_low &lt; score_medium) &amp; (score_medium &lt; score_high), \"Score did not increase with more relevant details.\"\n</pre> def test_increasingrelevance_medium():     score_low = relevance(\"What is a cephalopod?\",\"Squids are a member of the molluscan class\")     score_medium = relevance(\"What is a cephalopod?\",\"Squids are a member of the molluscan class characterized by bilateral body symmetry, a prominent head, and a set of arms or tentacles (muscular hydrostats) modified from the primitive molluscan foot.\")     score_high = relevance(\"What is a cephalopod?\",\"A cephalopod is any member of the molluscan class such as squid, octopus or cuttlefish. These exclusively marine animals are characterized by bilateral body symmetry, a prominent head, and a set of arms or tentacles (muscular hydrostats) modified from the primitive molluscan foot.\")     assert (score_low &lt; score_medium) &amp; (score_medium &lt; score_high), \"Score did not increase with more relevant details.\" In\u00a0[18]: Copied! <pre>def test_increasingrelevance_long():\n    score_low = relevance(\"Describe Abraham Lincoln's childhood\",\"Lincoln spent many of his early years in kentucky\")\n    score_medium = relevance(\"Describe Abraham Lincoln's childhood\",\"\"\"\n    Lincoln was born in a one-room log cabin in Hardin County, Kentucky. His parents were Thomas Lincoln and Nancy Hanks Lincoln. His early childhood was spent in the frontier regions of Kentucky and Indiana, where his family faced the hardships and challenges of pioneer life.\n    Kentucky Years (1809-1816): Abraham Lincoln's early years were spent in Kentucky. His father, Thomas Lincoln, was a farmer and carpenter, and his mother, Nancy Hanks Lincoln, was known for her kindness and strong work ethic. Sadly, Nancy died when Abraham was just nine years old. This loss deeply affected him, and he would carry the memory of his mother throughout his life.\n    Moving to Indiana (1816): In 1816, Thomas Lincoln decided to move his family to Indiana due to land disputes in Kentucky. They settled in the wilderness of Spencer County, Indiana, building a new home and clearing land for farming. Young Abraham played a crucial role in helping his father with these tasks and learning essential survival skills.\n    \"\"\")\n    score_high = relevance(\"Describe Abraham Lincoln's childhood\", \"\"\"\n    Abraham Lincoln was born on February 12, 1809, in a one-room log cabin on the Sinking Spring Farm in Hardin County, Kentucky (now part of LaRue County). His parents were Thomas Lincoln and Nancy Hanks Lincoln. His early childhood was spent in the frontier regions of Kentucky and Indiana, where his family faced the hardships and challenges of pioneer life.\n\nKentucky Years (1809-1816): Abraham Lincoln's early years were spent in Kentucky. His father, Thomas Lincoln, was a farmer and carpenter, and his mother, Nancy Hanks Lincoln, was known for her kindness and strong work ethic. Sadly, Nancy died when Abraham was just nine years old. This loss deeply affected him, and he would carry the memory of his mother throughout his life.\n\nMoving to Indiana (1816): In 1816, Thomas Lincoln decided to move his family to Indiana due to land disputes in Kentucky. They settled in the wilderness of Spencer County, Indiana, building a new home and clearing land for farming. Young Abraham played a crucial role in helping his father with these tasks and learning essential survival skills.\n\nSelf-Education: Lincoln's formal education was limited to a few months, amounting to less than a year, as schools were scarce on the frontier. However, he was an avid reader and had a strong thirst for knowledge. Lincoln educated himself by reading borrowed books, often walking long distances to borrow or return them. He later referred to this time as having been \"raised to farm work.\"\n\nEarly Tragedies: The Lincolns faced several tragedies during their time in Indiana. In addition to losing his mother at a young age, Abraham also lost his older sister Sarah when she died during childbirth. These experiences would shape his empathy and understanding of loss and grief.\n\nBeginning His Legal Career: In 1830, the Lincoln family moved to Illinois, where Abraham Lincoln began to work as a farm laborer, rail-splitter, and store clerk. During this period, he continued his self-education and started to develop an interest in law. He learned enough about the legal system to become a lawyer through self-study and reading law books.\n\nMarriage and Family: In 1842, Lincoln married Mary Todd, and they had four sons: Robert, Edward, William, and Thomas. Tragically, only one of their sons, Robert, survived to adulthood.\n\nAbraham Lincoln's childhood and early life were marked by poverty, hard work, and the challenges of frontier living. Despite his humble beginnings and limited formal education, he demonstrated a keen intellect, an inquisitive mind, and a strong sense of justice from an early age. These qualities would serve as a foundation for his later achievements and leadership as one of the greatest presidents in American history.\"\"\")\n    pass\n</pre> def test_increasingrelevance_long():     score_low = relevance(\"Describe Abraham Lincoln's childhood\",\"Lincoln spent many of his early years in kentucky\")     score_medium = relevance(\"Describe Abraham Lincoln's childhood\",\"\"\"     Lincoln was born in a one-room log cabin in Hardin County, Kentucky. His parents were Thomas Lincoln and Nancy Hanks Lincoln. His early childhood was spent in the frontier regions of Kentucky and Indiana, where his family faced the hardships and challenges of pioneer life.     Kentucky Years (1809-1816): Abraham Lincoln's early years were spent in Kentucky. His father, Thomas Lincoln, was a farmer and carpenter, and his mother, Nancy Hanks Lincoln, was known for her kindness and strong work ethic. Sadly, Nancy died when Abraham was just nine years old. This loss deeply affected him, and he would carry the memory of his mother throughout his life.     Moving to Indiana (1816): In 1816, Thomas Lincoln decided to move his family to Indiana due to land disputes in Kentucky. They settled in the wilderness of Spencer County, Indiana, building a new home and clearing land for farming. Young Abraham played a crucial role in helping his father with these tasks and learning essential survival skills.     \"\"\")     score_high = relevance(\"Describe Abraham Lincoln's childhood\", \"\"\"     Abraham Lincoln was born on February 12, 1809, in a one-room log cabin on the Sinking Spring Farm in Hardin County, Kentucky (now part of LaRue County). His parents were Thomas Lincoln and Nancy Hanks Lincoln. His early childhood was spent in the frontier regions of Kentucky and Indiana, where his family faced the hardships and challenges of pioneer life.  Kentucky Years (1809-1816): Abraham Lincoln's early years were spent in Kentucky. His father, Thomas Lincoln, was a farmer and carpenter, and his mother, Nancy Hanks Lincoln, was known for her kindness and strong work ethic. Sadly, Nancy died when Abraham was just nine years old. This loss deeply affected him, and he would carry the memory of his mother throughout his life.  Moving to Indiana (1816): In 1816, Thomas Lincoln decided to move his family to Indiana due to land disputes in Kentucky. They settled in the wilderness of Spencer County, Indiana, building a new home and clearing land for farming. Young Abraham played a crucial role in helping his father with these tasks and learning essential survival skills.  Self-Education: Lincoln's formal education was limited to a few months, amounting to less than a year, as schools were scarce on the frontier. However, he was an avid reader and had a strong thirst for knowledge. Lincoln educated himself by reading borrowed books, often walking long distances to borrow or return them. He later referred to this time as having been \"raised to farm work.\"  Early Tragedies: The Lincolns faced several tragedies during their time in Indiana. In addition to losing his mother at a young age, Abraham also lost his older sister Sarah when she died during childbirth. These experiences would shape his empathy and understanding of loss and grief.  Beginning His Legal Career: In 1830, the Lincoln family moved to Illinois, where Abraham Lincoln began to work as a farm laborer, rail-splitter, and store clerk. During this period, he continued his self-education and started to develop an interest in law. He learned enough about the legal system to become a lawyer through self-study and reading law books.  Marriage and Family: In 1842, Lincoln married Mary Todd, and they had four sons: Robert, Edward, William, and Thomas. Tragically, only one of their sons, Robert, survived to adulthood.  Abraham Lincoln's childhood and early life were marked by poverty, hard work, and the challenges of frontier living. Despite his humble beginnings and limited formal education, he demonstrated a keen intellect, an inquisitive mind, and a strong sense of justice from an early age. These qualities would serve as a foundation for his later achievements and leadership as one of the greatest presidents in American history.\"\"\")     pass In\u00a0[19]: Copied! <pre>#! pip install tabulate\n</pre> #! pip install tabulate In\u00a0[20]: Copied! <pre>from tabulate import tabulate\n\ndef run_all_smoke_tests():\n    smoke_tests = [\n        test_lowadherence_short, test_lowadherence_medium, test_lowadherence_long,\n        test_majorityadherence_short, test_majorityadherence_medium, test_majorityadherence_long, test_smallrelevance_long,\n        test_nonanswer_short, test_nonanswer_medium, test_nonanswer_long,\n        test_increasingrelevance_short, test_increasingrelevance_medium, test_increasingrelevance_long,\n        test_seemingrelevance_short, test_seemingrelevance_medium, test_seemingrelevance_long\n    ]\n\n    total_tests = len(smoke_tests)\n    total_passed = 0\n    total_failed = 0\n    type_passed = {}\n    type_failed = {}\n    length_passed = {}\n    length_failed = {}\n\n    for i, test in enumerate(smoke_tests):\n        try:\n            test()\n            total_passed += 1\n\n            # Update type_passed and length_passed dictionaries\n            test_name = test.__name__\n            test_type, test_length = test_name.split('_')[1], test_name.split('_')[2]\n            type_passed[test_type] = type_passed.get(test_type, 0) + 1\n            length_passed[test_length] = length_passed.get(test_length, 0) + 1\n        except AssertionError as e:\n            total_failed += 1\n\n            # Update type_failed and length_failed dictionaries\n            test_name = test.__name__\n            test_type, test_length = test_name.split('_')[1], test_name.split('_')[2]\n            type_failed[test_type] = type_failed.get(test_type, 0) + 1\n            length_failed[test_length] = length_failed.get(test_length, 0) + 1\n\n    # Prepare data for the table\n    overall_data = [[\"Total Tests\", total_tests],\n                    [\"Total Passed \u2705\", total_passed],\n                    [\"Total Failed \u274c\", total_failed]]\n\n    type_data = [[f\"{test_type.capitalize()} Tests\", type_passed.get(test_type, 0), type_failed.get(test_type, 0),\n                  f\"{(type_passed.get(test_type, 0) / (type_passed.get(test_type, 0) + type_failed.get(test_type, 0))) * 100:.2f}%\"]\n                 for test_type in sorted(set(type_passed.keys()) | set(type_failed.keys()))]\n\n    length_data = [[f\"{test_length.capitalize()} Tests\", length_passed.get(test_length, 0), length_failed.get(test_length, 0),\n                    f\"{(length_passed.get(test_length, 0) / (length_passed.get(test_length, 0) + length_failed.get(test_length, 0))) * 100:.2f}%\"]\n                   for test_length in sorted(set(length_passed.keys()) | set(length_failed.keys()))]\n\n    return overall_data, type_data, length_data\n\n# Get the data from the function\noverall_data, type_data, length_data = run_all_smoke_tests()\n</pre> from tabulate import tabulate  def run_all_smoke_tests():     smoke_tests = [         test_lowadherence_short, test_lowadherence_medium, test_lowadherence_long,         test_majorityadherence_short, test_majorityadherence_medium, test_majorityadherence_long, test_smallrelevance_long,         test_nonanswer_short, test_nonanswer_medium, test_nonanswer_long,         test_increasingrelevance_short, test_increasingrelevance_medium, test_increasingrelevance_long,         test_seemingrelevance_short, test_seemingrelevance_medium, test_seemingrelevance_long     ]      total_tests = len(smoke_tests)     total_passed = 0     total_failed = 0     type_passed = {}     type_failed = {}     length_passed = {}     length_failed = {}      for i, test in enumerate(smoke_tests):         try:             test()             total_passed += 1              # Update type_passed and length_passed dictionaries             test_name = test.__name__             test_type, test_length = test_name.split('_')[1], test_name.split('_')[2]             type_passed[test_type] = type_passed.get(test_type, 0) + 1             length_passed[test_length] = length_passed.get(test_length, 0) + 1         except AssertionError as e:             total_failed += 1              # Update type_failed and length_failed dictionaries             test_name = test.__name__             test_type, test_length = test_name.split('_')[1], test_name.split('_')[2]             type_failed[test_type] = type_failed.get(test_type, 0) + 1             length_failed[test_length] = length_failed.get(test_length, 0) + 1      # Prepare data for the table     overall_data = [[\"Total Tests\", total_tests],                     [\"Total Passed \u2705\", total_passed],                     [\"Total Failed \u274c\", total_failed]]      type_data = [[f\"{test_type.capitalize()} Tests\", type_passed.get(test_type, 0), type_failed.get(test_type, 0),                   f\"{(type_passed.get(test_type, 0) / (type_passed.get(test_type, 0) + type_failed.get(test_type, 0))) * 100:.2f}%\"]                  for test_type in sorted(set(type_passed.keys()) | set(type_failed.keys()))]      length_data = [[f\"{test_length.capitalize()} Tests\", length_passed.get(test_length, 0), length_failed.get(test_length, 0),                     f\"{(length_passed.get(test_length, 0) / (length_passed.get(test_length, 0) + length_failed.get(test_length, 0))) * 100:.2f}%\"]                    for test_length in sorted(set(length_passed.keys()) | set(length_failed.keys()))]      return overall_data, type_data, length_data  # Get the data from the function overall_data, type_data, length_data = run_all_smoke_tests() In\u00a0[21]: Copied! <pre># Print the table\nprint(\"\\nOverall Results:\")\nprint(tabulate(overall_data, headers=[\"Metric\", \"Count\"], tablefmt=\"grid\"))\n</pre> # Print the table print(\"\\nOverall Results:\") print(tabulate(overall_data, headers=[\"Metric\", \"Count\"], tablefmt=\"grid\")) <pre>\nOverall Results:\n+-----------------+---------+\n| Metric          |   Count |\n+=================+=========+\n| Total Tests     |      16 |\n+-----------------+---------+\n| Total Passed \u2705 |      14 |\n+-----------------+---------+\n| Total Failed \u274c |       2 |\n+-----------------+---------+\n</pre> In\u00a0[22]: Copied! <pre>print(\"\\nResults by Type:\")\nprint(tabulate(type_data, headers=[\"Test Type\", \"Passed \u2705\", \"Failed \u274c\", \"Percentage Passed\"], tablefmt=\"grid\"))\n</pre> print(\"\\nResults by Type:\") print(tabulate(type_data, headers=[\"Test Type\", \"Passed \u2705\", \"Failed \u274c\", \"Percentage Passed\"], tablefmt=\"grid\")) <pre>\nResults by Type:\n+---------------------------+-------------+-------------+---------------------+\n| Test Type                 |   Passed \u2705 |   Failed \u274c | Percentage Passed   |\n+===========================+=============+=============+=====================+\n| Increasingrelevance Tests |           3 |           0 | 100.00%             |\n+---------------------------+-------------+-------------+---------------------+\n| Lowadherence Tests        |           3 |           0 | 100.00%             |\n+---------------------------+-------------+-------------+---------------------+\n| Majorityadherence Tests   |           2 |           1 | 66.67%              |\n+---------------------------+-------------+-------------+---------------------+\n| Nonanswer Tests           |           3 |           0 | 100.00%             |\n+---------------------------+-------------+-------------+---------------------+\n| Seemingrelevance Tests    |           3 |           0 | 100.00%             |\n+---------------------------+-------------+-------------+---------------------+\n| Smallrelevance Tests      |           0 |           1 | 0.00%               |\n+---------------------------+-------------+-------------+---------------------+\n</pre> In\u00a0[23]: Copied! <pre>print(\"\\nResults by Length:\")\nprint(tabulate(length_data, headers=[\"Test Length\", \"Passed \u2705\", \"Failed \u274c\", \"Percentage Passed\"], tablefmt=\"grid\"))\n</pre> print(\"\\nResults by Length:\") print(tabulate(length_data, headers=[\"Test Length\", \"Passed \u2705\", \"Failed \u274c\", \"Percentage Passed\"], tablefmt=\"grid\")) <pre>\nResults by Length:\n+---------------+-------------+-------------+---------------------+\n| Test Length   |   Passed \u2705 |   Failed \u274c | Percentage Passed   |\n+===============+=============+=============+=====================+\n| Long Tests    |           4 |           2 | 66.67%              |\n+---------------+-------------+-------------+---------------------+\n| Medium Tests  |           5 |           0 | 100.00%             |\n+---------------+-------------+-------------+---------------------+\n| Short Tests   |           5 |           0 | 100.00%             |\n+---------------+-------------+-------------+---------------------+\n</pre>"},{"location":"trulens_eval/qs_relevance_smoke_tests/#qs-relevance-feedback-requirements","title":"QS Relevance Feedback Requirements\u00b6","text":"<ol> <li>Relevance requires adherence to the entire query.</li> <li>Long context with small relevant chunks are relevant.</li> <li>Context that provides no answer can still be relevant.</li> <li>Feedback mechanism should differentiate between seeming and actual relevance.</li> <li>Relevant but inconclusive statements should get increasingly high scores as they are more helpful for answering the query.</li> </ol>"},{"location":"trulens_eval/qs_relevance_smoke_tests/#relevance-requires-adherence-to-the-entire-query","title":"Relevance requires adherence to the entire query.\u00b6","text":""},{"location":"trulens_eval/qs_relevance_smoke_tests/#long-context-with-small-relevant-chunks-are-relevant","title":"Long context with small relevant chunks are relevant\u00b6","text":""},{"location":"trulens_eval/qs_relevance_smoke_tests/#context-that-provides-no-answer-can-still-be-relevant","title":"Context that provides no answer can still be relevant.\u00b6","text":""},{"location":"trulens_eval/qs_relevance_smoke_tests/#feedback-score-should-differentiate-between-seeming-and-actual-relevance","title":"Feedback score should differentiate between seeming and actual relevance.\u00b6","text":""},{"location":"trulens_eval/qs_relevance_smoke_tests/#relevant-but-inconclusive-statements-should-get-increasingly-high-scores-as-they-are-more-helpful-for-answering-the-query","title":"Relevant but inconclusive statements should get increasingly high scores as they are more helpful for answering the query.\u00b6","text":""},{"location":"trulens_eval/qs_relevance_smoke_tests/#run-smoke-tests","title":"Run smoke tests\u00b6","text":""},{"location":"trulens_eval/quickstart/","title":"Quickstart","text":"In\u00a0[\u00a0]: Copied! <pre>import os\nos.environ[\"OPENAI_API_KEY\"] = \"...\"\nos.environ[\"HUGGINGFACE_API_KEY\"] = \"...\"\n</pre> import os os.environ[\"OPENAI_API_KEY\"] = \"...\" os.environ[\"HUGGINGFACE_API_KEY\"] = \"...\" In\u00a0[\u00a0]: Copied! <pre>from IPython.display import JSON\n\n# Imports main tools:\nfrom trulens_eval import TruChain, Feedback, Huggingface, Tru\ntru = Tru()\n\n# Imports from langchain to build app. You may need to install langchain first\n# with the following:\n# ! pip install langchain&gt;=0.0.170\nfrom langchain.chains import LLMChain\nfrom langchain.llms import OpenAI\nfrom langchain.prompts.chat import ChatPromptTemplate, PromptTemplate\nfrom langchain.prompts.chat import HumanMessagePromptTemplate\n</pre> from IPython.display import JSON  # Imports main tools: from trulens_eval import TruChain, Feedback, Huggingface, Tru tru = Tru()  # Imports from langchain to build app. You may need to install langchain first # with the following: # ! pip install langchain&gt;=0.0.170 from langchain.chains import LLMChain from langchain.llms import OpenAI from langchain.prompts.chat import ChatPromptTemplate, PromptTemplate from langchain.prompts.chat import HumanMessagePromptTemplate In\u00a0[\u00a0]: Copied! <pre>full_prompt = HumanMessagePromptTemplate(\n    prompt=PromptTemplate(\n        template=\n        \"Provide a helpful response with relevant background information for the following: {prompt}\",\n        input_variables=[\"prompt\"],\n    )\n)\n\nchat_prompt_template = ChatPromptTemplate.from_messages([full_prompt])\n\nllm = OpenAI(temperature=0.9, max_tokens=128)\n\nchain = LLMChain(llm=llm, prompt=chat_prompt_template, verbose=True)\n</pre> full_prompt = HumanMessagePromptTemplate(     prompt=PromptTemplate(         template=         \"Provide a helpful response with relevant background information for the following: {prompt}\",         input_variables=[\"prompt\"],     ) )  chat_prompt_template = ChatPromptTemplate.from_messages([full_prompt])  llm = OpenAI(temperature=0.9, max_tokens=128)  chain = LLMChain(llm=llm, prompt=chat_prompt_template, verbose=True) In\u00a0[\u00a0]: Copied! <pre>prompt_input = '\u00bfque hora es?'\n</pre> prompt_input = '\u00bfque hora es?' In\u00a0[\u00a0]: Copied! <pre>llm_response = chain(prompt_input)\n\ndisplay(llm_response)\n</pre> llm_response = chain(prompt_input)  display(llm_response) In\u00a0[\u00a0]: Copied! <pre># Initialize Huggingface-based feedback function collection class:\nhugs = Huggingface()\n\n# Define a language match feedback function using HuggingFace.\nf_lang_match = Feedback(hugs.language_match).on_input_output()\n# By default this will check language match on the main app input and main app\n# output.\n</pre> # Initialize Huggingface-based feedback function collection class: hugs = Huggingface()  # Define a language match feedback function using HuggingFace. f_lang_match = Feedback(hugs.language_match).on_input_output() # By default this will check language match on the main app input and main app # output. In\u00a0[\u00a0]: Copied! <pre>truchain = TruChain(chain,\n    app_id='Chain1_ChatApplication',\n    feedbacks=[f_lang_match],\n    tags = \"prototype\")\n</pre> truchain = TruChain(chain,     app_id='Chain1_ChatApplication',     feedbacks=[f_lang_match],     tags = \"prototype\") In\u00a0[\u00a0]: Copied! <pre># Instrumented chain can operate like the original:\nllm_response = truchain(prompt_input)\n\ndisplay(llm_response)\n</pre> # Instrumented chain can operate like the original: llm_response = truchain(prompt_input)  display(llm_response) In\u00a0[\u00a0]: Copied! <pre>tru.run_dashboard() # open a local streamlit app to explore\n\n# tru.stop_dashboard() # stop if needed\n</pre> tru.run_dashboard() # open a local streamlit app to explore  # tru.stop_dashboard() # stop if needed <p>Alternatively, you can run <code>trulens-eval</code> from a command line in the same folder to start the dashboard.</p> <p>Note: Feedback functions evaluated in the deferred manner can be seen in the \"Progress\" page of the TruLens dashboard.</p> In\u00a0[\u00a0]: Copied! <pre>tru.get_records_and_feedback(app_ids=[])[0] # pass an empty list of app_ids to get all\n</pre> tru.get_records_and_feedback(app_ids=[])[0] # pass an empty list of app_ids to get all"},{"location":"trulens_eval/quickstart/#quickstart","title":"Quickstart\u00b6","text":"<p>In this quickstart you will create a simple LLM Chain and learn how to log it and get feedback on an LLM response.</p>"},{"location":"trulens_eval/quickstart/#setup","title":"Setup\u00b6","text":""},{"location":"trulens_eval/quickstart/#add-api-keys","title":"Add API keys\u00b6","text":"<p>For this quickstart you will need Open AI and Huggingface keys</p>"},{"location":"trulens_eval/quickstart/#import-from-langchain-and-trulens","title":"Import from LangChain and TruLens\u00b6","text":""},{"location":"trulens_eval/quickstart/#create-simple-llm-application","title":"Create Simple LLM Application\u00b6","text":"<p>This example uses a LangChain framework and OpenAI LLM</p>"},{"location":"trulens_eval/quickstart/#send-your-first-request","title":"Send your first request\u00b6","text":""},{"location":"trulens_eval/quickstart/#initialize-feedback-functions","title":"Initialize Feedback Function(s)\u00b6","text":""},{"location":"trulens_eval/quickstart/#instrument-chain-for-logging-with-trulens","title":"Instrument chain for logging with TruLens\u00b6","text":""},{"location":"trulens_eval/quickstart/#explore-in-a-dashboard","title":"Explore in a Dashboard\u00b6","text":""},{"location":"trulens_eval/quickstart/#chain-leaderboard","title":"Chain Leaderboard\u00b6","text":"<p>Understand how your LLM application is performing at a glance. Once you've set up logging and evaluation in your application, you can view key performance statistics including cost and average feedback value across all of your LLM apps using the chain leaderboard. As you iterate new versions of your LLM application, you can compare their performance across all of the different quality metrics you've set up.</p> <p>Note: Average feedback values are returned and displayed in a range from 0 (worst) to 1 (best).</p> <p></p> <p>To dive deeper on a particular chain, click \"Select Chain\".</p>"},{"location":"trulens_eval/quickstart/#understand-chain-performance-with-evaluations","title":"Understand chain performance with Evaluations\u00b6","text":"<p>To learn more about the performance of a particular chain or LLM model, we can select it to view its evaluations at the record level. LLM quality is assessed through the use of feedback functions. Feedback functions are extensible methods for determining the quality of LLM responses and can be applied to any downstream LLM task. Out of the box we provide a number of feedback functions for assessing model agreement, sentiment, relevance and more.</p> <p>The evaluations tab provides record-level metadata and feedback on the quality of your LLM application.</p> <p></p>"},{"location":"trulens_eval/quickstart/#deep-dive-into-full-chain-metadata","title":"Deep dive into full chain metadata\u00b6","text":"<p>Click on a record to dive deep into all of the details of your chain stack and underlying LLM, captured by tru_chain.</p> <p></p> <p>If you prefer the raw format, you can quickly get it using the \"Display full chain json\" or \"Display full record json\" buttons at the bottom of the page.</p>"},{"location":"trulens_eval/quickstart/#or-view-results-directly-in-your-notebook","title":"Or view results directly in your notebook\u00b6","text":""},{"location":"trulens_eval/trulens_eval_gh_top_readme/","title":"Trulens eval gh top readme","text":"In\u00a0[\u00a0]: Copied! <pre>from trulens_eval import Tru\nfrom trulens_eval import TruChain\n\ntru = Tru()\n</pre> from trulens_eval import Tru from trulens_eval import TruChain  tru = Tru() <p>This example uses LangChain and OpenAI, but the same process can be followed with any framework and model provider.</p> In\u00a0[\u00a0]: Copied! <pre># imports from LangChain to build app\nfrom langchain import PromptTemplate\nfrom langchain.chains import LLMChain\nfrom langchain.chat_models import ChatOpenAI\nfrom langchain.prompts.chat import ChatPromptTemplate\nfrom langchain.prompts.chat import HumanMessagePromptTemplate\n\nimport os\nos.environ[\"OPENAI_API_KEY\"] = \"...\"\nos.environ[\"HUGGINGFACE_API_KEY\"] = \"...\"\n</pre> # imports from LangChain to build app from langchain import PromptTemplate from langchain.chains import LLMChain from langchain.chat_models import ChatOpenAI from langchain.prompts.chat import ChatPromptTemplate from langchain.prompts.chat import HumanMessagePromptTemplate  import os os.environ[\"OPENAI_API_KEY\"] = \"...\" os.environ[\"HUGGINGFACE_API_KEY\"] = \"...\" In\u00a0[\u00a0]: Copied! <pre># create LLM chain\nfull_prompt = HumanMessagePromptTemplate(\n    prompt=PromptTemplate(\n        template=\"Provide a helpful response with relevant background information for the following: {prompt}\",\n            input_variables=[\"prompt\"],\n        )\n    )\nchat_prompt_template = ChatPromptTemplate.from_messages([full_prompt])\n\nchat = ChatOpenAI(model_name='gpt-3.5-turbo', temperature=0.9)\n\nchain = LLMChain(llm=chat, prompt=chat_prompt_template)\n</pre> # create LLM chain full_prompt = HumanMessagePromptTemplate(     prompt=PromptTemplate(         template=\"Provide a helpful response with relevant background information for the following: {prompt}\",             input_variables=[\"prompt\"],         )     ) chat_prompt_template = ChatPromptTemplate.from_messages([full_prompt])  chat = ChatOpenAI(model_name='gpt-3.5-turbo', temperature=0.9)  chain = LLMChain(llm=chat, prompt=chat_prompt_template) <p>Now that we created an LLM chain, we can set up our first feedback function. Here, we'll create a feedback function for language matching. After we've created the feedback function, we can include it in the TruChain wrapper. Now, whenever our wrapped chain is used we'll log both the metadata and feedback.</p> In\u00a0[\u00a0]: Copied! <pre># create a feedback function\n\nfrom trulens_eval.feedback import Feedback, Huggingface\n</pre> # create a feedback function  from trulens_eval.feedback import Feedback, Huggingface In\u00a0[\u00a0]: Copied! <pre># Initialize HuggingFace-based feedback function collection class:\nhugs = Huggingface()\n\n# Define a language match feedback function using HuggingFace.\nf_lang_match = Feedback(hugs.language_match).on_input_output()\n# By default this will check language match on the main app input and main app\n# output.\n\n# wrap your chain with TruChain\ntruchain = TruChain(\n    chain,\n    app_id='Chain1_ChatApplication',\n    feedbacks=[f_lang_match]\n)\n# Note: any `feedbacks` specified here will be evaluated and logged whenever the chain is used.\ntruchain(\"que hora es?\")\n</pre> # Initialize HuggingFace-based feedback function collection class: hugs = Huggingface()  # Define a language match feedback function using HuggingFace. f_lang_match = Feedback(hugs.language_match).on_input_output() # By default this will check language match on the main app input and main app # output.  # wrap your chain with TruChain truchain = TruChain(     chain,     app_id='Chain1_ChatApplication',     feedbacks=[f_lang_match] ) # Note: any `feedbacks` specified here will be evaluated and logged whenever the chain is used. truchain(\"que hora es?\") <p>Now you can explore your LLM-based application!</p> <p>Doing so will help you understand how your LLM application is performing at a glance. As you iterate new versions of your LLM application, you can compare their performance across all of the different quality metrics you've set up. You'll also be able to view evaluations at a record level, and explore the chain metadata for each record.</p> In\u00a0[\u00a0]: Copied! <pre>tru.run_dashboard() # open a Streamlit app to explore\n</pre> tru.run_dashboard() # open a Streamlit app to explore <p>Alternatively, you can run <code>trulens-eval</code> from a command line in the same folder to start the dashboard.</p> <p>For more information, see TruLens-Eval Documentation.</p>"},{"location":"trulens_eval/trulens_eval_gh_top_readme/","title":"Trulens eval gh top readme","text":"<pre><code>from trulens_eval import Tru\nfrom trulens_eval import TruChain\n\ntru = Tru()\n</code></pre> <p>This example uses LangChain and OpenAI, but the same process can be followed with any framework and model provider.</p> <pre><code># imports from LangChain to build app\nfrom langchain import PromptTemplate\nfrom langchain.chains import LLMChain\nfrom langchain.chat_models import ChatOpenAI\nfrom langchain.prompts.chat import ChatPromptTemplate\nfrom langchain.prompts.chat import HumanMessagePromptTemplate\n\nimport os\nos.environ[\"OPENAI_API_KEY\"] = \"...\"\nos.environ[\"HUGGINGFACE_API_KEY\"] = \"...\"\n</code></pre> <pre><code># create LLM chain\nfull_prompt = HumanMessagePromptTemplate(\n    prompt=PromptTemplate(\n        template=\"Provide a helpful response with relevant background information for the following: {prompt}\",\n            input_variables=[\"prompt\"],\n        )\n    )\nchat_prompt_template = ChatPromptTemplate.from_messages([full_prompt])\n\nchat = ChatOpenAI(model_name='gpt-3.5-turbo', temperature=0.9)\n\nchain = LLMChain(llm=chat, prompt=chat_prompt_template)\n</code></pre> <p>Now that we created an LLM chain, we can set up our first feedback function. Here, we'll create a feedback function for language matching. After we've created the feedback function, we can include it in the TruChain wrapper. Now, whenever our wrapped chain is used we'll log both the metadata and feedback.</p> <pre><code># create a feedback function\n\nfrom trulens_eval.feedback import Feedback, Huggingface\n</code></pre> <pre><code># Initialize HuggingFace-based feedback function collection class:\nhugs = Huggingface()\n\n# Define a language match feedback function using HuggingFace.\nf_lang_match = Feedback(hugs.language_match).on_input_output()\n# By default this will check language match on the main app input and main app\n# output.\n\n# wrap your chain with TruChain\ntruchain = TruChain(\n    chain,\n    app_id='Chain1_ChatApplication',\n    feedbacks=[f_lang_match]\n)\n# Note: any `feedbacks` specified here will be evaluated and logged whenever the chain is used.\ntruchain(\"que hora es?\")\n</code></pre> <p>Now you can explore your LLM-based application!</p> <p>Doing so will help you understand how your LLM application is performing at a glance. As you iterate new versions of your LLM application, you can compare their performance across all of the different quality metrics you've set up. You'll also be able to view evaluations at a record level, and explore the chain metadata for each record.</p> <pre><code>tru.run_dashboard() # open a Streamlit app to explore\n</code></pre> <p>Alternatively, you can run <code>trulens-eval</code> from a command line in the same folder to start the dashboard.</p> <p>For more information, see TruLens-Eval Documentation.</p>"},{"location":"trulens_eval/api/feedback/","title":"Feedback Functions","text":""},{"location":"trulens_eval/api/feedback/#trulens_eval.trulens_eval.feedback--feedback-functions","title":"Feedback Functions","text":"<p>The <code>Feedback</code> class contains the starting point for feedback function specification and evaluation. A typical use-case looks like this:</p> <pre><code>from trulens_eval import feedback, Select, Feedback\n\nhugs = feedback.Huggingface()\n\nf_lang_match = Feedback(hugs.language_match)\n    .on_input_output()\n</code></pre> <p>The components of this specifications are:</p> <ul> <li> <p>Provider classes -- <code>feedback.OpenAI</code> contains feedback function   implementations like <code>qs_relevance</code>. Other classes subtyping   <code>feedback.Provider</code> include <code>Huggingface</code> and <code>Cohere</code>.</p> </li> <li> <p>Feedback implementations -- <code>openai.qs_relevance</code> is a feedback function   implementation. Feedback implementations are simple callables that can be run   on any arguments matching their signatures. In the example, the implementation   has the following signature: </p> <pre><code>def language_match(self, text1: str, text2: str) -&gt; float:\n</code></pre> </li> </ul> <p>That is, <code>language_match</code> is a plain python method that accepts two pieces   of text, both strings, and produces a float (assumed to be between 0.0 and   1.0).</p> <ul> <li> <p>Feedback constructor -- The line <code>Feedback(openai.language_match)</code>   constructs a Feedback object with a feedback implementation. </p> </li> <li> <p>Argument specification -- The next line, <code>on_input_output</code>, specifies how   the <code>language_match</code> arguments are to be determined from an app record or app   definition. The general form of this specification is done using <code>on</code> but   several shorthands are provided. <code>on_input_output</code> states that the first two   argument to <code>language_match</code> (<code>text1</code> and <code>text2</code>) are to be the main app   input and the main output, respectively.</p> </li> </ul> <p>Several utility methods starting with <code>.on</code> provide shorthands:</p> <pre><code>- `on_input(arg) == on_prompt(arg: Optional[str])` -- both specify that the next\nunspecified argument or `arg` should be the main app input.\n\n- `on_output(arg) == on_response(arg: Optional[str])` -- specify that the next\nargument or `arg` should be the main app output.\n\n- `on_input_output() == on_input().on_output()` -- specifies that the first\ntwo arguments of implementation should be the main app input and main app\noutput, respectively.\n\n- `on_default()` -- depending on signature of implementation uses either\n`on_output()` if it has a single argument, or `on_input_output` if it has\ntwo arguments.\n\nSome wrappers include additional shorthands:\n\n### llama_index-specific selectors\n\n- `TruLlama.select_source_nodes()` -- outputs the selector of the source\n    documents part of the engine output.\n</code></pre>"},{"location":"trulens_eval/api/feedback/#trulens_eval.trulens_eval.feedback--fine-grained-selection-and-aggregation","title":"Fine-grained Selection and Aggregation","text":"<p>For more advanced control on the feedback function operation, we allow data selection and aggregation. Consider this feedback example:</p> <pre><code>f_qs_relevance = Feedback(openai.qs_relevance)\n    .on_input()\n    .on(Select.Record.app.combine_docs_chain._call.args.inputs.input_documents[:].page_content)\n    .aggregate(numpy.min)\n\n# Implementation signature:\n# def qs_relevance(self, question: str, statement: str) -&gt; float:\n</code></pre> <ul> <li> <p>**Argument Selection specification ** -- Where we previously set,   <code>on_input_output</code> , the <code>on(Select...)</code> line enables specification of where   the statement argument to the implementation comes from. The form of the   specification will be discussed in further details in the Specifying Arguments   section.</p> </li> <li> <p>Aggregation specification -- The last line <code>aggregate(numpy.min)</code> specifies   how feedback outputs are to be aggregated. This only applies to cases where   the argument specification names more than one value for an input. The second   specification, for <code>statement</code> was of this type. The input to <code>aggregate</code> must   be a method which can be imported globally. This requirement is further   elaborated in the next section. This function is called on the <code>float</code> results   of feedback function evaluations to produce a single float. The default is   <code>numpy.mean</code>.</p> </li> </ul> <p>The result of these lines is that <code>f_qs_relevance</code> can be now be run on app/records and will automatically select the specified components of those apps/records:</p> <pre><code>record: Record = ...\napp: App = ...\n\nfeedback_result: FeedbackResult = f_qs_relevance.run(app=app, record=record)\n</code></pre> <p>The object can also be provided to an app wrapper for automatic evaluation:</p> <pre><code>app: App = tru.Chain(...., feedbacks=[f_qs_relevance])\n</code></pre>"},{"location":"trulens_eval/api/feedback/#trulens_eval.trulens_eval.feedback--specifying-implementation-function-and-aggregate","title":"Specifying Implementation Function and Aggregate","text":"<p>The function or method provided to the <code>Feedback</code> constructor is the implementation of the feedback function which does the actual work of producing a float indicating some quantity of interest. </p> <p>Note regarding FeedbackMode.DEFERRED -- Any function or method (not static or class methods presently supported) can be provided here but there are additional requirements if your app uses the \"deferred\" feedback evaluation mode (when <code>feedback_mode=FeedbackMode.DEFERRED</code> are specified to app constructor). In those cases the callables must be functions or methods that are importable (see the next section for details). The function/method performing the aggregation has the same requirements.</p>"},{"location":"trulens_eval/api/feedback/#trulens_eval.trulens_eval.feedback--import-requirement-deferred-feedback-mode-only","title":"Import requirement (DEFERRED feedback mode only)","text":"<p>If using deferred evaluation, the feedback function implementations and aggregation implementations must be functions or methods from a Provider subclass that is importable. That is, the callables must be accessible were you to evaluate this code:</p> <pre><code>from somepackage.[...] import someproviderclass\nfrom somepackage.[...] import somefunction\n\n# [...] means optionally further package specifications\n\nprovider = someproviderclass(...) # constructor arguments can be included\nfeedback_implementation1 = provider.somemethod\nfeedback_implementation2 = somefunction\n</code></pre> <p>For provided feedback functions, <code>somepackage</code> is <code>trulens_eval.feedback</code> and <code>someproviderclass</code> is <code>OpenAI</code> or one of the other <code>Provider</code> subclasses. Custom feedback functions likewise need to be importable functions or methods of a provider subclass that can be imported. Critically, functions or classes defined locally in a notebook will not be importable this way.</p>"},{"location":"trulens_eval/api/feedback/#trulens_eval.trulens_eval.feedback--specifying-arguments","title":"Specifying Arguments","text":"<p>The mapping between app/records to feedback implementation arguments is specified by the <code>on...</code> methods of the <code>Feedback</code> objects. The general form is:</p> <pre><code>feedback: Feedback = feedback.on(argname1=selector1, argname2=selector2, ...)\n</code></pre> <p>That is, <code>Feedback.on(...)</code> returns a new <code>Feedback</code> object with additional argument mappings, the source of <code>argname1</code> is <code>selector1</code> and so on for further argument names. The types of <code>selector1</code> is <code>JSONPath</code> which we elaborate on in the \"Selector Details\".</p> <p>If argument names are ommitted, they are taken from the feedback function implementation signature in order. That is, </p> <pre><code>Feedback(...).on(argname1=selector1, argname2=selector2)\n</code></pre> <p>and</p> <pre><code>Feedback(...).on(selector1, selector2)\n</code></pre> <p>are equivalent assuming the feedback implementation has two arguments, <code>argname1</code> and <code>argname2</code>, in that order.</p>"},{"location":"trulens_eval/api/feedback/#trulens_eval.trulens_eval.feedback--running-feedback","title":"Running Feedback","text":"<p>Feedback implementations are simple callables that can be run on any arguments matching their signatures. However, once wrapped with <code>Feedback</code>, they are meant to be run on outputs of app evaluation (the \"Records\"). Specifically, <code>Feedback.run</code> has this definition:</p> <pre><code>def run(self, \n    app: Union[AppDefinition, JSON], \n    record: Record\n) -&gt; FeedbackResult:\n</code></pre> <p>That is, the context of a Feedback evaluation is an app (either as <code>AppDefinition</code> or a JSON-like object) and a <code>Record</code> of the execution of the aforementioned app. Both objects are indexable using \"Selectors\". By indexable here we mean that their internal components can be specified by a Selector and subsequently that internal component can be extracted using that selector. Selectors for Feedback start by specifying whether they are indexing into an App or a Record via the <code>__app__</code> and <code>__record__</code> special attributes (see Selectors section below).</p>"},{"location":"trulens_eval/api/feedback/#trulens_eval.trulens_eval.feedback--selector-details","title":"Selector Details","text":"<p>Selectors are of type <code>JSONPath</code> defined in <code>util.py</code> but are also aliased in <code>schema.py</code> as <code>Select.Query</code>. Objects of this type specify paths into JSON-like structures (enumerating <code>Record</code> or <code>App</code> contents). </p> <p>By JSON-like structures we mean python objects that can be converted into JSON or are base types. This includes:</p> <ul> <li> <p>base types: strings, integers, dates, etc.</p> </li> <li> <p>sequences</p> </li> <li> <p>dictionaries with string keys</p> </li> </ul> <p>Additionally, JSONPath also index into general python objects like <code>AppDefinition</code> or <code>Record</code> though each of these can be converted to JSON-like.</p> <p>When used to index json-like objects, JSONPath are used as generators: the path can be used to iterate over items from within the object:</p> <pre><code>class JSONPath...\n    ...\n    def __call__(self, obj: Any) -&gt; Iterable[Any]:\n    ...\n</code></pre> <p>In most cases, the generator produces only a single item but paths can also address multiple items (as opposed to a single item containing multiple).</p> <p>The syntax of this specification mirrors the syntax one would use with instantiations of JSON-like objects. For every <code>obj</code> generated by <code>query: JSONPath</code>:</p> <ul> <li> <p><code>query[somekey]</code> generates the <code>somekey</code> element of <code>obj</code> assuming it is a     dictionary with key <code>somekey</code>.</p> </li> <li> <p><code>query[someindex]</code> generates the index <code>someindex</code> of <code>obj</code> assuming it is     a sequence.</p> </li> <li> <p><code>query[slice]</code> generates the multiple elements of <code>obj</code> assuming it is a     sequence. Slices include <code>:</code> or in general <code>startindex:endindex:step</code>.</p> </li> <li> <p><code>query[somekey1, somekey2, ...]</code> generates multiple elements of <code>obj</code>     assuming <code>obj</code> is a dictionary and <code>somekey1</code>... are its keys.</p> </li> <li> <p><code>query[someindex1, someindex2, ...]</code> generates multiple elements     indexed by <code>someindex1</code>... from a sequence <code>obj</code>.</p> </li> <li> <p><code>query.someattr</code> depends on type of <code>obj</code>. If <code>obj</code> is a dictionary, then     <code>query.someattr</code> is an alias for <code>query[someattr]</code>. Otherwise if     <code>someattr</code> is an attribute of a python object <code>obj</code>, then <code>query.someattr</code>     generates the named attribute.</p> </li> </ul> <p>For feedback argument specification, the selectors should start with either <code>__record__</code> or <code>__app__</code> indicating which of the two JSON-like structures to select from (Records or Apps). <code>Select.Record</code> and <code>Select.App</code> are defined as <code>Query().__record__</code> and <code>Query().__app__</code> and thus can stand in for the start of a selector specification that wishes to select from a Record or App, respectively. The full set of Query aliases are as follows:</p> <ul> <li> <p><code>Record = Query().__record__</code> -- points to the Record.</p> </li> <li> <p>App = Query().app -- points to the App.</p> </li> <li> <p><code>RecordInput = Record.main_input</code> -- points to the main input part of a     Record. This is the first argument to the root method of an app (for     langchain Chains this is the <code>__call__</code> method).</p> </li> <li> <p><code>RecordOutput = Record.main_output</code> -- points to the main output part of a     Record. This is the output of the root method of an app (i.e. <code>__call__</code>     for langchain Chains).</p> </li> <li> <p><code>RecordCalls = Record.app</code> -- points to the root of the app-structured     mirror of calls in a record. See App-organized Calls Section above.</p> </li> </ul>"},{"location":"trulens_eval/api/feedback/#trulens_eval.trulens_eval.feedback--multiple-inputs-per-argument","title":"Multiple Inputs Per Argument","text":"<p>As in the <code>f_qs_relevance</code> example, a selector for a single argument may point to more than one aspect of a record/app. These are specified using the slice or lists in key/index poisitions. In that case, the feedback function is evaluated multiple times, its outputs collected, and finally aggregated into a main feedback result.</p> <p>The collection of values for each argument of feedback implementation is collected and every combination of argument-to-value mapping is evaluated with a feedback definition. This may produce a large number of evaluations if more than one argument names multiple values. In the dashboard, all individual invocations of a feedback implementation are shown alongside the final aggregate result.</p>"},{"location":"trulens_eval/api/feedback/#trulens_eval.trulens_eval.feedback--apprecord-organization-what-can-be-selected","title":"App/Record Organization (What can be selected)","text":"<p>Apps are serialized into JSON-like structures which are indexed via selectors. The exact makeup of this structure is app-dependent though always start with <code>app</code>, that is, the trulens wrappers (subtypes of <code>App</code>) contain the wrapped app in the attribute <code>app</code>:</p> <pre><code># app.py:\nclass App(AppDefinition, SerialModel):\n    ...\n    # The wrapped app.\n    app: Any = Field(exclude=True)\n    ...\n</code></pre> <p>For your app, you can inspect the JSON-like structure by using the <code>dict</code> method:</p> <pre><code>tru = ... # your app, extending App\nprint(tru.dict())\n</code></pre> <p>The other non-excluded fields accessible outside of the wrapped app are listed in the <code>AppDefinition</code> class in <code>schema.py</code>:</p> <pre><code>class AppDefinition(SerialModel, WithClassInfo, ABC):\n    ...\n\n    app_id: AppID\n\n    feedback_definitions: Sequence[FeedbackDefinition] = []\n\n    feedback_mode: FeedbackMode = FeedbackMode.WITH_APP_THREAD\n\n    root_class: Class\n\n    root_callable: ClassVar[FunctionOrMethod]\n\n    app: JSON\n</code></pre> <p>Note that <code>app</code> is in both classes. This distinction between <code>App</code> and <code>AppDefinition</code> here is that one corresponds to potentially non-serializable python objects (<code>App</code>) and their serializable versions (<code>AppDefinition</code>). Feedbacks should expect to be run with <code>AppDefinition</code>. Fields of <code>App</code> that are not part of <code>AppDefinition</code> may not be available.</p> <p>You can inspect the data available for feedback definitions in the dashboard by clicking on the \"See full app json\" button on the bottom of the page after selecting a record from a table.</p> <p>The other piece of context to Feedback evaluation are records. These contain the inputs/outputs and other information collected during the execution of an app:</p> <pre><code>class Record(SerialModel):\n    record_id: RecordID\n    app_id: AppID\n\n    cost: Optional[Cost] = None\n    perf: Optional[Perf] = None\n\n    ts: datetime = pydantic.Field(default_factory=lambda: datetime.now())\n\n    tags: str = \"\"\n\n    main_input: Optional[JSON] = None\n    main_output: Optional[JSON] = None  # if no error\n    main_error: Optional[JSON] = None  # if error\n\n    # The collection of calls recorded. Note that these can be converted into a\n    # json structure with the same paths as the app that generated this record\n    # via `layout_calls_as_app`.\n    calls: Sequence[RecordAppCall] = []\n</code></pre> <p>A listing of a record can be seen in the dashboard by clicking the \"see full record json\" button on the bottom of the page after selecting a record from the table.</p>"},{"location":"trulens_eval/api/feedback/#trulens_eval.trulens_eval.feedback--calls-made-by-app-components","title":"Calls made by App Components","text":"<p>When evaluating a feedback function, Records are augmented with app/component calls in app layout in the attribute <code>app</code>. By this we mean that in addition to the fields listed in the class definition above, the <code>app</code> field will contain the same information as <code>calls</code> but organized in a manner mirroring the organization of the app structure. For example, if the instrumented app contains a component <code>combine_docs_chain</code> then <code>app.combine_docs_chain</code> will contain calls to methods of this component. In the example at the top of this docstring, <code>_call</code> was an example of such a method. Thus <code>app.combine_docs_chain._call</code> further contains a <code>RecordAppCall</code> (see schema.py) structure with information about the inputs/outputs/metadata regarding the <code>_call</code> call to that component. Selecting this information is the reason behind the <code>Select.RecordCalls</code> alias (see next section).</p> <p>You can inspect the components making up your app via the <code>App</code> method <code>print_instrumented</code>.</p>"},{"location":"trulens_eval/api/feedback/#trulens_eval.trulens_eval.feedback.AzureOpenAI","title":"<code>AzureOpenAI</code>","text":"<p>         Bases: <code>OpenAI</code></p> Source code in <code>trulens_eval/trulens_eval/feedback.py</code> <pre><code>class AzureOpenAI(OpenAI):\n    deployment_id: str\n\n    def __init__(self, endpoint=None, **kwargs):\n        # NOTE(piotrm): pydantic adds endpoint to the signature of this\n        # constructor if we don't include it explicitly, even though we set it\n        # down below. Adding it as None here as a temporary hack.\n\"\"\"\n        Wrapper to use Azure OpenAI. Please export the following env variables\n\n        - OPENAI_API_BASE\n        - OPENAI_API_VERSION\n        - OPENAI_API_KEY\n\n        Parameters:\n\n        - model_engine (str, optional): The specific model version. Defaults to\n          \"gpt-35-turbo\".\n        - deployment_id (str): The specified deployment id\n        \"\"\"\n\n        super().__init__(\n            **kwargs\n        )  # need to include pydantic.BaseModel.__init__\n\n        set_openai_key()\n        openai.api_type = \"azure\"\n        openai.api_base = os.getenv(\"OPENAI_API_BASE\")\n        openai.api_version = os.getenv(\"OPENAI_API_VERSION\")\n\n    def _create_chat_completion(self, *args, **kwargs):\n\"\"\"\n        We need to pass `engine`\n        \"\"\"\n        return super()._create_chat_completion(\n            *args, deployment_id=self.deployment_id, **kwargs\n        )\n</code></pre>"},{"location":"trulens_eval/api/feedback/#trulens_eval.trulens_eval.feedback.AzureOpenAI.__init__","title":"<code>__init__(endpoint=None, **kwargs)</code>","text":"<p>Wrapper to use Azure OpenAI. Please export the following env variables</p> <ul> <li>OPENAI_API_BASE</li> <li>OPENAI_API_VERSION</li> <li>OPENAI_API_KEY</li> </ul> <ul> <li>model_engine (str, optional): The specific model version. Defaults to   \"gpt-35-turbo\".</li> <li>deployment_id (str): The specified deployment id</li> </ul> Source code in <code>trulens_eval/trulens_eval/feedback.py</code> <pre><code>def __init__(self, endpoint=None, **kwargs):\n    # NOTE(piotrm): pydantic adds endpoint to the signature of this\n    # constructor if we don't include it explicitly, even though we set it\n    # down below. Adding it as None here as a temporary hack.\n\"\"\"\n    Wrapper to use Azure OpenAI. Please export the following env variables\n\n    - OPENAI_API_BASE\n    - OPENAI_API_VERSION\n    - OPENAI_API_KEY\n\n    Parameters:\n\n    - model_engine (str, optional): The specific model version. Defaults to\n      \"gpt-35-turbo\".\n    - deployment_id (str): The specified deployment id\n    \"\"\"\n\n    super().__init__(\n        **kwargs\n    )  # need to include pydantic.BaseModel.__init__\n\n    set_openai_key()\n    openai.api_type = \"azure\"\n    openai.api_base = os.getenv(\"OPENAI_API_BASE\")\n    openai.api_version = os.getenv(\"OPENAI_API_VERSION\")\n</code></pre>"},{"location":"trulens_eval/api/feedback/#trulens_eval.trulens_eval.feedback.Feedback","title":"<code>Feedback</code>","text":"<p>         Bases: <code>FeedbackDefinition</code></p> Source code in <code>trulens_eval/trulens_eval/feedback.py</code> <pre><code>class Feedback(FeedbackDefinition):\n    # Implementation, not serializable, note that FeedbackDefinition contains\n    # `implementation` meant to serialize the below.\n    imp: Optional[ImpCallable] = pydantic.Field(exclude=True)\n\n    # Aggregator method for feedback functions that produce more than one\n    # result.\n    agg: Optional[AggCallable] = pydantic.Field(exclude=True)\n\n    def __init__(\n        self,\n        imp: Optional[Callable] = None,\n        agg: Optional[Callable] = None,\n        **kwargs\n    ):\n\"\"\"\n        A Feedback function container.\n\n        Parameters:\n\n        - imp: Optional[Callable] -- implementation of the feedback function.\n\n        - agg: Optional[Callable] -- aggregation function for producing a single\n          float for feedback implementations that are run more than once.\n        \"\"\"\n\n        agg = agg or np.mean\n\n        # imp is the python function/method while implementation is a serialized\n        # json structure. Create the one that is missing based on the one that\n        # is provided:\n\n        if imp is not None:\n            # These are for serialization to/from json and for db storage.\n            if 'implementation' not in kwargs:\n                try:\n                    kwargs['implementation'] = FunctionOrMethod.of_callable(\n                        imp, loadable=True\n                    )\n                except ImportError as e:\n                    logger.warning(\n                        f\"Feedback implementation {imp} cannot be serialized: {e}. \"\n                        f\"This may be ok unless you are using the deferred feedback mode.\"\n                    )\n\n                    kwargs['implementation'] = FunctionOrMethod.of_callable(\n                        imp, loadable=False\n                    )\n\n        else:\n            if \"implementation\" in kwargs:\n                imp: ImpCallable = FunctionOrMethod.pick(\n                    **(kwargs['implementation'])\n                ).load() if kwargs['implementation'] is not None else None\n\n        # Similarly with agg and aggregator.\n        if agg is not None:\n            if 'aggregator' not in kwargs:\n                try:\n                    # These are for serialization to/from json and for db storage.\n                    kwargs['aggregator'] = FunctionOrMethod.of_callable(\n                        agg, loadable=True\n                    )\n                except:\n                    # User defined functions in script do not have a module so cannot be serialized\n                    pass\n        else:\n            if 'aggregator' in kwargs:\n                agg: AggCallable = FunctionOrMethod.pick(\n                    **(kwargs['aggregator'])\n                ).load()\n\n        super().__init__(**kwargs)\n\n        self.imp = imp\n        self.agg = agg\n\n        # Verify that `imp` expects the arguments specified in `selectors`:\n        if self.imp is not None:\n            sig: Signature = signature(self.imp)\n            for argname in self.selectors.keys():\n                assert argname in sig.parameters, (\n                    f\"{argname} is not an argument to {self.imp.__name__}. \"\n                    f\"Its arguments are {list(sig.parameters.keys())}.\"\n                )\n\n    def on_input_output(self):\n\"\"\"\n        Specifies that the feedback implementation arguments are to be the main\n        app input and output in that order.\n\n        Returns a new Feedback object with the specification.\n        \"\"\"\n        return self.on_input().on_output()\n\n    def on_default(self):\n\"\"\"\n        Specifies that one argument feedbacks should be evaluated on the main\n        app output and two argument feedbacks should be evaluates on main input\n        and main output in that order.\n\n        Returns a new Feedback object with this specification.\n        \"\"\"\n\n        ret = Feedback().parse_obj(self)\n        ret._default_selectors()\n        return ret\n\n    def _print_guessed_selector(self, par_name, par_path):\n        if par_path == Select.RecordCalls:\n            alias_info = f\" or `Select.RecordCalls`\"\n        elif par_path == Select.RecordInput:\n            alias_info = f\" or `Select.RecordInput`\"\n        elif par_path == Select.RecordOutput:\n            alias_info = f\" or `Select.RecordOutput`\"\n        else:\n            alias_info = \"\"\n\n        print(\n            f\"{UNICODE_CHECK} In {self.name}, \"\n            f\"input {par_name} will be set to {par_path}{alias_info} .\"\n        )\n\n    def _default_selectors(self):\n\"\"\"\n        Fill in default selectors for any remaining feedback function arguments.\n        \"\"\"\n\n        assert self.imp is not None, \"Feedback function implementation is required to determine default argument names.\"\n\n        sig: Signature = signature(self.imp)\n        par_names = list(\n            k for k in sig.parameters.keys() if k not in self.selectors\n        )\n\n        if len(par_names) == 1:\n            # A single argument remaining. Assume it is record output.\n            selectors = {par_names[0]: Select.RecordOutput}\n            self._print_guessed_selector(par_names[0], Select.RecordOutput)\n\n            # TODO: replace with on_output ?\n\n        elif len(par_names) == 2:\n            # Two arguments remaining. Assume they are record input and output\n            # respectively.\n            selectors = {\n                par_names[0]: Select.RecordInput,\n                par_names[1]: Select.RecordOutput\n            }\n            self._print_guessed_selector(par_names[0], Select.RecordInput)\n            self._print_guessed_selector(par_names[1], Select.RecordOutput)\n\n            # TODO: replace on_input_output ?\n        else:\n            # Otherwise give up.\n\n            raise RuntimeError(\n                f\"Cannot determine default paths for feedback function arguments. \"\n                f\"The feedback function has signature {sig}.\"\n            )\n\n        self.selectors = selectors\n\n    @staticmethod\n    def evaluate_deferred(tru: 'Tru') -&gt; int:\n\"\"\"\n        Evaluates feedback functions that were specified to be deferred. Returns\n        an integer indicating how many evaluates were run.\n        \"\"\"\n\n        db = tru.db\n\n        def prepare_feedback(row):\n            record_json = row.record_json\n            record = Record(**record_json)\n\n            app_json = row.app_json\n\n            feedback = Feedback(**row.feedback_json)\n            feedback.run_and_log(\n                record=record,\n                app=app_json,\n                tru=tru,\n                feedback_result_id=row.feedback_result_id\n            )\n\n        feedbacks = db.get_feedback()\n\n        started_count = 0\n\n        for i, row in feedbacks.iterrows():\n            feedback_ident = f\"{row.fname} for app {row.app_json['app_id']}, record {row.record_id}\"\n\n            if row.status == FeedbackResultStatus.NONE:\n\n                print(\n                    f\"{UNICODE_YIELD} Feedback task starting: {feedback_ident}\"\n                )\n\n                TP().runlater(prepare_feedback, row)\n                started_count += 1\n\n            elif row.status in [FeedbackResultStatus.RUNNING]:\n                now = datetime.now().timestamp()\n                if now - row.last_ts &gt; 30:\n                    print(\n                        f\"{UNICODE_YIELD} Feedback task last made progress over 30 seconds ago. \"\n                        f\"Retrying: {feedback_ident}\"\n                    )\n                    TP().runlater(prepare_feedback, row)\n                    started_count += 1\n\n                else:\n                    print(\n                        f\"{UNICODE_CLOCK} Feedback task last made progress less than 30 seconds ago. \"\n                        f\"Giving it more time: {feedback_ident}\"\n                    )\n\n            elif row.status in [FeedbackResultStatus.FAILED]:\n                now = datetime.now().timestamp()\n                if now - row.last_ts &gt; 60 * 5:\n                    print(\n                        f\"{UNICODE_YIELD} Feedback task last made progress over 5 minutes ago. \"\n                        f\"Retrying: {feedback_ident}\"\n                    )\n                    TP().runlater(prepare_feedback, row)\n                    started_count += 1\n\n                else:\n                    print(\n                        f\"{UNICODE_CLOCK} Feedback task last made progress less than 5 minutes ago. \"\n                        f\"Not touching it for now: {feedback_ident}\"\n                    )\n\n            elif row.status == FeedbackResultStatus.DONE:\n                pass\n\n        return started_count\n\n    def __call__(self, *args, **kwargs) -&gt; Any:\n        assert self.imp is not None, \"Feedback definition needs an implementation to call.\"\n        return self.imp(*args, **kwargs)\n\n    def aggregate(self, func: Callable) -&gt; 'Feedback':\n\"\"\"\n        Specify the aggregation function in case the selectors for this feedback\n        generate more than one value for implementation argument(s).\n\n        Returns a new Feedback object with the given aggregation function.\n        \"\"\"\n\n        return Feedback(imp=self.imp, selectors=self.selectors, agg=func)\n\n    @staticmethod\n    def of_feedback_definition(f: FeedbackDefinition):\n        implementation = f.implementation\n        aggregator = f.aggregator\n\n        imp_func = implementation.load()\n        agg_func = aggregator.load()\n\n        return Feedback(imp=imp_func, agg=agg_func, **f.dict())\n\n    def _next_unselected_arg_name(self):\n        if self.imp is not None:\n            sig = signature(self.imp)\n            par_names = list(\n                k for k in sig.parameters.keys() if k not in self.selectors\n            )\n            if \"self\" in par_names:\n                logger.warning(\n                    f\"Feedback function `{self.imp.__name__}` has `self` as argument. \"\n                    \"Perhaps it is static method or its Provider class was not initialized?\"\n                )\n            return par_names[0]\n        else:\n            raise RuntimeError(\n                \"Cannot determine name of feedback function parameter without its definition.\"\n            )\n\n    def on_prompt(self, arg: Optional[str] = None):\n\"\"\"\n        Create a variant of `self` that will take in the main app input or\n        \"prompt\" as input, sending it as an argument `arg` to implementation.\n        \"\"\"\n\n        new_selectors = self.selectors.copy()\n\n        if arg is None:\n            arg = self._next_unselected_arg_name()\n            self._print_guessed_selector(arg, Select.RecordInput)\n\n        new_selectors[arg] = Select.RecordInput\n\n        return Feedback(imp=self.imp, selectors=new_selectors, agg=self.agg)\n\n    on_input = on_prompt\n\n    def on_response(self, arg: Optional[str] = None):\n\"\"\"\n        Create a variant of `self` that will take in the main app output or\n        \"response\" as input, sending it as an argument `arg` to implementation.\n        \"\"\"\n\n        new_selectors = self.selectors.copy()\n\n        if arg is None:\n            arg = self._next_unselected_arg_name()\n            self._print_guessed_selector(arg, Select.RecordOutput)\n\n        new_selectors[arg] = Select.RecordOutput\n\n        return Feedback(imp=self.imp, selectors=new_selectors, agg=self.agg)\n\n    on_output = on_response\n\n    def on(self, *args, **kwargs):\n\"\"\"\n        Create a variant of `self` with the same implementation but the given\n        selectors. Those provided positionally get their implementation argument\n        name guessed and those provided as kwargs get their name from the kwargs\n        key.\n        \"\"\"\n\n        new_selectors = self.selectors.copy()\n        new_selectors.update(kwargs)\n\n        for path in args:\n            argname = self._next_unselected_arg_name()\n            new_selectors[argname] = path\n            self._print_guessed_selector(argname, path)\n\n        return Feedback(imp=self.imp, selectors=new_selectors, agg=self.agg)\n\n    def run(\n        self, app: Union[AppDefinition, JSON], record: Record\n    ) -&gt; FeedbackResult:\n\"\"\"\n        Run the feedback function on the given `record`. The `app` that\n        produced the record is also required to determine input/output argument\n        names.\n\n        Might not have a AppDefinitionhere but only the serialized app_json .\n        \"\"\"\n\n        if isinstance(app, AppDefinition):\n            app_json = jsonify(app)\n        else:\n            app_json = app\n\n        result_vals = []\n\n        feedback_calls = []\n\n        feedback_result = FeedbackResult(\n            feedback_definition_id=self.feedback_definition_id,\n            record_id=record.record_id,\n            name=self.name\n        )\n\n        try:\n            # Total cost, will accumulate.\n            cost = Cost()\n\n            for ins in self.extract_selection(app=app_json, record=record):\n\n                result_and_meta, part_cost = Endpoint.track_all_costs_tally(\n                    lambda: self.imp(**ins)\n                )\n                cost += part_cost\n\n                if isinstance(result_and_meta, Tuple):\n                    # If output is a tuple of two, we assume it is the float and the metadata.\n                    assert len(\n                        result_and_meta\n                    ) == 2, \"Feedback functions must return either a single float or a float and a dictionary.\"\n                    result_val, meta = result_and_meta\n\n                    assert isinstance(\n                        meta, dict\n                    ), f\"Feedback metadata output must be a dictionary but was {type(call_meta)}.\"\n                else:\n                    # Otherwise it is just the float. We create empty metadata dict.\n                    result_val = result_and_meta\n                    meta = dict()\n\n                if isinstance(result_val, dict):\n                    for val in result_val.values():\n                        assert isinstance(\n                            val, float\n                        ), f\"Feedback function output with multivalue must be a dict with float values but encountered {type(val)}.\"\n                    # TODO: Better handling of multi-output\n                    result_val = list(result_val.values())\n                    feedback_call = FeedbackCall(\n                        args=ins, ret=np.mean(result_val), meta=meta\n                    )\n\n                else:\n                    assert isinstance(\n                        result_val, float\n                    ), f\"Feedback function output must be a float or dict but was {type(result_val)}.\"\n                    feedback_call = FeedbackCall(\n                        args=ins, ret=result_val, meta=meta\n                    )\n\n                result_vals.append(result_val)\n                feedback_calls.append(feedback_call)\n\n            result_vals = np.array(result_vals)\n            if len(result_vals) == 0:\n                logger.warning(\n                    f\"Feedback function {self.name} with aggregation {self.agg} had no inputs.\"\n                )\n                result = np.nan\n            else:\n                result = self.agg(result_vals)\n\n            feedback_result.update(\n                result=result,\n                status=FeedbackResultStatus.DONE,\n                cost=cost,\n                calls=feedback_calls\n            )\n\n            return feedback_result\n\n        except:\n            exc_tb = traceback.format_exc()\n            logger.warning(f\"Feedback Function Exception Caught: {exc_tb}\")\n            feedback_result.update(\n                error=exc_tb, status=FeedbackResultStatus.FAILED\n            )\n            return feedback_result\n\n    def run_and_log(\n        self,\n        record: Record,\n        tru: 'Tru',\n        app: Union[AppDefinition, JSON] = None,\n        feedback_result_id: Optional[FeedbackResultID] = None\n    ) -&gt; FeedbackResult:\n        record_id = record.record_id\n        app_id = record.app_id\n\n        db = tru.db\n\n        # Placeholder result to indicate a run.\n        feedback_result = FeedbackResult(\n            feedback_definition_id=self.feedback_definition_id,\n            feedback_result_id=feedback_result_id,\n            record_id=record_id,\n            name=self.name\n        )\n\n        if feedback_result_id is None:\n            feedback_result_id = feedback_result.feedback_result_id\n\n        try:\n            db.insert_feedback(\n                feedback_result.update(\n                    status=FeedbackResultStatus.RUNNING  # in progress\n                )\n            )\n\n            feedback_result = self.run(\n                app=app, record=record\n            ).update(feedback_result_id=feedback_result_id)\n\n        except Exception as e:\n            exc_tb = traceback.format_exc()\n            db.insert_feedback(\n                feedback_result.update(\n                    error=exc_tb, status=FeedbackResultStatus.FAILED\n                )\n            )\n            return\n\n        # Otherwise update based on what Feedback.run produced (could be success or failure).\n        db.insert_feedback(feedback_result)\n\n        return feedback_result\n\n    @property\n    def name(self):\n\"\"\"\n        Name of the feedback function. Presently derived from the name of the\n        function implementing it.\n        \"\"\"\n\n        if self.imp is None:\n            raise RuntimeError(\"This feedback function has no implementation.\")\n\n        return self.imp.__name__\n\n    def extract_selection(\n        self, app: Union[AppDefinition, JSON], record: Record\n    ) -&gt; Iterable[Dict[str, Any]]:\n\"\"\"\n        Given the `app` that produced the given `record`, extract from\n        `record` the values that will be sent as arguments to the implementation\n        as specified by `self.selectors`.\n        \"\"\"\n\n        arg_vals = {}\n\n        for k, v in self.selectors.items():\n            if isinstance(v, Select.Query):\n                q = v\n\n            else:\n                raise RuntimeError(f\"Unhandled selection type {type(v)}.\")\n\n            if q.path[0] == Select.Record.path[0]:\n                o = record.layout_calls_as_app()\n            elif q.path[0] == Select.App.path[0]:\n                o = app\n            else:\n                raise ValueError(\n                    f\"Query {q} does not indicate whether it is about a record or about a app.\"\n                )\n\n            q_within_o = Select.Query(path=q.path[1:])\n            arg_vals[k] = list(q_within_o(o))\n\n        keys = arg_vals.keys()\n        vals = arg_vals.values()\n\n        assignments = itertools.product(*vals)\n\n        for assignment in assignments:\n            yield {k: v for k, v in zip(keys, assignment)}\n</code></pre>"},{"location":"trulens_eval/api/feedback/#trulens_eval.trulens_eval.feedback.Feedback.name","title":"<code>name</code>  <code>property</code>","text":"<p>Name of the feedback function. Presently derived from the name of the function implementing it.</p>"},{"location":"trulens_eval/api/feedback/#trulens_eval.trulens_eval.feedback.Feedback.__init__","title":"<code>__init__(imp=None, agg=None, **kwargs)</code>","text":"<p>A Feedback function container.</p> <ul> <li> <p>imp: Optional[Callable] -- implementation of the feedback function.</p> </li> <li> <p>agg: Optional[Callable] -- aggregation function for producing a single   float for feedback implementations that are run more than once.</p> </li> </ul> Source code in <code>trulens_eval/trulens_eval/feedback.py</code> <pre><code>def __init__(\n    self,\n    imp: Optional[Callable] = None,\n    agg: Optional[Callable] = None,\n    **kwargs\n):\n\"\"\"\n    A Feedback function container.\n\n    Parameters:\n\n    - imp: Optional[Callable] -- implementation of the feedback function.\n\n    - agg: Optional[Callable] -- aggregation function for producing a single\n      float for feedback implementations that are run more than once.\n    \"\"\"\n\n    agg = agg or np.mean\n\n    # imp is the python function/method while implementation is a serialized\n    # json structure. Create the one that is missing based on the one that\n    # is provided:\n\n    if imp is not None:\n        # These are for serialization to/from json and for db storage.\n        if 'implementation' not in kwargs:\n            try:\n                kwargs['implementation'] = FunctionOrMethod.of_callable(\n                    imp, loadable=True\n                )\n            except ImportError as e:\n                logger.warning(\n                    f\"Feedback implementation {imp} cannot be serialized: {e}. \"\n                    f\"This may be ok unless you are using the deferred feedback mode.\"\n                )\n\n                kwargs['implementation'] = FunctionOrMethod.of_callable(\n                    imp, loadable=False\n                )\n\n    else:\n        if \"implementation\" in kwargs:\n            imp: ImpCallable = FunctionOrMethod.pick(\n                **(kwargs['implementation'])\n            ).load() if kwargs['implementation'] is not None else None\n\n    # Similarly with agg and aggregator.\n    if agg is not None:\n        if 'aggregator' not in kwargs:\n            try:\n                # These are for serialization to/from json and for db storage.\n                kwargs['aggregator'] = FunctionOrMethod.of_callable(\n                    agg, loadable=True\n                )\n            except:\n                # User defined functions in script do not have a module so cannot be serialized\n                pass\n    else:\n        if 'aggregator' in kwargs:\n            agg: AggCallable = FunctionOrMethod.pick(\n                **(kwargs['aggregator'])\n            ).load()\n\n    super().__init__(**kwargs)\n\n    self.imp = imp\n    self.agg = agg\n\n    # Verify that `imp` expects the arguments specified in `selectors`:\n    if self.imp is not None:\n        sig: Signature = signature(self.imp)\n        for argname in self.selectors.keys():\n            assert argname in sig.parameters, (\n                f\"{argname} is not an argument to {self.imp.__name__}. \"\n                f\"Its arguments are {list(sig.parameters.keys())}.\"\n            )\n</code></pre>"},{"location":"trulens_eval/api/feedback/#trulens_eval.trulens_eval.feedback.Feedback.aggregate","title":"<code>aggregate(func)</code>","text":"<p>Specify the aggregation function in case the selectors for this feedback generate more than one value for implementation argument(s).</p> <p>Returns a new Feedback object with the given aggregation function.</p> Source code in <code>trulens_eval/trulens_eval/feedback.py</code> <pre><code>def aggregate(self, func: Callable) -&gt; 'Feedback':\n\"\"\"\n    Specify the aggregation function in case the selectors for this feedback\n    generate more than one value for implementation argument(s).\n\n    Returns a new Feedback object with the given aggregation function.\n    \"\"\"\n\n    return Feedback(imp=self.imp, selectors=self.selectors, agg=func)\n</code></pre>"},{"location":"trulens_eval/api/feedback/#trulens_eval.trulens_eval.feedback.Feedback.evaluate_deferred","title":"<code>evaluate_deferred(tru)</code>  <code>staticmethod</code>","text":"<p>Evaluates feedback functions that were specified to be deferred. Returns an integer indicating how many evaluates were run.</p> Source code in <code>trulens_eval/trulens_eval/feedback.py</code> <pre><code>@staticmethod\ndef evaluate_deferred(tru: 'Tru') -&gt; int:\n\"\"\"\n    Evaluates feedback functions that were specified to be deferred. Returns\n    an integer indicating how many evaluates were run.\n    \"\"\"\n\n    db = tru.db\n\n    def prepare_feedback(row):\n        record_json = row.record_json\n        record = Record(**record_json)\n\n        app_json = row.app_json\n\n        feedback = Feedback(**row.feedback_json)\n        feedback.run_and_log(\n            record=record,\n            app=app_json,\n            tru=tru,\n            feedback_result_id=row.feedback_result_id\n        )\n\n    feedbacks = db.get_feedback()\n\n    started_count = 0\n\n    for i, row in feedbacks.iterrows():\n        feedback_ident = f\"{row.fname} for app {row.app_json['app_id']}, record {row.record_id}\"\n\n        if row.status == FeedbackResultStatus.NONE:\n\n            print(\n                f\"{UNICODE_YIELD} Feedback task starting: {feedback_ident}\"\n            )\n\n            TP().runlater(prepare_feedback, row)\n            started_count += 1\n\n        elif row.status in [FeedbackResultStatus.RUNNING]:\n            now = datetime.now().timestamp()\n            if now - row.last_ts &gt; 30:\n                print(\n                    f\"{UNICODE_YIELD} Feedback task last made progress over 30 seconds ago. \"\n                    f\"Retrying: {feedback_ident}\"\n                )\n                TP().runlater(prepare_feedback, row)\n                started_count += 1\n\n            else:\n                print(\n                    f\"{UNICODE_CLOCK} Feedback task last made progress less than 30 seconds ago. \"\n                    f\"Giving it more time: {feedback_ident}\"\n                )\n\n        elif row.status in [FeedbackResultStatus.FAILED]:\n            now = datetime.now().timestamp()\n            if now - row.last_ts &gt; 60 * 5:\n                print(\n                    f\"{UNICODE_YIELD} Feedback task last made progress over 5 minutes ago. \"\n                    f\"Retrying: {feedback_ident}\"\n                )\n                TP().runlater(prepare_feedback, row)\n                started_count += 1\n\n            else:\n                print(\n                    f\"{UNICODE_CLOCK} Feedback task last made progress less than 5 minutes ago. \"\n                    f\"Not touching it for now: {feedback_ident}\"\n                )\n\n        elif row.status == FeedbackResultStatus.DONE:\n            pass\n\n    return started_count\n</code></pre>"},{"location":"trulens_eval/api/feedback/#trulens_eval.trulens_eval.feedback.Feedback.extract_selection","title":"<code>extract_selection(app, record)</code>","text":"<p>Given the <code>app</code> that produced the given <code>record</code>, extract from <code>record</code> the values that will be sent as arguments to the implementation as specified by <code>self.selectors</code>.</p> Source code in <code>trulens_eval/trulens_eval/feedback.py</code> <pre><code>def extract_selection(\n    self, app: Union[AppDefinition, JSON], record: Record\n) -&gt; Iterable[Dict[str, Any]]:\n\"\"\"\n    Given the `app` that produced the given `record`, extract from\n    `record` the values that will be sent as arguments to the implementation\n    as specified by `self.selectors`.\n    \"\"\"\n\n    arg_vals = {}\n\n    for k, v in self.selectors.items():\n        if isinstance(v, Select.Query):\n            q = v\n\n        else:\n            raise RuntimeError(f\"Unhandled selection type {type(v)}.\")\n\n        if q.path[0] == Select.Record.path[0]:\n            o = record.layout_calls_as_app()\n        elif q.path[0] == Select.App.path[0]:\n            o = app\n        else:\n            raise ValueError(\n                f\"Query {q} does not indicate whether it is about a record or about a app.\"\n            )\n\n        q_within_o = Select.Query(path=q.path[1:])\n        arg_vals[k] = list(q_within_o(o))\n\n    keys = arg_vals.keys()\n    vals = arg_vals.values()\n\n    assignments = itertools.product(*vals)\n\n    for assignment in assignments:\n        yield {k: v for k, v in zip(keys, assignment)}\n</code></pre>"},{"location":"trulens_eval/api/feedback/#trulens_eval.trulens_eval.feedback.Feedback.on","title":"<code>on(*args, **kwargs)</code>","text":"<p>Create a variant of <code>self</code> with the same implementation but the given selectors. Those provided positionally get their implementation argument name guessed and those provided as kwargs get their name from the kwargs key.</p> Source code in <code>trulens_eval/trulens_eval/feedback.py</code> <pre><code>def on(self, *args, **kwargs):\n\"\"\"\n    Create a variant of `self` with the same implementation but the given\n    selectors. Those provided positionally get their implementation argument\n    name guessed and those provided as kwargs get their name from the kwargs\n    key.\n    \"\"\"\n\n    new_selectors = self.selectors.copy()\n    new_selectors.update(kwargs)\n\n    for path in args:\n        argname = self._next_unselected_arg_name()\n        new_selectors[argname] = path\n        self._print_guessed_selector(argname, path)\n\n    return Feedback(imp=self.imp, selectors=new_selectors, agg=self.agg)\n</code></pre>"},{"location":"trulens_eval/api/feedback/#trulens_eval.trulens_eval.feedback.Feedback.on_default","title":"<code>on_default()</code>","text":"<p>Specifies that one argument feedbacks should be evaluated on the main app output and two argument feedbacks should be evaluates on main input and main output in that order.</p> <p>Returns a new Feedback object with this specification.</p> Source code in <code>trulens_eval/trulens_eval/feedback.py</code> <pre><code>def on_default(self):\n\"\"\"\n    Specifies that one argument feedbacks should be evaluated on the main\n    app output and two argument feedbacks should be evaluates on main input\n    and main output in that order.\n\n    Returns a new Feedback object with this specification.\n    \"\"\"\n\n    ret = Feedback().parse_obj(self)\n    ret._default_selectors()\n    return ret\n</code></pre>"},{"location":"trulens_eval/api/feedback/#trulens_eval.trulens_eval.feedback.Feedback.on_input_output","title":"<code>on_input_output()</code>","text":"<p>Specifies that the feedback implementation arguments are to be the main app input and output in that order.</p> <p>Returns a new Feedback object with the specification.</p> Source code in <code>trulens_eval/trulens_eval/feedback.py</code> <pre><code>def on_input_output(self):\n\"\"\"\n    Specifies that the feedback implementation arguments are to be the main\n    app input and output in that order.\n\n    Returns a new Feedback object with the specification.\n    \"\"\"\n    return self.on_input().on_output()\n</code></pre>"},{"location":"trulens_eval/api/feedback/#trulens_eval.trulens_eval.feedback.Feedback.on_prompt","title":"<code>on_prompt(arg=None)</code>","text":"<p>Create a variant of <code>self</code> that will take in the main app input or \"prompt\" as input, sending it as an argument <code>arg</code> to implementation.</p> Source code in <code>trulens_eval/trulens_eval/feedback.py</code> <pre><code>def on_prompt(self, arg: Optional[str] = None):\n\"\"\"\n    Create a variant of `self` that will take in the main app input or\n    \"prompt\" as input, sending it as an argument `arg` to implementation.\n    \"\"\"\n\n    new_selectors = self.selectors.copy()\n\n    if arg is None:\n        arg = self._next_unselected_arg_name()\n        self._print_guessed_selector(arg, Select.RecordInput)\n\n    new_selectors[arg] = Select.RecordInput\n\n    return Feedback(imp=self.imp, selectors=new_selectors, agg=self.agg)\n</code></pre>"},{"location":"trulens_eval/api/feedback/#trulens_eval.trulens_eval.feedback.Feedback.on_response","title":"<code>on_response(arg=None)</code>","text":"<p>Create a variant of <code>self</code> that will take in the main app output or \"response\" as input, sending it as an argument <code>arg</code> to implementation.</p> Source code in <code>trulens_eval/trulens_eval/feedback.py</code> <pre><code>def on_response(self, arg: Optional[str] = None):\n\"\"\"\n    Create a variant of `self` that will take in the main app output or\n    \"response\" as input, sending it as an argument `arg` to implementation.\n    \"\"\"\n\n    new_selectors = self.selectors.copy()\n\n    if arg is None:\n        arg = self._next_unselected_arg_name()\n        self._print_guessed_selector(arg, Select.RecordOutput)\n\n    new_selectors[arg] = Select.RecordOutput\n\n    return Feedback(imp=self.imp, selectors=new_selectors, agg=self.agg)\n</code></pre>"},{"location":"trulens_eval/api/feedback/#trulens_eval.trulens_eval.feedback.Feedback.run","title":"<code>run(app, record)</code>","text":"<p>Run the feedback function on the given <code>record</code>. The <code>app</code> that produced the record is also required to determine input/output argument names.</p> <p>Might not have a AppDefinitionhere but only the serialized app_json .</p> Source code in <code>trulens_eval/trulens_eval/feedback.py</code> <pre><code>def run(\n    self, app: Union[AppDefinition, JSON], record: Record\n) -&gt; FeedbackResult:\n\"\"\"\n    Run the feedback function on the given `record`. The `app` that\n    produced the record is also required to determine input/output argument\n    names.\n\n    Might not have a AppDefinitionhere but only the serialized app_json .\n    \"\"\"\n\n    if isinstance(app, AppDefinition):\n        app_json = jsonify(app)\n    else:\n        app_json = app\n\n    result_vals = []\n\n    feedback_calls = []\n\n    feedback_result = FeedbackResult(\n        feedback_definition_id=self.feedback_definition_id,\n        record_id=record.record_id,\n        name=self.name\n    )\n\n    try:\n        # Total cost, will accumulate.\n        cost = Cost()\n\n        for ins in self.extract_selection(app=app_json, record=record):\n\n            result_and_meta, part_cost = Endpoint.track_all_costs_tally(\n                lambda: self.imp(**ins)\n            )\n            cost += part_cost\n\n            if isinstance(result_and_meta, Tuple):\n                # If output is a tuple of two, we assume it is the float and the metadata.\n                assert len(\n                    result_and_meta\n                ) == 2, \"Feedback functions must return either a single float or a float and a dictionary.\"\n                result_val, meta = result_and_meta\n\n                assert isinstance(\n                    meta, dict\n                ), f\"Feedback metadata output must be a dictionary but was {type(call_meta)}.\"\n            else:\n                # Otherwise it is just the float. We create empty metadata dict.\n                result_val = result_and_meta\n                meta = dict()\n\n            if isinstance(result_val, dict):\n                for val in result_val.values():\n                    assert isinstance(\n                        val, float\n                    ), f\"Feedback function output with multivalue must be a dict with float values but encountered {type(val)}.\"\n                # TODO: Better handling of multi-output\n                result_val = list(result_val.values())\n                feedback_call = FeedbackCall(\n                    args=ins, ret=np.mean(result_val), meta=meta\n                )\n\n            else:\n                assert isinstance(\n                    result_val, float\n                ), f\"Feedback function output must be a float or dict but was {type(result_val)}.\"\n                feedback_call = FeedbackCall(\n                    args=ins, ret=result_val, meta=meta\n                )\n\n            result_vals.append(result_val)\n            feedback_calls.append(feedback_call)\n\n        result_vals = np.array(result_vals)\n        if len(result_vals) == 0:\n            logger.warning(\n                f\"Feedback function {self.name} with aggregation {self.agg} had no inputs.\"\n            )\n            result = np.nan\n        else:\n            result = self.agg(result_vals)\n\n        feedback_result.update(\n            result=result,\n            status=FeedbackResultStatus.DONE,\n            cost=cost,\n            calls=feedback_calls\n        )\n\n        return feedback_result\n\n    except:\n        exc_tb = traceback.format_exc()\n        logger.warning(f\"Feedback Function Exception Caught: {exc_tb}\")\n        feedback_result.update(\n            error=exc_tb, status=FeedbackResultStatus.FAILED\n        )\n        return feedback_result\n</code></pre>"},{"location":"trulens_eval/api/feedback/#trulens_eval.trulens_eval.feedback.GroundTruthAgreement","title":"<code>GroundTruthAgreement</code>","text":"<p>         Bases: <code>SerialModel</code>, <code>WithClassInfo</code></p> Source code in <code>trulens_eval/trulens_eval/feedback.py</code> <pre><code>class GroundTruthAgreement(SerialModel, WithClassInfo):\n    ground_truth: Union[List[str], FunctionOrMethod]\n    provider: Provider\n\n    ground_truth_imp: Optional[Callable] = pydantic.Field(exclude=True)\n\n    def __init__(\n        self,\n        ground_truth: Union[List[str], Callable, FunctionOrMethod],\n        provider: Provider = None\n    ):\n        if provider is None:\n            provider = OpenAI()\n        if isinstance(ground_truth, List):\n            ground_truth_imp = None\n        elif isinstance(ground_truth, FunctionOrMethod):\n            ground_truth_imp = ground_truth.load()\n        elif isinstance(ground_truth, Callable):\n            ground_truth_imp = ground_truth\n            ground_truth = FunctionOrMethod.of_callable(ground_truth)\n        elif isinstance(ground_truth, Dict):\n            # Serialized FunctionOrMethod?\n            ground_truth = FunctionOrMethod.pick(**ground_truth)\n            ground_truth_imp = ground_truth.load()\n        else:\n            raise RuntimeError(\n                f\"Unhandled ground_truth type: {type(ground_truth)}.\"\n            )\n\n        super().__init__(\n            ground_truth=ground_truth,\n            ground_truth_imp=ground_truth_imp,\n            provider=provider,\n            obj=self  # for WithClassInfo\n        )\n\n    def _find_response(self, prompt: str) -&gt; Optional[str]:\n        if self.ground_truth_imp is not None:\n            return self.ground_truth_imp(prompt)\n\n        responses = [\n            qr[\"response\"] for qr in self.ground_truth if qr[\"query\"] == prompt\n        ]\n        if responses:\n            return responses[0]\n        else:\n            return None\n\n    def agreement_measure(\n        self, prompt: str, response: str\n    ) -&gt; Union[float, Tuple[float, Dict[str, str]]]:\n\"\"\"\n        Uses OpenAI's Chat GPT Model. A function that that measures\n        similarity to ground truth. A second template is given to Chat GPT\n        with a prompt that the original response is correct, and measures\n        whether previous Chat GPT's response is similar.\n\n        Parameters:\n            prompt (str): A text prompt to an agent. response (str): The\n            agent's response to the prompt.\n\n        Returns:\n            - float: A value between 0 and 1. 0 being \"not in agreement\" and 1\n                being \"in agreement\".\n            - dict: with key 'ground_truth_response'\n        \"\"\"\n        ground_truth_response = self._find_response(prompt)\n        if ground_truth_response:\n            agreement_txt = self.provider._get_answer_agreement(\n                prompt, response, ground_truth_response\n            )\n            ret = _re_1_10_rating(agreement_txt) / 10, dict(\n                ground_truth_response=ground_truth_response\n            )\n        else:\n            ret = np.nan\n        return ret\n</code></pre>"},{"location":"trulens_eval/api/feedback/#trulens_eval.trulens_eval.feedback.GroundTruthAgreement.agreement_measure","title":"<code>agreement_measure(prompt, response)</code>","text":"<p>Uses OpenAI's Chat GPT Model. A function that that measures similarity to ground truth. A second template is given to Chat GPT with a prompt that the original response is correct, and measures whether previous Chat GPT's response is similar.</p> <p>Parameters:</p> Name Type Description Default <code>prompt</code> <code>str</code> <p>A text prompt to an agent. response (str): The</p> required <p>Returns:</p> Type Description <code>Union[float, Tuple[float, Dict[str, str]]]</code> <ul> <li>float: A value between 0 and 1. 0 being \"not in agreement\" and 1 being \"in agreement\".</li> </ul> <code>Union[float, Tuple[float, Dict[str, str]]]</code> <ul> <li>dict: with key 'ground_truth_response'</li> </ul> Source code in <code>trulens_eval/trulens_eval/feedback.py</code> <pre><code>def agreement_measure(\n    self, prompt: str, response: str\n) -&gt; Union[float, Tuple[float, Dict[str, str]]]:\n\"\"\"\n    Uses OpenAI's Chat GPT Model. A function that that measures\n    similarity to ground truth. A second template is given to Chat GPT\n    with a prompt that the original response is correct, and measures\n    whether previous Chat GPT's response is similar.\n\n    Parameters:\n        prompt (str): A text prompt to an agent. response (str): The\n        agent's response to the prompt.\n\n    Returns:\n        - float: A value between 0 and 1. 0 being \"not in agreement\" and 1\n            being \"in agreement\".\n        - dict: with key 'ground_truth_response'\n    \"\"\"\n    ground_truth_response = self._find_response(prompt)\n    if ground_truth_response:\n        agreement_txt = self.provider._get_answer_agreement(\n            prompt, response, ground_truth_response\n        )\n        ret = _re_1_10_rating(agreement_txt) / 10, dict(\n            ground_truth_response=ground_truth_response\n        )\n    else:\n        ret = np.nan\n    return ret\n</code></pre>"},{"location":"trulens_eval/api/feedback/#trulens_eval.trulens_eval.feedback.Groundedness","title":"<code>Groundedness</code>","text":"<p>         Bases: <code>SerialModel</code>, <code>WithClassInfo</code></p> Source code in <code>trulens_eval/trulens_eval/feedback.py</code> <pre><code>class Groundedness(SerialModel, WithClassInfo):\n    summarize_provider: Provider\n    groundedness_provider: Provider\n\n    def __init__(self, groundedness_provider: Provider = None):\n\"\"\"Instantiates the groundedness providers. Currently the groundedness functions work well with a summarizer.\n        This class will use an OpenAI summarizer to find the relevant strings in a text. The groundedness_provider can \n        either be an llm with OpenAI or NLI with huggingface.\n\n        Args:\n            groundedness_provider (Provider, optional): groundedness provider options: OpenAI LLM or HuggingFace NLI. Defaults to OpenAI().\n        \"\"\"\n        if groundedness_provider is None:\n            groundedness_provider = OpenAI()\n        summarize_provider = OpenAI()\n        if not isinstance(groundedness_provider, (OpenAI, Huggingface)):\n            raise Exception(\n                \"Groundedness is only supported groundedness_provider as OpenAI or Huggingface Providers.\"\n            )\n        super().__init__(\n            summarize_provider=summarize_provider,\n            groundedness_provider=groundedness_provider,\n            obj=self  # for WithClassInfo\n        )\n\n    def groundedness_measure(self, source: str, statement: str) -&gt; float:\n\"\"\"A measure to track if the source material supports each sentence in the statement. \n        This groundedness measure is faster; but less accurate than `groundedness_measure_with_summarize_step` \n\n        ```\n        grounded = feedback.Groundedness(groundedness_provider=OpenAI())\n\n\n        f_groundedness = feedback.Feedback(grounded.groundedness_measure).on(\n            Select.Record.app.combine_documents_chain._call.args.inputs.input_documents[:].page_content\n        ).on_output().aggregate(grounded.grounded_statements_aggregator)\n        ```\n        Args:\n            source (str): The source that should support the statement\n            statement (str): The statement to check groundedness\n\n        Returns:\n            float: A measure between 0 and 1, where 1 means each sentence is grounded in the source.\n        \"\"\"\n        groundedness_scores = {}\n        if isinstance(self.groundedness_provider, OpenAI):\n            plausible_junk_char_min = 4  # very likely \"sentences\" under 4 characters are punctuation, spaces, etc\n            if len(statement) &gt; plausible_junk_char_min:\n                reason = self.summarize_provider._groundedness_doc_in_out(\n                    source, statement\n                )\n            i = 0\n            for line in reason.split('\\n'):\n                if \"Score\" in line:\n                    groundedness_scores[f\"statement_{i}\"\n                                       ] = _re_1_10_rating(line) / 10\n                    i += 1\n            return groundedness_scores, {\"reason\": reason}\n        if isinstance(self.groundedness_provider, Huggingface):\n            reason = \"\"\n            for i, hypothesis in enumerate(\n                    tqdm(statement.split(\".\"),\n                         desc=\"Groundendess per statement in source\")):\n                plausible_junk_char_min = 4  # very likely \"sentences\" under 4 characters are punctuation, spaces, etc\n                if len(hypothesis) &gt; plausible_junk_char_min:\n                    score = self.groundedness_provider._doc_groundedness(\n                        premise=source, hypothesis=hypothesis\n                    )\n                    reason = reason + str.format(\n                        feedback_prompts.GROUNDEDNESS_REASON_TEMPLATE,\n                        statement_sentence=hypothesis,\n                        supporting_evidence=\"[Doc NLI Used full source]\",\n                        score=score * 10,\n                    )\n                    groundedness_scores[f\"statement_{i}\"] = score\n\n            return groundedness_scores, {\"reason\": reason}\n\n    def groundedness_measure_with_summarize_step(\n        self, source: str, statement: str\n    ) -&gt; float:\n\"\"\"A measure to track if the source material supports each sentence in the statement. \n        This groundedness measure is more accurate; but slower using a two step process.\n        - First find supporting evidence with an LLM\n        - Then for each statement sentence, check groundendness\n        ```\n        grounded = feedback.Groundedness(groundedness_provider=OpenAI())\n\n\n        f_groundedness = feedback.Feedback(grounded.groundedness_measure_with_summarize_step).on(\n            Select.Record.app.combine_documents_chain._call.args.inputs.input_documents[:].page_content\n        ).on_output().aggregate(grounded.grounded_statements_aggregator)\n        ```\n        Args:\n            source (str): The source that should support the statement\n            statement (str): The statement to check groundedness\n\n        Returns:\n            float: A measure between 0 and 1, where 1 means each sentence is grounded in the source.\n        \"\"\"\n        groundedness_scores = {}\n        reason = \"\"\n        for i, hypothesis in enumerate(\n                tqdm(statement.split(\".\"),\n                     desc=\"Groundendess per statement in source\")):\n            plausible_junk_char_min = 4  # very likely \"sentences\" under 4 characters are punctuation, spaces, etc\n            if len(hypothesis) &gt; plausible_junk_char_min:\n                supporting_premise = self.summarize_provider._find_relevant_string(\n                    source, hypothesis\n                )\n                score = self.groundedness_provider._summarized_groundedness(\n                    premise=supporting_premise, hypothesis=hypothesis\n                )\n                reason = reason + str.format(\n                    feedback_prompts.GROUNDEDNESS_REASON_TEMPLATE,\n                    statement_sentence=hypothesis,\n                    supporting_evidence=supporting_premise,\n                    score=score * 10,\n                )\n                groundedness_scores[f\"statement_{i}\"] = score\n        return groundedness_scores, {\"reason\": reason}\n\n    def grounded_statements_aggregator(\n        self, source_statements_matrix: np.ndarray\n    ) -&gt; float:\n\"\"\"Aggregates multi-input, mulit-output information from the _groundedness_measure_experimental methods.\n\n\n        Args:\n            source_statements_matrix (np.ndarray): a 2D array with the first dimension corresponding to a source text,\n                and the second dimension corresponding to each sentence in a statement; it's groundedness score\n\n        Returns:\n            float: for each statement, gets the max groundedness, then averages over that.\n        \"\"\"\n        max_over_sources = np.max(source_statements_matrix, axis=0)\n        return np.mean(max_over_sources)\n</code></pre>"},{"location":"trulens_eval/api/feedback/#trulens_eval.trulens_eval.feedback.Groundedness.__init__","title":"<code>__init__(groundedness_provider=None)</code>","text":"<p>Instantiates the groundedness providers. Currently the groundedness functions work well with a summarizer. This class will use an OpenAI summarizer to find the relevant strings in a text. The groundedness_provider can  either be an llm with OpenAI or NLI with huggingface.</p> <p>Parameters:</p> Name Type Description Default <code>groundedness_provider</code> <code>Provider</code> <p>groundedness provider options: OpenAI LLM or HuggingFace NLI. Defaults to OpenAI().</p> <code>None</code> Source code in <code>trulens_eval/trulens_eval/feedback.py</code> <pre><code>def __init__(self, groundedness_provider: Provider = None):\n\"\"\"Instantiates the groundedness providers. Currently the groundedness functions work well with a summarizer.\n    This class will use an OpenAI summarizer to find the relevant strings in a text. The groundedness_provider can \n    either be an llm with OpenAI or NLI with huggingface.\n\n    Args:\n        groundedness_provider (Provider, optional): groundedness provider options: OpenAI LLM or HuggingFace NLI. Defaults to OpenAI().\n    \"\"\"\n    if groundedness_provider is None:\n        groundedness_provider = OpenAI()\n    summarize_provider = OpenAI()\n    if not isinstance(groundedness_provider, (OpenAI, Huggingface)):\n        raise Exception(\n            \"Groundedness is only supported groundedness_provider as OpenAI or Huggingface Providers.\"\n        )\n    super().__init__(\n        summarize_provider=summarize_provider,\n        groundedness_provider=groundedness_provider,\n        obj=self  # for WithClassInfo\n    )\n</code></pre>"},{"location":"trulens_eval/api/feedback/#trulens_eval.trulens_eval.feedback.Groundedness.grounded_statements_aggregator","title":"<code>grounded_statements_aggregator(source_statements_matrix)</code>","text":"<p>Aggregates multi-input, mulit-output information from the _groundedness_measure_experimental methods.</p> <p>Parameters:</p> Name Type Description Default <code>source_statements_matrix</code> <code>np.ndarray</code> <p>a 2D array with the first dimension corresponding to a source text, and the second dimension corresponding to each sentence in a statement; it's groundedness score</p> required <p>Returns:</p> Name Type Description <code>float</code> <code>float</code> <p>for each statement, gets the max groundedness, then averages over that.</p> Source code in <code>trulens_eval/trulens_eval/feedback.py</code> <pre><code>def grounded_statements_aggregator(\n    self, source_statements_matrix: np.ndarray\n) -&gt; float:\n\"\"\"Aggregates multi-input, mulit-output information from the _groundedness_measure_experimental methods.\n\n\n    Args:\n        source_statements_matrix (np.ndarray): a 2D array with the first dimension corresponding to a source text,\n            and the second dimension corresponding to each sentence in a statement; it's groundedness score\n\n    Returns:\n        float: for each statement, gets the max groundedness, then averages over that.\n    \"\"\"\n    max_over_sources = np.max(source_statements_matrix, axis=0)\n    return np.mean(max_over_sources)\n</code></pre>"},{"location":"trulens_eval/api/feedback/#trulens_eval.trulens_eval.feedback.Groundedness.groundedness_measure","title":"<code>groundedness_measure(source, statement)</code>","text":"<p>A measure to track if the source material supports each sentence in the statement.  This groundedness measure is faster; but less accurate than <code>groundedness_measure_with_summarize_step</code> </p> <pre><code>grounded = feedback.Groundedness(groundedness_provider=OpenAI())\n\n\nf_groundedness = feedback.Feedback(grounded.groundedness_measure).on(\n    Select.Record.app.combine_documents_chain._call.args.inputs.input_documents[:].page_content\n).on_output().aggregate(grounded.grounded_statements_aggregator)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>source</code> <code>str</code> <p>The source that should support the statement</p> required <code>statement</code> <code>str</code> <p>The statement to check groundedness</p> required <p>Returns:</p> Name Type Description <code>float</code> <code>float</code> <p>A measure between 0 and 1, where 1 means each sentence is grounded in the source.</p> Source code in <code>trulens_eval/trulens_eval/feedback.py</code> <pre><code>def groundedness_measure(self, source: str, statement: str) -&gt; float:\n\"\"\"A measure to track if the source material supports each sentence in the statement. \n    This groundedness measure is faster; but less accurate than `groundedness_measure_with_summarize_step` \n\n    ```\n    grounded = feedback.Groundedness(groundedness_provider=OpenAI())\n\n\n    f_groundedness = feedback.Feedback(grounded.groundedness_measure).on(\n        Select.Record.app.combine_documents_chain._call.args.inputs.input_documents[:].page_content\n    ).on_output().aggregate(grounded.grounded_statements_aggregator)\n    ```\n    Args:\n        source (str): The source that should support the statement\n        statement (str): The statement to check groundedness\n\n    Returns:\n        float: A measure between 0 and 1, where 1 means each sentence is grounded in the source.\n    \"\"\"\n    groundedness_scores = {}\n    if isinstance(self.groundedness_provider, OpenAI):\n        plausible_junk_char_min = 4  # very likely \"sentences\" under 4 characters are punctuation, spaces, etc\n        if len(statement) &gt; plausible_junk_char_min:\n            reason = self.summarize_provider._groundedness_doc_in_out(\n                source, statement\n            )\n        i = 0\n        for line in reason.split('\\n'):\n            if \"Score\" in line:\n                groundedness_scores[f\"statement_{i}\"\n                                   ] = _re_1_10_rating(line) / 10\n                i += 1\n        return groundedness_scores, {\"reason\": reason}\n    if isinstance(self.groundedness_provider, Huggingface):\n        reason = \"\"\n        for i, hypothesis in enumerate(\n                tqdm(statement.split(\".\"),\n                     desc=\"Groundendess per statement in source\")):\n            plausible_junk_char_min = 4  # very likely \"sentences\" under 4 characters are punctuation, spaces, etc\n            if len(hypothesis) &gt; plausible_junk_char_min:\n                score = self.groundedness_provider._doc_groundedness(\n                    premise=source, hypothesis=hypothesis\n                )\n                reason = reason + str.format(\n                    feedback_prompts.GROUNDEDNESS_REASON_TEMPLATE,\n                    statement_sentence=hypothesis,\n                    supporting_evidence=\"[Doc NLI Used full source]\",\n                    score=score * 10,\n                )\n                groundedness_scores[f\"statement_{i}\"] = score\n\n        return groundedness_scores, {\"reason\": reason}\n</code></pre>"},{"location":"trulens_eval/api/feedback/#trulens_eval.trulens_eval.feedback.Groundedness.groundedness_measure_with_summarize_step","title":"<code>groundedness_measure_with_summarize_step(source, statement)</code>","text":"<p>A measure to track if the source material supports each sentence in the statement.  This groundedness measure is more accurate; but slower using a two step process. - First find supporting evidence with an LLM - Then for each statement sentence, check groundendness <pre><code>grounded = feedback.Groundedness(groundedness_provider=OpenAI())\n\n\nf_groundedness = feedback.Feedback(grounded.groundedness_measure_with_summarize_step).on(\n    Select.Record.app.combine_documents_chain._call.args.inputs.input_documents[:].page_content\n).on_output().aggregate(grounded.grounded_statements_aggregator)\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>source</code> <code>str</code> <p>The source that should support the statement</p> required <code>statement</code> <code>str</code> <p>The statement to check groundedness</p> required <p>Returns:</p> Name Type Description <code>float</code> <code>float</code> <p>A measure between 0 and 1, where 1 means each sentence is grounded in the source.</p> Source code in <code>trulens_eval/trulens_eval/feedback.py</code> <pre><code>def groundedness_measure_with_summarize_step(\n    self, source: str, statement: str\n) -&gt; float:\n\"\"\"A measure to track if the source material supports each sentence in the statement. \n    This groundedness measure is more accurate; but slower using a two step process.\n    - First find supporting evidence with an LLM\n    - Then for each statement sentence, check groundendness\n    ```\n    grounded = feedback.Groundedness(groundedness_provider=OpenAI())\n\n\n    f_groundedness = feedback.Feedback(grounded.groundedness_measure_with_summarize_step).on(\n        Select.Record.app.combine_documents_chain._call.args.inputs.input_documents[:].page_content\n    ).on_output().aggregate(grounded.grounded_statements_aggregator)\n    ```\n    Args:\n        source (str): The source that should support the statement\n        statement (str): The statement to check groundedness\n\n    Returns:\n        float: A measure between 0 and 1, where 1 means each sentence is grounded in the source.\n    \"\"\"\n    groundedness_scores = {}\n    reason = \"\"\n    for i, hypothesis in enumerate(\n            tqdm(statement.split(\".\"),\n                 desc=\"Groundendess per statement in source\")):\n        plausible_junk_char_min = 4  # very likely \"sentences\" under 4 characters are punctuation, spaces, etc\n        if len(hypothesis) &gt; plausible_junk_char_min:\n            supporting_premise = self.summarize_provider._find_relevant_string(\n                source, hypothesis\n            )\n            score = self.groundedness_provider._summarized_groundedness(\n                premise=supporting_premise, hypothesis=hypothesis\n            )\n            reason = reason + str.format(\n                feedback_prompts.GROUNDEDNESS_REASON_TEMPLATE,\n                statement_sentence=hypothesis,\n                supporting_evidence=supporting_premise,\n                score=score * 10,\n            )\n            groundedness_scores[f\"statement_{i}\"] = score\n    return groundedness_scores, {\"reason\": reason}\n</code></pre>"},{"location":"trulens_eval/api/feedback/#trulens_eval.trulens_eval.feedback.Huggingface","title":"<code>Huggingface</code>","text":"<p>         Bases: <code>Provider</code></p> Source code in <code>trulens_eval/trulens_eval/feedback.py</code> <pre><code>class Huggingface(Provider):\n\n    endpoint: Endpoint\n\n    def __init__(self, endpoint=None, **kwargs):\n        # NOTE(piotrm): pydantic adds endpoint to the signature of this\n        # constructor if we don't include it explicitly, even though we set it\n        # down below. Adding it as None here as a temporary hack.\n\"\"\"\n        A set of Huggingface Feedback Functions.\n\n        All args/kwargs passed to HuggingfaceEndpoint constructor.\n        \"\"\"\n\n        self_kwargs = dict()\n        self_kwargs['endpoint'] = HuggingfaceEndpoint(**kwargs)\n\n        super().__init__(\n            **self_kwargs\n        )  # need to include pydantic.BaseModel.__init__\n\n    def language_match(self, text1: str, text2: str) -&gt; float:\n\"\"\"\n        Uses Huggingface's papluca/xlm-roberta-base-language-detection model. A\n        function that uses language detection on `text1` and `text2` and\n        calculates the probit difference on the language detected on text1. The\n        function is: `1.0 - (|probit_language_text1(text1) -\n        probit_language_text1(text2))`\n\n        Parameters:\n\n            text1 (str): Text to evaluate.\n\n            text2 (str): Comparative text to evaluate.\n\n        Returns:\n\n            float: A value between 0 and 1. 0 being \"different languages\" and 1\n            being \"same languages\".\n        \"\"\"\n\n        def get_scores(text):\n            payload = {\"inputs\": text}\n            hf_response = self.endpoint.post(\n                url=HUGS_LANGUAGE_API_URL, payload=payload, timeout=30\n            )\n            return {r['label']: r['score'] for r in hf_response}\n\n        max_length = 500\n        scores1: AsyncResult[Dict] = TP().promise(\n            get_scores, text=text1[:max_length]\n        )\n        scores2: AsyncResult[Dict] = TP().promise(\n            get_scores, text=text2[:max_length]\n        )\n\n        scores1: Dict = scores1.get()\n        scores2: Dict = scores2.get()\n\n        langs = list(scores1.keys())\n        prob1 = np.array([scores1[k] for k in langs])\n        prob2 = np.array([scores2[k] for k in langs])\n        diff = prob1 - prob2\n\n        l1 = 1.0 - (np.linalg.norm(diff, ord=1)) / 2.0\n\n        return l1, dict(text1_scores=scores1, text2_scores=scores2)\n\n    def positive_sentiment(self, text: str) -&gt; float:\n\"\"\"\n        Uses Huggingface's cardiffnlp/twitter-roberta-base-sentiment model. A\n        function that uses a sentiment classifier on `text`.\n\n        Parameters:\n            text (str): Text to evaluate.\n\n        Returns:\n            float: A value between 0 and 1. 0 being \"negative sentiment\" and 1\n            being \"positive sentiment\".\n        \"\"\"\n        max_length = 500\n        truncated_text = text[:max_length]\n        payload = {\"inputs\": truncated_text}\n\n        hf_response = self.endpoint.post(\n            url=HUGS_SENTIMENT_API_URL, payload=payload\n        )\n\n        for label in hf_response:\n            if label['label'] == 'LABEL_2':\n                return label['score']\n\n    def not_toxic(self, text: str) -&gt; float:\n\"\"\"\n        Uses Huggingface's martin-ha/toxic-comment-model model. A function that\n        uses a toxic comment classifier on `text`.\n\n        Parameters:\n            text (str): Text to evaluate.\n\n        Returns:\n            float: A value between 0 and 1. 0 being \"toxic\" and 1 being \"not\n            toxic\".\n        \"\"\"\n        max_length = 500\n        truncated_text = text[:max_length]\n        payload = {\"inputs\": truncated_text}\n        hf_response = self.endpoint.post(\n            url=HUGS_TOXIC_API_URL, payload=payload\n        )\n\n        for label in hf_response:\n            if label['label'] == 'toxic':\n                return label['score']\n\n    def _summarized_groundedness(self, premise: str, hypothesis: str) -&gt; float:\n\"\"\" A groundedness measure best used for summarized premise against simple hypothesis.\n        This Huggingface implementation uses NLI.\n\n        Args:\n            premise (str): NLI Premise\n            hypothesis (str): NLI Hypothesis\n\n        Returns:\n            float: NLI Entailment\n        \"\"\"\n        if not '.' == premise[len(premise) - 1]:\n            premise = premise + '.'\n        nli_string = premise + ' ' + hypothesis\n        payload = {\"inputs\": nli_string}\n        hf_response = self.endpoint.post(url=HUGS_NLI_API_URL, payload=payload)\n\n        for label in hf_response:\n            if label['label'] == 'entailment':\n                return label['score']\n\n    def _doc_groundedness(self, premise, hypothesis):\n\"\"\" A groundedness measure for full document premise against hypothesis.\n        This Huggingface implementation uses DocNLI. The Hypoethsis still only works on single small hypothesis.\n\n        Args:\n            premise (str): NLI Premise\n            hypothesis (str): NLI Hypothesis\n\n        Returns:\n            float: NLI Entailment\n        \"\"\"\n        nli_string = premise + ' [SEP] ' + hypothesis\n        payload = {\"inputs\": nli_string}\n        hf_response = self.endpoint.post(\n            url=HUGS_DOCNLI_API_URL, payload=payload\n        )\n\n        for label in hf_response:\n            if label['label'] == 'entailment':\n                return label['score']\n</code></pre>"},{"location":"trulens_eval/api/feedback/#trulens_eval.trulens_eval.feedback.Huggingface.__init__","title":"<code>__init__(endpoint=None, **kwargs)</code>","text":"<p>A set of Huggingface Feedback Functions.</p> <p>All args/kwargs passed to HuggingfaceEndpoint constructor.</p> Source code in <code>trulens_eval/trulens_eval/feedback.py</code> <pre><code>def __init__(self, endpoint=None, **kwargs):\n    # NOTE(piotrm): pydantic adds endpoint to the signature of this\n    # constructor if we don't include it explicitly, even though we set it\n    # down below. Adding it as None here as a temporary hack.\n\"\"\"\n    A set of Huggingface Feedback Functions.\n\n    All args/kwargs passed to HuggingfaceEndpoint constructor.\n    \"\"\"\n\n    self_kwargs = dict()\n    self_kwargs['endpoint'] = HuggingfaceEndpoint(**kwargs)\n\n    super().__init__(\n        **self_kwargs\n    )  # need to include pydantic.BaseModel.__init__\n</code></pre>"},{"location":"trulens_eval/api/feedback/#trulens_eval.trulens_eval.feedback.Huggingface.language_match","title":"<code>language_match(text1, text2)</code>","text":"<p>Uses Huggingface's papluca/xlm-roberta-base-language-detection model. A function that uses language detection on <code>text1</code> and <code>text2</code> and calculates the probit difference on the language detected on text1. The function is: <code>1.0 - (|probit_language_text1(text1) - probit_language_text1(text2))</code></p> <p>Parameters:</p> Name Type Description Default <code>text1</code> <code>str</code> <p>Text to evaluate.</p> required <code>text2</code> <code>str</code> <p>Comparative text to evaluate.</p> required <p>Returns:</p> Name Type Description <code>float</code> <code>float</code> <p>A value between 0 and 1. 0 being \"different languages\" and 1</p> <code>float</code> <p>being \"same languages\".</p> Source code in <code>trulens_eval/trulens_eval/feedback.py</code> <pre><code>def language_match(self, text1: str, text2: str) -&gt; float:\n\"\"\"\n    Uses Huggingface's papluca/xlm-roberta-base-language-detection model. A\n    function that uses language detection on `text1` and `text2` and\n    calculates the probit difference on the language detected on text1. The\n    function is: `1.0 - (|probit_language_text1(text1) -\n    probit_language_text1(text2))`\n\n    Parameters:\n\n        text1 (str): Text to evaluate.\n\n        text2 (str): Comparative text to evaluate.\n\n    Returns:\n\n        float: A value between 0 and 1. 0 being \"different languages\" and 1\n        being \"same languages\".\n    \"\"\"\n\n    def get_scores(text):\n        payload = {\"inputs\": text}\n        hf_response = self.endpoint.post(\n            url=HUGS_LANGUAGE_API_URL, payload=payload, timeout=30\n        )\n        return {r['label']: r['score'] for r in hf_response}\n\n    max_length = 500\n    scores1: AsyncResult[Dict] = TP().promise(\n        get_scores, text=text1[:max_length]\n    )\n    scores2: AsyncResult[Dict] = TP().promise(\n        get_scores, text=text2[:max_length]\n    )\n\n    scores1: Dict = scores1.get()\n    scores2: Dict = scores2.get()\n\n    langs = list(scores1.keys())\n    prob1 = np.array([scores1[k] for k in langs])\n    prob2 = np.array([scores2[k] for k in langs])\n    diff = prob1 - prob2\n\n    l1 = 1.0 - (np.linalg.norm(diff, ord=1)) / 2.0\n\n    return l1, dict(text1_scores=scores1, text2_scores=scores2)\n</code></pre>"},{"location":"trulens_eval/api/feedback/#trulens_eval.trulens_eval.feedback.Huggingface.not_toxic","title":"<code>not_toxic(text)</code>","text":"<p>Uses Huggingface's martin-ha/toxic-comment-model model. A function that uses a toxic comment classifier on <code>text</code>.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>Text to evaluate.</p> required <p>Returns:</p> Name Type Description <code>float</code> <code>float</code> <p>A value between 0 and 1. 0 being \"toxic\" and 1 being \"not</p> <code>float</code> <p>toxic\".</p> Source code in <code>trulens_eval/trulens_eval/feedback.py</code> <pre><code>def not_toxic(self, text: str) -&gt; float:\n\"\"\"\n    Uses Huggingface's martin-ha/toxic-comment-model model. A function that\n    uses a toxic comment classifier on `text`.\n\n    Parameters:\n        text (str): Text to evaluate.\n\n    Returns:\n        float: A value between 0 and 1. 0 being \"toxic\" and 1 being \"not\n        toxic\".\n    \"\"\"\n    max_length = 500\n    truncated_text = text[:max_length]\n    payload = {\"inputs\": truncated_text}\n    hf_response = self.endpoint.post(\n        url=HUGS_TOXIC_API_URL, payload=payload\n    )\n\n    for label in hf_response:\n        if label['label'] == 'toxic':\n            return label['score']\n</code></pre>"},{"location":"trulens_eval/api/feedback/#trulens_eval.trulens_eval.feedback.Huggingface.positive_sentiment","title":"<code>positive_sentiment(text)</code>","text":"<p>Uses Huggingface's cardiffnlp/twitter-roberta-base-sentiment model. A function that uses a sentiment classifier on <code>text</code>.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>Text to evaluate.</p> required <p>Returns:</p> Name Type Description <code>float</code> <code>float</code> <p>A value between 0 and 1. 0 being \"negative sentiment\" and 1</p> <code>float</code> <p>being \"positive sentiment\".</p> Source code in <code>trulens_eval/trulens_eval/feedback.py</code> <pre><code>def positive_sentiment(self, text: str) -&gt; float:\n\"\"\"\n    Uses Huggingface's cardiffnlp/twitter-roberta-base-sentiment model. A\n    function that uses a sentiment classifier on `text`.\n\n    Parameters:\n        text (str): Text to evaluate.\n\n    Returns:\n        float: A value between 0 and 1. 0 being \"negative sentiment\" and 1\n        being \"positive sentiment\".\n    \"\"\"\n    max_length = 500\n    truncated_text = text[:max_length]\n    payload = {\"inputs\": truncated_text}\n\n    hf_response = self.endpoint.post(\n        url=HUGS_SENTIMENT_API_URL, payload=payload\n    )\n\n    for label in hf_response:\n        if label['label'] == 'LABEL_2':\n            return label['score']\n</code></pre>"},{"location":"trulens_eval/api/feedback/#trulens_eval.trulens_eval.feedback.OpenAI","title":"<code>OpenAI</code>","text":"<p>         Bases: <code>Provider</code></p> Source code in <code>trulens_eval/trulens_eval/feedback.py</code> <pre><code>class OpenAI(Provider):\n    model_engine: str\n    endpoint: Endpoint\n\n    def __init__(\n        self, *args, endpoint=None, model_engine=\"gpt-3.5-turbo\", **kwargs\n    ):\n        # NOTE(piotrm): pydantic adds endpoint to the signature of this\n        # constructor if we don't include it explicitly, even though we set it\n        # down below. Adding it as None here as a temporary hack.\n\"\"\"\n        A set of OpenAI Feedback Functions.\n\n        Parameters:\n\n        - model_engine (str, optional): The specific model version. Defaults to\n          \"gpt-3.5-turbo\".\n\n        - All other args/kwargs passed to OpenAIEndpoint constructor.\n        \"\"\"\n\n        # TODO: why was self_kwargs required here independently of kwargs?\n        self_kwargs = dict()\n        self_kwargs['model_engine'] = model_engine\n        self_kwargs['endpoint'] = OpenAIEndpoint(*args, **kwargs)\n\n        super().__init__(\n            **self_kwargs\n        )  # need to include pydantic.BaseModel.__init__\n\n        set_openai_key()\n\n\"\"\"\n    def to_json(self) -&gt; Dict:\n        return Provider.to_json(self, model_engine=self.model_engine)\n    \"\"\"\n\n    def _create_chat_completion(self, *args, **kwargs):\n        return openai.ChatCompletion.create(*args, **kwargs)\n\n    def _moderation(self, text: str):\n        return self.endpoint.run_me(\n            lambda: openai.Moderation.create(input=text)\n        )\n\n    def moderation_not_hate(self, text: str) -&gt; float:\n\"\"\"\n        Uses OpenAI's Moderation API. A function that checks if text is hate\n        speech.\n\n        Parameters:\n            text (str): Text to evaluate.\n\n        Returns:\n            float: A value between 0 and 1. 0 being \"hate\" and 1 being \"not\n            hate\".\n        \"\"\"\n        openai_response = self._moderation(text)\n        return 1 - float(\n            openai_response[\"results\"][0][\"category_scores\"][\"hate\"]\n        )\n\n    def moderation_not_hatethreatening(self, text: str) -&gt; float:\n\"\"\"\n        Uses OpenAI's Moderation API. A function that checks if text is\n        threatening speech.\n\n        Parameters:\n            text (str): Text to evaluate.\n\n        Returns:\n            float: A value between 0 and 1. 0 being \"threatening\" and 1 being\n            \"not threatening\".\n        \"\"\"\n        openai_response = self._moderation(text)\n\n        return 1 - int(\n            openai_response[\"results\"][0][\"category_scores\"][\"hate/threatening\"]\n        )\n\n    def moderation_not_selfharm(self, text: str) -&gt; float:\n\"\"\"\n        Uses OpenAI's Moderation API. A function that checks if text is about\n        self harm.\n\n        Parameters:\n            text (str): Text to evaluate.\n\n        Returns:\n            float: A value between 0 and 1. 0 being \"self harm\" and 1 being \"not\n            self harm\".\n        \"\"\"\n        openai_response = self._moderation(text)\n\n        return 1 - int(\n            openai_response[\"results\"][0][\"category_scores\"][\"self-harm\"]\n        )\n\n    def moderation_not_sexual(self, text: str) -&gt; float:\n\"\"\"\n        Uses OpenAI's Moderation API. A function that checks if text is sexual\n        speech.\n\n        Parameters:\n            text (str): Text to evaluate.\n\n        Returns:\n            float: A value between 0 and 1. 0 being \"sexual\" and 1 being \"not\n            sexual\".\n        \"\"\"\n        openai_response = self._moderation(text)\n\n        return 1 - int(\n            openai_response[\"results\"][0][\"category_scores\"][\"sexual\"]\n        )\n\n    def moderation_not_sexualminors(self, text: str) -&gt; float:\n\"\"\"\n        Uses OpenAI's Moderation API. A function that checks if text is about\n        sexual minors.\n\n        Parameters:\n            text (str): Text to evaluate.\n\n        Returns:\n            float: A value between 0 and 1. 0 being \"sexual minors\" and 1 being\n            \"not sexual minors\".\n        \"\"\"\n        openai_response = self._moderation(text)\n\n        return 1 - int(\n            openai_response[\"results\"][0][\"category_scores\"][\"sexual/minors\"]\n        )\n\n    def moderation_not_violence(self, text: str) -&gt; float:\n\"\"\"\n        Uses OpenAI's Moderation API. A function that checks if text is about\n        violence.\n\n        Parameters:\n            text (str): Text to evaluate.\n\n        Returns:\n            float: A value between 0 and 1. 0 being \"violence\" and 1 being \"not\n            violence\".\n        \"\"\"\n        openai_response = self._moderation(text)\n\n        return 1 - int(\n            openai_response[\"results\"][0][\"category_scores\"][\"violence\"]\n        )\n\n    def moderation_not_violencegraphic(self, text: str) -&gt; float:\n\"\"\"\n        Uses OpenAI's Moderation API. A function that checks if text is about\n        graphic violence.\n\n        Parameters:\n            text (str): Text to evaluate.\n\n        Returns:\n            float: A value between 0 and 1. 0 being \"graphic violence\" and 1\n            being \"not graphic violence\".\n        \"\"\"\n        openai_response = self._moderation(text)\n\n        return 1 - int(\n            openai_response[\"results\"][0][\"category_scores\"][\"violence/graphic\"]\n        )\n\n    def _find_relevant_string(self, full_source, hypothesis):\n        return self.endpoint.run_me(\n            lambda: self._create_chat_completion(\n                model=self.model_engine,\n                temperature=0.0,\n                messages=[\n                    {\n                        \"role\":\n                            \"system\",\n                        \"content\":\n                            str.format(\n                                feedback_prompts.SYSTEM_FIND_SUPPORTING,\n                                prompt=full_source,\n                            )\n                    }, {\n                        \"role\":\n                            \"user\",\n                        \"content\":\n                            str.format(\n                                feedback_prompts.USER_FIND_SUPPORTING,\n                                response=hypothesis\n                            )\n                    }\n                ]\n            )[\"choices\"][0][\"message\"][\"content\"]\n        )\n\n    def _summarized_groundedness(self, premise: str, hypothesis: str) -&gt; float:\n\"\"\" A groundedness measure best used for summarized premise against simple hypothesis.\n        This OpenAI implementation uses information overlap prompts.\n\n        Args:\n            premise (str): Summarized source sentences.\n            hypothesis (str): Single statement setnece.\n\n        Returns:\n            float: Information Overlap\n        \"\"\"\n        return _re_1_10_rating(\n            self.endpoint.run_me(\n                lambda: self._create_chat_completion(\n                    model=self.model_engine,\n                    temperature=0.0,\n                    messages=[\n                        {\n                            \"role\":\n                                \"system\",\n                            \"content\":\n                                str.format(\n                                    feedback_prompts.LLM_GROUNDEDNESS,\n                                    premise=premise,\n                                    hypothesis=hypothesis,\n                                )\n                        }\n                    ]\n                )[\"choices\"][0][\"message\"][\"content\"]\n            )\n        ) / 10\n\n    def _groundedness_doc_in_out(self, premise: str, hypothesis: str) -&gt; str:\n\"\"\"An LLM prompt using the entire document for premise and entire statement document for hypothesis\n\n        Args:\n            premise (str): A source document\n            hypothesis (str): A statement to check\n\n        Returns:\n            str: An LLM response using a scorecard template\n        \"\"\"\n        return self.endpoint.run_me(\n            lambda: self._create_chat_completion(\n                model=self.model_engine,\n                temperature=0.0,\n                messages=[\n                    {\n                        \"role\":\n                            \"system\",\n                        \"content\":\n                            str.format(\n                                feedback_prompts.LLM_GROUNDEDNESS_FULL_SYSTEM,\n                            )\n                    }, {\n                        \"role\":\n                            \"user\",\n                        \"content\":\n                            str.format(\n                                feedback_prompts.LLM_GROUNDEDNESS_FULL_PROMPT,\n                                premise=premise,\n                                hypothesis=hypothesis\n                            )\n                    }\n                ]\n            )[\"choices\"][0][\"message\"][\"content\"]\n        )\n\n    def qs_relevance(self, question: str, statement: str) -&gt; float:\n\"\"\"\n        Uses OpenAI's Chat Completion App. A function that completes a\n        template to check the relevance of the statement to the question.\n\n        Parameters:\n            question (str): A question being asked. statement (str): A statement\n            to the question.\n\n        Returns:\n            float: A value between 0 and 1. 0 being \"not relevant\" and 1 being\n            \"relevant\".\n        \"\"\"\n        return _re_1_10_rating(\n            self.endpoint.run_me(\n                lambda: self._create_chat_completion(\n                    model=self.model_engine,\n                    temperature=0.0,\n                    messages=[\n                        {\n                            \"role\":\n                                \"system\",\n                            \"content\":\n                                str.format(\n                                    feedback_prompts.QS_RELEVANCE,\n                                    question=question,\n                                    statement=statement\n                                )\n                        }\n                    ]\n                )[\"choices\"][0][\"message\"][\"content\"]\n            )\n        ) / 10\n\n    def relevance(self, prompt: str, response: str) -&gt; float:\n\"\"\"\n        Uses OpenAI's Chat Completion Model. A function that completes a\n        template to check the relevance of the response to a prompt.\n\n        Parameters:\n            prompt (str): A text prompt to an agent. response (str): The agent's\n            response to the prompt.\n\n        Returns:\n            float: A value between 0 and 1. 0 being \"not relevant\" and 1 being\n            \"relevant\".\n        \"\"\"\n        return _re_1_10_rating(\n            self.endpoint.run_me(\n                lambda: self._create_chat_completion(\n                    model=self.model_engine,\n                    temperature=0.0,\n                    messages=[\n                        {\n                            \"role\":\n                                \"system\",\n                            \"content\":\n                                str.format(\n                                    feedback_prompts.PR_RELEVANCE,\n                                    prompt=prompt,\n                                    response=response\n                                )\n                        },\n                    ]\n                )[\"choices\"][0][\"message\"][\"content\"]\n            )\n        ) / 10\n\n    def sentiment(self, text: str) -&gt; float:\n\"\"\"\n        Uses OpenAI's Chat Completion Model. A function that completes a\n        template to check the sentiment of some text.\n\n        Parameters:\n            text (str): A prompt to an agent. response (str): The agent's\n            response to the prompt.\n\n        Returns:\n            float: A value between 0 and 1. 0 being \"negative sentiment\" and 1\n            being \"positive sentiment\".\n        \"\"\"\n\n        return _re_1_10_rating(\n            self.endpoint.run_me(\n                lambda: self._create_chat_completion(\n                    model=self.model_engine,\n                    temperature=0.0,\n                    messages=[\n                        {\n                            \"role\": \"system\",\n                            \"content\": feedback_prompts.SENTIMENT_SYSTEM_PROMPT\n                        }, {\n                            \"role\": \"user\",\n                            \"content\": text\n                        }\n                    ]\n                )[\"choices\"][0][\"message\"][\"content\"]\n            )\n        )\n\n    def model_agreement(self, prompt: str, response: str) -&gt; float:\n\"\"\"\n        Uses OpenAI's Chat GPT Model. A function that gives Chat GPT the same\n        prompt and gets a response, encouraging truthfulness. A second template\n        is given to Chat GPT with a prompt that the original response is\n        correct, and measures whether previous Chat GPT's response is similar.\n\n        Parameters:\n            prompt (str): A text prompt to an agent. response (str): The agent's\n            response to the prompt.\n\n        Returns:\n            float: A value between 0 and 1. 0 being \"not in agreement\" and 1\n            being \"in agreement\".\n        \"\"\"\n        logger.warning(\n            \"model_agreement has been deprecated. Use GroundTruthAgreement(ground_truth) instead.\"\n        )\n        oai_chat_response = self.endpoint.run_me(\n            lambda: self._create_chat_completion(\n                model=self.model_engine,\n                temperature=0.0,\n                messages=[\n                    {\n                        \"role\": \"system\",\n                        \"content\": feedback_prompts.CORRECT_SYSTEM_PROMPT\n                    }, {\n                        \"role\": \"user\",\n                        \"content\": prompt\n                    }\n                ]\n            )[\"choices\"][0][\"message\"][\"content\"]\n        )\n        agreement_txt = self._get_answer_agreement(\n            prompt, response, oai_chat_response, self.model_engine\n        )\n        return _re_1_10_rating(agreement_txt) / 10\n\n    def _get_answer_agreement(\n        self, prompt, response, check_response, model_engine=\"gpt-3.5-turbo\"\n    ):\n        oai_chat_response = self.endpoint.run_me(\n            lambda: self._create_chat_completion(\n                model=model_engine,\n                temperature=0.0,\n                messages=[\n                    {\n                        \"role\":\n                            \"system\",\n                        \"content\":\n                            feedback_prompts.AGREEMENT_SYSTEM_PROMPT %\n                            (prompt, response)\n                    }, {\n                        \"role\": \"user\",\n                        \"content\": check_response\n                    }\n                ]\n            )[\"choices\"][0][\"message\"][\"content\"]\n        )\n        return oai_chat_response\n</code></pre>"},{"location":"trulens_eval/api/feedback/#trulens_eval.trulens_eval.feedback.OpenAI.__init__","title":"<code>__init__(*args, endpoint=None, model_engine='gpt-3.5-turbo', **kwargs)</code>","text":"<p>A set of OpenAI Feedback Functions.</p> <ul> <li> <p>model_engine (str, optional): The specific model version. Defaults to   \"gpt-3.5-turbo\".</p> </li> <li> <p>All other args/kwargs passed to OpenAIEndpoint constructor.</p> </li> </ul> Source code in <code>trulens_eval/trulens_eval/feedback.py</code> <pre><code>def __init__(\n    self, *args, endpoint=None, model_engine=\"gpt-3.5-turbo\", **kwargs\n):\n    # NOTE(piotrm): pydantic adds endpoint to the signature of this\n    # constructor if we don't include it explicitly, even though we set it\n    # down below. Adding it as None here as a temporary hack.\n\"\"\"\n    A set of OpenAI Feedback Functions.\n\n    Parameters:\n\n    - model_engine (str, optional): The specific model version. Defaults to\n      \"gpt-3.5-turbo\".\n\n    - All other args/kwargs passed to OpenAIEndpoint constructor.\n    \"\"\"\n\n    # TODO: why was self_kwargs required here independently of kwargs?\n    self_kwargs = dict()\n    self_kwargs['model_engine'] = model_engine\n    self_kwargs['endpoint'] = OpenAIEndpoint(*args, **kwargs)\n\n    super().__init__(\n        **self_kwargs\n    )  # need to include pydantic.BaseModel.__init__\n\n    set_openai_key()\n</code></pre>"},{"location":"trulens_eval/api/feedback/#trulens_eval.trulens_eval.feedback.OpenAI.model_agreement","title":"<code>model_agreement(prompt, response)</code>","text":"<p>Uses OpenAI's Chat GPT Model. A function that gives Chat GPT the same prompt and gets a response, encouraging truthfulness. A second template is given to Chat GPT with a prompt that the original response is correct, and measures whether previous Chat GPT's response is similar.</p> <p>Parameters:</p> Name Type Description Default <code>prompt</code> <code>str</code> <p>A text prompt to an agent. response (str): The agent's</p> required <p>Returns:</p> Name Type Description <code>float</code> <code>float</code> <p>A value between 0 and 1. 0 being \"not in agreement\" and 1</p> <code>float</code> <p>being \"in agreement\".</p> Source code in <code>trulens_eval/trulens_eval/feedback.py</code> <pre><code>def model_agreement(self, prompt: str, response: str) -&gt; float:\n\"\"\"\n    Uses OpenAI's Chat GPT Model. A function that gives Chat GPT the same\n    prompt and gets a response, encouraging truthfulness. A second template\n    is given to Chat GPT with a prompt that the original response is\n    correct, and measures whether previous Chat GPT's response is similar.\n\n    Parameters:\n        prompt (str): A text prompt to an agent. response (str): The agent's\n        response to the prompt.\n\n    Returns:\n        float: A value between 0 and 1. 0 being \"not in agreement\" and 1\n        being \"in agreement\".\n    \"\"\"\n    logger.warning(\n        \"model_agreement has been deprecated. Use GroundTruthAgreement(ground_truth) instead.\"\n    )\n    oai_chat_response = self.endpoint.run_me(\n        lambda: self._create_chat_completion(\n            model=self.model_engine,\n            temperature=0.0,\n            messages=[\n                {\n                    \"role\": \"system\",\n                    \"content\": feedback_prompts.CORRECT_SYSTEM_PROMPT\n                }, {\n                    \"role\": \"user\",\n                    \"content\": prompt\n                }\n            ]\n        )[\"choices\"][0][\"message\"][\"content\"]\n    )\n    agreement_txt = self._get_answer_agreement(\n        prompt, response, oai_chat_response, self.model_engine\n    )\n    return _re_1_10_rating(agreement_txt) / 10\n</code></pre>"},{"location":"trulens_eval/api/feedback/#trulens_eval.trulens_eval.feedback.OpenAI.moderation_not_hate","title":"<code>moderation_not_hate(text)</code>","text":"<p>Uses OpenAI's Moderation API. A function that checks if text is hate speech.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>Text to evaluate.</p> required <p>Returns:</p> Name Type Description <code>float</code> <code>float</code> <p>A value between 0 and 1. 0 being \"hate\" and 1 being \"not</p> <code>float</code> <p>hate\".</p> Source code in <code>trulens_eval/trulens_eval/feedback.py</code> <pre><code>def moderation_not_hate(self, text: str) -&gt; float:\n\"\"\"\n    Uses OpenAI's Moderation API. A function that checks if text is hate\n    speech.\n\n    Parameters:\n        text (str): Text to evaluate.\n\n    Returns:\n        float: A value between 0 and 1. 0 being \"hate\" and 1 being \"not\n        hate\".\n    \"\"\"\n    openai_response = self._moderation(text)\n    return 1 - float(\n        openai_response[\"results\"][0][\"category_scores\"][\"hate\"]\n    )\n</code></pre>"},{"location":"trulens_eval/api/feedback/#trulens_eval.trulens_eval.feedback.OpenAI.moderation_not_hatethreatening","title":"<code>moderation_not_hatethreatening(text)</code>","text":"<p>Uses OpenAI's Moderation API. A function that checks if text is threatening speech.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>Text to evaluate.</p> required <p>Returns:</p> Name Type Description <code>float</code> <code>float</code> <p>A value between 0 and 1. 0 being \"threatening\" and 1 being</p> <code>float</code> <p>\"not threatening\".</p> Source code in <code>trulens_eval/trulens_eval/feedback.py</code> <pre><code>def moderation_not_hatethreatening(self, text: str) -&gt; float:\n\"\"\"\n    Uses OpenAI's Moderation API. A function that checks if text is\n    threatening speech.\n\n    Parameters:\n        text (str): Text to evaluate.\n\n    Returns:\n        float: A value between 0 and 1. 0 being \"threatening\" and 1 being\n        \"not threatening\".\n    \"\"\"\n    openai_response = self._moderation(text)\n\n    return 1 - int(\n        openai_response[\"results\"][0][\"category_scores\"][\"hate/threatening\"]\n    )\n</code></pre>"},{"location":"trulens_eval/api/feedback/#trulens_eval.trulens_eval.feedback.OpenAI.moderation_not_selfharm","title":"<code>moderation_not_selfharm(text)</code>","text":"<p>Uses OpenAI's Moderation API. A function that checks if text is about self harm.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>Text to evaluate.</p> required <p>Returns:</p> Name Type Description <code>float</code> <code>float</code> <p>A value between 0 and 1. 0 being \"self harm\" and 1 being \"not</p> <code>float</code> <p>self harm\".</p> Source code in <code>trulens_eval/trulens_eval/feedback.py</code> <pre><code>def moderation_not_selfharm(self, text: str) -&gt; float:\n\"\"\"\n    Uses OpenAI's Moderation API. A function that checks if text is about\n    self harm.\n\n    Parameters:\n        text (str): Text to evaluate.\n\n    Returns:\n        float: A value between 0 and 1. 0 being \"self harm\" and 1 being \"not\n        self harm\".\n    \"\"\"\n    openai_response = self._moderation(text)\n\n    return 1 - int(\n        openai_response[\"results\"][0][\"category_scores\"][\"self-harm\"]\n    )\n</code></pre>"},{"location":"trulens_eval/api/feedback/#trulens_eval.trulens_eval.feedback.OpenAI.moderation_not_sexual","title":"<code>moderation_not_sexual(text)</code>","text":"<p>Uses OpenAI's Moderation API. A function that checks if text is sexual speech.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>Text to evaluate.</p> required <p>Returns:</p> Name Type Description <code>float</code> <code>float</code> <p>A value between 0 and 1. 0 being \"sexual\" and 1 being \"not</p> <code>float</code> <p>sexual\".</p> Source code in <code>trulens_eval/trulens_eval/feedback.py</code> <pre><code>def moderation_not_sexual(self, text: str) -&gt; float:\n\"\"\"\n    Uses OpenAI's Moderation API. A function that checks if text is sexual\n    speech.\n\n    Parameters:\n        text (str): Text to evaluate.\n\n    Returns:\n        float: A value between 0 and 1. 0 being \"sexual\" and 1 being \"not\n        sexual\".\n    \"\"\"\n    openai_response = self._moderation(text)\n\n    return 1 - int(\n        openai_response[\"results\"][0][\"category_scores\"][\"sexual\"]\n    )\n</code></pre>"},{"location":"trulens_eval/api/feedback/#trulens_eval.trulens_eval.feedback.OpenAI.moderation_not_sexualminors","title":"<code>moderation_not_sexualminors(text)</code>","text":"<p>Uses OpenAI's Moderation API. A function that checks if text is about sexual minors.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>Text to evaluate.</p> required <p>Returns:</p> Name Type Description <code>float</code> <code>float</code> <p>A value between 0 and 1. 0 being \"sexual minors\" and 1 being</p> <code>float</code> <p>\"not sexual minors\".</p> Source code in <code>trulens_eval/trulens_eval/feedback.py</code> <pre><code>def moderation_not_sexualminors(self, text: str) -&gt; float:\n\"\"\"\n    Uses OpenAI's Moderation API. A function that checks if text is about\n    sexual minors.\n\n    Parameters:\n        text (str): Text to evaluate.\n\n    Returns:\n        float: A value between 0 and 1. 0 being \"sexual minors\" and 1 being\n        \"not sexual minors\".\n    \"\"\"\n    openai_response = self._moderation(text)\n\n    return 1 - int(\n        openai_response[\"results\"][0][\"category_scores\"][\"sexual/minors\"]\n    )\n</code></pre>"},{"location":"trulens_eval/api/feedback/#trulens_eval.trulens_eval.feedback.OpenAI.moderation_not_violence","title":"<code>moderation_not_violence(text)</code>","text":"<p>Uses OpenAI's Moderation API. A function that checks if text is about violence.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>Text to evaluate.</p> required <p>Returns:</p> Name Type Description <code>float</code> <code>float</code> <p>A value between 0 and 1. 0 being \"violence\" and 1 being \"not</p> <code>float</code> <p>violence\".</p> Source code in <code>trulens_eval/trulens_eval/feedback.py</code> <pre><code>def moderation_not_violence(self, text: str) -&gt; float:\n\"\"\"\n    Uses OpenAI's Moderation API. A function that checks if text is about\n    violence.\n\n    Parameters:\n        text (str): Text to evaluate.\n\n    Returns:\n        float: A value between 0 and 1. 0 being \"violence\" and 1 being \"not\n        violence\".\n    \"\"\"\n    openai_response = self._moderation(text)\n\n    return 1 - int(\n        openai_response[\"results\"][0][\"category_scores\"][\"violence\"]\n    )\n</code></pre>"},{"location":"trulens_eval/api/feedback/#trulens_eval.trulens_eval.feedback.OpenAI.moderation_not_violencegraphic","title":"<code>moderation_not_violencegraphic(text)</code>","text":"<p>Uses OpenAI's Moderation API. A function that checks if text is about graphic violence.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>Text to evaluate.</p> required <p>Returns:</p> Name Type Description <code>float</code> <code>float</code> <p>A value between 0 and 1. 0 being \"graphic violence\" and 1</p> <code>float</code> <p>being \"not graphic violence\".</p> Source code in <code>trulens_eval/trulens_eval/feedback.py</code> <pre><code>def moderation_not_violencegraphic(self, text: str) -&gt; float:\n\"\"\"\n    Uses OpenAI's Moderation API. A function that checks if text is about\n    graphic violence.\n\n    Parameters:\n        text (str): Text to evaluate.\n\n    Returns:\n        float: A value between 0 and 1. 0 being \"graphic violence\" and 1\n        being \"not graphic violence\".\n    \"\"\"\n    openai_response = self._moderation(text)\n\n    return 1 - int(\n        openai_response[\"results\"][0][\"category_scores\"][\"violence/graphic\"]\n    )\n</code></pre>"},{"location":"trulens_eval/api/feedback/#trulens_eval.trulens_eval.feedback.OpenAI.qs_relevance","title":"<code>qs_relevance(question, statement)</code>","text":"<p>Uses OpenAI's Chat Completion App. A function that completes a template to check the relevance of the statement to the question.</p> <p>Parameters:</p> Name Type Description Default <code>question</code> <code>str</code> <p>A question being asked. statement (str): A statement</p> required <p>Returns:</p> Name Type Description <code>float</code> <code>float</code> <p>A value between 0 and 1. 0 being \"not relevant\" and 1 being</p> <code>float</code> <p>\"relevant\".</p> Source code in <code>trulens_eval/trulens_eval/feedback.py</code> <pre><code>def qs_relevance(self, question: str, statement: str) -&gt; float:\n\"\"\"\n    Uses OpenAI's Chat Completion App. A function that completes a\n    template to check the relevance of the statement to the question.\n\n    Parameters:\n        question (str): A question being asked. statement (str): A statement\n        to the question.\n\n    Returns:\n        float: A value between 0 and 1. 0 being \"not relevant\" and 1 being\n        \"relevant\".\n    \"\"\"\n    return _re_1_10_rating(\n        self.endpoint.run_me(\n            lambda: self._create_chat_completion(\n                model=self.model_engine,\n                temperature=0.0,\n                messages=[\n                    {\n                        \"role\":\n                            \"system\",\n                        \"content\":\n                            str.format(\n                                feedback_prompts.QS_RELEVANCE,\n                                question=question,\n                                statement=statement\n                            )\n                    }\n                ]\n            )[\"choices\"][0][\"message\"][\"content\"]\n        )\n    ) / 10\n</code></pre>"},{"location":"trulens_eval/api/feedback/#trulens_eval.trulens_eval.feedback.OpenAI.relevance","title":"<code>relevance(prompt, response)</code>","text":"<p>Uses OpenAI's Chat Completion Model. A function that completes a template to check the relevance of the response to a prompt.</p> <p>Parameters:</p> Name Type Description Default <code>prompt</code> <code>str</code> <p>A text prompt to an agent. response (str): The agent's</p> required <p>Returns:</p> Name Type Description <code>float</code> <code>float</code> <p>A value between 0 and 1. 0 being \"not relevant\" and 1 being</p> <code>float</code> <p>\"relevant\".</p> Source code in <code>trulens_eval/trulens_eval/feedback.py</code> <pre><code>def relevance(self, prompt: str, response: str) -&gt; float:\n\"\"\"\n    Uses OpenAI's Chat Completion Model. A function that completes a\n    template to check the relevance of the response to a prompt.\n\n    Parameters:\n        prompt (str): A text prompt to an agent. response (str): The agent's\n        response to the prompt.\n\n    Returns:\n        float: A value between 0 and 1. 0 being \"not relevant\" and 1 being\n        \"relevant\".\n    \"\"\"\n    return _re_1_10_rating(\n        self.endpoint.run_me(\n            lambda: self._create_chat_completion(\n                model=self.model_engine,\n                temperature=0.0,\n                messages=[\n                    {\n                        \"role\":\n                            \"system\",\n                        \"content\":\n                            str.format(\n                                feedback_prompts.PR_RELEVANCE,\n                                prompt=prompt,\n                                response=response\n                            )\n                    },\n                ]\n            )[\"choices\"][0][\"message\"][\"content\"]\n        )\n    ) / 10\n</code></pre>"},{"location":"trulens_eval/api/feedback/#trulens_eval.trulens_eval.feedback.OpenAI.sentiment","title":"<code>sentiment(text)</code>","text":"<p>Uses OpenAI's Chat Completion Model. A function that completes a template to check the sentiment of some text.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>A prompt to an agent. response (str): The agent's</p> required <p>Returns:</p> Name Type Description <code>float</code> <code>float</code> <p>A value between 0 and 1. 0 being \"negative sentiment\" and 1</p> <code>float</code> <p>being \"positive sentiment\".</p> Source code in <code>trulens_eval/trulens_eval/feedback.py</code> <pre><code>def sentiment(self, text: str) -&gt; float:\n\"\"\"\n    Uses OpenAI's Chat Completion Model. A function that completes a\n    template to check the sentiment of some text.\n\n    Parameters:\n        text (str): A prompt to an agent. response (str): The agent's\n        response to the prompt.\n\n    Returns:\n        float: A value between 0 and 1. 0 being \"negative sentiment\" and 1\n        being \"positive sentiment\".\n    \"\"\"\n\n    return _re_1_10_rating(\n        self.endpoint.run_me(\n            lambda: self._create_chat_completion(\n                model=self.model_engine,\n                temperature=0.0,\n                messages=[\n                    {\n                        \"role\": \"system\",\n                        \"content\": feedback_prompts.SENTIMENT_SYSTEM_PROMPT\n                    }, {\n                        \"role\": \"user\",\n                        \"content\": text\n                    }\n                ]\n            )[\"choices\"][0][\"message\"][\"content\"]\n        )\n    )\n</code></pre>"},{"location":"trulens_eval/api/tru/","title":"Tru","text":"<p>         Bases: <code>SingletonPerName</code></p> <p>Tru is the main class that provides an entry points to trulens-eval. Tru lets you:</p> <ul> <li>Log app prompts and outputs</li> <li>Log app Metadata</li> <li>Run and log feedback functions</li> <li>Run streamlit dashboard to view experiment results</li> </ul> <p>All data is logged to the current working directory to default.sqlite.</p> Source code in <code>trulens_eval/trulens_eval/tru.py</code> <pre><code>class Tru(SingletonPerName):\n\"\"\"\n    Tru is the main class that provides an entry points to trulens-eval. Tru lets you:\n\n    * Log app prompts and outputs\n    * Log app Metadata\n    * Run and log feedback functions\n    * Run streamlit dashboard to view experiment results\n\n    All data is logged to the current working directory to default.sqlite.\n    \"\"\"\n    DEFAULT_DATABASE_FILE = \"default.sqlite\"\n\n    # Process or Thread of the deferred feedback function evaluator.\n    evaluator_proc = None\n\n    # Process of the dashboard app.\n    dashboard_proc = None\n\n    def Chain(self, chain, **kwargs):\n\"\"\"\n        Create a TruChain with database managed by self.\n        \"\"\"\n\n        from trulens_eval.tru_chain import TruChain\n\n        return TruChain(tru=self, app=chain, **kwargs)\n\n    def Llama(self, engine, **kwargs):\n\"\"\"\n        Create a llama_index engine with database managed by self.\n        \"\"\"\n\n        from trulens_eval.tru_llama import TruLlama\n\n        return TruLlama(tru=self, app=engine, **kwargs)\n\n    def __init__(self):\n\"\"\"\n        TruLens instrumentation, logging, and feedback functions for apps.\n        Creates a local database 'default.sqlite' in current working directory.\n        \"\"\"\n\n        if hasattr(self, \"db\"):\n            # Already initialized by SingletonByName mechanism.\n            return\n\n        self.db = LocalSQLite(filename=Path(Tru.DEFAULT_DATABASE_FILE))\n\n    def reset_database(self):\n\"\"\"\n        Reset the database. Clears all tables.\n        \"\"\"\n\n        self.db.reset_database()\n\n    def migrate_database(self):\n\"\"\"\n        Migrates the database. \n        \"\"\"\n\n        self.db.migrate_database()\n\n    def add_record(self, record: Optional[Record] = None, **kwargs):\n\"\"\"\n        Add a record to the database.\n\n        Parameters:\n\n        - record: Record\n\n        - **kwargs: Record fields.\n\n        Returns:\n            RecordID: Unique record identifier.\n\n        \"\"\"\n\n        if record is None:\n            record = Record(**kwargs)\n        else:\n            record.update(**kwargs)\n\n        return self.db.insert_record(record=record)\n\n    def run_feedback_functions(\n        self,\n        record: Record,\n        feedback_functions: Sequence[Feedback],\n        app: Optional[AppDefinition] = None,\n    ) -&gt; Sequence[JSON]:\n\"\"\"\n        Run a collection of feedback functions and report their result.\n\n        Parameters:\n\n            record (Record): The record on which to evaluate the feedback\n            functions.\n\n            app (App, optional): The app that produced the given record.\n            If not provided, it is looked up from the given database `db`.\n\n            feedback_functions (Sequence[Feedback]): A collection of feedback\n            functions to evaluate.\n\n        Returns nothing.\n        \"\"\"\n\n        app_id = record.app_id\n\n        if app is None:\n            app = self.db.get_app(app_id=app_id)\n            if app is None:\n                raise RuntimeError(\n                    \"App {app_id} not present in db. \"\n                    \"Either add it with `tru.add_app` or provide `app_json` to `tru.run_feedback_functions`.\"\n                )\n\n        else:\n            assert app_id == app.app_id, \"Record was produced by a different app.\"\n\n            if self.db.get_app(app_id=app.app_id) is None:\n                logger.warn(\n                    \"App {app_id} was not present in database. Adding it.\"\n                )\n                self.add_app(app=app)\n\n        evals = []\n\n        for func in feedback_functions:\n            evals.append(\n                TP().promise(lambda f: f.run(app=app, record=record), func)\n            )\n\n        evals = map(lambda p: p.get(), evals)\n\n        return list(evals)\n\n    def add_app(self, app: AppDefinition) -&gt; None:\n\"\"\"\n        Add a app to the database.        \n        \"\"\"\n\n        self.db.insert_app(app=app)\n\n    def add_feedback(\n        self, feedback_result: FeedbackResult = None, **kwargs\n    ) -&gt; None:\n\"\"\"\n        Add a single feedback result to the database.\n        \"\"\"\n\n        if feedback_result is None:\n            feedback_result = FeedbackResult(**kwargs)\n        else:\n            feedback_result.update(**kwargs)\n\n        self.db.insert_feedback(feedback_result=feedback_result)\n\n    def add_feedbacks(self, feedback_results: Iterable[FeedbackResult]) -&gt; None:\n\"\"\"\n        Add multiple feedback results to the database.\n        \"\"\"\n\n        for feedback_result in feedback_results:\n            self.add_feedback(feedback_result=feedback_result)\n\n    def get_app(self, app_id: Optional[str] = None) -&gt; JSON:\n\"\"\"\n        Look up a app from the database.\n        \"\"\"\n\n        # TODO: unserialize\n        return self.db.get_app(app_id)\n\n    def get_records_and_feedback(self, app_ids: List[str]):\n\"\"\"\n        Get records, their feeback results, and feedback names from the database.\n        \"\"\"\n\n        df, feedback_columns = self.db.get_records_and_feedback(app_ids)\n\n        return df, feedback_columns\n\n    def start_evaluator(self,\n                        restart=False,\n                        fork=False) -&gt; Union[Process, Thread]:\n\"\"\"\n        Start a deferred feedback function evaluation thread.\n        \"\"\"\n\n        assert not fork, \"Fork mode not yet implemented.\"\n\n        if self.evaluator_proc is not None:\n            if restart:\n                self.stop_evaluator()\n            else:\n                raise RuntimeError(\n                    \"Evaluator is already running in this process.\"\n                )\n\n        from trulens_eval.feedback import Feedback\n\n        if not fork:\n            self.evaluator_stop = threading.Event()\n\n        def runloop():\n            while fork or not self.evaluator_stop.is_set():\n                #print(\n                #    \"Looking for things to do. Stop me with `tru.stop_evaluator()`.\",\n                #    end=''\n                #)\n                started_count = Feedback.evaluate_deferred(tru=self)\n\n                if started_count &gt; 0:\n                    print(\n                        f\"{UNICODE_YIELD}{UNICODE_YIELD}{UNICODE_YIELD} Started {started_count} deferred feedback functions.\"\n                    )\n                    TP().finish()\n                    print(\n                        f\"{UNICODE_CHECK}{UNICODE_CHECK}{UNICODE_CHECK} Finished evaluating deferred feedback functions.\"\n                    )\n\n                if fork:\n                    sleep(10)\n                else:\n                    self.evaluator_stop.wait(10)\n\n            print(\"Evaluator stopped.\")\n\n        if fork:\n            proc = Process(target=runloop)\n        else:\n            proc = Thread(target=runloop)\n\n        # Start a persistent thread or process that evaluates feedback functions.\n\n        self.evaluator_proc = proc\n        proc.start()\n\n        return proc\n\n    def stop_evaluator(self):\n\"\"\"\n        Stop the deferred feedback evaluation thread.\n        \"\"\"\n\n        if self.evaluator_proc is None:\n            raise RuntimeError(\"Evaluator not running this process.\")\n\n        if isinstance(self.evaluator_proc, Process):\n            self.evaluator_proc.terminate()\n\n        elif isinstance(self.evaluator_proc, Thread):\n            self.evaluator_stop.set()\n            self.evaluator_proc.join()\n            self.evaluator_stop = None\n\n        self.evaluator_proc = None\n\n    def stop_dashboard(self, force: bool = False) -&gt; None:\n\"\"\"\n        Stop existing dashboard(s) if running.\n\n        Args:\n\n            - force: bool: Also try to find any other dashboard processes not\n              started in this notebook and shut them down too.\n\n        Raises:\n\n            - ValueError: Dashboard is not running.\n        \"\"\"\n        if Tru.dashboard_proc is None:\n            if not force:\n                raise ValueError(\n                    \"Dashboard not running in this workspace. \"\n                    \"You may be able to shut other instances by setting the `force` flag.\"\n                )\n\n            else:\n                if sys.platform.startswith(\"win\"):\n                    raise RuntimeError(\n                        \"Force stop option is not supported on windows.\"\n                    )\n\n                print(\"Force stopping dashboard ...\")\n                import os\n                import pwd  # PROBLEM: does not exist on windows\n\n                import psutil\n                username = pwd.getpwuid(os.getuid())[0]\n                for p in psutil.process_iter():\n                    try:\n                        cmd = \" \".join(p.cmdline())\n                        if \"streamlit\" in cmd and \"Leaderboard.py\" in cmd and p.username(\n                        ) == username:\n                            print(f\"killing {p}\")\n                            p.kill()\n                    except Exception as e:\n                        continue\n\n        else:\n            Tru.dashboard_proc.kill()\n            Tru.dashboard_proc = None\n\n    def run_dashboard(\n        self, force: bool = False, _dev: Optional[Path] = None\n    ) -&gt; Process:\n\"\"\"\n        Run a streamlit dashboard to view logged results and apps.\n\n        Args:\n\n            - force: bool: Stop existing dashboard(s) first.\n\n            - _dev: Optional[Path]: If given, run dashboard with the given\n              PYTHONPATH. This can be used to run the dashboard from outside of\n              its pip package installation folder.\n\n        Raises:\n\n            - ValueError: Dashboard is already running.\n\n        Returns:\n\n            - Process: Process containing streamlit dashboard.\n        \"\"\"\n\n        if force:\n            self.stop_dashboard(force=force)\n\n        if Tru.dashboard_proc is not None:\n            raise ValueError(\n                \"Dashboard already running. \"\n                \"Run tru.stop_dashboard() to stop existing dashboard.\"\n            )\n\n        print(\"Starting dashboard ...\")\n\n        # Create .streamlit directory if it doesn't exist\n        streamlit_dir = os.path.join(os.getcwd(), '.streamlit')\n        os.makedirs(streamlit_dir, exist_ok=True)\n\n        # Create config.toml file\n        config_path = os.path.join(streamlit_dir, 'config.toml')\n        with open(config_path, 'w') as f:\n            f.write('[theme]\\n')\n            f.write('primaryColor=\"#0A2C37\"\\n')\n            f.write('backgroundColor=\"#FFFFFF\"\\n')\n            f.write('secondaryBackgroundColor=\"F5F5F5\"\\n')\n            f.write('textColor=\"#0A2C37\"\\n')\n            f.write('font=\"sans serif\"\\n')\n\n        cred_path = os.path.join(streamlit_dir, 'credentials.toml')\n        with open(cred_path, 'w') as f:\n            f.write('[general]\\n')\n            f.write('email=\"\"\\n')\n\n        #run leaderboard with subprocess\n        leaderboard_path = pkg_resources.resource_filename(\n            'trulens_eval', 'Leaderboard.py'\n        )\n\n        env_opts = {}\n        if _dev is not None:\n            env_opts['env'] = os.environ\n            env_opts['env']['PYTHONPATH'] = str(_dev)\n\n        proc = subprocess.Popen(\n            [\"streamlit\", \"run\", \"--server.headless=True\", leaderboard_path],\n            stdout=subprocess.PIPE,\n            stderr=subprocess.PIPE,\n            text=True,\n            **env_opts\n        )\n\n        started = threading.Event()\n        tunnel_started = threading.Event()\n        if is_notebook():\n            out_stdout, out_stderr = setup_widget_stdout_stderr()\n        else:\n            out_stdout = None\n            out_stderr = None\n\n        IN_COLAB = 'google.colab' in sys.modules\n        if IN_COLAB:\n            tunnel_proc = subprocess.Popen(\n                [\"npx\", \"localtunnel\", \"--port\", \"8501\"],\n                stdout=subprocess.PIPE,\n                stderr=subprocess.PIPE,\n                text=True,\n                **env_opts\n            )\n\n            def listen_to_tunnel(proc: subprocess.Popen, pipe, out, started):\n                while proc.poll() is None:\n\n                    line = pipe.readline()\n                    if \"url\" in line:\n                        started.set()\n                        line = \"Go to this url and submit the ip given here. \" + line\n\n                    if out is not None:\n                        out.append_stdout(line)\n\n                    else:\n                        print(line)\n\n            Tru.tunnel_listener_stdout = Thread(\n                target=listen_to_tunnel,\n                args=(\n                    tunnel_proc, tunnel_proc.stdout, out_stdout, tunnel_started\n                )\n            )\n            Tru.tunnel_listener_stderr = Thread(\n                target=listen_to_tunnel,\n                args=(\n                    tunnel_proc, tunnel_proc.stderr, out_stderr, tunnel_started\n                )\n            )\n            Tru.tunnel_listener_stdout.start()\n            Tru.tunnel_listener_stderr.start()\n            if not tunnel_started.wait(timeout=DASHBOARD_START_TIMEOUT\n                                      ):  # This might not work on windows.\n                raise RuntimeError(\"Tunnel failed to start in time. \")\n\n        def listen_to_dashboard(proc: subprocess.Popen, pipe, out, started):\n            while proc.poll() is None:\n                line = pipe.readline()\n                if IN_COLAB:\n                    if \"External URL: \" in line:\n                        started.set()\n                        line = line.replace(\n                            \"External URL: http://\", \"Submit this IP Address: \"\n                        )\n                        line = line.replace(\":8501\", \"\")\n                        if out is not None:\n                            out.append_stdout(line)\n                        else:\n                            print(line)\n                else:\n                    if \"Network URL: \" in line:\n                        url = line.split(\": \")[1]\n                        url = url.rstrip()\n                        print(f\"Dashboard started at {url} .\")\n                        started.set()\n                    if out is not None:\n                        out.append_stdout(line)\n                    else:\n                        print(line)\n            if out is not None:\n                out.append_stdout(\"Dashboard closed.\")\n            else:\n                print(\"Dashboard closed.\")\n\n        Tru.dashboard_listener_stdout = Thread(\n            target=listen_to_dashboard,\n            args=(proc, proc.stdout, out_stdout, started)\n        )\n        Tru.dashboard_listener_stderr = Thread(\n            target=listen_to_dashboard,\n            args=(proc, proc.stderr, out_stderr, started)\n        )\n        Tru.dashboard_listener_stdout.start()\n        Tru.dashboard_listener_stderr.start()\n\n        Tru.dashboard_proc = proc\n\n        wait_period = DASHBOARD_START_TIMEOUT\n        if IN_COLAB:\n            # Need more time to setup 2 processes tunnel and dashboard\n            wait_period = wait_period * 3\n        if not started.wait(timeout=wait_period\n                           ):  # This might not work on windows.\n            raise RuntimeError(\n                \"Dashboard failed to start in time. \"\n                \"Please inspect dashboard logs for additional information.\"\n            )\n\n        return proc\n\n    start_dashboard = run_dashboard\n</code></pre>"},{"location":"trulens_eval/api/tru/#trulens_eval.trulens_eval.tru.Tru.Chain","title":"<code>Chain(chain, **kwargs)</code>","text":"<p>Create a TruChain with database managed by self.</p> Source code in <code>trulens_eval/trulens_eval/tru.py</code> <pre><code>def Chain(self, chain, **kwargs):\n\"\"\"\n    Create a TruChain with database managed by self.\n    \"\"\"\n\n    from trulens_eval.tru_chain import TruChain\n\n    return TruChain(tru=self, app=chain, **kwargs)\n</code></pre>"},{"location":"trulens_eval/api/tru/#trulens_eval.trulens_eval.tru.Tru.Llama","title":"<code>Llama(engine, **kwargs)</code>","text":"<p>Create a llama_index engine with database managed by self.</p> Source code in <code>trulens_eval/trulens_eval/tru.py</code> <pre><code>def Llama(self, engine, **kwargs):\n\"\"\"\n    Create a llama_index engine with database managed by self.\n    \"\"\"\n\n    from trulens_eval.tru_llama import TruLlama\n\n    return TruLlama(tru=self, app=engine, **kwargs)\n</code></pre>"},{"location":"trulens_eval/api/tru/#trulens_eval.trulens_eval.tru.Tru.__init__","title":"<code>__init__()</code>","text":"<p>TruLens instrumentation, logging, and feedback functions for apps. Creates a local database 'default.sqlite' in current working directory.</p> Source code in <code>trulens_eval/trulens_eval/tru.py</code> <pre><code>def __init__(self):\n\"\"\"\n    TruLens instrumentation, logging, and feedback functions for apps.\n    Creates a local database 'default.sqlite' in current working directory.\n    \"\"\"\n\n    if hasattr(self, \"db\"):\n        # Already initialized by SingletonByName mechanism.\n        return\n\n    self.db = LocalSQLite(filename=Path(Tru.DEFAULT_DATABASE_FILE))\n</code></pre>"},{"location":"trulens_eval/api/tru/#trulens_eval.trulens_eval.tru.Tru.add_app","title":"<code>add_app(app)</code>","text":"<p>Add a app to the database.</p> Source code in <code>trulens_eval/trulens_eval/tru.py</code> <pre><code>def add_app(self, app: AppDefinition) -&gt; None:\n\"\"\"\n    Add a app to the database.        \n    \"\"\"\n\n    self.db.insert_app(app=app)\n</code></pre>"},{"location":"trulens_eval/api/tru/#trulens_eval.trulens_eval.tru.Tru.add_feedback","title":"<code>add_feedback(feedback_result=None, **kwargs)</code>","text":"<p>Add a single feedback result to the database.</p> Source code in <code>trulens_eval/trulens_eval/tru.py</code> <pre><code>def add_feedback(\n    self, feedback_result: FeedbackResult = None, **kwargs\n) -&gt; None:\n\"\"\"\n    Add a single feedback result to the database.\n    \"\"\"\n\n    if feedback_result is None:\n        feedback_result = FeedbackResult(**kwargs)\n    else:\n        feedback_result.update(**kwargs)\n\n    self.db.insert_feedback(feedback_result=feedback_result)\n</code></pre>"},{"location":"trulens_eval/api/tru/#trulens_eval.trulens_eval.tru.Tru.add_feedbacks","title":"<code>add_feedbacks(feedback_results)</code>","text":"<p>Add multiple feedback results to the database.</p> Source code in <code>trulens_eval/trulens_eval/tru.py</code> <pre><code>def add_feedbacks(self, feedback_results: Iterable[FeedbackResult]) -&gt; None:\n\"\"\"\n    Add multiple feedback results to the database.\n    \"\"\"\n\n    for feedback_result in feedback_results:\n        self.add_feedback(feedback_result=feedback_result)\n</code></pre>"},{"location":"trulens_eval/api/tru/#trulens_eval.trulens_eval.tru.Tru.add_record","title":"<code>add_record(record=None, **kwargs)</code>","text":"<p>Add a record to the database.</p> <ul> <li> <p>record: Record</p> </li> <li> <p>**kwargs: Record fields.</p> </li> </ul> <p>Returns:</p> Name Type Description <code>RecordID</code> <p>Unique record identifier.</p> Source code in <code>trulens_eval/trulens_eval/tru.py</code> <pre><code>def add_record(self, record: Optional[Record] = None, **kwargs):\n\"\"\"\n    Add a record to the database.\n\n    Parameters:\n\n    - record: Record\n\n    - **kwargs: Record fields.\n\n    Returns:\n        RecordID: Unique record identifier.\n\n    \"\"\"\n\n    if record is None:\n        record = Record(**kwargs)\n    else:\n        record.update(**kwargs)\n\n    return self.db.insert_record(record=record)\n</code></pre>"},{"location":"trulens_eval/api/tru/#trulens_eval.trulens_eval.tru.Tru.get_app","title":"<code>get_app(app_id=None)</code>","text":"<p>Look up a app from the database.</p> Source code in <code>trulens_eval/trulens_eval/tru.py</code> <pre><code>def get_app(self, app_id: Optional[str] = None) -&gt; JSON:\n\"\"\"\n    Look up a app from the database.\n    \"\"\"\n\n    # TODO: unserialize\n    return self.db.get_app(app_id)\n</code></pre>"},{"location":"trulens_eval/api/tru/#trulens_eval.trulens_eval.tru.Tru.get_records_and_feedback","title":"<code>get_records_and_feedback(app_ids)</code>","text":"<p>Get records, their feeback results, and feedback names from the database.</p> Source code in <code>trulens_eval/trulens_eval/tru.py</code> <pre><code>def get_records_and_feedback(self, app_ids: List[str]):\n\"\"\"\n    Get records, their feeback results, and feedback names from the database.\n    \"\"\"\n\n    df, feedback_columns = self.db.get_records_and_feedback(app_ids)\n\n    return df, feedback_columns\n</code></pre>"},{"location":"trulens_eval/api/tru/#trulens_eval.trulens_eval.tru.Tru.migrate_database","title":"<code>migrate_database()</code>","text":"<p>Migrates the database.</p> Source code in <code>trulens_eval/trulens_eval/tru.py</code> <pre><code>def migrate_database(self):\n\"\"\"\n    Migrates the database. \n    \"\"\"\n\n    self.db.migrate_database()\n</code></pre>"},{"location":"trulens_eval/api/tru/#trulens_eval.trulens_eval.tru.Tru.reset_database","title":"<code>reset_database()</code>","text":"<p>Reset the database. Clears all tables.</p> Source code in <code>trulens_eval/trulens_eval/tru.py</code> <pre><code>def reset_database(self):\n\"\"\"\n    Reset the database. Clears all tables.\n    \"\"\"\n\n    self.db.reset_database()\n</code></pre>"},{"location":"trulens_eval/api/tru/#trulens_eval.trulens_eval.tru.Tru.run_dashboard","title":"<code>run_dashboard(force=False, _dev=None)</code>","text":"<p>Run a streamlit dashboard to view logged results and apps.</p> <p>Parameters:</p> Name Type Description Default <code>-</code> <code>force</code> <p>bool: Stop existing dashboard(s) first.</p> required <code>-</code> <code>_dev</code> <p>Optional[Path]: If given, run dashboard with the given PYTHONPATH. This can be used to run the dashboard from outside of its pip package installation folder.</p> required <p>Raises:</p> Type Description <code>-ValueError</code> <p>Dashboard is already running.</p> <p>Returns:</p> Type Description <code>Process</code> <ul> <li>Process: Process containing streamlit dashboard.</li> </ul> Source code in <code>trulens_eval/trulens_eval/tru.py</code> <pre><code>def run_dashboard(\n    self, force: bool = False, _dev: Optional[Path] = None\n) -&gt; Process:\n\"\"\"\n    Run a streamlit dashboard to view logged results and apps.\n\n    Args:\n\n        - force: bool: Stop existing dashboard(s) first.\n\n        - _dev: Optional[Path]: If given, run dashboard with the given\n          PYTHONPATH. This can be used to run the dashboard from outside of\n          its pip package installation folder.\n\n    Raises:\n\n        - ValueError: Dashboard is already running.\n\n    Returns:\n\n        - Process: Process containing streamlit dashboard.\n    \"\"\"\n\n    if force:\n        self.stop_dashboard(force=force)\n\n    if Tru.dashboard_proc is not None:\n        raise ValueError(\n            \"Dashboard already running. \"\n            \"Run tru.stop_dashboard() to stop existing dashboard.\"\n        )\n\n    print(\"Starting dashboard ...\")\n\n    # Create .streamlit directory if it doesn't exist\n    streamlit_dir = os.path.join(os.getcwd(), '.streamlit')\n    os.makedirs(streamlit_dir, exist_ok=True)\n\n    # Create config.toml file\n    config_path = os.path.join(streamlit_dir, 'config.toml')\n    with open(config_path, 'w') as f:\n        f.write('[theme]\\n')\n        f.write('primaryColor=\"#0A2C37\"\\n')\n        f.write('backgroundColor=\"#FFFFFF\"\\n')\n        f.write('secondaryBackgroundColor=\"F5F5F5\"\\n')\n        f.write('textColor=\"#0A2C37\"\\n')\n        f.write('font=\"sans serif\"\\n')\n\n    cred_path = os.path.join(streamlit_dir, 'credentials.toml')\n    with open(cred_path, 'w') as f:\n        f.write('[general]\\n')\n        f.write('email=\"\"\\n')\n\n    #run leaderboard with subprocess\n    leaderboard_path = pkg_resources.resource_filename(\n        'trulens_eval', 'Leaderboard.py'\n    )\n\n    env_opts = {}\n    if _dev is not None:\n        env_opts['env'] = os.environ\n        env_opts['env']['PYTHONPATH'] = str(_dev)\n\n    proc = subprocess.Popen(\n        [\"streamlit\", \"run\", \"--server.headless=True\", leaderboard_path],\n        stdout=subprocess.PIPE,\n        stderr=subprocess.PIPE,\n        text=True,\n        **env_opts\n    )\n\n    started = threading.Event()\n    tunnel_started = threading.Event()\n    if is_notebook():\n        out_stdout, out_stderr = setup_widget_stdout_stderr()\n    else:\n        out_stdout = None\n        out_stderr = None\n\n    IN_COLAB = 'google.colab' in sys.modules\n    if IN_COLAB:\n        tunnel_proc = subprocess.Popen(\n            [\"npx\", \"localtunnel\", \"--port\", \"8501\"],\n            stdout=subprocess.PIPE,\n            stderr=subprocess.PIPE,\n            text=True,\n            **env_opts\n        )\n\n        def listen_to_tunnel(proc: subprocess.Popen, pipe, out, started):\n            while proc.poll() is None:\n\n                line = pipe.readline()\n                if \"url\" in line:\n                    started.set()\n                    line = \"Go to this url and submit the ip given here. \" + line\n\n                if out is not None:\n                    out.append_stdout(line)\n\n                else:\n                    print(line)\n\n        Tru.tunnel_listener_stdout = Thread(\n            target=listen_to_tunnel,\n            args=(\n                tunnel_proc, tunnel_proc.stdout, out_stdout, tunnel_started\n            )\n        )\n        Tru.tunnel_listener_stderr = Thread(\n            target=listen_to_tunnel,\n            args=(\n                tunnel_proc, tunnel_proc.stderr, out_stderr, tunnel_started\n            )\n        )\n        Tru.tunnel_listener_stdout.start()\n        Tru.tunnel_listener_stderr.start()\n        if not tunnel_started.wait(timeout=DASHBOARD_START_TIMEOUT\n                                  ):  # This might not work on windows.\n            raise RuntimeError(\"Tunnel failed to start in time. \")\n\n    def listen_to_dashboard(proc: subprocess.Popen, pipe, out, started):\n        while proc.poll() is None:\n            line = pipe.readline()\n            if IN_COLAB:\n                if \"External URL: \" in line:\n                    started.set()\n                    line = line.replace(\n                        \"External URL: http://\", \"Submit this IP Address: \"\n                    )\n                    line = line.replace(\":8501\", \"\")\n                    if out is not None:\n                        out.append_stdout(line)\n                    else:\n                        print(line)\n            else:\n                if \"Network URL: \" in line:\n                    url = line.split(\": \")[1]\n                    url = url.rstrip()\n                    print(f\"Dashboard started at {url} .\")\n                    started.set()\n                if out is not None:\n                    out.append_stdout(line)\n                else:\n                    print(line)\n        if out is not None:\n            out.append_stdout(\"Dashboard closed.\")\n        else:\n            print(\"Dashboard closed.\")\n\n    Tru.dashboard_listener_stdout = Thread(\n        target=listen_to_dashboard,\n        args=(proc, proc.stdout, out_stdout, started)\n    )\n    Tru.dashboard_listener_stderr = Thread(\n        target=listen_to_dashboard,\n        args=(proc, proc.stderr, out_stderr, started)\n    )\n    Tru.dashboard_listener_stdout.start()\n    Tru.dashboard_listener_stderr.start()\n\n    Tru.dashboard_proc = proc\n\n    wait_period = DASHBOARD_START_TIMEOUT\n    if IN_COLAB:\n        # Need more time to setup 2 processes tunnel and dashboard\n        wait_period = wait_period * 3\n    if not started.wait(timeout=wait_period\n                       ):  # This might not work on windows.\n        raise RuntimeError(\n            \"Dashboard failed to start in time. \"\n            \"Please inspect dashboard logs for additional information.\"\n        )\n\n    return proc\n</code></pre>"},{"location":"trulens_eval/api/tru/#trulens_eval.trulens_eval.tru.Tru.run_feedback_functions","title":"<code>run_feedback_functions(record, feedback_functions, app=None)</code>","text":"<p>Run a collection of feedback functions and report their result.</p> <p>Parameters:</p> Name Type Description Default <code>record</code> <code>Record</code> <p>The record on which to evaluate the feedback</p> required <code>app</code> <code>App</code> <p>The app that produced the given record.</p> <code>None</code> <code>feedback_functions</code> <code>Sequence[Feedback]</code> <p>A collection of feedback</p> required <p>Returns nothing.</p> Source code in <code>trulens_eval/trulens_eval/tru.py</code> <pre><code>def run_feedback_functions(\n    self,\n    record: Record,\n    feedback_functions: Sequence[Feedback],\n    app: Optional[AppDefinition] = None,\n) -&gt; Sequence[JSON]:\n\"\"\"\n    Run a collection of feedback functions and report their result.\n\n    Parameters:\n\n        record (Record): The record on which to evaluate the feedback\n        functions.\n\n        app (App, optional): The app that produced the given record.\n        If not provided, it is looked up from the given database `db`.\n\n        feedback_functions (Sequence[Feedback]): A collection of feedback\n        functions to evaluate.\n\n    Returns nothing.\n    \"\"\"\n\n    app_id = record.app_id\n\n    if app is None:\n        app = self.db.get_app(app_id=app_id)\n        if app is None:\n            raise RuntimeError(\n                \"App {app_id} not present in db. \"\n                \"Either add it with `tru.add_app` or provide `app_json` to `tru.run_feedback_functions`.\"\n            )\n\n    else:\n        assert app_id == app.app_id, \"Record was produced by a different app.\"\n\n        if self.db.get_app(app_id=app.app_id) is None:\n            logger.warn(\n                \"App {app_id} was not present in database. Adding it.\"\n            )\n            self.add_app(app=app)\n\n    evals = []\n\n    for func in feedback_functions:\n        evals.append(\n            TP().promise(lambda f: f.run(app=app, record=record), func)\n        )\n\n    evals = map(lambda p: p.get(), evals)\n\n    return list(evals)\n</code></pre>"},{"location":"trulens_eval/api/tru/#trulens_eval.trulens_eval.tru.Tru.start_evaluator","title":"<code>start_evaluator(restart=False, fork=False)</code>","text":"<p>Start a deferred feedback function evaluation thread.</p> Source code in <code>trulens_eval/trulens_eval/tru.py</code> <pre><code>def start_evaluator(self,\n                    restart=False,\n                    fork=False) -&gt; Union[Process, Thread]:\n\"\"\"\n    Start a deferred feedback function evaluation thread.\n    \"\"\"\n\n    assert not fork, \"Fork mode not yet implemented.\"\n\n    if self.evaluator_proc is not None:\n        if restart:\n            self.stop_evaluator()\n        else:\n            raise RuntimeError(\n                \"Evaluator is already running in this process.\"\n            )\n\n    from trulens_eval.feedback import Feedback\n\n    if not fork:\n        self.evaluator_stop = threading.Event()\n\n    def runloop():\n        while fork or not self.evaluator_stop.is_set():\n            #print(\n            #    \"Looking for things to do. Stop me with `tru.stop_evaluator()`.\",\n            #    end=''\n            #)\n            started_count = Feedback.evaluate_deferred(tru=self)\n\n            if started_count &gt; 0:\n                print(\n                    f\"{UNICODE_YIELD}{UNICODE_YIELD}{UNICODE_YIELD} Started {started_count} deferred feedback functions.\"\n                )\n                TP().finish()\n                print(\n                    f\"{UNICODE_CHECK}{UNICODE_CHECK}{UNICODE_CHECK} Finished evaluating deferred feedback functions.\"\n                )\n\n            if fork:\n                sleep(10)\n            else:\n                self.evaluator_stop.wait(10)\n\n        print(\"Evaluator stopped.\")\n\n    if fork:\n        proc = Process(target=runloop)\n    else:\n        proc = Thread(target=runloop)\n\n    # Start a persistent thread or process that evaluates feedback functions.\n\n    self.evaluator_proc = proc\n    proc.start()\n\n    return proc\n</code></pre>"},{"location":"trulens_eval/api/tru/#trulens_eval.trulens_eval.tru.Tru.stop_dashboard","title":"<code>stop_dashboard(force=False)</code>","text":"<p>Stop existing dashboard(s) if running.</p> <p>Parameters:</p> Name Type Description Default <code>-</code> <code>force</code> <p>bool: Also try to find any other dashboard processes not started in this notebook and shut them down too.</p> required <p>Raises:</p> Type Description <code>-ValueError</code> <p>Dashboard is not running.</p> Source code in <code>trulens_eval/trulens_eval/tru.py</code> <pre><code>def stop_dashboard(self, force: bool = False) -&gt; None:\n\"\"\"\n    Stop existing dashboard(s) if running.\n\n    Args:\n\n        - force: bool: Also try to find any other dashboard processes not\n          started in this notebook and shut them down too.\n\n    Raises:\n\n        - ValueError: Dashboard is not running.\n    \"\"\"\n    if Tru.dashboard_proc is None:\n        if not force:\n            raise ValueError(\n                \"Dashboard not running in this workspace. \"\n                \"You may be able to shut other instances by setting the `force` flag.\"\n            )\n\n        else:\n            if sys.platform.startswith(\"win\"):\n                raise RuntimeError(\n                    \"Force stop option is not supported on windows.\"\n                )\n\n            print(\"Force stopping dashboard ...\")\n            import os\n            import pwd  # PROBLEM: does not exist on windows\n\n            import psutil\n            username = pwd.getpwuid(os.getuid())[0]\n            for p in psutil.process_iter():\n                try:\n                    cmd = \" \".join(p.cmdline())\n                    if \"streamlit\" in cmd and \"Leaderboard.py\" in cmd and p.username(\n                    ) == username:\n                        print(f\"killing {p}\")\n                        p.kill()\n                except Exception as e:\n                    continue\n\n    else:\n        Tru.dashboard_proc.kill()\n        Tru.dashboard_proc = None\n</code></pre>"},{"location":"trulens_eval/api/tru/#trulens_eval.trulens_eval.tru.Tru.stop_evaluator","title":"<code>stop_evaluator()</code>","text":"<p>Stop the deferred feedback evaluation thread.</p> Source code in <code>trulens_eval/trulens_eval/tru.py</code> <pre><code>def stop_evaluator(self):\n\"\"\"\n    Stop the deferred feedback evaluation thread.\n    \"\"\"\n\n    if self.evaluator_proc is None:\n        raise RuntimeError(\"Evaluator not running this process.\")\n\n    if isinstance(self.evaluator_proc, Process):\n        self.evaluator_proc.terminate()\n\n    elif isinstance(self.evaluator_proc, Thread):\n        self.evaluator_stop.set()\n        self.evaluator_proc.join()\n        self.evaluator_stop = None\n\n    self.evaluator_proc = None\n</code></pre>"},{"location":"trulens_eval/api/trubasicapp/","title":"Tru Basic App","text":""},{"location":"trulens_eval/api/trubasicapp/#trulens_eval.trulens_eval.tru_basic_app--basic-input-output-instrumentation-and-monitoring","title":"Basic input output instrumentation and monitoring.","text":""},{"location":"trulens_eval/api/trubasicapp/#trulens_eval.trulens_eval.tru_basic_app.TruBasicApp","title":"<code>TruBasicApp</code>","text":"<p>         Bases: <code>App</code></p> <p>A Basic app that makes little assumptions. Assumes input text and output text</p> Source code in <code>trulens_eval/trulens_eval/tru_basic_app.py</code> <pre><code>class TruBasicApp(App):\n\"\"\"\n    A Basic app that makes little assumptions. Assumes input text and output text \n    \"\"\"\n\n    app: TruWrapperApp\n\n    root_callable: ClassVar[FunctionOrMethod] = Field(\n        default_factory=lambda: FunctionOrMethod.of_callable(TruBasicApp._call),\n        const=True\n    )\n\n    def __init__(self, text_to_text: Callable, **kwargs):\n\"\"\"\n        Wrap a callable for monitoring.\n\n        Arguments:\n        - text_to_text: A callable string to string\n        - More args in App\n        - More args in AppDefinition\n        - More args in WithClassInfo\n        \"\"\"\n        assert isinstance(text_to_text(\"This should return a string\"), str)\n        super().update_forward_refs()\n        app = TruWrapperApp(text_to_text)\n        kwargs['app'] = TruWrapperApp(text_to_text)\n        kwargs['root_class'] = Class.of_object(app)\n        kwargs['instrument'] = TruBasicCallableInstrument()\n        super().__init__(**kwargs)\n\n    def call_with_record(self, input: str, **kwargs):\n\"\"\" Run the callable and pass any kwargs.\n\n        Returns:\n            dict: record metadata\n        \"\"\"\n\n        # Wrapped calls will look this up by traversing the call stack. This\n        # should work with threads.\n        record: Sequence[RecordAppCall] = []\n\n        ret = None\n        error = None\n\n        cost: Cost = Cost()\n\n        start_time = None\n        end_time = None\n\n        try:\n            start_time = datetime.now()\n            ret, cost = Endpoint.track_all_costs_tally(\n                lambda: self.app._call(input, **kwargs)\n            )\n            end_time = datetime.now()\n\n        except BaseException as e:\n            end_time = datetime.now()\n            error = e\n            logger.error(f\"App raised an exception: {e}\")\n\n        assert len(record) &gt; 0, \"No information recorded in call.\"\n\n        ret_record_args = dict()\n\n        ret_record_args['main_input'] = input\n        if ret is not None:\n            ret_record_args['main_output'] = ret\n\n        ret_record = self._post_record(\n            ret_record_args, error, cost, start_time, end_time, record\n        )\n\n        return ret, ret_record\n</code></pre>"},{"location":"trulens_eval/api/trubasicapp/#trulens_eval.trulens_eval.tru_basic_app.TruBasicApp.__init__","title":"<code>__init__(text_to_text, **kwargs)</code>","text":"<p>Wrap a callable for monitoring.</p> <ul> <li>text_to_text: A callable string to string</li> <li>More args in App</li> <li>More args in AppDefinition</li> <li>More args in WithClassInfo</li> </ul> Source code in <code>trulens_eval/trulens_eval/tru_basic_app.py</code> <pre><code>def __init__(self, text_to_text: Callable, **kwargs):\n\"\"\"\n    Wrap a callable for monitoring.\n\n    Arguments:\n    - text_to_text: A callable string to string\n    - More args in App\n    - More args in AppDefinition\n    - More args in WithClassInfo\n    \"\"\"\n    assert isinstance(text_to_text(\"This should return a string\"), str)\n    super().update_forward_refs()\n    app = TruWrapperApp(text_to_text)\n    kwargs['app'] = TruWrapperApp(text_to_text)\n    kwargs['root_class'] = Class.of_object(app)\n    kwargs['instrument'] = TruBasicCallableInstrument()\n    super().__init__(**kwargs)\n</code></pre>"},{"location":"trulens_eval/api/trubasicapp/#trulens_eval.trulens_eval.tru_basic_app.TruBasicApp.call_with_record","title":"<code>call_with_record(input, **kwargs)</code>","text":"<p>Run the callable and pass any kwargs.</p> <p>Returns:</p> Name Type Description <code>dict</code> <p>record metadata</p> Source code in <code>trulens_eval/trulens_eval/tru_basic_app.py</code> <pre><code>def call_with_record(self, input: str, **kwargs):\n\"\"\" Run the callable and pass any kwargs.\n\n    Returns:\n        dict: record metadata\n    \"\"\"\n\n    # Wrapped calls will look this up by traversing the call stack. This\n    # should work with threads.\n    record: Sequence[RecordAppCall] = []\n\n    ret = None\n    error = None\n\n    cost: Cost = Cost()\n\n    start_time = None\n    end_time = None\n\n    try:\n        start_time = datetime.now()\n        ret, cost = Endpoint.track_all_costs_tally(\n            lambda: self.app._call(input, **kwargs)\n        )\n        end_time = datetime.now()\n\n    except BaseException as e:\n        end_time = datetime.now()\n        error = e\n        logger.error(f\"App raised an exception: {e}\")\n\n    assert len(record) &gt; 0, \"No information recorded in call.\"\n\n    ret_record_args = dict()\n\n    ret_record_args['main_input'] = input\n    if ret is not None:\n        ret_record_args['main_output'] = ret\n\n    ret_record = self._post_record(\n        ret_record_args, error, cost, start_time, end_time, record\n    )\n\n    return ret, ret_record\n</code></pre>"},{"location":"trulens_eval/api/truchain/","title":"Tru Chain","text":""},{"location":"trulens_eval/api/truchain/#trulens_eval.trulens_eval.tru_chain--langchain-instrumentation-and-monitoring","title":"Langchain instrumentation and monitoring.","text":""},{"location":"trulens_eval/api/truchain/#trulens_eval.trulens_eval.tru_chain.LangChainInstrument","title":"<code>LangChainInstrument</code>","text":"<p>         Bases: <code>Instrument</code></p> Source code in <code>trulens_eval/trulens_eval/tru_chain.py</code> <pre><code>class LangChainInstrument(Instrument):\n\n    class Default:\n        MODULES = {\"langchain.\"}\n\n        # Thunk because langchain is optional.\n        CLASSES = lambda: {\n            langchain.chains.base.Chain, langchain.vectorstores.base.\n            BaseRetriever, langchain.schema.BaseRetriever, langchain.llms.base.\n            BaseLLM, langchain.prompts.base.BasePromptTemplate, langchain.schema\n            .BaseMemory, langchain.schema.BaseChatMessageHistory\n        }\n\n        # Instrument only methods with these names and of these classes.\n        METHODS = {\n            \"_call\": lambda o: isinstance(o, langchain.chains.base.Chain),\n            # \"get_relevant_documents\": lambda o: True,  # VectorStoreRetriever\n            \"_get_relevant_documents\":\n                lambda o: True,  # VectorStoreRetriever, langchain &gt;= 0.230\n        }\n\n    def __init__(self):\n        super().__init__(\n            root_method=TruChain.call_with_record,\n            modules=LangChainInstrument.Default.MODULES,\n            classes=LangChainInstrument.Default.CLASSES(),\n            methods=LangChainInstrument.Default.METHODS\n        )\n\n    def _instrument_dict(self, cls, obj: Any, with_class_info: bool = False):\n\"\"\"\n        Replacement for langchain's dict method to one that does not fail under\n        non-serialization situations.\n        \"\"\"\n\n        return jsonify\n\n    def _instrument_type_method(self, obj, prop):\n\"\"\"\n        Instrument the Langchain class's method _*_type which is presently used\n        to control chain saving. Override the exception behaviour. Note that\n        _chain_type is defined as a property in langchain.\n        \"\"\"\n\n        # Properties doesn't let us new define new attributes like \"_instrument\"\n        # so we put it on fget instead.\n        if hasattr(prop.fget, Instrument.INSTRUMENT):\n            prop = getattr(prop.fget, Instrument.INSTRUMENT)\n\n        def safe_type(s) -&gt; Union[str, Dict]:\n            # self should be chain\n            try:\n                ret = prop.fget(s)\n                return ret\n\n            except NotImplementedError as e:\n\n                return noserio(obj, error=f\"{e.__class__.__name__}='{str(e)}'\")\n\n        safe_type._instrumented = prop\n        new_prop = property(fget=safe_type)\n\n        return new_prop\n</code></pre>"},{"location":"trulens_eval/api/truchain/#trulens_eval.trulens_eval.tru_chain.TruChain","title":"<code>TruChain</code>","text":"<p>         Bases: <code>App</code></p> <p>Wrap a langchain Chain to capture its configuration and evaluation steps.</p> Source code in <code>trulens_eval/trulens_eval/tru_chain.py</code> <pre><code>class TruChain(App):\n\"\"\"\n    Wrap a langchain Chain to capture its configuration and evaluation steps. \n    \"\"\"\n\n    app: Chain\n\n    root_callable: ClassVar[FunctionOrMethod] = Field(\n        default_factory=lambda: FunctionOrMethod.of_callable(TruChain._call),\n        const=True\n    )\n\n    # Normally pydantic does not like positional args but chain here is\n    # important enough to make an exception.\n    def __init__(self, app: Chain, **kwargs):\n\"\"\"\n        Wrap a langchain chain for monitoring.\n\n        Arguments:\n        - app: Chain -- the chain to wrap.\n        - More args in App\n        - More args in AppDefinition\n        - More args in WithClassInfo\n        \"\"\"\n\n        super().update_forward_refs()\n\n        # TruChain specific:\n        kwargs['app'] = app\n        kwargs['root_class'] = Class.of_object(app)\n        kwargs['instrument'] = LangChainInstrument()\n\n        super().__init__(**kwargs)\n\n    # Chain requirement\n    @property\n    def _chain_type(self):\n        return \"TruChain\"\n\n    # Chain requirement\n    @property\n    def input_keys(self) -&gt; List[str]:\n        return self.app.input_keys\n\n    # Chain requirement\n    @property\n    def output_keys(self) -&gt; List[str]:\n        return self.app.output_keys\n\n    def __getattr__(self, __name: str) -&gt; Any:\n        # A message for cases where a user calls something that the wrapped\n        # chain has but we do not wrap yet.\n\n        if hasattr(self.app, __name):\n            return RuntimeError(\n                f\"TruChain has no attribute {__name} but the wrapped app ({type(self.app)}) does. \",\n                f\"If you are calling a {type(self.app)} method, retrieve it from that app instead of from `TruChain`. \"\n                f\"TruChain only wraps the the Chain.__call__ and Chain._call methods presently.\"\n            )\n        else:\n            raise RuntimeError(f\"TruChain has no attribute named {__name}.\")\n\n    # NOTE: Input signature compatible with langchain.chains.base.Chain.__call__\n    def call_with_record(self, inputs: Union[Dict[str, Any], Any], **kwargs):\n\"\"\" Run the chain and also return a record metadata object.\n\n        Returns:\n            Any: chain output\n            dict: record metadata\n        \"\"\"\n\n        # Wrapped calls will look this up by traversing the call stack. This\n        # should work with threads.\n        record: Sequence[RecordAppCall] = []\n\n        ret = None\n        error = None\n\n        cost: Cost = Cost()\n\n        start_time = None\n        end_time = None\n\n        try:\n            # TODO: do this only if there is an openai model inside the chain:\n            with get_openai_callback() as cb:\n                start_time = datetime.now()\n                ret, cost = Endpoint.track_all_costs_tally(\n                    lambda: self.app.__call__(inputs=inputs, **kwargs)\n                )\n                end_time = datetime.now()\n\n        except BaseException as e:\n            end_time = datetime.now()\n            error = e\n            logger.error(f\"App raised an exception: {e}\")\n\n        assert len(record) &gt; 0, \"No information recorded in call.\"\n\n        ret_record_args = dict()\n\n        inputs = self.app.prep_inputs(inputs)\n\n        # Figure out the content of the \"inputs\" arg that __call__ constructs\n        # for _call so we can lookup main input and output.\n        input_key = self.input_keys[0]\n        output_key = self.output_keys[0]\n\n        ret_record_args['main_input'] = jsonify(inputs[input_key])\n\n        if ret is not None:\n            ret_record_args['main_output'] = jsonify(ret[output_key])\n\n        if error is not None:\n            ret_record_args['main_error'] = jsonify(error)\n\n        ret_record = self._post_record(\n            ret_record_args, error, cost, start_time, end_time, record\n        )\n\n        return ret, ret_record\n\n    # langchain.chains.base.py:Chain requirement:\n    def __call__(self, *args, **kwargs) -&gt; Dict[str, Any]:\n\"\"\"\n        Wrapped call to self.app.__call__ with instrumentation. If you need to\n        get the record, use `call_with_record` instead. \n        \"\"\"\n\n        ret, _ = self.call_with_record(*args, **kwargs)\n\n        return ret\n\n    # langchain.chains.base.py:Chain requirement:\n    def _call(self, *args, **kwargs) -&gt; Any:\n        # TODO(piotrm): figure out whether the combination of _call and __call__ is\n        # working right.\n\n        # TODO(piotrm): potentially remove this. We don't want to be\n        # wrapping/passing through all of the methods that a langchain Chain\n        # supports.\n\n        return self.app._call(*args, **kwargs)\n</code></pre>"},{"location":"trulens_eval/api/truchain/#trulens_eval.trulens_eval.tru_chain.TruChain.__call__","title":"<code>__call__(*args, **kwargs)</code>","text":"<p>Wrapped call to self.app.call with instrumentation. If you need to get the record, use <code>call_with_record</code> instead.</p> Source code in <code>trulens_eval/trulens_eval/tru_chain.py</code> <pre><code>def __call__(self, *args, **kwargs) -&gt; Dict[str, Any]:\n\"\"\"\n    Wrapped call to self.app.__call__ with instrumentation. If you need to\n    get the record, use `call_with_record` instead. \n    \"\"\"\n\n    ret, _ = self.call_with_record(*args, **kwargs)\n\n    return ret\n</code></pre>"},{"location":"trulens_eval/api/truchain/#trulens_eval.trulens_eval.tru_chain.TruChain.__init__","title":"<code>__init__(app, **kwargs)</code>","text":"<p>Wrap a langchain chain for monitoring.</p> <ul> <li>app: Chain -- the chain to wrap.</li> <li>More args in App</li> <li>More args in AppDefinition</li> <li>More args in WithClassInfo</li> </ul> Source code in <code>trulens_eval/trulens_eval/tru_chain.py</code> <pre><code>def __init__(self, app: Chain, **kwargs):\n\"\"\"\n    Wrap a langchain chain for monitoring.\n\n    Arguments:\n    - app: Chain -- the chain to wrap.\n    - More args in App\n    - More args in AppDefinition\n    - More args in WithClassInfo\n    \"\"\"\n\n    super().update_forward_refs()\n\n    # TruChain specific:\n    kwargs['app'] = app\n    kwargs['root_class'] = Class.of_object(app)\n    kwargs['instrument'] = LangChainInstrument()\n\n    super().__init__(**kwargs)\n</code></pre>"},{"location":"trulens_eval/api/truchain/#trulens_eval.trulens_eval.tru_chain.TruChain.call_with_record","title":"<code>call_with_record(inputs, **kwargs)</code>","text":"<p>Run the chain and also return a record metadata object.</p> <p>Returns:</p> Name Type Description <code>Any</code> <p>chain output</p> <code>dict</code> <p>record metadata</p> Source code in <code>trulens_eval/trulens_eval/tru_chain.py</code> <pre><code>def call_with_record(self, inputs: Union[Dict[str, Any], Any], **kwargs):\n\"\"\" Run the chain and also return a record metadata object.\n\n    Returns:\n        Any: chain output\n        dict: record metadata\n    \"\"\"\n\n    # Wrapped calls will look this up by traversing the call stack. This\n    # should work with threads.\n    record: Sequence[RecordAppCall] = []\n\n    ret = None\n    error = None\n\n    cost: Cost = Cost()\n\n    start_time = None\n    end_time = None\n\n    try:\n        # TODO: do this only if there is an openai model inside the chain:\n        with get_openai_callback() as cb:\n            start_time = datetime.now()\n            ret, cost = Endpoint.track_all_costs_tally(\n                lambda: self.app.__call__(inputs=inputs, **kwargs)\n            )\n            end_time = datetime.now()\n\n    except BaseException as e:\n        end_time = datetime.now()\n        error = e\n        logger.error(f\"App raised an exception: {e}\")\n\n    assert len(record) &gt; 0, \"No information recorded in call.\"\n\n    ret_record_args = dict()\n\n    inputs = self.app.prep_inputs(inputs)\n\n    # Figure out the content of the \"inputs\" arg that __call__ constructs\n    # for _call so we can lookup main input and output.\n    input_key = self.input_keys[0]\n    output_key = self.output_keys[0]\n\n    ret_record_args['main_input'] = jsonify(inputs[input_key])\n\n    if ret is not None:\n        ret_record_args['main_output'] = jsonify(ret[output_key])\n\n    if error is not None:\n        ret_record_args['main_error'] = jsonify(error)\n\n    ret_record = self._post_record(\n        ret_record_args, error, cost, start_time, end_time, record\n    )\n\n    return ret, ret_record\n</code></pre>"},{"location":"trulens_eval/api/trullama/","title":"Tru Llama","text":""},{"location":"trulens_eval/api/trullama/#trulens_eval.trulens_eval.tru_llama--llama_index-instrumentation-and-monitoring","title":"Llama_index instrumentation and monitoring.","text":""},{"location":"trulens_eval/api/trullama/#trulens_eval.trulens_eval.tru_llama.TruLlama","title":"<code>TruLlama</code>","text":"<p>         Bases: <code>App</code></p> <p>Wrap a llama index engine for monitoring.</p> <ul> <li>app: RetrieverQueryEngine -- the engine to wrap.</li> <li>More args in App</li> <li>More args in AppDefinition</li> <li>More args in WithClassInfo</li> </ul> Source code in <code>trulens_eval/trulens_eval/tru_llama.py</code> <pre><code>class TruLlama(App):\n\"\"\"\n    Wrap a llama index engine for monitoring.\n\n    Arguments:\n    - app: RetrieverQueryEngine -- the engine to wrap.\n    - More args in App\n    - More args in AppDefinition\n    - More args in WithClassInfo\n    \"\"\"\n\n    class Config:\n        arbitrary_types_allowed = True\n\n    app: BaseQueryEngine\n\n    root_callable: ClassVar[FunctionOrMethod] = Field(\n        default_factory=lambda: FunctionOrMethod.of_callable(TruLlama.query),\n        const=True\n    )\n\n    def __init__(self, app: BaseQueryEngine, **kwargs):\n\n        super().update_forward_refs()\n\n        # TruLlama specific:\n        kwargs['app'] = app\n        kwargs['root_class'] = Class.of_object(app)  # TODO: make class property\n        kwargs['instrument'] = LlamaInstrument()\n\n        super().__init__(**kwargs)\n\n    def query(self, *args, **kwargs) -&gt; Response:\n        res, _ = self.query_with_record(*args, **kwargs)\n        return res\n\n    @classmethod\n    def select_source_nodes(cls) -&gt; JSONPath:\n\"\"\"\n        Get the path to the source nodes in the query output.\n        \"\"\"\n        return cls.select_outputs().source_nodes[:]\n\n    def query_with_record(self, str_or_query_bundle) -&gt; Tuple[Response, Record]:\n        # Wrapped calls will look this up by traversing the call stack. This\n        # should work with threads.\n        record: Sequence[RecordAppCall] = []\n\n        ret = None\n        error = None\n\n        start_time = None\n        end_time = None\n\n        cost = Cost()\n\n        try:\n            start_time = datetime.now()\n            ret, cost = Endpoint.track_all_costs_tally(\n                lambda: self.app.query(str_or_query_bundle)\n            )\n\n            end_time = datetime.now()\n\n        except BaseException as e:\n            end_time = datetime.now()\n            error = e\n            logger.error(f\"Engine raised an exception: {e}\")\n\n        assert len(record) &gt; 0, \"No information recorded in call.\"\n\n        ret_record_args = dict()\n\n        # TODO: generalize\n        ret_record_args['main_input'] = str_or_query_bundle\n        if ret is not None:\n            # TODO: generalize and error check\n            ret_record_args['main_output'] = ret.response\n\n        ret_record = self._post_record(\n            ret_record_args, error, cost, start_time, end_time, record\n        )\n\n        return ret, ret_record\n</code></pre>"},{"location":"trulens_eval/api/trullama/#trulens_eval.trulens_eval.tru_llama.TruLlama.select_source_nodes","title":"<code>select_source_nodes()</code>  <code>classmethod</code>","text":"<p>Get the path to the source nodes in the query output.</p> Source code in <code>trulens_eval/trulens_eval/tru_llama.py</code> <pre><code>@classmethod\ndef select_source_nodes(cls) -&gt; JSONPath:\n\"\"\"\n    Get the path to the source nodes in the query output.\n    \"\"\"\n    return cls.select_outputs().source_nodes[:]\n</code></pre>"},{"location":"trulens_explain/attribution_parameterization/","title":"Attributions for Different Use Cases","text":""},{"location":"trulens_explain/attribution_parameterization/#attribution-parameterization","title":"Attribution Parameterization","text":"<p>Attributions for different models and use cases can range from simple to more complex. This page provides guidelines on how to set various attribution parameters to achieve your LLM explainability goals.</p>"},{"location":"trulens_explain/attribution_parameterization/#basic-definitions-and-terminology","title":"Basic Definitions and Terminology","text":"<p>What is a tensor? A tensor is a multidimensional object that can be model inputs, or layer activations.</p> <p>What is a layer? A layer is a set of neurons that can be thought of as a function on input tensors. Layer inputs are tensors. Layer outputs are modified tensors.</p> <p>What are anchors? Anchors are ways of specifying which tensors you want. You may want the input tensor of a layer, or the output tensor of a layer. </p> <p>E.g. Say you have a concat layer and you want to explain the 2 concatenated tensors. The concat operation is not usually a layer tracked by the model. If you try the 'in' anchor of the layer after the operation, you get a single tensor with all the information you need.</p> <p>What is a Quantity of Interest (QoI)? A QoI is a scalar number that is being explained. </p> <p>E.g. With saliency maps, you get <code>dx/dy</code> (i.e. the effect of input on output). <code>y</code> in this case is the QoI scalar. It is usually the output of a neuron, but could be a sum of multiple neurons.</p> <p>What is an attribution? An attribution is a numerical value associated with every element in a tensor that explains a QoI. </p> <p>E.g. With saliency maps, you get <code>dx/dy</code>. <code>x</code> is the associated tensor. The entirety of <code>dx/dy</code> is the explanation.</p> <p>What are cuts? Cuts are tensors that cut a network into two parts. They are composed of a layer and an anchor.</p> <p>What are slices? Slices are two cuts leaving a <code>slice</code> of the network. The attribution will be on the first cut, explaining the QoI on the second cut of the slice.</p> <p>E.g. With saliency maps, the TruLens slice would be AttributionCut: <code>Cut(x)</code> to QoICut: <code>Cut(y)</code>, denoted by <code>Slice(Cut(x),Cut(y))</code>.</p>"},{"location":"trulens_explain/attribution_parameterization/#how-to-use-trulens","title":"How to use TruLens?","text":"<p>This section will cover different use cases from the most basic to the most complex. For the following use cases, it may help to refer to Summary.</p>"},{"location":"trulens_explain/attribution_parameterization/#case-1-input-output-cut-basic-configuration","title":"Case 1: Input-Output cut (Basic configuration)","text":"<p>Use case: Explain the input given the output. Cuts needed: TruLens defaults. Attribution Cut (The tensor we would like to assign importance) \u2192 InputCut (model args / kwargs) QoI Cut (The tensor that we are interested to explain) \u2192 OutputCut</p>"},{"location":"trulens_explain/attribution_parameterization/#case-2-the-qoi-cut","title":"Case 2: The QoI Cut","text":"<p>Now suppose you want to explain some internal (intermediate) layer\u2019s output (i.e. how the input is affecting the output at some intermediate layer).</p> <p>Use case: Explain something that isn't the default model output. </p> <p>E.g. If you want to explain a logit layer instead of the probit (final) layer.</p> <p>Cuts needed: As you want to explain something different than the default output, you need to change the QoI from the default to the layer that you are interested. Attribution Cut \u2192 InputCut QoI Cut \u2192 Your logit layer, anchor:'out'</p>"},{"location":"trulens_explain/attribution_parameterization/#case-3-the-attribution-cut","title":"Case 3: The Attribution Cut","text":"<p>Now suppose you want to know the attribution of some internal layer on the final output. </p> <p>Use cases: </p> <ul> <li>As a preprocessing step, you drop a feature, so do not need attributions on that.</li> <li>For PyTorch models, model inputs are not tensors, so you'd want the 'in' anchor of the first layer.  </li> </ul> <p>Cuts needed: As you want to know the affect of some other layer rather than the input layer, you need to customize the attribution cut. Model inputs \u2192 InputCut Attribution Cut \u2192 Your attribution layer (The layer you want to assign importance/attributions with respect to output), anchor:'in' QoI Cut \u2192 OutputCut</p>"},{"location":"trulens_explain/attribution_parameterization/#advanced-use-cases","title":"Advanced Use Cases","text":"<p>For the following use cases, it may help to refer to Advanced Definitions.</p>"},{"location":"trulens_explain/attribution_parameterization/#case-4-the-distribution-of-interest-doi-cut-explanation-flexibility","title":"Case 4: The Distribution of Interest (DoI) Cut / Explanation flexibility","text":"<p>Usually, we explain the output with respect to each point in the input. All cases up to now were using a default called <code>PointDoI</code>. Now, suppose you want to explain using an aggregate over samples of points.  </p> <p>Use case: You want to perform approaches like Integrated Gradients, Grad-CAM, Shapley values instead of saliency maps. These only differ by sampling strategies.</p> <p>E.g. Integrated Gradients is a sample from a straight line from a baseline to a value.</p> <p>Cuts needed: Define a DoI that samples from the default attribution cut. Model inputs \u2192 InputCut DoI/Attribution Cut \u2192 Your baseline/DoI/attribution layer, anchor:'in' QoI Cut \u2192 OutputCut</p>"},{"location":"trulens_explain/attribution_parameterization/#case-5-internal-explanations","title":"Case 5: Internal explanations","text":"<p>Use case: You want to explain an internal layer. Methods like Integrated Gradients are a DoI on the baseline to the value, but it is located on the layer the baseline is defined. If you want to explain an internal layer, you do not move the DoI layer. Cuts needed: Attribution layer different from DoI. Model inputs \u2192 InputCut DoI Cut \u2192 Your baseline/DoI layer, anchor:'in' Attribution Cut \u2192 Your internal attribution layer, anchor:'out' or 'in' QoI Cut \u2192 OutputCut</p>"},{"location":"trulens_explain/attribution_parameterization/#case-6-your-baseline-happens-at-a-different-layer-than-your-sampling","title":"Case 6: Your baseline happens at a different layer than your sampling.","text":"<p>Use Case: in NLP, baselines are tokens, but the interpolation is on the embedding layer. Cuts needed: Baseline different from DoI. Model inputs \u2192 InputCut Baseline Cut \u2192 Tokens, anchor:'out' DoI/Attribution Cut \u2192 Embeddings, anchor:'out' QoI Cut \u2192 OutputCut</p>"},{"location":"trulens_explain/attribution_parameterization/#case-7-putting-it-together-the-most-complex-case-we-can-perform-with-trulens","title":"Case 7: Putting it together - The most complex case we can perform with TruLens","text":"<p>Use Case: Internal layer explanations of NLP, on the logit layer of a model with probit outputs. Model inputs \u2192 InputCut Baseline Cut \u2192 Tokens, anchor:'out' DoI Cut \u2192 Embeddings, anchor:'out' Attribution Cut \u2192 Internal layer, anchor:'out' QoI Cut \u2192 Logit layer, anchor:'out'</p>"},{"location":"trulens_explain/attribution_parameterization/#summary","title":"Summary","text":"<p>InputCut is model args / kwargs. OutputCut is the model output.</p> <p>Baseline Cut is the tensor associated with the Integrated Gradients baseline. Can be the InputCut or later. DoI Cut is the tensor associated with explanation sampling. Can be the BaselineCut or later. Attribution Cut is the tensor that should be explained. Can be the DoICut or later. QoI Cut is what is being explained with a QoI. Must be after the AttributionCut.</p>"},{"location":"trulens_explain/attribution_parameterization/#advanced-definitions","title":"Advanced Definitions","text":"<p>What is a Distribution of Interest (DoI)?</p> <p>The distribution of interest is a concept of aggregating attributions over a sample or distribution. </p> <ul> <li>Grad-CAM (Paper, GitHub, Docs) does this over a Gaussian distribution of inputs. </li> <li>Shapley values (GitHub, Docs) do this over different background data. </li> <li>Integrated Gradients (Paper, Tutorial) do this over an interpolation from a baseline to the input.</li> </ul> <p>How does this relate to the Attribution Cut?</p> <p>The sample or distributions are taken at a place that is humanly considered the input, even if this differs from the programmatic model input.</p> <p>For attributions, all parts of a network can have an attribution towards the QoI. The most common use case is to explain the tensors that are also humanly considered the input (which is where the DoI occurs).</p> <p>How does this relate to the Baseline Cut?</p> <p>The Baseline Cut is only applicable to the Integrated Gradients method. It is also only needed when there is no mathematical way to interpolate the baseline to the input.</p> <p>E.g. if the input is <code>'Hello'</code>, but the baseline is a <code>'[MASK]'</code> token, we cannot interpolate that. We define the baseline at the token layer, but interpolate on a numeric layer like the embeddings.</p>"},{"location":"trulens_explain/gh_top_intro/","title":"Gh top intro","text":""},{"location":"trulens_explain/gh_top_intro/#trulens-explain","title":"TruLens-Explain","text":"<p>TruLens-Explain is a cross-framework library for deep learning explainability. It provides a uniform abstraction over a number of different frameworks. It provides a uniform abstraction layer over TensorFlow, Pytorch, and Keras and allows input and internal explanations.</p>"},{"location":"trulens_explain/gh_top_intro/#get-going-with-trulens-explain","title":"Get going with TruLens-Explain","text":"<p>These installation instructions assume that you have conda installed and added to your path.</p> <ol> <li> <p>Create a virtual environment (or modify an existing one). <pre><code>conda create -n \"&lt;my_name&gt;\" python=3  # Skip if using existing environment.\nconda activate &lt;my_name&gt;\n</code></pre></p> </li> <li> <p>Install dependencies. <pre><code>conda install tensorflow-gpu=1  # Or whatever backend you're using.\nconda install keras             # Or whatever backend you're using.\nconda install matplotlib        # For visualizations.\n</code></pre></p> </li> <li> <p>[Pip installation] Install the trulens pip package from PyPI. <pre><code>pip install trulens\n</code></pre></p> </li> <li> <p>Get started! To quickly play around with the TruLens library, check out the following Colab notebooks:</p> </li> <li> <p>PyTorch: </p> </li> <li>TensorFlow 2 / Keras: </li> </ol> <p>For more information, see TruLens-Explain Documentation.</p>"},{"location":"trulens_explain/install/","title":"Installation","text":""},{"location":"trulens_explain/install/#getting-access-to-trulens","title":"Getting access to TruLens","text":"<p>These installation instructions assume that you have conda installed and added to your path.</p> <ol> <li> <p>Create a virtual environment (or modify an existing one). <pre><code>conda create -n \"&lt;my_name&gt;\" python=3.7  # Skip if using existing environment.\nconda activate &lt;my_name&gt;\n</code></pre></p> </li> <li> <p>Install dependencies. <pre><code>conda install tensorflow-gpu=1  # Or whatever backend you're using.\nconda install keras             # Or whatever backend you're using.\nconda install matplotlib        # For visualizations.\n</code></pre></p> </li> <li> <p>[Pip installation] Install the trulens pip package from PyPI. <pre><code>pip install trulens\n</code></pre></p> </li> <li> <p>[Local installation] If you would like to develop or modify TruLens, you can download the source code by cloning the TruLens repo. <pre><code>git clone https://github.com/truera/trulens.git\n</code></pre></p> </li> <li> <p>[Local installation] Install the TruLens repo. <pre><code>cd trulens_explain\npip install -e .\n</code></pre></p> </li> </ol>"},{"location":"trulens_explain/quickstart/","title":"Quickstart","text":""},{"location":"trulens_explain/quickstart/#quickstart","title":"Quickstart","text":""},{"location":"trulens_explain/quickstart/#playground","title":"Playground","text":"<p>To quickly play around with the TruLens library, check out the following Colab notebooks:</p> <ul> <li>PyTorch: </li> <li>TensorFlow 2 / Keras: </li> </ul>"},{"location":"trulens_explain/quickstart/#install-use","title":"Install &amp; Use","text":"<p>Check out the Installation instructions for information on how to install the library, use it, and contribute. </p>"},{"location":"trulens_explain/api/attribution/","title":"Attribution Methods","text":"<p>Attribution methods quantitatively measure the contribution of each of a  function's individual inputs to its output. Gradient-based attribution methods compute the gradient of a model with respect to its inputs to describe how important each input is towards the output prediction. These methods can be applied to assist in explaining deep networks.</p> <p>TruLens provides implementations of several such techniques, found in this package.</p>"},{"location":"trulens_explain/api/attribution/#trulens_explain.trulens.nn.attribution.AttributionMethod","title":"<code>AttributionMethod</code>","text":"<p>         Bases: <code>AbstractBaseClass</code></p> <p>Interface used by all attribution methods.</p> <p>An attribution method takes a neural network model and provides the ability to assign values to the variables of the network that specify the importance of each variable towards particular predictions.</p> Source code in <code>trulens_explain/trulens/nn/attribution.py</code> <pre><code>class AttributionMethod(AbstractBaseClass):\n\"\"\"\n    Interface used by all attribution methods.\n\n    An attribution method takes a neural network model and provides the ability\n    to assign values to the variables of the network that specify the importance\n    of each variable towards particular predictions.\n    \"\"\"\n\n    @abstractmethod\n    def __init__(\n        self, model: ModelWrapper, rebatch_size: int = None, *args, **kwargs\n    ):\n\"\"\"\n        Abstract constructor.\n\n        Parameters:\n            model: ModelWrapper\n                Model for which attributions are calculated.\n\n            rebatch_size: int (optional)\n                Will rebatch instances to this size if given. This may be\n                required for GPU usage if using a DoI which produces multiple\n                instances per user-provided instance. Many valued DoIs will\n                expand the tensors sent to each layer to original_batch_size *\n                doi_size. The rebatch size will break up original_batch_size *\n                doi_size into rebatch_size chunks to send to model.\n        \"\"\"\n        self._model = model\n\n        self.rebatch_size = rebatch_size\n\n    @property\n    def model(self) -&gt; ModelWrapper:\n\"\"\"\n        Model for which attributions are calculated.\n        \"\"\"\n        return self._model\n\n    @abstractmethod\n    def _attributions(self, model_inputs: ModelInputs) -&gt; AttributionResult:\n\"\"\"\n        For attributions that have options to return multiple things depending\n        on configuration, wrap those multiple things in the AttributionResult\n        tuple.\n        \"\"\"\n        ...\n\n    def attributions(\n        self, *model_args: ArgsLike, **model_kwargs: KwargsLike\n    ) -&gt; Union[TensorLike, ArgsLike[TensorLike],\n               ArgsLike[ArgsLike[TensorLike]]]:\n\"\"\"\n        Returns attributions for the given input. Attributions are in the same\n        shape as the layer that attributions are being generated for. \n\n        The numeric scale of the attributions will depend on the specific\n        implementations of the Distribution of Interest and Quantity of\n        Interest. However it is generally related to the scale of gradients on\n        the Quantity of Interest. \n\n        For example, Integrated Gradients uses the linear interpolation\n        Distribution of Interest which subsumes the completeness axiom which\n        ensures the sum of all attributions of a record equals the output\n        determined by the Quantity of Interest on the same record. \n\n        The Point Distribution of Interest will be determined by the gradient at\n        a single point, thus being a good measure of model sensitivity. \n\n        Parameters:\n            model_args: ArgsLike, model_kwargs: KwargsLike\n                The args and kwargs given to the call method of a model. This\n                should represent the records to obtain attributions for, assumed\n                to be a *batched* input. if `self.model` supports evaluation on\n                *data tensors*, the  appropriate tensor type may be used (e.g.,\n                Pytorch models may accept Pytorch tensors in addition to\n                `np.ndarray`s). The shape of the inputs must match the input\n                shape of `self.model`. \n\n        Returns\n            - np.ndarray when single attribution_cut input, single qoi output\n            - or ArgsLike[np.ndarray] when single input, multiple output (or\n              vice versa) \n            - or ArgsLike[ArgsLike[np.ndarray]] when multiple output (outer),\n              multiple input (inner)\n\n            An array of attributions, matching the shape and type of `from_cut`\n            of the slice. Each entry in the returned array represents the degree\n            to which the corresponding feature affected the model's outcome on\n            the corresponding point.\n\n            If attributing to a component with multiple inputs, a list for each\n            will be returned.\n\n            If the quantity of interest features multiple outputs, a list for\n            each will be returned.\n        \"\"\"\n\n        # Calls like: attributions([arg1, arg2]) will get read as model_args =\n        # ([arg1, arg2],), that is, a tuple with a single element containing the\n        # model args. Test below checks for this. TODO: Disallow such\n        # invocations? They should be given as attributions(arg1, arg2).\n        if isinstance(model_args,\n                      tuple) and len(model_args) == 1 and isinstance(\n                          model_args[0], DATA_CONTAINER_TYPE):\n            model_args = model_args[0]\n\n        model_inputs = ModelInputs(\n            args=many_of_om(model_args), kwargs=model_kwargs\n        )\n        # Will cast results to this data container type.\n        return_type = type(model_inputs.first_batchable(get_backend()))\n\n        pieces = self._attributions(model_inputs)\n\n        # Format attributions into the public structure which throws out output\n        # lists and input lists if there is only one output or only one input.\n        # Also cast to whatever the input type was.\n        attributions: Outputs[Inputs[np.ndarray]] = nested_cast(\n            backend=get_backend(), astype=return_type, args=pieces.attributions\n        )\n        attributions: Outputs[OM[Inputs, np.ndarray]\n                             ] = [om_of_many(attr) for attr in attributions]\n        attributions: OM[Outputs, OM[Inputs,\n                                     np.ndarray]] = om_of_many(attributions)\n\n        if pieces.gradients is not None or pieces.interventions is not None:\n            tru_logger.warning(\n                \"AttributionMethod configured to return gradients or interventions. \"\n                \"Use the internal _attribution call to retrieve those.\"\n            )\n\n        return attributions\n</code></pre>"},{"location":"trulens_explain/api/attribution/#trulens_explain.trulens.nn.attribution.AttributionMethod.model","title":"<code>model: ModelWrapper</code>  <code>property</code>","text":"<p>Model for which attributions are calculated.</p>"},{"location":"trulens_explain/api/attribution/#trulens_explain.trulens.nn.attribution.AttributionMethod.__init__","title":"<code>__init__(model, rebatch_size=None, *args, **kwargs)</code>  <code>abstractmethod</code>","text":"<p>Abstract constructor.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>ModelWrapper</code> <p>ModelWrapper Model for which attributions are calculated.</p> required <code>rebatch_size</code> <code>int</code> <p>int (optional) Will rebatch instances to this size if given. This may be required for GPU usage if using a DoI which produces multiple instances per user-provided instance. Many valued DoIs will expand the tensors sent to each layer to original_batch_size * doi_size. The rebatch size will break up original_batch_size * doi_size into rebatch_size chunks to send to model.</p> <code>None</code> Source code in <code>trulens_explain/trulens/nn/attribution.py</code> <pre><code>@abstractmethod\ndef __init__(\n    self, model: ModelWrapper, rebatch_size: int = None, *args, **kwargs\n):\n\"\"\"\n    Abstract constructor.\n\n    Parameters:\n        model: ModelWrapper\n            Model for which attributions are calculated.\n\n        rebatch_size: int (optional)\n            Will rebatch instances to this size if given. This may be\n            required for GPU usage if using a DoI which produces multiple\n            instances per user-provided instance. Many valued DoIs will\n            expand the tensors sent to each layer to original_batch_size *\n            doi_size. The rebatch size will break up original_batch_size *\n            doi_size into rebatch_size chunks to send to model.\n    \"\"\"\n    self._model = model\n\n    self.rebatch_size = rebatch_size\n</code></pre>"},{"location":"trulens_explain/api/attribution/#trulens_explain.trulens.nn.attribution.AttributionMethod.attributions","title":"<code>attributions(*model_args, **model_kwargs)</code>","text":"<p>Returns attributions for the given input. Attributions are in the same shape as the layer that attributions are being generated for. </p> <p>The numeric scale of the attributions will depend on the specific implementations of the Distribution of Interest and Quantity of Interest. However it is generally related to the scale of gradients on the Quantity of Interest. </p> <p>For example, Integrated Gradients uses the linear interpolation Distribution of Interest which subsumes the completeness axiom which ensures the sum of all attributions of a record equals the output determined by the Quantity of Interest on the same record. </p> <p>The Point Distribution of Interest will be determined by the gradient at a single point, thus being a good measure of model sensitivity. </p> <p>Parameters:</p> Name Type Description Default <code>model_args</code> <code>ArgsLike</code> <p>ArgsLike, model_kwargs: KwargsLike The args and kwargs given to the call method of a model. This should represent the records to obtain attributions for, assumed to be a batched input. if <code>self.model</code> supports evaluation on data tensors, the  appropriate tensor type may be used (e.g., Pytorch models may accept Pytorch tensors in addition to <code>np.ndarray</code>s). The shape of the inputs must match the input shape of <code>self.model</code>. </p> <code>()</code> <p>Returns     - np.ndarray when single attribution_cut input, single qoi output     - or ArgsLike[np.ndarray] when single input, multiple output (or       vice versa)      - or ArgsLike[ArgsLike[np.ndarray]] when multiple output (outer),       multiple input (inner)</p> <pre><code>An array of attributions, matching the shape and type of `from_cut`\nof the slice. Each entry in the returned array represents the degree\nto which the corresponding feature affected the model's outcome on\nthe corresponding point.\n\nIf attributing to a component with multiple inputs, a list for each\nwill be returned.\n\nIf the quantity of interest features multiple outputs, a list for\neach will be returned.\n</code></pre> Source code in <code>trulens_explain/trulens/nn/attribution.py</code> <pre><code>def attributions(\n    self, *model_args: ArgsLike, **model_kwargs: KwargsLike\n) -&gt; Union[TensorLike, ArgsLike[TensorLike],\n           ArgsLike[ArgsLike[TensorLike]]]:\n\"\"\"\n    Returns attributions for the given input. Attributions are in the same\n    shape as the layer that attributions are being generated for. \n\n    The numeric scale of the attributions will depend on the specific\n    implementations of the Distribution of Interest and Quantity of\n    Interest. However it is generally related to the scale of gradients on\n    the Quantity of Interest. \n\n    For example, Integrated Gradients uses the linear interpolation\n    Distribution of Interest which subsumes the completeness axiom which\n    ensures the sum of all attributions of a record equals the output\n    determined by the Quantity of Interest on the same record. \n\n    The Point Distribution of Interest will be determined by the gradient at\n    a single point, thus being a good measure of model sensitivity. \n\n    Parameters:\n        model_args: ArgsLike, model_kwargs: KwargsLike\n            The args and kwargs given to the call method of a model. This\n            should represent the records to obtain attributions for, assumed\n            to be a *batched* input. if `self.model` supports evaluation on\n            *data tensors*, the  appropriate tensor type may be used (e.g.,\n            Pytorch models may accept Pytorch tensors in addition to\n            `np.ndarray`s). The shape of the inputs must match the input\n            shape of `self.model`. \n\n    Returns\n        - np.ndarray when single attribution_cut input, single qoi output\n        - or ArgsLike[np.ndarray] when single input, multiple output (or\n          vice versa) \n        - or ArgsLike[ArgsLike[np.ndarray]] when multiple output (outer),\n          multiple input (inner)\n\n        An array of attributions, matching the shape and type of `from_cut`\n        of the slice. Each entry in the returned array represents the degree\n        to which the corresponding feature affected the model's outcome on\n        the corresponding point.\n\n        If attributing to a component with multiple inputs, a list for each\n        will be returned.\n\n        If the quantity of interest features multiple outputs, a list for\n        each will be returned.\n    \"\"\"\n\n    # Calls like: attributions([arg1, arg2]) will get read as model_args =\n    # ([arg1, arg2],), that is, a tuple with a single element containing the\n    # model args. Test below checks for this. TODO: Disallow such\n    # invocations? They should be given as attributions(arg1, arg2).\n    if isinstance(model_args,\n                  tuple) and len(model_args) == 1 and isinstance(\n                      model_args[0], DATA_CONTAINER_TYPE):\n        model_args = model_args[0]\n\n    model_inputs = ModelInputs(\n        args=many_of_om(model_args), kwargs=model_kwargs\n    )\n    # Will cast results to this data container type.\n    return_type = type(model_inputs.first_batchable(get_backend()))\n\n    pieces = self._attributions(model_inputs)\n\n    # Format attributions into the public structure which throws out output\n    # lists and input lists if there is only one output or only one input.\n    # Also cast to whatever the input type was.\n    attributions: Outputs[Inputs[np.ndarray]] = nested_cast(\n        backend=get_backend(), astype=return_type, args=pieces.attributions\n    )\n    attributions: Outputs[OM[Inputs, np.ndarray]\n                         ] = [om_of_many(attr) for attr in attributions]\n    attributions: OM[Outputs, OM[Inputs,\n                                 np.ndarray]] = om_of_many(attributions)\n\n    if pieces.gradients is not None or pieces.interventions is not None:\n        tru_logger.warning(\n            \"AttributionMethod configured to return gradients or interventions. \"\n            \"Use the internal _attribution call to retrieve those.\"\n        )\n\n    return attributions\n</code></pre>"},{"location":"trulens_explain/api/attribution/#trulens_explain.trulens.nn.attribution.AttributionResult","title":"<code>AttributionResult</code>  <code>dataclass</code>","text":"<p>_attribution method output container.</p> Source code in <code>trulens_explain/trulens/nn/attribution.py</code> <pre><code>@dataclass\nclass AttributionResult:\n\"\"\"\n    _attribution method output container.\n    \"\"\"\n\n    attributions: Outputs[Inputs[TensorLike]] = None\n    gradients: Outputs[Inputs[Uniform[TensorLike]]] = None\n    interventions: Inputs[Uniform[TensorLike]] = None\n</code></pre>"},{"location":"trulens_explain/api/attribution/#trulens_explain.trulens.nn.attribution.InputAttribution","title":"<code>InputAttribution</code>","text":"<p>         Bases: <code>InternalInfluence</code></p> <p>Attributions of input features on either internal or output quantities. This is essentially an alias for</p> <pre><code>InternalInfluence(\n    model,\n    (trulens.nn.slices.InputCut(), cut),\n    qoi,\n    doi,\n    multiply_activation)\n</code></pre> Source code in <code>trulens_explain/trulens/nn/attribution.py</code> <pre><code>class InputAttribution(InternalInfluence):\n\"\"\"\n    Attributions of input features on either internal or output quantities. This\n    is essentially an alias for\n\n    ```python\n    InternalInfluence(\n        model,\n        (trulens.nn.slices.InputCut(), cut),\n        qoi,\n        doi,\n        multiply_activation)\n    ```\n    \"\"\"\n\n    def __init__(\n        self,\n        model: ModelWrapper,\n        qoi_cut: CutLike = None,  # see WARNING-LOAD-INIT\n        qoi: QoiLike = 'max',\n        doi_cut: CutLike = None,  # see WARNING-LOAD-INIT\n        doi: DoiLike = 'point',\n        multiply_activation: bool = True,\n        *args,\n        **kwargs\n    ):\n\"\"\"\n        Parameters:\n            model :\n                Model for which attributions are calculated.\n\n            qoi_cut :\n                The cut determining the layer from which the QoI is derived.\n                Expects a `Cut` object, or a related type that can be\n                interpreted as a `Cut`, as documented below.\n\n                If an `int` is given, it represents the index of a layer in\n                `model`. \n\n                If a `str` is given, it represents the name of a layer in\n                `model`. \n\n                `None` is an alternative for `slices.OutputCut()`.\n\n            qoi : quantities.QoI | int | tuple | str\n                Quantity of interest to attribute. Expects a `QoI` object, or a\n                related type that can be interpreted as a `QoI`, as documented\n                below.\n\n                If an `int` is given, the quantity of interest is taken to be\n                the slice output for the class/neuron/channel specified by the\n                given integer, i.e., ```python\n                quantities.InternalChannelQoI(qoi) ```\n\n                If a tuple or list of two integers is given, then the quantity\n                of interest is taken to be the comparative quantity for the\n                class given by the first integer against the class given by the\n                second integer, i.e., ```python quantities.ComparativeQoI(*qoi)\n                ```\n\n                If a callable is given, it is interpreted as a function\n                representing the QoI, i.e., ```python quantities.LambdaQoI(qoi)\n                ```\n\n                If the string, `'max'`, is given, the quantity of interest is\n                taken to be the output for the class with the maximum score,\n                i.e., ```python quantities.MaxClassQoI() ```\n\n            doi_cut :\n                For models which have non-differentiable pre-processing at the\n                start of the model, specify the cut of the initial\n                differentiable input form. For NLP models, for example, this\n                could point to the embedding layer. If not provided, InputCut is\n                assumed.\n\n            doi : distributions.DoI | str\n                Distribution of interest over inputs. Expects a `DoI` object, or\n                a related type that can be interpreted as a `DoI`, as documented\n                below.\n\n                If the string, `'point'`, is given, the distribution is taken to\n                be the single point passed to `attributions`, i.e., ```python\n                distributions.PointDoi() ```\n\n                If the string, `'linear'`, is given, the distribution is taken\n                to be the linear interpolation from the zero input to the point\n                passed to `attributions`, i.e., ```python\n                distributions.LinearDoi() ```\n\n            multiply_activation : bool, optional\n                Whether to multiply the gradient result by its corresponding\n                activation, thus converting from \"*influence space*\" to\n                \"*attribution space*.\"\n        \"\"\"\n        if doi_cut is None:\n            # WARNING-LOAD-INIT: Do not put this as a default arg in the def\n            # line. That would cause an instantiation of InputCut when this\n            # class is loaded and before it is used. Because get_backend gets\n            # called in Cut.__init__, it may fail if this class is loaded before\n            # trulens.nn.models.get_model_wrapper is called on some model.\n            doi_cut = InputCut()\n\n        super().__init__(\n            model, (doi_cut, qoi_cut),\n            qoi,\n            doi,\n            multiply_activation=multiply_activation,\n            *args,\n            **kwargs\n        )\n</code></pre>"},{"location":"trulens_explain/api/attribution/#trulens_explain.trulens.nn.attribution.InputAttribution.__init__","title":"<code>__init__(model, qoi_cut=None, qoi='max', doi_cut=None, doi='point', multiply_activation=True, *args, **kwargs)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>model</code> <p>Model for which attributions are calculated.</p> required <code>qoi_cut</code> <p>The cut determining the layer from which the QoI is derived. Expects a <code>Cut</code> object, or a related type that can be interpreted as a <code>Cut</code>, as documented below.</p> <p>If an <code>int</code> is given, it represents the index of a layer in <code>model</code>. </p> <p>If a <code>str</code> is given, it represents the name of a layer in <code>model</code>. </p> <p><code>None</code> is an alternative for <code>slices.OutputCut()</code>.</p> <code>None</code> <code>qoi</code> <p>quantities.QoI | int | tuple | str Quantity of interest to attribute. Expects a <code>QoI</code> object, or a related type that can be interpreted as a <code>QoI</code>, as documented below.</p> <p>If an <code>int</code> is given, the quantity of interest is taken to be the slice output for the class/neuron/channel specified by the given integer, i.e., <code>python quantities.InternalChannelQoI(qoi)</code></p> <p>If a tuple or list of two integers is given, then the quantity of interest is taken to be the comparative quantity for the class given by the first integer against the class given by the second integer, i.e., ```python quantities.ComparativeQoI(*qoi) <pre><code>If a callable is given, it is interpreted as a function\nrepresenting the QoI, i.e., ```python quantities.LambdaQoI(qoi)\n</code></pre></p> <p>If the string, <code>'max'</code>, is given, the quantity of interest is taken to be the output for the class with the maximum score, i.e., <code>python quantities.MaxClassQoI()</code></p> <code>'max'</code> <code>doi_cut</code> <p>For models which have non-differentiable pre-processing at the start of the model, specify the cut of the initial differentiable input form. For NLP models, for example, this could point to the embedding layer. If not provided, InputCut is assumed.</p> <code>None</code> <code>doi</code> <p>distributions.DoI | str Distribution of interest over inputs. Expects a <code>DoI</code> object, or a related type that can be interpreted as a <code>DoI</code>, as documented below.</p> <p>If the string, <code>'point'</code>, is given, the distribution is taken to be the single point passed to <code>attributions</code>, i.e., <code>python distributions.PointDoi()</code></p> <p>If the string, <code>'linear'</code>, is given, the distribution is taken to be the linear interpolation from the zero input to the point passed to <code>attributions</code>, i.e., <code>python distributions.LinearDoi()</code></p> <code>'point'</code> <code>multiply_activation</code> <p>bool, optional Whether to multiply the gradient result by its corresponding activation, thus converting from \"influence space\" to \"attribution space.\"</p> <code>True</code> Source code in <code>trulens_explain/trulens/nn/attribution.py</code> <pre><code>def __init__(\n    self,\n    model: ModelWrapper,\n    qoi_cut: CutLike = None,  # see WARNING-LOAD-INIT\n    qoi: QoiLike = 'max',\n    doi_cut: CutLike = None,  # see WARNING-LOAD-INIT\n    doi: DoiLike = 'point',\n    multiply_activation: bool = True,\n    *args,\n    **kwargs\n):\n\"\"\"\n    Parameters:\n        model :\n            Model for which attributions are calculated.\n\n        qoi_cut :\n            The cut determining the layer from which the QoI is derived.\n            Expects a `Cut` object, or a related type that can be\n            interpreted as a `Cut`, as documented below.\n\n            If an `int` is given, it represents the index of a layer in\n            `model`. \n\n            If a `str` is given, it represents the name of a layer in\n            `model`. \n\n            `None` is an alternative for `slices.OutputCut()`.\n\n        qoi : quantities.QoI | int | tuple | str\n            Quantity of interest to attribute. Expects a `QoI` object, or a\n            related type that can be interpreted as a `QoI`, as documented\n            below.\n\n            If an `int` is given, the quantity of interest is taken to be\n            the slice output for the class/neuron/channel specified by the\n            given integer, i.e., ```python\n            quantities.InternalChannelQoI(qoi) ```\n\n            If a tuple or list of two integers is given, then the quantity\n            of interest is taken to be the comparative quantity for the\n            class given by the first integer against the class given by the\n            second integer, i.e., ```python quantities.ComparativeQoI(*qoi)\n            ```\n\n            If a callable is given, it is interpreted as a function\n            representing the QoI, i.e., ```python quantities.LambdaQoI(qoi)\n            ```\n\n            If the string, `'max'`, is given, the quantity of interest is\n            taken to be the output for the class with the maximum score,\n            i.e., ```python quantities.MaxClassQoI() ```\n\n        doi_cut :\n            For models which have non-differentiable pre-processing at the\n            start of the model, specify the cut of the initial\n            differentiable input form. For NLP models, for example, this\n            could point to the embedding layer. If not provided, InputCut is\n            assumed.\n\n        doi : distributions.DoI | str\n            Distribution of interest over inputs. Expects a `DoI` object, or\n            a related type that can be interpreted as a `DoI`, as documented\n            below.\n\n            If the string, `'point'`, is given, the distribution is taken to\n            be the single point passed to `attributions`, i.e., ```python\n            distributions.PointDoi() ```\n\n            If the string, `'linear'`, is given, the distribution is taken\n            to be the linear interpolation from the zero input to the point\n            passed to `attributions`, i.e., ```python\n            distributions.LinearDoi() ```\n\n        multiply_activation : bool, optional\n            Whether to multiply the gradient result by its corresponding\n            activation, thus converting from \"*influence space*\" to\n            \"*attribution space*.\"\n    \"\"\"\n    if doi_cut is None:\n        # WARNING-LOAD-INIT: Do not put this as a default arg in the def\n        # line. That would cause an instantiation of InputCut when this\n        # class is loaded and before it is used. Because get_backend gets\n        # called in Cut.__init__, it may fail if this class is loaded before\n        # trulens.nn.models.get_model_wrapper is called on some model.\n        doi_cut = InputCut()\n\n    super().__init__(\n        model, (doi_cut, qoi_cut),\n        qoi,\n        doi,\n        multiply_activation=multiply_activation,\n        *args,\n        **kwargs\n    )\n</code></pre>"},{"location":"trulens_explain/api/attribution/#trulens_explain.trulens.nn.attribution.IntegratedGradients","title":"<code>IntegratedGradients</code>","text":"<p>         Bases: <code>InputAttribution</code></p> <p>Implementation for the Integrated Gradients method from the following paper:</p> <p>Axiomatic Attribution for Deep Networks</p> <p>This should be cited using:</p> <pre><code>@INPROCEEDINGS{\nsundararajan17axiomatic,\nauthor={Mukund Sundararajan and Ankur Taly, and Qiqi Yan},\ntitle={Axiomatic Attribution for Deep Networks},\nbooktitle={International Conference on Machine Learning (ICML)},\nyear={2017},\n}\n</code></pre> <p>This is essentially an alias for</p> <pre><code>InternalInfluence(\n    model,\n    (trulens.nn.slices.InputCut(), trulens.nn.slices.OutputCut()),\n    'max',\n    trulens.nn.distributions.LinearDoi(baseline, resolution),\n    multiply_activation=True)\n</code></pre> Source code in <code>trulens_explain/trulens/nn/attribution.py</code> <pre><code>class IntegratedGradients(InputAttribution):\n\"\"\"\n    Implementation for the Integrated Gradients method from the following paper:\n\n    [Axiomatic Attribution for Deep Networks](\n        https://arxiv.org/pdf/1703.01365)\n\n    This should be cited using:\n\n    ```bibtex\n    @INPROCEEDINGS{\n        sundararajan17axiomatic,\n        author={Mukund Sundararajan and Ankur Taly, and Qiqi Yan},\n        title={Axiomatic Attribution for Deep Networks},\n        booktitle={International Conference on Machine Learning (ICML)},\n        year={2017},\n    }\n    ```\n\n    This is essentially an alias for\n\n    ```python\n    InternalInfluence(\n        model,\n        (trulens.nn.slices.InputCut(), trulens.nn.slices.OutputCut()),\n        'max',\n        trulens.nn.distributions.LinearDoi(baseline, resolution),\n        multiply_activation=True)\n    ```\n    \"\"\"\n\n    def __init__(\n        self,\n        model: ModelWrapper,\n        baseline=None,\n        resolution: int = 50,\n        doi_cut=None,  # see WARNING-LOAD-INIT\n        qoi='max',\n        qoi_cut=None,  # see WARNING-LOAD-INIT\n        *args,\n        **kwargs\n    ):\n\"\"\"\n        Parameters:\n            model:\n                Model for which attributions are calculated.\n\n            baseline:\n                The baseline to interpolate from. Must be same shape as the \n                input. If `None` is given, the zero vector in the appropriate \n                shape will be used.\n\n            resolution:\n                Number of points to use in the approximation. A higher \n                resolution is more computationally expensive, but gives a better\n                approximation of the mathematical formula this attribution \n                method represents.\n        \"\"\"\n\n        if doi_cut is None:\n            doi_cut = InputCut()\n\n        if qoi_cut is None:\n            qoi_cut = OutputCut()\n\n        super().__init__(\n            model=model,\n            qoi_cut=qoi_cut,\n            qoi=qoi,\n            doi_cut=doi_cut,\n            doi=LinearDoi(baseline, resolution, cut=doi_cut),\n            multiply_activation=True,\n            *args,\n            **kwargs\n        )\n</code></pre>"},{"location":"trulens_explain/api/attribution/#trulens_explain.trulens.nn.attribution.IntegratedGradients.__init__","title":"<code>__init__(model, baseline=None, resolution=50, doi_cut=None, qoi='max', qoi_cut=None, *args, **kwargs)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>model</code> <code>ModelWrapper</code> <p>Model for which attributions are calculated.</p> required <code>baseline</code> <p>The baseline to interpolate from. Must be same shape as the  input. If <code>None</code> is given, the zero vector in the appropriate  shape will be used.</p> <code>None</code> <code>resolution</code> <code>int</code> <p>Number of points to use in the approximation. A higher  resolution is more computationally expensive, but gives a better approximation of the mathematical formula this attribution  method represents.</p> <code>50</code> Source code in <code>trulens_explain/trulens/nn/attribution.py</code> <pre><code>def __init__(\n    self,\n    model: ModelWrapper,\n    baseline=None,\n    resolution: int = 50,\n    doi_cut=None,  # see WARNING-LOAD-INIT\n    qoi='max',\n    qoi_cut=None,  # see WARNING-LOAD-INIT\n    *args,\n    **kwargs\n):\n\"\"\"\n    Parameters:\n        model:\n            Model for which attributions are calculated.\n\n        baseline:\n            The baseline to interpolate from. Must be same shape as the \n            input. If `None` is given, the zero vector in the appropriate \n            shape will be used.\n\n        resolution:\n            Number of points to use in the approximation. A higher \n            resolution is more computationally expensive, but gives a better\n            approximation of the mathematical formula this attribution \n            method represents.\n    \"\"\"\n\n    if doi_cut is None:\n        doi_cut = InputCut()\n\n    if qoi_cut is None:\n        qoi_cut = OutputCut()\n\n    super().__init__(\n        model=model,\n        qoi_cut=qoi_cut,\n        qoi=qoi,\n        doi_cut=doi_cut,\n        doi=LinearDoi(baseline, resolution, cut=doi_cut),\n        multiply_activation=True,\n        *args,\n        **kwargs\n    )\n</code></pre>"},{"location":"trulens_explain/api/attribution/#trulens_explain.trulens.nn.attribution.InternalInfluence","title":"<code>InternalInfluence</code>","text":"<p>         Bases: <code>AttributionMethod</code></p> <p>Internal attributions parameterized by a slice, quantity of interest, and distribution of interest.</p> <p>The slice specifies the layers at which the internals of the model are to be exposed; it is represented by two cuts, which specify the layer the attributions are assigned to and the layer from which the quantity of interest is derived. The Quantity of Interest (QoI) is a function of the output specified by the slice that determines the network output behavior that the attributions are to describe. The Distribution of Interest (DoI) specifies the records over which the attributions are aggregated.</p> <p>More information can be found in the following paper:</p> <p>Influence-Directed Explanations for Deep Convolutional Networks</p> <p>This should be cited using:</p> <pre><code>@INPROCEEDINGS{\nleino18influence,\nauthor={\n        Klas Leino and\n        Shayak Sen and\n        Anupam Datta and\n        Matt Fredrikson and\n        Linyi Li},\ntitle={\n        Influence-Directed Explanations\n        for Deep Convolutional Networks},\nbooktitle={IEEE International Test Conference (ITC)},\nyear={2018},\n}\n</code></pre> Source code in <code>trulens_explain/trulens/nn/attribution.py</code> <pre><code>class InternalInfluence(AttributionMethod):\n\"\"\"Internal attributions parameterized by a slice, quantity of interest, and\n    distribution of interest.\n\n    The *slice* specifies the layers at which the internals of the model are to\n    be exposed; it is represented by two *cuts*, which specify the layer the\n    attributions are assigned to and the layer from which the quantity of\n    interest is derived. The *Quantity of Interest* (QoI) is a function of the\n    output specified by the slice that determines the network output behavior\n    that the attributions are to describe. The *Distribution of Interest* (DoI)\n    specifies the records over which the attributions are aggregated.\n\n    More information can be found in the following paper:\n\n    [Influence-Directed Explanations for Deep Convolutional Networks](\n        https://arxiv.org/pdf/1802.03788.pdf)\n\n    This should be cited using:\n\n    ```bibtex\n    @INPROCEEDINGS{\n        leino18influence,\n        author={\n            Klas Leino and\n            Shayak Sen and\n            Anupam Datta and\n            Matt Fredrikson and\n            Linyi Li},\n        title={\n            Influence-Directed Explanations\n            for Deep Convolutional Networks},\n        booktitle={IEEE International Test Conference (ITC)},\n        year={2018},\n    }\n    ```\n    \"\"\"\n\n    def __init__(\n        self,\n        model: ModelWrapper,\n        cuts: SliceLike,\n        qoi: QoiLike,\n        doi: DoiLike,\n        multiply_activation: bool = True,\n        return_grads: bool = False,\n        return_doi: bool = False,\n        *args,\n        **kwargs\n    ):\n\"\"\"\n        Parameters:\n            model:\n                Model for which attributions are calculated.\n\n            cuts: \n                The slice to use when computing the attributions. The slice \n                keeps track of the layer whose output attributions are \n                calculated and the layer for which the quantity of interest is \n                computed. Expects a `Slice` object, or a related type that can\n                be interpreted as a `Slice`, as documented below.\n\n                If a single `Cut` object is given, it is assumed to be the cut \n                representing the layer for which attributions are calculated \n                (i.e., `from_cut` in `Slice`) and the layer for the quantity of \n                interest (i.e., `to_cut` in `slices.Slice`) is taken to be the \n                output of the network. If a tuple or list of two `Cut`s is \n                given, they are assumed to be `from_cut` and `to_cut`, \n                respectively.\n\n                A cut (or the cuts within the tuple) can also be represented as \n                an `int`, `str`, or `None`. If an `int` is given, it represents \n                the index of a layer in `model`. If a `str` is given, it \n                represents the name of a layer in `model`. `None` is an \n                alternative for `slices.InputCut`.\n\n            qoi:\n                Quantity of interest to attribute. Expects a `QoI` object, or a\n                related type that can be interpreted as a `QoI`, as documented\n                below.\n\n                If an `int` is given, the quantity of interest is taken to be \n                the slice output for the class/neuron/channel specified by the \n                given integer, i.e., \n                ```python\n                quantities.InternalChannelQoI(qoi)\n                ```\n\n                If a tuple or list of two integers is given, then the quantity \n                of interest is taken to be the comparative quantity for the \n                class given by the first integer against the class given by the \n                second integer, i.e., \n                ```python\n                quantities.ComparativeQoI(*qoi)\n                ```\n\n                If a callable is given, it is interpreted as a function\n                representing the QoI, i.e.,\n                ```python\n                quantities.LambdaQoI(qoi)\n                ```\n\n                If the string, `'max'`, is given, the quantity of interest is \n                taken to be the output for the class with the maximum score, \n                i.e., \n                ```python\n                quantities.MaxClassQoI()\n                ```\n\n            doi:\n                Distribution of interest over inputs. Expects a `DoI` object, or\n                a related type that can be interpreted as a `DoI`, as documented\n                below.\n\n                If the string, `'point'`, is given, the distribution is taken to\n                be the single point passed to `attributions`, i.e., \n                ```python\n                distributions.PointDoi()\n                ```\n\n                If the string, `'linear'`, is given, the distribution is taken \n                to be the linear interpolation from the zero input to the point \n                passed to `attributions`, i.e., \n                ```python\n                distributions.LinearDoi()\n                ```\n\n            multiply_activation:\n                Whether to multiply the gradient result by its corresponding\n                activation, thus converting from \"*influence space*\" to \n                \"*attribution space*.\"\n        \"\"\"\n        super().__init__(model, *args, **kwargs)\n\n        self.slice = InternalInfluence.__get_slice(cuts)\n        self.qoi = InternalInfluence.__get_qoi(qoi)\n        self.doi = InternalInfluence.__get_doi(doi, cut=self.slice.from_cut)\n        self._do_multiply = multiply_activation\n        self._return_grads = return_grads\n        self._return_doi = return_doi\n\n    def _attributions(self, model_inputs: ModelInputs) -&gt; AttributionResult:\n        # NOTE: not symbolic\n\n        B = get_backend()\n        results = AttributionResult()\n\n        # Create a message for out-of-memory errors regarding float and batch size.\n        first_batchable = model_inputs.first_batchable(B)\n        if first_batchable is None:\n            batch_size = 1\n        else:\n            batch_size = first_batchable.shape[0]\n\n        param_msgs = [\n            f\"float size = {B.floatX_size} ({B.floatX}); consider changing to a smaller type.\",\n            f\"batch size = {batch_size}; consider reducing the size of the batch you send to the attributions method.\"\n        ]\n\n        doi_cut = self.doi.cut() if self.doi.cut() else InputCut()\n\n        with memory_suggestions(*param_msgs):  # Handles out-of-memory messages.\n            doi_val: List[B.Tensor] = self.model._fprop(\n                model_inputs=model_inputs,\n                to_cut=doi_cut,\n                doi_cut=InputCut(),\n                attribution_cut=None,  # InputCut(),\n                intervention=model_inputs\n            )[0]\n\n        doi_val = nested_map(doi_val, B.as_array)\n\n        D = self.doi._wrap_public_call(doi_val, model_inputs=model_inputs)\n\n        if self._return_doi:\n            results.interventions = D  # : Inputs[Uniform[TensorLike]]\n\n        D_tensors = D[0]\n        n_doi = len(D_tensors)\n        if isinstance(D_tensors, MAP_CONTAINER_TYPE):\n            for k in D_tensors.keys():\n                if isinstance(D_tensors[k], DATA_CONTAINER_TYPE):\n                    n_doi = len(D_tensors[k])\n        D = self.__concatenate_doi(D)\n        rebatch_size = self.rebatch_size\n        if rebatch_size is None:\n            rebatch_size = len(D[0])\n\n        intervention = TensorArgs(args=D)\n        model_inputs_expanded = tile(what=model_inputs, onto=intervention)\n        # Create a message for out-of-memory errors regarding doi_size.\n        # TODO: Generalize this message to doi other than LinearDoI:\n        doi_size_msg = f\"distribution of interest size = {n_doi}; consider reducing intervention resolution.\"\n\n        combined_batch_size = n_doi * batch_size\n        combined_batch_msg = f\"combined batch size = {combined_batch_size}; consider reducing batch size, intervention size\"\n\n        rebatch_size_msg = f\"rebatch_size = {rebatch_size}; consider reducing this AttributionMethod constructor parameter (default is same as combined batch size).\"\n\n        # Calculate the gradient of each of the points in the DoI.\n        with memory_suggestions(\n                param_msgs +\n            [doi_size_msg, combined_batch_msg, rebatch_size_msg]\n        ):  # Handles out-of-memory messages.\n            qoi_grads_expanded: List[Outputs[Inputs[TensorLike]]] = []\n\n            for inputs_batch, intervention_batch in rebatch(\n                    model_inputs_expanded, intervention,\n                    batch_size=rebatch_size):\n\n                qoi_grads_expanded_batch: Outputs[\n                    Inputs[TensorLike]] = self.model._qoi_bprop(\n                        qoi=self.qoi,\n                        model_inputs=inputs_batch,\n                        attribution_cut=self.slice.from_cut,\n                        to_cut=self.slice.to_cut,\n                        intervention=intervention_batch,\n                        doi_cut=doi_cut\n                    )\n\n                # important to cast to numpy inside loop:\n                qoi_grads_expanded.append(\n                    nested_map(qoi_grads_expanded_batch, B.as_array)\n                )\n\n        num_outputs = len(qoi_grads_expanded[0])\n        num_inputs = len(qoi_grads_expanded[0][0])\n        transpose = [\n            [[] for _ in range(num_inputs)] for _ in range(num_outputs)\n        ]\n        for o in range(num_outputs):\n            for i in range(num_inputs):\n                for qoi_grads_batch in qoi_grads_expanded:\n                    transpose[o][i].append(qoi_grads_batch[o][i])\n\n        def container_concat(x):\n\"\"\"Applies np concatenate on a container. If it is a map type, it will apply it on each key.\n\n            Args:\n                x (map or data container): A container of tensors\n\n            Returns:\n                concatenated tensors of the container.\n            \"\"\"\n            if isinstance(x[0], MAP_CONTAINER_TYPE):\n                ret_map = {}\n                for k in x[0].keys():\n                    ret_map[k] = np.concatenate([_dict[k] for _dict in x])\n                return ret_map\n            else:\n                return np.concatenate(x)\n\n        qoi_grads_expanded: Outputs[Inputs[np.ndarray]] = nested_map(\n            transpose, container_concat, nest=2\n        )\n        qoi_grads_expanded: Outputs[Inputs[np.ndarray]] = nested_map(\n            qoi_grads_expanded,\n            lambda grad: np.reshape(grad, (n_doi, -1) + grad.shape[1:]),\n            nest=2\n        )\n        if self._return_grads:\n            results.gradients = qoi_grads_expanded  # : Outputs[Inputs[Uniform[TensorLike]]]\n\n        # TODO: Does this need to be done in numpy?\n        attrs: Outputs[Inputs[TensorLike]] = nested_map(\n            qoi_grads_expanded, lambda grad: np.mean(grad, axis=0), nest=2\n        )\n\n        # Multiply by the activation multiplier if specified.\n        if self._do_multiply:\n            with memory_suggestions(param_msgs):\n                z_val = self.model._fprop(\n                    model_inputs=model_inputs,\n                    doi_cut=InputCut(),\n                    attribution_cut=None,\n                    to_cut=self.slice.from_cut,\n                    intervention=model_inputs  # intentional\n                )[0]\n\n            mults: Inputs[TensorLike\n                         ] = self.doi._wrap_public_get_activation_multiplier(\n                             z_val, model_inputs=model_inputs\n                         )\n            mults: Inputs[np.ndarray] = nested_cast(\n                backend=B, args=mults, astype=np.ndarray\n            )\n            mult_attrs = []\n            for attr in attrs:  # Outputs\n\n                zipped = nested_zip(attr, mults)\n\n                def zip_mult(zipped_attr_mults):\n                    attr = zipped_attr_mults[0]\n                    mults = zipped_attr_mults[1]\n                    return attr * mults\n\n                attr = nested_map(\n                    zipped, zip_mult, check_accessor=lambda x: x[0]\n                )\n                mult_attrs.append(attr)\n            attrs = mult_attrs\n        results.attributions = attrs  # : Outputs[Inputs[TensorLike]]\n\n        return results\n\n    @staticmethod\n    def __get_qoi(qoi_arg):\n\"\"\"\n        Helper function to get a `QoI` object from more user-friendly primitive \n        arguments.\n        \"\"\"\n        # TODO(klas): we could potentially do some basic error catching here,\n        #   for example, making sure the index for a given channel is in range.\n\n        if isinstance(qoi_arg, QoI):\n            # We were already given a QoI, so return it.\n            return qoi_arg\n\n        elif callable(qoi_arg):\n            # If we were given a callable, treat that function as a QoI.\n            return LambdaQoI(qoi_arg)\n\n        elif isinstance(qoi_arg, int):\n            # If we receive an int, we take it to be the class/channel index\n            # (whether it's a class or channel depends on the layer the quantity\n            # is for, but `InternalChannelQoI` generalizes to both).\n            return InternalChannelQoI(qoi_arg)\n\n        elif isinstance(qoi_arg, DATA_CONTAINER_TYPE):\n            # If we receive a DATA_CONTAINER_TYPE, we take it to be two classes\n            # for which we are performing a comparative quantity of interest.\n            if len(qoi_arg) == 2:\n                return ComparativeQoI(*qoi_arg)\n\n            else:\n                raise ValueError(\n                    'Tuple or list argument for `qoi` must have length 2'\n                )\n\n        elif isinstance(qoi_arg, str):\n            # We can specify `MaxClassQoI` via the string 'max'.\n            if qoi_arg == 'max':\n                return MaxClassQoI()\n\n            else:\n                raise ValueError(\n                    'String argument for `qoi` must be one of the following:\\n'\n                    '  - \"max\"'\n                )\n\n        else:\n            raise ValueError('Unrecognized argument type for `qoi`')\n\n    @staticmethod\n    def __get_doi(doi_arg, cut=None):\n\"\"\"\n        Helper function to get a `DoI` object from more user-friendly primitive \n        arguments.\n        \"\"\"\n        if isinstance(doi_arg, DoI):\n            # We were already given a DoI, so return it.\n            return doi_arg\n\n        elif isinstance(doi_arg, str):\n            # We can specify `PointDoi` via the string 'point', or `LinearDoi`\n            # via the string 'linear'.\n            if doi_arg == 'point':\n                return PointDoi(cut=cut)\n\n            elif doi_arg == 'linear':\n                return LinearDoi(cut=cut)\n\n            else:\n                raise ValueError(\n                    'String argument for `doi` must be one of the following:\\n'\n                    '  - \"point\"\\n'\n                    '  - \"linear\"'\n                )\n\n        else:\n            raise ValueError('Unrecognized argument type for `doi`')\n\n    @staticmethod\n    def __get_slice(slice_arg):\n\"\"\"\n        Helper function to get a `Slice` object from more user-friendly\n        primitive arguments.\n        \"\"\"\n        if isinstance(slice_arg, Slice):\n            # We are already given a Slice, so return it.\n            return slice_arg\n\n        elif (isinstance(slice_arg, Cut) or isinstance(slice_arg, int) or\n              isinstance(slice_arg, str) or slice_arg is None or\n              slice_arg == 0):\n\n            # If we receive a Cut, we take it to be the Cut of the start layer.\n            return Slice(InternalInfluence.__get_cut(slice_arg), OutputCut())\n\n        elif isinstance(slice_arg, DATA_CONTAINER_TYPE):\n            # If we receive a DATA_CONTAINER_TYPE, we take it to be the start\n            # and end layer of the slice.\n            if len(slice_arg) == 2:\n                if slice_arg[1] is None:\n                    return Slice(\n                        InternalInfluence.__get_cut(slice_arg[0]), OutputCut()\n                    )\n                else:\n                    return Slice(\n                        InternalInfluence.__get_cut(slice_arg[0]),\n                        InternalInfluence.__get_cut(slice_arg[1])\n                    )\n\n            else:\n                raise ValueError(\n                    'Tuple or list argument for `cuts` must have length 2'\n                )\n\n        else:\n            raise ValueError('Unrecognized argument type for `cuts`')\n\n    @staticmethod\n    def __get_cut(cut_arg):\n\"\"\"\n        Helper function to get a `Cut` object from more user-friendly primitive\n        arguments.\n        \"\"\"\n        if isinstance(cut_arg, Cut):\n            # We are already given a Cut, so return it.\n            return cut_arg\n\n        elif cut_arg is None or cut_arg == 0:\n            # If we receive None or zero, we take it to be the input cut.\n            return InputCut()\n\n        # TODO(klas): may want a bit more validation here.\n        elif isinstance(cut_arg, int) or isinstance(cut_arg, str):\n            return Cut(cut_arg)\n\n        else:\n            raise ValueError('Unrecognized argument type for cut')\n\n    @staticmethod\n    def __concatenate_doi(D: Inputs[Uniform[TensorLike]]) -&gt; Inputs[TensorLike]:\n        # Returns one TensorLike for each model input.\n        if len(D[0]) == 0:\n            raise ValueError(\n                'Got empty distribution of interest. `DoI` must return at '\n                'least one point.'\n            )\n        # TODO: should this always be done in numpy or can we do it in backend?\n        D = nested_cast(backend=get_backend(), args=D, astype=np.ndarray)\n        ret = nested_map(D, np.concatenate, nest=1)\n        return ret\n</code></pre>"},{"location":"trulens_explain/api/attribution/#trulens_explain.trulens.nn.attribution.InternalInfluence.__init__","title":"<code>__init__(model, cuts, qoi, doi, multiply_activation=True, return_grads=False, return_doi=False, *args, **kwargs)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>model</code> <code>ModelWrapper</code> <p>Model for which attributions are calculated.</p> required <code>cuts</code> <code>SliceLike</code> <p>The slice to use when computing the attributions. The slice  keeps track of the layer whose output attributions are  calculated and the layer for which the quantity of interest is  computed. Expects a <code>Slice</code> object, or a related type that can be interpreted as a <code>Slice</code>, as documented below.</p> <p>If a single <code>Cut</code> object is given, it is assumed to be the cut  representing the layer for which attributions are calculated  (i.e., <code>from_cut</code> in <code>Slice</code>) and the layer for the quantity of  interest (i.e., <code>to_cut</code> in <code>slices.Slice</code>) is taken to be the  output of the network. If a tuple or list of two <code>Cut</code>s is  given, they are assumed to be <code>from_cut</code> and <code>to_cut</code>,  respectively.</p> <p>A cut (or the cuts within the tuple) can also be represented as  an <code>int</code>, <code>str</code>, or <code>None</code>. If an <code>int</code> is given, it represents  the index of a layer in <code>model</code>. If a <code>str</code> is given, it  represents the name of a layer in <code>model</code>. <code>None</code> is an  alternative for <code>slices.InputCut</code>.</p> required <code>qoi</code> <code>QoiLike</code> <p>Quantity of interest to attribute. Expects a <code>QoI</code> object, or a related type that can be interpreted as a <code>QoI</code>, as documented below.</p> <p>If an <code>int</code> is given, the quantity of interest is taken to be  the slice output for the class/neuron/channel specified by the  given integer, i.e.,  <pre><code>quantities.InternalChannelQoI(qoi)\n</code></pre></p> <p>If a tuple or list of two integers is given, then the quantity  of interest is taken to be the comparative quantity for the  class given by the first integer against the class given by the  second integer, i.e.,  <pre><code>quantities.ComparativeQoI(*qoi)\n</code></pre></p> <p>If a callable is given, it is interpreted as a function representing the QoI, i.e., <pre><code>quantities.LambdaQoI(qoi)\n</code></pre></p> <p>If the string, <code>'max'</code>, is given, the quantity of interest is  taken to be the output for the class with the maximum score,  i.e.,  <pre><code>quantities.MaxClassQoI()\n</code></pre></p> required <code>doi</code> <code>DoiLike</code> <p>Distribution of interest over inputs. Expects a <code>DoI</code> object, or a related type that can be interpreted as a <code>DoI</code>, as documented below.</p> <p>If the string, <code>'point'</code>, is given, the distribution is taken to be the single point passed to <code>attributions</code>, i.e.,  <pre><code>distributions.PointDoi()\n</code></pre></p> <p>If the string, <code>'linear'</code>, is given, the distribution is taken  to be the linear interpolation from the zero input to the point  passed to <code>attributions</code>, i.e.,  <pre><code>distributions.LinearDoi()\n</code></pre></p> required <code>multiply_activation</code> <code>bool</code> <p>Whether to multiply the gradient result by its corresponding activation, thus converting from \"influence space\" to  \"attribution space.\"</p> <code>True</code> Source code in <code>trulens_explain/trulens/nn/attribution.py</code> <pre><code>def __init__(\n    self,\n    model: ModelWrapper,\n    cuts: SliceLike,\n    qoi: QoiLike,\n    doi: DoiLike,\n    multiply_activation: bool = True,\n    return_grads: bool = False,\n    return_doi: bool = False,\n    *args,\n    **kwargs\n):\n\"\"\"\n    Parameters:\n        model:\n            Model for which attributions are calculated.\n\n        cuts: \n            The slice to use when computing the attributions. The slice \n            keeps track of the layer whose output attributions are \n            calculated and the layer for which the quantity of interest is \n            computed. Expects a `Slice` object, or a related type that can\n            be interpreted as a `Slice`, as documented below.\n\n            If a single `Cut` object is given, it is assumed to be the cut \n            representing the layer for which attributions are calculated \n            (i.e., `from_cut` in `Slice`) and the layer for the quantity of \n            interest (i.e., `to_cut` in `slices.Slice`) is taken to be the \n            output of the network. If a tuple or list of two `Cut`s is \n            given, they are assumed to be `from_cut` and `to_cut`, \n            respectively.\n\n            A cut (or the cuts within the tuple) can also be represented as \n            an `int`, `str`, or `None`. If an `int` is given, it represents \n            the index of a layer in `model`. If a `str` is given, it \n            represents the name of a layer in `model`. `None` is an \n            alternative for `slices.InputCut`.\n\n        qoi:\n            Quantity of interest to attribute. Expects a `QoI` object, or a\n            related type that can be interpreted as a `QoI`, as documented\n            below.\n\n            If an `int` is given, the quantity of interest is taken to be \n            the slice output for the class/neuron/channel specified by the \n            given integer, i.e., \n            ```python\n            quantities.InternalChannelQoI(qoi)\n            ```\n\n            If a tuple or list of two integers is given, then the quantity \n            of interest is taken to be the comparative quantity for the \n            class given by the first integer against the class given by the \n            second integer, i.e., \n            ```python\n            quantities.ComparativeQoI(*qoi)\n            ```\n\n            If a callable is given, it is interpreted as a function\n            representing the QoI, i.e.,\n            ```python\n            quantities.LambdaQoI(qoi)\n            ```\n\n            If the string, `'max'`, is given, the quantity of interest is \n            taken to be the output for the class with the maximum score, \n            i.e., \n            ```python\n            quantities.MaxClassQoI()\n            ```\n\n        doi:\n            Distribution of interest over inputs. Expects a `DoI` object, or\n            a related type that can be interpreted as a `DoI`, as documented\n            below.\n\n            If the string, `'point'`, is given, the distribution is taken to\n            be the single point passed to `attributions`, i.e., \n            ```python\n            distributions.PointDoi()\n            ```\n\n            If the string, `'linear'`, is given, the distribution is taken \n            to be the linear interpolation from the zero input to the point \n            passed to `attributions`, i.e., \n            ```python\n            distributions.LinearDoi()\n            ```\n\n        multiply_activation:\n            Whether to multiply the gradient result by its corresponding\n            activation, thus converting from \"*influence space*\" to \n            \"*attribution space*.\"\n    \"\"\"\n    super().__init__(model, *args, **kwargs)\n\n    self.slice = InternalInfluence.__get_slice(cuts)\n    self.qoi = InternalInfluence.__get_qoi(qoi)\n    self.doi = InternalInfluence.__get_doi(doi, cut=self.slice.from_cut)\n    self._do_multiply = multiply_activation\n    self._return_grads = return_grads\n    self._return_doi = return_doi\n</code></pre>"},{"location":"trulens_explain/api/distributions/","title":"Distributions of Interest","text":"<p>The distribution of interest lets us specify the set of samples over which we  want our explanations to be faithful. In some cases, we may want to explain the  model\u2019s behavior on a particular record, whereas other times we may be  interested in a more general behavior over a distribution of samples.</p>"},{"location":"trulens_explain/api/distributions/#trulens_explain.trulens.nn.distributions.DoI","title":"<code>DoI</code>","text":"<p>         Bases: <code>AbstractBaseClass</code></p> <p>Interface for distributions of interest. The Distribution of Interest  (DoI) specifies the samples over which an attribution method is  aggregated.</p> Source code in <code>trulens_explain/trulens/nn/distributions.py</code> <pre><code>class DoI(AbstractBaseClass):\n\"\"\"\n    Interface for distributions of interest. The *Distribution of Interest* \n    (DoI) specifies the samples over which an attribution method is \n    aggregated.\n    \"\"\"\n\n    def __init__(self, cut: Cut = None):\n\"\"\"\"Initialize DoI\n\n        Parameters:\n            cut (Cut, optional): \n                The Cut in which the DoI will be applied. If `None`, the DoI will be\n                applied to the input. otherwise, the distribution should be applied\n                to the latent space defined by the cut. \n        \"\"\"\n        self._cut = cut\n\n    def __str__(self):\n        return render_object(self, ['_cut'])\n\n    def _wrap_public_call(\n        self, z: Inputs[TensorLike], *, model_inputs: ModelInputs\n    ) -&gt; Inputs[Uniform[TensorLike]]:\n\"\"\"Same as __call__ but input and output types are more specific and\n        less permissive. Formats the inputs for special cases that might be more\n        convenient for the user's __call__ implementation and formats its return\n        back to the consistent type.\"\"\"\n\n        z: Inputs[TensorLike] = om_of_many(z)\n\n        if accepts_model_inputs(self.__call__):\n            ret = self.__call__(z, model_inputs=model_inputs)\n        else:\n            ret = self.__call__(z)\n        # Wrap the public doi generator with appropriate type aliases.\n        if isinstance(ret, DATA_CONTAINER_TYPE):\n            if isinstance(ret[0], DATA_CONTAINER_TYPE):\n                ret = Inputs(Uniform(x) for x in ret)\n            else:\n                ret = Uniform(ret)\n\n            ret: Inputs[Uniform[TensorLike]] = many_of_om(\n                ret, innertype=Uniform\n            )\n        else:\n            ret: ArgsLike = [ret]\n        return ret\n\n    @abstractmethod\n    def __call__(\n        self,\n        z: OM[Inputs, TensorLike],\n        *,\n        model_inputs: Optional[ModelInputs] = None\n    ) -&gt; OM[Inputs, Uniform[TensorLike]]:\n\"\"\"\n        Computes the distribution of interest from an initial point. If z:\n        TensorLike is given, we assume there is only 1 input to the DoI layer. If\n        z: List[TensorLike] is given, it provides all of the inputs to the DoI\n        layer. \n\n        Either way, we always return List[List[TensorLike]] (alias\n        Inputs[Uniform[TensorLike]]) with outer list spanning layer inputs, and\n        inner list spanning a distribution's instance.\n\n        Parameters:\n            z:\n                Input point from which the distribution is derived. If\n                list/tuple, the point is defined by multiple tensors.\n            model_inputs:\n                Optional wrapped model input arguments that produce value z at\n                cut.\n\n        Returns:\n            List of points which are all assigned equal probability mass in the\n            distribution of interest, i.e., the distribution of interest is a\n            discrete, uniform distribution over the list of returned points. If\n            z is multi-input, returns a distribution for each input.\n        \"\"\"\n        raise NotImplementedError\n\n    # @property\n    def cut(self) -&gt; Cut:\n\"\"\"\n        Returns:\n            The Cut in which the DoI will be applied. If `None`, the DoI will be\n            applied to the input. otherwise, the distribution should be applied\n            to the latent space defined by the cut. \n        \"\"\"\n        return self._cut\n\n    def _wrap_public_get_activation_multiplier(\n        self, activation: Inputs[TensorLike], *, model_inputs: ModelInputs\n    ) -&gt; Inputs[TensorLike]:\n\"\"\"Same as get_activation_multiplier but without \"one-or-more\". \"\"\"\n\n        activations: OM[Inputs, TensorLike] = om_of_many(activation)\n\n        # get_activation_multiplier is public\n        if accepts_model_inputs(self.get_activation_multiplier):\n            ret: OM[Inputs, TensorLike] = self.get_activation_multiplier(\n                activations, model_inputs=model_inputs\n            )\n        else:\n            ret: OM[Inputs,\n                    TensorLike] = self.get_activation_multiplier(activations)\n\n        ret: Inputs[TensorLike] = many_of_om(ret)\n\n        return ret\n\n    def get_activation_multiplier(\n        self,\n        activation: OM[Inputs, TensorLike],\n        *,\n        model_inputs: Optional[ModelInputs] = None\n    ) -&gt; OM[Inputs, TensorLike]:\n\"\"\"\n        Returns a term to multiply the gradient by to convert from \"*influence\n        space*\" to \"*attribution space*\". Conceptually, \"influence space\"\n        corresponds to the potential effect of a slight increase in each\n        feature, while \"attribution space\" corresponds to an approximation of\n        the net marginal contribution to the quantity of interest of each\n        feature.\n\n        Parameters:\n            activation:\n                The activation of the layer the DoI is applied to. DoI may be\n                multi-input in which case activation will be a list.\n            model_inputs:\n                Optional wrapped model input arguments that produce activation\n                at cut.\n\n        Returns:\n            An array with the same shape as ``activation`` that will be\n            multiplied by the gradient to obtain the attribution. The default\n            implementation of this method simply returns ``activation``. If\n            activation is multi-input, returns one multiplier for each.\n        \"\"\"\n        return om_of_many(activation)\n\n    def _assert_cut_contains_only_one_tensor(self, x):\n        if isinstance(x, DATA_CONTAINER_TYPE) and len(x) == 1:\n            x = x[0]\n        if isinstance(x, MAP_CONTAINER_TYPE) and len(x) == 1:\n            x = list(x.values())[0]\n\n        if isinstance(x, list):\n            raise DoiCutSupportError(\n                '\\n\\n'\n                'Cut provided to distribution of interest was comprised of '\n                'multiple tensors, but `{}` is only defined for cuts comprised '\n                'of a single tensor (received a list of {} tensors).\\n'\n                '\\n'\n                'Either (1) select a slice where the `to_cut` corresponds to a '\n                'single tensor, or (2) implement/use a `DoI` object that '\n                'supports lists of tensors, i.e., where the parameter, `z`, to '\n                '`__call__` is expected/allowed to be a list of {} tensors.'.\n                format(self.__class__.__name__, len(x), len(x))\n            )\n\n        elif not (isinstance(x, np.ndarray) or get_backend().is_tensor(x)):\n            raise ValueError(\n                '`{}` expected to receive an instance of `Tensor` or '\n                '`np.ndarray`, but received an instance of {}'.format(\n                    self.__class__.__name__, type(x)\n                )\n            )\n</code></pre>"},{"location":"trulens_explain/api/distributions/#trulens_explain.trulens.nn.distributions.DoI.__call__","title":"<code>__call__(z, *, model_inputs=None)</code>  <code>abstractmethod</code>","text":"<p>Computes the distribution of interest from an initial point. If z: TensorLike is given, we assume there is only 1 input to the DoI layer. If z: List[TensorLike] is given, it provides all of the inputs to the DoI layer. </p> <p>Either way, we always return List[List[TensorLike]] (alias Inputs[Uniform[TensorLike]]) with outer list spanning layer inputs, and inner list spanning a distribution's instance.</p> <p>Parameters:</p> Name Type Description Default <code>z</code> <code>OM[Inputs, TensorLike]</code> <p>Input point from which the distribution is derived. If list/tuple, the point is defined by multiple tensors.</p> required <code>model_inputs</code> <code>Optional[ModelInputs]</code> <p>Optional wrapped model input arguments that produce value z at cut.</p> <code>None</code> <p>Returns:</p> Type Description <code>OM[Inputs, Uniform[TensorLike]]</code> <p>List of points which are all assigned equal probability mass in the</p> <code>OM[Inputs, Uniform[TensorLike]]</code> <p>distribution of interest, i.e., the distribution of interest is a</p> <code>OM[Inputs, Uniform[TensorLike]]</code> <p>discrete, uniform distribution over the list of returned points. If</p> <code>OM[Inputs, Uniform[TensorLike]]</code> <p>z is multi-input, returns a distribution for each input.</p> Source code in <code>trulens_explain/trulens/nn/distributions.py</code> <pre><code>@abstractmethod\ndef __call__(\n    self,\n    z: OM[Inputs, TensorLike],\n    *,\n    model_inputs: Optional[ModelInputs] = None\n) -&gt; OM[Inputs, Uniform[TensorLike]]:\n\"\"\"\n    Computes the distribution of interest from an initial point. If z:\n    TensorLike is given, we assume there is only 1 input to the DoI layer. If\n    z: List[TensorLike] is given, it provides all of the inputs to the DoI\n    layer. \n\n    Either way, we always return List[List[TensorLike]] (alias\n    Inputs[Uniform[TensorLike]]) with outer list spanning layer inputs, and\n    inner list spanning a distribution's instance.\n\n    Parameters:\n        z:\n            Input point from which the distribution is derived. If\n            list/tuple, the point is defined by multiple tensors.\n        model_inputs:\n            Optional wrapped model input arguments that produce value z at\n            cut.\n\n    Returns:\n        List of points which are all assigned equal probability mass in the\n        distribution of interest, i.e., the distribution of interest is a\n        discrete, uniform distribution over the list of returned points. If\n        z is multi-input, returns a distribution for each input.\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"trulens_explain/api/distributions/#trulens_explain.trulens.nn.distributions.DoI.__init__","title":"<code>__init__(cut=None)</code>","text":"<p>\"Initialize DoI</p> <p>Parameters:</p> Name Type Description Default <code>cut</code> <code>Cut</code> <p>The Cut in which the DoI will be applied. If <code>None</code>, the DoI will be applied to the input. otherwise, the distribution should be applied to the latent space defined by the cut.</p> <code>None</code> Source code in <code>trulens_explain/trulens/nn/distributions.py</code> <pre><code>def __init__(self, cut: Cut = None):\n\"\"\"\"Initialize DoI\n\n    Parameters:\n        cut (Cut, optional): \n            The Cut in which the DoI will be applied. If `None`, the DoI will be\n            applied to the input. otherwise, the distribution should be applied\n            to the latent space defined by the cut. \n    \"\"\"\n    self._cut = cut\n</code></pre>"},{"location":"trulens_explain/api/distributions/#trulens_explain.trulens.nn.distributions.DoI.cut","title":"<code>cut()</code>","text":"<p>Returns:</p> Type Description <code>Cut</code> <p>The Cut in which the DoI will be applied. If <code>None</code>, the DoI will be</p> <code>Cut</code> <p>applied to the input. otherwise, the distribution should be applied</p> <code>Cut</code> <p>to the latent space defined by the cut.</p> Source code in <code>trulens_explain/trulens/nn/distributions.py</code> <pre><code>def cut(self) -&gt; Cut:\n\"\"\"\n    Returns:\n        The Cut in which the DoI will be applied. If `None`, the DoI will be\n        applied to the input. otherwise, the distribution should be applied\n        to the latent space defined by the cut. \n    \"\"\"\n    return self._cut\n</code></pre>"},{"location":"trulens_explain/api/distributions/#trulens_explain.trulens.nn.distributions.DoI.get_activation_multiplier","title":"<code>get_activation_multiplier(activation, *, model_inputs=None)</code>","text":"<p>Returns a term to multiply the gradient by to convert from \"influence space\" to \"attribution space\". Conceptually, \"influence space\" corresponds to the potential effect of a slight increase in each feature, while \"attribution space\" corresponds to an approximation of the net marginal contribution to the quantity of interest of each feature.</p> <p>Parameters:</p> Name Type Description Default <code>activation</code> <code>OM[Inputs, TensorLike]</code> <p>The activation of the layer the DoI is applied to. DoI may be multi-input in which case activation will be a list.</p> required <code>model_inputs</code> <code>Optional[ModelInputs]</code> <p>Optional wrapped model input arguments that produce activation at cut.</p> <code>None</code> <p>Returns:</p> Type Description <code>OM[Inputs, TensorLike]</code> <p>An array with the same shape as <code>activation</code> that will be</p> <code>OM[Inputs, TensorLike]</code> <p>multiplied by the gradient to obtain the attribution. The default</p> <code>OM[Inputs, TensorLike]</code> <p>implementation of this method simply returns <code>activation</code>. If</p> <code>OM[Inputs, TensorLike]</code> <p>activation is multi-input, returns one multiplier for each.</p> Source code in <code>trulens_explain/trulens/nn/distributions.py</code> <pre><code>def get_activation_multiplier(\n    self,\n    activation: OM[Inputs, TensorLike],\n    *,\n    model_inputs: Optional[ModelInputs] = None\n) -&gt; OM[Inputs, TensorLike]:\n\"\"\"\n    Returns a term to multiply the gradient by to convert from \"*influence\n    space*\" to \"*attribution space*\". Conceptually, \"influence space\"\n    corresponds to the potential effect of a slight increase in each\n    feature, while \"attribution space\" corresponds to an approximation of\n    the net marginal contribution to the quantity of interest of each\n    feature.\n\n    Parameters:\n        activation:\n            The activation of the layer the DoI is applied to. DoI may be\n            multi-input in which case activation will be a list.\n        model_inputs:\n            Optional wrapped model input arguments that produce activation\n            at cut.\n\n    Returns:\n        An array with the same shape as ``activation`` that will be\n        multiplied by the gradient to obtain the attribution. The default\n        implementation of this method simply returns ``activation``. If\n        activation is multi-input, returns one multiplier for each.\n    \"\"\"\n    return om_of_many(activation)\n</code></pre>"},{"location":"trulens_explain/api/distributions/#trulens_explain.trulens.nn.distributions.DoiCutSupportError","title":"<code>DoiCutSupportError</code>","text":"<p>         Bases: <code>ValueError</code></p> <p>Exception raised if the distribution of interest is called on a cut whose output is not supported by the distribution of interest.</p> Source code in <code>trulens_explain/trulens/nn/distributions.py</code> <pre><code>class DoiCutSupportError(ValueError):\n\"\"\"\n    Exception raised if the distribution of interest is called on a cut whose\n    output is not supported by the distribution of interest.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"trulens_explain/api/distributions/#trulens_explain.trulens.nn.distributions.GaussianDoi","title":"<code>GaussianDoi</code>","text":"<p>         Bases: <code>DoI</code></p> <p>Distribution representing a Gaussian ball around the point. Used by Smooth Gradients.</p> Source code in <code>trulens_explain/trulens/nn/distributions.py</code> <pre><code>class GaussianDoi(DoI):\n\"\"\"\n    Distribution representing a Gaussian ball around the point. Used by Smooth\n    Gradients.\n    \"\"\"\n\n    def __init__(self, var: float, resolution: int, cut: Cut = None):\n\"\"\"\n        Parameters:\n            var:\n                The variance of the Gaussian noise to be added around the point.\n\n            resolution:\n                Number of samples returned by each call to this DoI.\n            cut (Cut, optional): \n                The Cut in which the DoI will be applied. If `None`, the DoI will be\n                applied to the input. otherwise, the distribution should be applied\n                to the latent space defined by the cut. \n        \"\"\"\n        super(GaussianDoi, self).__init__(cut)\n        self._var = var\n        self._resolution = resolution\n\n    def __str__(self):\n        return render_object(self, ['_cut', '_var', '_resolution'])\n\n    def __call__(self, z: OM[Inputs,\n                             TensorLike]) -&gt; OM[Inputs, Uniform[TensorLike]]:\n        # Public interface.\n\n        B = get_backend()\n        self._assert_cut_contains_only_one_tensor(z)\n\n        def gauss_of_input(z: TensorLike) -&gt; Uniform[TensorLike]:\n            # TODO: make a pytorch backend with the same interface to use in places like these.\n\n            if B.is_tensor(z):\n                # Tensor implementation.\n                return [\n                    z + B.random_normal_like(z, var=self._var)\n                    for _ in range(self._resolution)\n                ]  # Uniform\n\n            else:\n                # Array implementation.\n                return [\n                    z + np.random.normal(0., np.sqrt(self._var), z.shape)\n                    for _ in range(self._resolution)\n                ]  # Uniform\n\n        z: Inputs[TensorLike] = many_of_om(z)\n\n        return om_of_many(nested_map(z, gauss_of_input))\n</code></pre>"},{"location":"trulens_explain/api/distributions/#trulens_explain.trulens.nn.distributions.GaussianDoi.__init__","title":"<code>__init__(var, resolution, cut=None)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>var</code> <code>float</code> <p>The variance of the Gaussian noise to be added around the point.</p> required <code>resolution</code> <code>int</code> <p>Number of samples returned by each call to this DoI.</p> required <code>cut</code> <code>Cut</code> <p>The Cut in which the DoI will be applied. If <code>None</code>, the DoI will be applied to the input. otherwise, the distribution should be applied to the latent space defined by the cut.</p> <code>None</code> Source code in <code>trulens_explain/trulens/nn/distributions.py</code> <pre><code>def __init__(self, var: float, resolution: int, cut: Cut = None):\n\"\"\"\n    Parameters:\n        var:\n            The variance of the Gaussian noise to be added around the point.\n\n        resolution:\n            Number of samples returned by each call to this DoI.\n        cut (Cut, optional): \n            The Cut in which the DoI will be applied. If `None`, the DoI will be\n            applied to the input. otherwise, the distribution should be applied\n            to the latent space defined by the cut. \n    \"\"\"\n    super(GaussianDoi, self).__init__(cut)\n    self._var = var\n    self._resolution = resolution\n</code></pre>"},{"location":"trulens_explain/api/distributions/#trulens_explain.trulens.nn.distributions.LinearDoi","title":"<code>LinearDoi</code>","text":"<p>         Bases: <code>DoI</code></p> <p>Distribution representing the linear interpolation between a baseline and  the given point. Used by Integrated Gradients.</p> Source code in <code>trulens_explain/trulens/nn/distributions.py</code> <pre><code>class LinearDoi(DoI):\n\"\"\"\n    Distribution representing the linear interpolation between a baseline and \n    the given point. Used by Integrated Gradients.\n    \"\"\"\n\n    def __init__(\n        self,\n        baseline: BaselineLike = None,\n        resolution: int = 10,\n        *,\n        cut: Cut = None,\n    ):\n\"\"\"\n        The DoI for point, `z`, will be a uniform distribution over the points\n        on the line segment connecting `z` to `baseline`, approximated by a\n        sample of `resolution` points equally spaced along this segment.\n\n        Parameters:\n            cut (Cut, optional, from DoI): \n                The Cut in which the DoI will be applied. If `None`, the DoI\n                will be applied to the input. otherwise, the distribution should\n                be applied to the latent space defined by the cut. \n            baseline (BaselineLike, optional):\n                The baseline to interpolate from. Must be same shape as the\n                space the distribution acts over, i.e., the shape of the points,\n                `z`, eventually passed to `__call__`. If `cut` is `None`, this\n                must be the same shape as the input, otherwise this must be the\n                same shape as the latent space defined by the cut. If `None` is\n                given, `baseline` will be the zero vector in the appropriate\n                shape. If the baseline is callable, it is expected to return the\n                `baseline`, given `z` and optional model arguments.\n            resolution (int):\n                Number of points returned by each call to this DoI. A higher\n                resolution is more computationally expensive, but gives a better\n                approximation of the DoI this object mathematically represents.\n        \"\"\"\n        super(LinearDoi, self).__init__(cut)\n        self._baseline = baseline\n        self._resolution = resolution\n\n    @property\n    def baseline(self) -&gt; BaselineLike:\n        return self._baseline\n\n    @property\n    def resolution(self) -&gt; int:\n        return self._resolution\n\n    def __str__(self):\n        return render_object(self, ['_cut', '_baseline', '_resolution'])\n\n    def __call__(\n        self,\n        z: OM[Inputs, TensorLike],\n        *,\n        model_inputs: Optional[ModelInputs] = None\n    ) -&gt; OM[Inputs, Uniform[TensorLike]]:\n\n        self._assert_cut_contains_only_one_tensor(z)\n\n        z: Inputs[TensorLike] = many_of_om(z)\n\n        baseline = self._compute_baseline(z, model_inputs=model_inputs)\n\n        r = 1. if self._resolution == 1 else self._resolution - 1.\n        zipped = nested_zip(z, baseline)\n\n        def zipped_interpolate(zipped_z_baseline):\n\"\"\"interpolates zipped elements\n\n            Args:\n                zipped_z_baseline: A tuple expecting the first element to be the z_val, and second to be the baseline.\n\n            Returns:\n                a list of interpolations from z to baseline\n            \"\"\"\n            z_ = zipped_z_baseline[0]\n            b_ = zipped_z_baseline[1]\n            return [ # Uniform\n                (1. - i / r) * z_ + i / r * b_\n                for i in range(self._resolution)\n            ]\n\n        ret = om_of_many(\n            nested_map(\n                zipped, zipped_interpolate, check_accessor=lambda x: x[0]\n            )\n        )\n\n        return ret\n\n    def get_activation_multiplier(\n        self,\n        activation: OM[Inputs, TensorLike],\n        *,\n        model_inputs: Optional[ModelInputs] = None\n    ) -&gt; Inputs[TensorLike]:\n\"\"\"\n        Returns a term to multiply the gradient by to convert from \"*influence \n        space*\" to \"*attribution space*\". Conceptually, \"influence space\"\n        corresponds to the potential effect of a slight increase in each \n        feature, while \"attribution space\" corresponds to an approximation of\n        the net marginal contribution to the quantity of interest of each \n        feature.\n\n        Parameters:\n            activation:\n                The activation of the layer the DoI is applied to.\n\n        Returns:\n            The activation adjusted by the baseline passed to the constructor.\n        \"\"\"\n\n        activation: Inputs[TensorLike] = many_of_om(activation)\n\n        baseline: Inputs[TensorLike] = self._compute_baseline(\n            activation, model_inputs=model_inputs\n        )\n\n        if baseline is None:\n            return activation\n\n        zipped = nested_zip(activation, baseline)\n\n        def zipped_subtract(zipped_activation_baseline):\n\"\"\"subtracts zipped elements\n\n            Args:\n                zipped_activation_baseline: A tuple expecting the first element to be the activation, and second to be the baseline.\n\n            Returns:\n                a subtraction of activation and baseline\n            \"\"\"\n            activation = zipped_activation_baseline[0]\n            baseline = zipped_activation_baseline[1]\n            return activation - baseline\n\n        ret = nested_map(zipped, zipped_subtract, check_accessor=lambda x: x[0])\n        return ret\n\n    def _compute_baseline(\n        self,\n        z: Inputs[TensorLike],\n        *,\n        model_inputs: Optional[ModelInputs] = None\n    ) -&gt; Inputs[TensorLike]:\n\n        B = get_backend()\n\n        _baseline: BaselineLike = self.baseline  # user-provided\n\n        if isinstance(_baseline, Callable):\n            if accepts_model_inputs(_baseline):\n                _baseline: OM[Inputs, TensorLike] = many_of_om(\n                    _baseline(om_of_many(z), model_inputs=model_inputs)\n                )\n            else:\n                _baseline: OM[Inputs, TensorLike] = many_of_om(\n                    _baseline(om_of_many(z))\n                )\n\n        else:\n            _baseline: OM[Inputs, TensorLike]\n\n        if _baseline is None:\n            _baseline: Inputs[TensorLike] = nested_map(z, B.zeros_like)\n        else:\n            _baseline: Inputs[TensorLike] = many_of_om(_baseline)\n            # Came from user; could have been single or multiple inputs.\n        tensor_wrapper = TensorAKs(args=z)\n        # Cast to either Tensor or numpy.ndarray to match what was given in z.\n        return nested_cast(\n            backend=B,\n            args=_baseline,\n            astype=type(tensor_wrapper.first_batchable(B))\n        )\n</code></pre>"},{"location":"trulens_explain/api/distributions/#trulens_explain.trulens.nn.distributions.LinearDoi.__init__","title":"<code>__init__(baseline=None, resolution=10, *, cut=None)</code>","text":"<p>The DoI for point, <code>z</code>, will be a uniform distribution over the points on the line segment connecting <code>z</code> to <code>baseline</code>, approximated by a sample of <code>resolution</code> points equally spaced along this segment.</p> <p>Parameters:</p> Name Type Description Default <code>cut</code> <code>Cut, optional, from DoI</code> <p>The Cut in which the DoI will be applied. If <code>None</code>, the DoI will be applied to the input. otherwise, the distribution should be applied to the latent space defined by the cut. </p> <code>None</code> <code>baseline</code> <code>BaselineLike</code> <p>The baseline to interpolate from. Must be same shape as the space the distribution acts over, i.e., the shape of the points, <code>z</code>, eventually passed to <code>__call__</code>. If <code>cut</code> is <code>None</code>, this must be the same shape as the input, otherwise this must be the same shape as the latent space defined by the cut. If <code>None</code> is given, <code>baseline</code> will be the zero vector in the appropriate shape. If the baseline is callable, it is expected to return the <code>baseline</code>, given <code>z</code> and optional model arguments.</p> <code>None</code> <code>resolution</code> <code>int</code> <p>Number of points returned by each call to this DoI. A higher resolution is more computationally expensive, but gives a better approximation of the DoI this object mathematically represents.</p> <code>10</code> Source code in <code>trulens_explain/trulens/nn/distributions.py</code> <pre><code>def __init__(\n    self,\n    baseline: BaselineLike = None,\n    resolution: int = 10,\n    *,\n    cut: Cut = None,\n):\n\"\"\"\n    The DoI for point, `z`, will be a uniform distribution over the points\n    on the line segment connecting `z` to `baseline`, approximated by a\n    sample of `resolution` points equally spaced along this segment.\n\n    Parameters:\n        cut (Cut, optional, from DoI): \n            The Cut in which the DoI will be applied. If `None`, the DoI\n            will be applied to the input. otherwise, the distribution should\n            be applied to the latent space defined by the cut. \n        baseline (BaselineLike, optional):\n            The baseline to interpolate from. Must be same shape as the\n            space the distribution acts over, i.e., the shape of the points,\n            `z`, eventually passed to `__call__`. If `cut` is `None`, this\n            must be the same shape as the input, otherwise this must be the\n            same shape as the latent space defined by the cut. If `None` is\n            given, `baseline` will be the zero vector in the appropriate\n            shape. If the baseline is callable, it is expected to return the\n            `baseline`, given `z` and optional model arguments.\n        resolution (int):\n            Number of points returned by each call to this DoI. A higher\n            resolution is more computationally expensive, but gives a better\n            approximation of the DoI this object mathematically represents.\n    \"\"\"\n    super(LinearDoi, self).__init__(cut)\n    self._baseline = baseline\n    self._resolution = resolution\n</code></pre>"},{"location":"trulens_explain/api/distributions/#trulens_explain.trulens.nn.distributions.LinearDoi.get_activation_multiplier","title":"<code>get_activation_multiplier(activation, *, model_inputs=None)</code>","text":"<p>Returns a term to multiply the gradient by to convert from \"influence  space\" to \"attribution space\". Conceptually, \"influence space\" corresponds to the potential effect of a slight increase in each  feature, while \"attribution space\" corresponds to an approximation of the net marginal contribution to the quantity of interest of each  feature.</p> <p>Parameters:</p> Name Type Description Default <code>activation</code> <code>OM[Inputs, TensorLike]</code> <p>The activation of the layer the DoI is applied to.</p> required <p>Returns:</p> Type Description <code>Inputs[TensorLike]</code> <p>The activation adjusted by the baseline passed to the constructor.</p> Source code in <code>trulens_explain/trulens/nn/distributions.py</code> <pre><code>def get_activation_multiplier(\n    self,\n    activation: OM[Inputs, TensorLike],\n    *,\n    model_inputs: Optional[ModelInputs] = None\n) -&gt; Inputs[TensorLike]:\n\"\"\"\n    Returns a term to multiply the gradient by to convert from \"*influence \n    space*\" to \"*attribution space*\". Conceptually, \"influence space\"\n    corresponds to the potential effect of a slight increase in each \n    feature, while \"attribution space\" corresponds to an approximation of\n    the net marginal contribution to the quantity of interest of each \n    feature.\n\n    Parameters:\n        activation:\n            The activation of the layer the DoI is applied to.\n\n    Returns:\n        The activation adjusted by the baseline passed to the constructor.\n    \"\"\"\n\n    activation: Inputs[TensorLike] = many_of_om(activation)\n\n    baseline: Inputs[TensorLike] = self._compute_baseline(\n        activation, model_inputs=model_inputs\n    )\n\n    if baseline is None:\n        return activation\n\n    zipped = nested_zip(activation, baseline)\n\n    def zipped_subtract(zipped_activation_baseline):\n\"\"\"subtracts zipped elements\n\n        Args:\n            zipped_activation_baseline: A tuple expecting the first element to be the activation, and second to be the baseline.\n\n        Returns:\n            a subtraction of activation and baseline\n        \"\"\"\n        activation = zipped_activation_baseline[0]\n        baseline = zipped_activation_baseline[1]\n        return activation - baseline\n\n    ret = nested_map(zipped, zipped_subtract, check_accessor=lambda x: x[0])\n    return ret\n</code></pre>"},{"location":"trulens_explain/api/distributions/#trulens_explain.trulens.nn.distributions.PointDoi","title":"<code>PointDoi</code>","text":"<p>         Bases: <code>DoI</code></p> <p>Distribution that puts all probability mass on a single point.</p> Source code in <code>trulens_explain/trulens/nn/distributions.py</code> <pre><code>class PointDoi(DoI):\n\"\"\"\n    Distribution that puts all probability mass on a single point.\n    \"\"\"\n\n    def __init__(self, cut: Cut = None):\n\"\"\"\"Initialize PointDoI\n\n        Parameters:\n            cut (Cut, optional): \n                The Cut in which the DoI will be applied. If `None`, the DoI will be\n                applied to the input. otherwise, the distribution should be applied\n                to the latent space defined by the cut. \n        \"\"\"\n        super(PointDoi, self).__init__(cut)\n\n    def __call__(\n        self,\n        z: OM[Inputs, TensorLike],\n        *,\n        model_inputs: Optional[ModelInputs] = None\n    ) -&gt; OM[Inputs, Uniform[TensorLike]]:\n\n        z: Inputs[TensorLike] = many_of_om(z)\n\n        return om_of_many(nested_map(z, lambda x: [x]))\n</code></pre>"},{"location":"trulens_explain/api/distributions/#trulens_explain.trulens.nn.distributions.PointDoi.__init__","title":"<code>__init__(cut=None)</code>","text":"<p>\"Initialize PointDoI</p> <p>Parameters:</p> Name Type Description Default <code>cut</code> <code>Cut</code> <p>The Cut in which the DoI will be applied. If <code>None</code>, the DoI will be applied to the input. otherwise, the distribution should be applied to the latent space defined by the cut.</p> <code>None</code> Source code in <code>trulens_explain/trulens/nn/distributions.py</code> <pre><code>def __init__(self, cut: Cut = None):\n\"\"\"\"Initialize PointDoI\n\n    Parameters:\n        cut (Cut, optional): \n            The Cut in which the DoI will be applied. If `None`, the DoI will be\n            applied to the input. otherwise, the distribution should be applied\n            to the latent space defined by the cut. \n    \"\"\"\n    super(PointDoi, self).__init__(cut)\n</code></pre>"},{"location":"trulens_explain/api/model_wrappers/","title":"Model Wrappers","text":"<p>The TruLens library is designed to support models implemented via a variety of different popular python neural network frameworks: Keras (with TensorFlow or  Theano backend), TensorFlow, and Pytorch. Models developed with different frameworks  implement things (e.g., gradient computations) a number of different ways. We define  framework specific <code>ModelWrapper</code> instances to create a unified model API, providing the same  functionality to models that are implemented in disparate frameworks. In order to compute  attributions for a model, we provide a <code>trulens.nn.models.get_model_wrapper</code> function that will return an appropriate <code>ModelWrapper</code> instance.</p> <p>Some parameters are exclusively utilized for specific frameworks and are outlined  in the parameter descriptions.</p>"},{"location":"trulens_explain/api/model_wrappers/#trulens_explain.trulens.nn.models.get_model_wrapper","title":"<code>get_model_wrapper(model, *, logit_layer=None, replace_softmax=False, softmax_layer=-1, custom_objects=None, device=None, input_tensors=None, output_tensors=None, internal_tensor_dict=None, default_feed_dict=None, session=None, backend=None, force_eval=True, **kwargs)</code>","text":"<p>Returns a ModelWrapper implementation that exposes the components needed for computing attributions.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>ModelLike</code> <p>The model to wrap. If using the TensorFlow 1 backend, this is  expected to be a graph object.</p> required <code>logit_layer</code> <p>Supported for Keras and Pytorch models.  Specifies the name or index of the layer that produces the logit predictions. </p> <code>None</code> <code>replace_softmax</code> <code>bool</code> <p>Supported for Keras models only. If true, the activation function in the softmax layer (specified by <code>softmax_layer</code>)  will be changed to a <code>'linear'</code> activation. </p> <code>False</code> <code>softmax_layer</code> <p>Supported for Keras models only. Specifies the layer that performs the softmax. This layer should have an <code>activation</code> attribute. Only used when <code>replace_softmax</code> is true.</p> <code>-1</code> <code>custom_objects</code> <p>Optional, for use with Keras models only. A dictionary of custom objects used by the Keras model.</p> <code>None</code> <code>device</code> <code>str</code> <p>Optional, for use with Pytorch models only. A string specifying the device to run the model on.</p> <code>None</code> <code>input_tensors</code> <p>Required for use with TensorFlow 1 graph models only. A list of tensors representing the input to the model graph.</p> <code>None</code> <code>output_tensors</code> <p>Required for use with TensorFlow 1 graph models only. A list of tensors representing the output to the model graph.</p> <code>None</code> <code>internal_tensor_dict</code> <p>Optional, for use with TensorFlow 1 graph models only. A dictionary mapping user-selected layer names to the internal tensors in the model graph that the user would like to expose. This is provided to give more human-readable names to the layers if desired. Internal tensors can also be accessed via the name given to them by tensorflow.</p> <code>None</code> <code>default_feed_dict</code> <p>Optional, for use with TensorFlow 1 graph models only. A dictionary of default values to give to tensors in the model graph.</p> <code>None</code> <code>session</code> <p>Optional, for use with TensorFlow 1 graph models only. A  <code>tf.Session</code> object to run the model graph in. If <code>None</code>, a new temporary session will be generated every time the model is run.</p> <code>None</code> <code>backend</code> <p>Optional, for forcing a specific backend. String values recognized are pytorch, tensorflow, keras, or tf.keras.</p> <code>None</code> <code>force_eval</code> <p>_Optional, True will force a model.eval() call for PyTorch models. False will retain current model state</p> <code>True</code> Source code in <code>trulens_explain/trulens/nn/models/__init__.py</code> <pre><code>def get_model_wrapper(\n    model: ModelLike,\n    *,\n    logit_layer=None,\n    replace_softmax: bool = False,\n    softmax_layer=-1,\n    custom_objects=None,\n    device: str = None,\n    input_tensors=None,\n    output_tensors=None,\n    internal_tensor_dict=None,\n    default_feed_dict=None,\n    session=None,\n    backend=None,\n    force_eval=True,\n    **kwargs\n):\n\"\"\"\n    Returns a ModelWrapper implementation that exposes the components needed for computing attributions.\n\n    Parameters:\n        model:\n            The model to wrap. If using the TensorFlow 1 backend, this is \n            expected to be a graph object.\n\n        logit_layer:\n            _Supported for Keras and Pytorch models._ \n            Specifies the name or index of the layer that produces the\n            logit predictions. \n\n        replace_softmax:\n            _Supported for Keras models only._ If true, the activation\n            function in the softmax layer (specified by `softmax_layer`) \n            will be changed to a `'linear'` activation. \n\n        softmax_layer:\n            _Supported for Keras models only._ Specifies the layer that\n            performs the softmax. This layer should have an `activation`\n            attribute. Only used when `replace_softmax` is true.\n\n        custom_objects:\n            _Optional, for use with Keras models only._ A dictionary of\n            custom objects used by the Keras model.\n\n        device:\n            _Optional, for use with Pytorch models only._ A string\n            specifying the device to run the model on.\n\n        input_tensors:\n            _Required for use with TensorFlow 1 graph models only._ A list\n            of tensors representing the input to the model graph.\n\n        output_tensors:\n            _Required for use with TensorFlow 1 graph models only._ A list\n            of tensors representing the output to the model graph.\n\n        internal_tensor_dict:\n            _Optional, for use with TensorFlow 1 graph models only._ A\n            dictionary mapping user-selected layer names to the internal\n            tensors in the model graph that the user would like to expose.\n            This is provided to give more human-readable names to the layers\n            if desired. Internal tensors can also be accessed via the name\n            given to them by tensorflow.\n\n        default_feed_dict:\n            _Optional, for use with TensorFlow 1 graph models only._ A\n            dictionary of default values to give to tensors in the model\n            graph.\n\n        session:\n            _Optional, for use with TensorFlow 1 graph models only._ A \n            `tf.Session` object to run the model graph in. If `None`, a new\n            temporary session will be generated every time the model is run.\n\n        backend:\n            _Optional, for forcing a specific backend._ String values recognized\n            are pytorch, tensorflow, keras, or tf.keras.\n\n        force_eval:\n            _Optional, True will force a model.eval() call for PyTorch models. False\n            will retain current model state\n\n    Returns: ModelWrapper\n    \"\"\"\n\n    if 'input_shape' in kwargs:\n        tru_logger.deprecate(\n            f\"get_model_wrapper: input_shape parameter is no longer used and will be removed in the future\"\n        )\n        del kwargs['input_shape']\n    if 'input_dtype' in kwargs:\n        tru_logger.deprecate(\n            f\"get_model_wrapper: input_dtype parameter is no longer used and will be removed in the future\"\n        )\n        del kwargs['input_dtype']\n\n    # get existing backend\n    B = get_backend(suppress_warnings=True)\n\n    if backend is None:\n        backend = discern_backend(model)\n        tru_logger.info(\n            \"Detected {} backend for {}.\".format(\n                backend.name.lower(), type(model)\n            )\n        )\n    else:\n        backend = Backend.from_name(backend)\n    if B is None or (backend is not Backend.UNKNOWN and B.backend != backend):\n        tru_logger.info(\n            \"Changing backend from {} to {}.\".format(\n                None if B is None else B.backend, backend\n            )\n        )\n        os.environ['TRULENS_BACKEND'] = backend.name.lower()\n        B = get_backend()\n    else:\n        tru_logger.info(\"Using backend {}.\".format(B.backend))\n    tru_logger.info(\n        \"If this seems incorrect, you can force the correct backend by passing the `backend` parameter directly into your get_model_wrapper call.\"\n    )\n    if B.backend.is_keras_derivative():\n        from trulens.nn.models.keras import KerasModelWrapper\n        return KerasModelWrapper(\n            model,\n            logit_layer=logit_layer,\n            replace_softmax=replace_softmax,\n            softmax_layer=softmax_layer,\n            custom_objects=custom_objects\n        )\n\n    elif B.backend == Backend.PYTORCH:\n        from trulens.nn.models.pytorch import PytorchModelWrapper\n        return PytorchModelWrapper(\n            model,\n            logit_layer=logit_layer,\n            device=device,\n            force_eval=force_eval\n        )\n    elif B.backend == Backend.TENSORFLOW:\n        import tensorflow as tf\n        if tf.__version__.startswith('2'):\n            from trulens.nn.models.tensorflow_v2 import Tensorflow2ModelWrapper\n            return Tensorflow2ModelWrapper(\n                model,\n                logit_layer=logit_layer,\n                replace_softmax=replace_softmax,\n                softmax_layer=softmax_layer,\n                custom_objects=custom_objects\n            )\n        else:\n            from trulens.nn.models.tensorflow_v1 import TensorflowModelWrapper\n            if input_tensors is None:\n                tru_logger.error(\n                    'tensorflow1 model must pass parameter: input_tensors'\n                )\n            if output_tensors is None:\n                tru_logger.error(\n                    'tensorflow1 model must pass parameter: output_tensors'\n                )\n            return TensorflowModelWrapper(\n                model,\n                input_tensors=input_tensors,\n                output_tensors=output_tensors,\n                internal_tensor_dict=internal_tensor_dict,\n                session=session\n            )\n</code></pre>"},{"location":"trulens_explain/api/quantities/","title":"Quantities of Interest","text":"<p>A Quantity of Interest (QoI) is a function of the output that determines the  network output behavior that the attributions describe.</p> <p>The quantity of interest lets us specify what we want to explain. Often, this is the output of the network corresponding to a particular class, addressing, e.g., \"Why did the model classify a given image as a car?\" However, we could also  consider various combinations of outputs, allowing us to ask more specific  questions, such as, \"Why did the model classify a given image as a sedan and  not a convertible?\" The former may highlight general \u201ccar features,\u201d such as  tires, while the latter (called a comparative explanation) might focus on the  roof of the car, a \u201ccar feature\u201d not shared by convertibles.</p>"},{"location":"trulens_explain/api/quantities/#trulens_explain.trulens.nn.quantities.ClassQoI","title":"<code>ClassQoI</code>","text":"<p>         Bases: <code>QoI</code></p> <p>Quantity of interest for attributing output towards a specified class.</p> Source code in <code>trulens_explain/trulens/nn/quantities.py</code> <pre><code>class ClassQoI(QoI):\n\"\"\"\n    Quantity of interest for attributing output towards a specified class.\n    \"\"\"\n\n    def __init__(self, cl: int):\n\"\"\"\n        Parameters:\n            cl:\n                The index of the class the QoI is for.\n        \"\"\"\n        self.cl = cl\n\n    def __str__(self):\n        return render_object(self, [\"cl\"])\n\n    def __call__(self, y: TensorLike) -&gt; TensorLike:\n        self._assert_cut_contains_only_one_tensor(y)\n\n        return y[:, self.cl]\n</code></pre>"},{"location":"trulens_explain/api/quantities/#trulens_explain.trulens.nn.quantities.ClassQoI.__init__","title":"<code>__init__(cl)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>cl</code> <code>int</code> <p>The index of the class the QoI is for.</p> required Source code in <code>trulens_explain/trulens/nn/quantities.py</code> <pre><code>def __init__(self, cl: int):\n\"\"\"\n    Parameters:\n        cl:\n            The index of the class the QoI is for.\n    \"\"\"\n    self.cl = cl\n</code></pre>"},{"location":"trulens_explain/api/quantities/#trulens_explain.trulens.nn.quantities.ClassSeqQoI","title":"<code>ClassSeqQoI</code>","text":"<p>         Bases: <code>QoI</code></p> <p>Quantity of interest for attributing output towards a sequence of classes  for each input.</p> Source code in <code>trulens_explain/trulens/nn/quantities.py</code> <pre><code>class ClassSeqQoI(QoI):\n\"\"\"\n    Quantity of interest for attributing output towards a sequence of classes \n    for each input.\n    \"\"\"\n\n    def __init__(self, seq_labels: List[int]):\n\"\"\"\n        Parameters:\n            seq_labels:\n                A sequence of classes corresponding to each input.\n        \"\"\"\n        self.seq_labels = seq_labels\n\n    def __call__(self, y):\n\n        self._assert_cut_contains_only_one_tensor(y)\n        assert get_backend().shape(y)[0] == len(self.seq_labels)\n\n        return y[:, self.seq_labels]\n</code></pre>"},{"location":"trulens_explain/api/quantities/#trulens_explain.trulens.nn.quantities.ClassSeqQoI.__init__","title":"<code>__init__(seq_labels)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>seq_labels</code> <code>List[int]</code> <p>A sequence of classes corresponding to each input.</p> required Source code in <code>trulens_explain/trulens/nn/quantities.py</code> <pre><code>def __init__(self, seq_labels: List[int]):\n\"\"\"\n    Parameters:\n        seq_labels:\n            A sequence of classes corresponding to each input.\n    \"\"\"\n    self.seq_labels = seq_labels\n</code></pre>"},{"location":"trulens_explain/api/quantities/#trulens_explain.trulens.nn.quantities.ComparativeQoI","title":"<code>ComparativeQoI</code>","text":"<p>         Bases: <code>QoI</code></p> <p>Quantity of interest for attributing network output towards a given class,  relative to another.</p> Source code in <code>trulens_explain/trulens/nn/quantities.py</code> <pre><code>class ComparativeQoI(QoI):\n\"\"\"\n    Quantity of interest for attributing network output towards a given class, \n    relative to another.\n    \"\"\"\n\n    def __init__(self, cl1: int, cl2: int):\n\"\"\"\n        Parameters:\n            cl1:\n                The index of the class the QoI is for.\n            cl2:\n                The index of the class to compare against.\n        \"\"\"\n        self.cl1 = cl1\n        self.cl2 = cl2\n\n    def __str__(self):\n        return render_object(self, [\"cl1\", \"cl2\"])\n\n    def __call__(self, y: TensorLike) -&gt; TensorLike:\n\n        self._assert_cut_contains_only_one_tensor(y)\n\n        return y[:, self.cl1] - y[:, self.cl2]\n</code></pre>"},{"location":"trulens_explain/api/quantities/#trulens_explain.trulens.nn.quantities.ComparativeQoI.__init__","title":"<code>__init__(cl1, cl2)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>cl1</code> <code>int</code> <p>The index of the class the QoI is for.</p> required <code>cl2</code> <code>int</code> <p>The index of the class to compare against.</p> required Source code in <code>trulens_explain/trulens/nn/quantities.py</code> <pre><code>def __init__(self, cl1: int, cl2: int):\n\"\"\"\n    Parameters:\n        cl1:\n            The index of the class the QoI is for.\n        cl2:\n            The index of the class to compare against.\n    \"\"\"\n    self.cl1 = cl1\n    self.cl2 = cl2\n</code></pre>"},{"location":"trulens_explain/api/quantities/#trulens_explain.trulens.nn.quantities.InternalChannelQoI","title":"<code>InternalChannelQoI</code>","text":"<p>         Bases: <code>QoI</code></p> <p>Quantity of interest for attributing output towards the output of an  internal convolutional layer channel, aggregating using a specified  operation.</p> <p>Also works for non-convolutional dense layers, where the given neuron's activation is returned.</p> Source code in <code>trulens_explain/trulens/nn/quantities.py</code> <pre><code>class InternalChannelQoI(QoI):\n\"\"\"\n    Quantity of interest for attributing output towards the output of an \n    internal convolutional layer channel, aggregating using a specified \n    operation.\n\n    Also works for non-convolutional dense layers, where the given neuron's\n    activation is returned.\n    \"\"\"\n\n    @staticmethod\n    def _batch_sum(x):\n\"\"\"\n        Sums batched 2D channels, leaving the batch dimension unchanged.\n        \"\"\"\n        return get_backend().sum(x, axis=(1, 2))\n\n    def __init__(\n        self,\n        channel: Union[int, List[int]],\n        channel_axis: Optional[int] = None,\n        agg_fn: Optional[Callable] = None\n    ):\n\"\"\"\n        Parameters:\n            channel:\n                Channel to return. If a list is provided, then the quantity sums \n                over each of the channels in the list.\n\n            channel_axis:\n                Channel dimension index, if relevant, e.g., for 2D convolutional\n                layers. If `channel_axis` is `None`, then the channel axis of \n                the relevant backend will be used. This argument is not used \n                when the channels are scalars, e.g., for dense layers.\n\n            agg_fn:\n                Function with which to aggregate the remaining dimensions \n                (except the batch dimension) in order to get a single scalar \n                value for each channel. If `agg_fn` is `None` then a sum over \n                each neuron in the channel will be taken. This argument is not \n                used when the channels are scalars, e.g., for dense layers.\n        \"\"\"\n        if channel_axis is None:\n            channel_axis = get_backend().channel_axis\n        if agg_fn is None:\n            agg_fn = InternalChannelQoI._batch_sum\n\n        self._channel_ax = channel_axis\n        self._agg_fn = agg_fn\n        self._channels = channel if isinstance(channel, list) else [channel]\n\n    def __call__(self, y: TensorLike) -&gt; TensorLike:\n        B = get_backend()\n        self._assert_cut_contains_only_one_tensor(y)\n\n        if len(B.int_shape(y)) == 2:\n            return sum([y[:, ch] for ch in self._channels])\n\n        elif len(B.int_shape(y)) == 3:\n            return sum([self._agg_fn(y[:, :, ch]) for ch in self._channel])\n\n        elif len(B.int_shape(y)) == 4:\n            if self._channel_ax == 1:\n                return sum([self._agg_fn(y[:, ch]) for ch in self._channels])\n\n            elif self._channel_ax == 3:\n                return sum(\n                    [self._agg_fn(y[:, :, :, ch]) for ch in self._channels]\n                )\n\n            else:\n                raise ValueError(\n                    'Unsupported channel axis for convolutional layer: {}'.\n                    format(self._channel_ax)\n                )\n\n        else:\n            raise QoiCutSupportError(\n                'Unsupported tensor rank for `InternalChannelQoI`: {}'.format(\n                    len(B.int_shape(y))\n                )\n            )\n</code></pre>"},{"location":"trulens_explain/api/quantities/#trulens_explain.trulens.nn.quantities.InternalChannelQoI.__init__","title":"<code>__init__(channel, channel_axis=None, agg_fn=None)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>channel</code> <code>Union[int, List[int]]</code> <p>Channel to return. If a list is provided, then the quantity sums  over each of the channels in the list.</p> required <code>channel_axis</code> <code>Optional[int]</code> <p>Channel dimension index, if relevant, e.g., for 2D convolutional layers. If <code>channel_axis</code> is <code>None</code>, then the channel axis of  the relevant backend will be used. This argument is not used  when the channels are scalars, e.g., for dense layers.</p> <code>None</code> <code>agg_fn</code> <code>Optional[Callable]</code> <p>Function with which to aggregate the remaining dimensions  (except the batch dimension) in order to get a single scalar  value for each channel. If <code>agg_fn</code> is <code>None</code> then a sum over  each neuron in the channel will be taken. This argument is not  used when the channels are scalars, e.g., for dense layers.</p> <code>None</code> Source code in <code>trulens_explain/trulens/nn/quantities.py</code> <pre><code>def __init__(\n    self,\n    channel: Union[int, List[int]],\n    channel_axis: Optional[int] = None,\n    agg_fn: Optional[Callable] = None\n):\n\"\"\"\n    Parameters:\n        channel:\n            Channel to return. If a list is provided, then the quantity sums \n            over each of the channels in the list.\n\n        channel_axis:\n            Channel dimension index, if relevant, e.g., for 2D convolutional\n            layers. If `channel_axis` is `None`, then the channel axis of \n            the relevant backend will be used. This argument is not used \n            when the channels are scalars, e.g., for dense layers.\n\n        agg_fn:\n            Function with which to aggregate the remaining dimensions \n            (except the batch dimension) in order to get a single scalar \n            value for each channel. If `agg_fn` is `None` then a sum over \n            each neuron in the channel will be taken. This argument is not \n            used when the channels are scalars, e.g., for dense layers.\n    \"\"\"\n    if channel_axis is None:\n        channel_axis = get_backend().channel_axis\n    if agg_fn is None:\n        agg_fn = InternalChannelQoI._batch_sum\n\n    self._channel_ax = channel_axis\n    self._agg_fn = agg_fn\n    self._channels = channel if isinstance(channel, list) else [channel]\n</code></pre>"},{"location":"trulens_explain/api/quantities/#trulens_explain.trulens.nn.quantities.LambdaQoI","title":"<code>LambdaQoI</code>","text":"<p>         Bases: <code>QoI</code></p> <p>Generic quantity of interest allowing the user to specify a function of the model's output as the QoI.</p> Source code in <code>trulens_explain/trulens/nn/quantities.py</code> <pre><code>class LambdaQoI(QoI):\n\"\"\"\n    Generic quantity of interest allowing the user to specify a function of the\n    model's output as the QoI.\n    \"\"\"\n\n    def __init__(self, function: Callable):\n\"\"\"\n        Parameters:\n            function:\n                A callable that takes a single argument representing the model's \n                tensor output and returns a differentiable batched scalar tensor \n                representing the QoI.\n        \"\"\"\n        if len(signature(function).parameters) != 1:\n            raise ValueError(\n                'QoI function must take exactly 1 argument, but provided '\n                'function takes {} arguments'.format(\n                    len(signature(function).parameters)\n                )\n            )\n\n        self.function = function\n\n    def __call__(self, y: TensorLike) -&gt; TensorLike:\n        return self.function(y)\n</code></pre>"},{"location":"trulens_explain/api/quantities/#trulens_explain.trulens.nn.quantities.LambdaQoI.__init__","title":"<code>__init__(function)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>function</code> <code>Callable</code> <p>A callable that takes a single argument representing the model's  tensor output and returns a differentiable batched scalar tensor  representing the QoI.</p> required Source code in <code>trulens_explain/trulens/nn/quantities.py</code> <pre><code>def __init__(self, function: Callable):\n\"\"\"\n    Parameters:\n        function:\n            A callable that takes a single argument representing the model's \n            tensor output and returns a differentiable batched scalar tensor \n            representing the QoI.\n    \"\"\"\n    if len(signature(function).parameters) != 1:\n        raise ValueError(\n            'QoI function must take exactly 1 argument, but provided '\n            'function takes {} arguments'.format(\n                len(signature(function).parameters)\n            )\n        )\n\n    self.function = function\n</code></pre>"},{"location":"trulens_explain/api/quantities/#trulens_explain.trulens.nn.quantities.MaxClassQoI","title":"<code>MaxClassQoI</code>","text":"<p>         Bases: <code>QoI</code></p> <p>Quantity of interest for attributing output towards the maximum-predicted  class.</p> Source code in <code>trulens_explain/trulens/nn/quantities.py</code> <pre><code>class MaxClassQoI(QoI):\n\"\"\"\n    Quantity of interest for attributing output towards the maximum-predicted \n    class.\n    \"\"\"\n\n    def __init__(\n        self, axis: int = 1, activation: Union[Callable, str, None] = None\n    ):\n\"\"\"\n        Parameters:\n            axis:\n                Output dimension over which max operation is taken.\n\n            activation:\n                Activation function to be applied to the output before taking \n                the max. If `activation` is a string, use the corresponding \n                named activation function implemented by the backend. The \n                following strings are currently supported as shorthands for the\n                respective standard activation functions:\n\n                - `'sigmoid'` \n                - `'softmax'` \n\n                If `activation` is `None`, no activation function is applied to\n                the input.\n        \"\"\"\n        self._axis = axis\n        self.activation = activation\n\n    def __str__(self):\n        return render_object(self, [\"_axis\", \"activation\"])\n\n    def __call__(self, y: TensorLike) -&gt; TensorLike:\n        self._assert_cut_contains_only_one_tensor(y)\n\n        if self.activation is not None:\n            if isinstance(self.activation, str):\n                self.activation = self.activation.lower()\n                if self.activation in ['sigmoid', 'softmax']:\n                    y = getattr(get_backend(), self.activation)(y)\n\n                else:\n                    raise NotImplementedError(\n                        'This activation function is not currently supported '\n                        'by the backend'\n                    )\n            else:\n                y = self.activation(y)\n\n        return get_backend().max(y, axis=self._axis)\n</code></pre>"},{"location":"trulens_explain/api/quantities/#trulens_explain.trulens.nn.quantities.MaxClassQoI.__init__","title":"<code>__init__(axis=1, activation=None)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>axis</code> <code>int</code> <p>Output dimension over which max operation is taken.</p> <code>1</code> <code>activation</code> <code>Union[Callable, str, None]</code> <p>Activation function to be applied to the output before taking  the max. If <code>activation</code> is a string, use the corresponding  named activation function implemented by the backend. The  following strings are currently supported as shorthands for the respective standard activation functions:</p> <ul> <li><code>'sigmoid'</code> </li> <li><code>'softmax'</code> </li> </ul> <p>If <code>activation</code> is <code>None</code>, no activation function is applied to the input.</p> <code>None</code> Source code in <code>trulens_explain/trulens/nn/quantities.py</code> <pre><code>def __init__(\n    self, axis: int = 1, activation: Union[Callable, str, None] = None\n):\n\"\"\"\n    Parameters:\n        axis:\n            Output dimension over which max operation is taken.\n\n        activation:\n            Activation function to be applied to the output before taking \n            the max. If `activation` is a string, use the corresponding \n            named activation function implemented by the backend. The \n            following strings are currently supported as shorthands for the\n            respective standard activation functions:\n\n            - `'sigmoid'` \n            - `'softmax'` \n\n            If `activation` is `None`, no activation function is applied to\n            the input.\n    \"\"\"\n    self._axis = axis\n    self.activation = activation\n</code></pre>"},{"location":"trulens_explain/api/quantities/#trulens_explain.trulens.nn.quantities.QoI","title":"<code>QoI</code>","text":"<p>         Bases: <code>AbstractBaseClass</code></p> <p>Interface for quantities of interest. The Quantity of Interest (QoI) is a function of the output specified by the slice that determines the network  output behavior that the attributions describe.</p> Source code in <code>trulens_explain/trulens/nn/quantities.py</code> <pre><code>class QoI(AbstractBaseClass):\n\"\"\"\n    Interface for quantities of interest. The *Quantity of Interest* (QoI) is a\n    function of the output specified by the slice that determines the network \n    output behavior that the attributions describe.\n    \"\"\"\n\n    def __str__(self):\n        return render_object(self, [])\n\n    # TODO: Need to give a seperate value of y at target instance here since\n    # these are values are interventions. Cannot presently define a QoI that says:\n    # logits of the predicted class for each instance.\n    # Issue GH-72 . Task MLNN-415 .\n\n    def _wrap_public_call(self, y: Outputs[Tensor]) -&gt; Outputs[Tensor]:\n\"\"\"\n        Wrap a public call that may result in one or more tensors. Signature of\n        this class is not specific while public calls are flexible.\n        \"\"\"\n\n        return many_of_om(self.__call__(om_of_many(y)))\n\n    @abstractmethod\n    def __call__(self, y: OM[Outputs, Tensor]) -&gt; OM[Outputs, Tensor]:\n\"\"\"\n        Computes the distribution of interest from an initial point.\n\n        Parameters:\n            y:\n                Output point from which the quantity is derived. Must be a\n                differentiable tensor.\n\n        Returns:\n            A differentiable batched scalar tensor representing the QoI.\n        \"\"\"\n        raise NotImplementedError\n\n    def _assert_cut_contains_only_one_tensor(self, x):\n        if isinstance(x, DATA_CONTAINER_TYPE):\n            raise QoiCutSupportError(\n                'Cut provided to quantity of interest was comprised of '\n                'multiple tensors, but `{}` is only defined for cuts comprised '\n                'of a single tensor (received a list of {} tensors).\\n'\n                '\\n'\n                'Either (1) select a slice where the `to_cut` corresponds to a '\n                'single tensor, or (2) implement/use a `QoI` object that '\n                'supports lists of tensors, i.e., where the parameter, `x`, to '\n                '`__call__` is expected/allowed to be a list of {} tensors.'.\n                format(self.__class__.__name__, len(x), len(x))\n            )\n\n        elif not get_backend().is_tensor(x):\n            raise ValueError(\n                '`{}` expected to receive an instance of `Tensor`, but '\n                'received an instance of {}'.format(\n                    self.__class__.__name__, type(x)\n                )\n            )\n</code></pre>"},{"location":"trulens_explain/api/quantities/#trulens_explain.trulens.nn.quantities.QoI.__call__","title":"<code>__call__(y)</code>  <code>abstractmethod</code>","text":"<p>Computes the distribution of interest from an initial point.</p> <p>Parameters:</p> Name Type Description Default <code>y</code> <code>OM[Outputs, Tensor]</code> <p>Output point from which the quantity is derived. Must be a differentiable tensor.</p> required <p>Returns:</p> Type Description <code>OM[Outputs, Tensor]</code> <p>A differentiable batched scalar tensor representing the QoI.</p> Source code in <code>trulens_explain/trulens/nn/quantities.py</code> <pre><code>@abstractmethod\ndef __call__(self, y: OM[Outputs, Tensor]) -&gt; OM[Outputs, Tensor]:\n\"\"\"\n    Computes the distribution of interest from an initial point.\n\n    Parameters:\n        y:\n            Output point from which the quantity is derived. Must be a\n            differentiable tensor.\n\n    Returns:\n        A differentiable batched scalar tensor representing the QoI.\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"trulens_explain/api/quantities/#trulens_explain.trulens.nn.quantities.QoiCutSupportError","title":"<code>QoiCutSupportError</code>","text":"<p>         Bases: <code>ValueError</code></p> <p>Exception raised if the quantity of interest is called on a cut whose output is not supported by the quantity of interest.</p> Source code in <code>trulens_explain/trulens/nn/quantities.py</code> <pre><code>class QoiCutSupportError(ValueError):\n\"\"\"\n    Exception raised if the quantity of interest is called on a cut whose output\n    is not supported by the quantity of interest.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"trulens_explain/api/quantities/#trulens_explain.trulens.nn.quantities.ThresholdQoI","title":"<code>ThresholdQoI</code>","text":"<p>         Bases: <code>QoI</code></p> <p>Quantity of interest for attributing network output toward the difference  between two regions seperated by a given threshold. I.e., the quantity of interest is the \"high\" elements minus the \"low\" elements, where the high elements have activations above the threshold and the low elements have  activations below the threshold.</p> <p>Use case: bianry segmentation.</p> Source code in <code>trulens_explain/trulens/nn/quantities.py</code> <pre><code>class ThresholdQoI(QoI):\n\"\"\"\n    Quantity of interest for attributing network output toward the difference \n    between two regions seperated by a given threshold. I.e., the quantity of\n    interest is the \"high\" elements minus the \"low\" elements, where the high\n    elements have activations above the threshold and the low elements have \n    activations below the threshold.\n\n    Use case: bianry segmentation.\n    \"\"\"\n\n    def __init__(\n        self,\n        threshold: float,\n        low_minus_high: bool = False,\n        activation: Union[Callable, str, None] = None\n    ):\n\"\"\"\n        Parameters:\n            threshold:\n                A threshold to determine the element-wise sign of the input \n                tensor. The elements with activations higher than the threshold \n                will retain their sign, while the elements with activations \n                lower than the threshold will have their sign flipped (or vice \n                versa if `low_minus_high` is set to `True`).\n            low_minus_high:\n                If `True`, substract the output with activations above the \n                threshold from the output with activations below the threshold. \n                If `False`, substract the output with activations below the \n                threshold from the output with activations above the threshold.\n            activation: str or function, optional\n                Activation function to be applied to the quantity before taking\n                the threshold. If `activation` is a string, use the \n                corresponding activation function implemented by the backend \n                (currently supported: `'sigmoid'` and `'softmax'`). Otherwise, \n                if `activation` is not `None`, it will be treated as a callable.\n                If `activation` is `None`, do not apply an activation function \n                to the quantity.\n        \"\"\"\n        # TODO(klas):should this support an aggregation function? By default\n        #   this is a sum, but it could, for example, subtract the greatest\n        #   positive element from the least negative element.\n        self.threshold = threshold\n        self.low_minus_high = low_minus_high\n        self.activation = activation\n\n    def __call__(self, x: TensorLike) -&gt; TensorLike:\n        B = get_backend()\n        self._assert_cut_contains_only_one_tensor(x)\n\n        if self.activation is not None:\n            if isinstance(self.activation, str):\n                self.activation = self.activation.lower()\n                if self.activation in ['sigmoid', 'softmax']:\n                    x = getattr(B, self.activation)(x)\n                else:\n                    raise NotImplementedError(\n                        'This activation function is not currently supported '\n                        'by the backend'\n                    )\n            else:\n                x = self.activation(x)\n\n        # TODO(klas): is the `clone` necessary here? Not sure why it was\n        #   included.\n        mask = B.sign(B.clone(x) - self.threshold)\n        if self.low_minus_high:\n            mask = -mask\n\n        non_batch_dimensions = tuple(range(len(B.int_shape(x)))[1:])\n\n        return B.sum(mask * x, axis=non_batch_dimensions)\n</code></pre>"},{"location":"trulens_explain/api/quantities/#trulens_explain.trulens.nn.quantities.ThresholdQoI.__init__","title":"<code>__init__(threshold, low_minus_high=False, activation=None)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>threshold</code> <code>float</code> <p>A threshold to determine the element-wise sign of the input  tensor. The elements with activations higher than the threshold  will retain their sign, while the elements with activations  lower than the threshold will have their sign flipped (or vice  versa if <code>low_minus_high</code> is set to <code>True</code>).</p> required <code>low_minus_high</code> <code>bool</code> <p>If <code>True</code>, substract the output with activations above the  threshold from the output with activations below the threshold.  If <code>False</code>, substract the output with activations below the  threshold from the output with activations above the threshold.</p> <code>False</code> <code>activation</code> <code>Union[Callable, str, None]</code> <p>str or function, optional Activation function to be applied to the quantity before taking the threshold. If <code>activation</code> is a string, use the  corresponding activation function implemented by the backend  (currently supported: <code>'sigmoid'</code> and <code>'softmax'</code>). Otherwise,  if <code>activation</code> is not <code>None</code>, it will be treated as a callable. If <code>activation</code> is <code>None</code>, do not apply an activation function  to the quantity.</p> <code>None</code> Source code in <code>trulens_explain/trulens/nn/quantities.py</code> <pre><code>def __init__(\n    self,\n    threshold: float,\n    low_minus_high: bool = False,\n    activation: Union[Callable, str, None] = None\n):\n\"\"\"\n    Parameters:\n        threshold:\n            A threshold to determine the element-wise sign of the input \n            tensor. The elements with activations higher than the threshold \n            will retain their sign, while the elements with activations \n            lower than the threshold will have their sign flipped (or vice \n            versa if `low_minus_high` is set to `True`).\n        low_minus_high:\n            If `True`, substract the output with activations above the \n            threshold from the output with activations below the threshold. \n            If `False`, substract the output with activations below the \n            threshold from the output with activations above the threshold.\n        activation: str or function, optional\n            Activation function to be applied to the quantity before taking\n            the threshold. If `activation` is a string, use the \n            corresponding activation function implemented by the backend \n            (currently supported: `'sigmoid'` and `'softmax'`). Otherwise, \n            if `activation` is not `None`, it will be treated as a callable.\n            If `activation` is `None`, do not apply an activation function \n            to the quantity.\n    \"\"\"\n    # TODO(klas):should this support an aggregation function? By default\n    #   this is a sum, but it could, for example, subtract the greatest\n    #   positive element from the least negative element.\n    self.threshold = threshold\n    self.low_minus_high = low_minus_high\n    self.activation = activation\n</code></pre>"},{"location":"trulens_explain/api/slices/","title":"Slices","text":"<p>The slice, or layer, of the network provides flexibility over the level of  abstraction for the explanation. In a low layer, an explanation may highlight  the edges that were most important in identifying an object like a face, while  in a higher layer, the explanation might highlight high-level features such as a nose or mouth. By raising the level of abstraction, explanations that generalize over larger sets of samples are possible.</p> <p>Formally, A network, \\(f\\), can be broken into a slice, \\(f = g \\circ h\\), where  \\(h\\) can be thought of as a pre-processor that computes features, and \\(g\\) can be thought of as a sub-model that uses the features computed by \\(h\\).</p>"},{"location":"trulens_explain/api/slices/#trulens_explain.trulens.nn.slices.Cut","title":"<code>Cut</code>","text":"<p>         Bases: <code>object</code></p> <p>A cut is the primary building block for a slice. It determines an internal component of a network to expose. A slice if formed by two cuts.</p> Source code in <code>trulens_explain/trulens/nn/slices.py</code> <pre><code>class Cut(object):\n\"\"\"\n    A cut is the primary building block for a slice. It determines an internal\n    component of a network to expose. A slice if formed by two cuts.\n    \"\"\"\n\n    def __init__(\n        self,\n        name: LayerIdentifier,\n        anchor: str = 'out',\n        accessor: Optional[Callable] = None\n    ):\n\"\"\"\n        Parameters:\n            name:\n                The name or index of a layer in the model, or a list containing\n                the names/indices of mutliple layers.\n\n            anchor: \n                Determines whether input (`'in'`) or the output (`'out'`) tensor\n                of the spcified layer should be used.\n\n            accessor:\n                An accessor function that operates on the layer, mapping the \n                tensor (or list thereof) corresponding to the layer's \n                input/output to another tensor (or list thereof). This can be \n                used to, e.g., extract a particular output from a layer that \n                produces a sequence of outputs. If `accessor` is `None`, the \n                following accessor function will be used: \n                ```python\n                lambda t: t[-1] if isinstance(t, list) else t\n                ```\n        \"\"\"\n        assert name is None or isinstance(\n            name, (list, int, str)\n        ), \"Cut.name must be one of: layer index, layer name, or list of names/indices of multiple layers\"\n        if isinstance(name, list):\n            for n in name:\n                assert isinstance(\n                    n, (int, str)\n                ), f\"Elements in Cut.name must be layer names (str) or indices (int). Got type {type(n)}\"\n        anchor = str(anchor)\n        assert anchor in [\n            'in', 'out'\n        ], \"Cut.anchor must be one of ('in', 'out')\"\n        assert accessor is None or isinstance(\n            accessor, Callable\n        ), \"Cut.accessor must be callable or None\"\n\n        if get_backend().backend == 'pytorch':\n            if (isinstance(name, int) or\n                (isinstance(name, list) and isinstance(name[0], int))):\n\n                tru_logger.warning(\n                    '\\n\\nPytorch does not have native support for indexed '\n                    'layers. Using layer indices is not recommended.\\n'\n                )\n\n        self.name = name\n        self.accessor = accessor\n        self.anchor = anchor\n\n    def __str__(self):\n        return render_object(self, ['name', 'accessor', 'anchor'])\n\n    # TODO: layer arg might need to be more specific\n    def access_layer(self, layer: TensorLike) -&gt; TensorLike:\n\"\"\"\n        Applies `self.accessor` to the result of collecting the relevant \n        tensor(s) associated with a layer's output.\n\n        Parameters:\n            layer:\n                The tensor output (or input, if so specified by the anchor) of \n                the layer(s) specified by this cut.\n\n        Returns:\n            The result of applying `self.accessor` to the given layer.\n        \"\"\"\n        if layer is None:\n            return layer\n        elif self.accessor is None:\n            return layer\n        else:\n            layer = (\n                layer[0]\n                if isinstance(layer, list) and len(layer) == 1 else layer\n            )\n            return self.accessor(layer)\n</code></pre>"},{"location":"trulens_explain/api/slices/#trulens_explain.trulens.nn.slices.Cut.__init__","title":"<code>__init__(name, anchor='out', accessor=None)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>name</code> <code>LayerIdentifier</code> <p>The name or index of a layer in the model, or a list containing the names/indices of mutliple layers.</p> required <code>anchor</code> <code>str</code> <p>Determines whether input (<code>'in'</code>) or the output (<code>'out'</code>) tensor of the spcified layer should be used.</p> <code>'out'</code> <code>accessor</code> <code>Optional[Callable]</code> <p>An accessor function that operates on the layer, mapping the  tensor (or list thereof) corresponding to the layer's  input/output to another tensor (or list thereof). This can be  used to, e.g., extract a particular output from a layer that  produces a sequence of outputs. If <code>accessor</code> is <code>None</code>, the  following accessor function will be used:  <pre><code>lambda t: t[-1] if isinstance(t, list) else t\n</code></pre></p> <code>None</code> Source code in <code>trulens_explain/trulens/nn/slices.py</code> <pre><code>def __init__(\n    self,\n    name: LayerIdentifier,\n    anchor: str = 'out',\n    accessor: Optional[Callable] = None\n):\n\"\"\"\n    Parameters:\n        name:\n            The name or index of a layer in the model, or a list containing\n            the names/indices of mutliple layers.\n\n        anchor: \n            Determines whether input (`'in'`) or the output (`'out'`) tensor\n            of the spcified layer should be used.\n\n        accessor:\n            An accessor function that operates on the layer, mapping the \n            tensor (or list thereof) corresponding to the layer's \n            input/output to another tensor (or list thereof). This can be \n            used to, e.g., extract a particular output from a layer that \n            produces a sequence of outputs. If `accessor` is `None`, the \n            following accessor function will be used: \n            ```python\n            lambda t: t[-1] if isinstance(t, list) else t\n            ```\n    \"\"\"\n    assert name is None or isinstance(\n        name, (list, int, str)\n    ), \"Cut.name must be one of: layer index, layer name, or list of names/indices of multiple layers\"\n    if isinstance(name, list):\n        for n in name:\n            assert isinstance(\n                n, (int, str)\n            ), f\"Elements in Cut.name must be layer names (str) or indices (int). Got type {type(n)}\"\n    anchor = str(anchor)\n    assert anchor in [\n        'in', 'out'\n    ], \"Cut.anchor must be one of ('in', 'out')\"\n    assert accessor is None or isinstance(\n        accessor, Callable\n    ), \"Cut.accessor must be callable or None\"\n\n    if get_backend().backend == 'pytorch':\n        if (isinstance(name, int) or\n            (isinstance(name, list) and isinstance(name[0], int))):\n\n            tru_logger.warning(\n                '\\n\\nPytorch does not have native support for indexed '\n                'layers. Using layer indices is not recommended.\\n'\n            )\n\n    self.name = name\n    self.accessor = accessor\n    self.anchor = anchor\n</code></pre>"},{"location":"trulens_explain/api/slices/#trulens_explain.trulens.nn.slices.Cut.access_layer","title":"<code>access_layer(layer)</code>","text":"<p>Applies <code>self.accessor</code> to the result of collecting the relevant  tensor(s) associated with a layer's output.</p> <p>Parameters:</p> Name Type Description Default <code>layer</code> <code>TensorLike</code> <p>The tensor output (or input, if so specified by the anchor) of  the layer(s) specified by this cut.</p> required <p>Returns:</p> Type Description <code>TensorLike</code> <p>The result of applying <code>self.accessor</code> to the given layer.</p> Source code in <code>trulens_explain/trulens/nn/slices.py</code> <pre><code>def access_layer(self, layer: TensorLike) -&gt; TensorLike:\n\"\"\"\n    Applies `self.accessor` to the result of collecting the relevant \n    tensor(s) associated with a layer's output.\n\n    Parameters:\n        layer:\n            The tensor output (or input, if so specified by the anchor) of \n            the layer(s) specified by this cut.\n\n    Returns:\n        The result of applying `self.accessor` to the given layer.\n    \"\"\"\n    if layer is None:\n        return layer\n    elif self.accessor is None:\n        return layer\n    else:\n        layer = (\n            layer[0]\n            if isinstance(layer, list) and len(layer) == 1 else layer\n        )\n        return self.accessor(layer)\n</code></pre>"},{"location":"trulens_explain/api/slices/#trulens_explain.trulens.nn.slices.InputCut","title":"<code>InputCut</code>","text":"<p>         Bases: <code>Cut</code></p> <p>Special cut that selects the input(s) of a model.</p> Source code in <code>trulens_explain/trulens/nn/slices.py</code> <pre><code>class InputCut(Cut):\n\"\"\"\n    Special cut that selects the input(s) of a model.\n    \"\"\"\n\n    def __init__(self, anchor: str = 'in', accessor: Optional[Callable] = None):\n\"\"\"\n        Parameters:\n            anchor: \n                Determines whether input (`'in'`) or the output (`'out'`) tensor\n                of the spcified layer should be used.\n\n            accessor:\n                An accessor function that operates on the layer, mapping the \n                tensor (or list thereof) corresponding to the layer's \n                input/output to another tensor (or list thereof). This can be \n                used to, e.g., extract a particular output from a layer that \n                produces a sequence of outputs. If `accessor` is `None`, the \n                following accessor function will be used: \n                ```python\n                lambda t: t[-1] if isinstance(t, list) else t\n                ```\n        \"\"\"\n        super().__init__(None, anchor, accessor)\n</code></pre>"},{"location":"trulens_explain/api/slices/#trulens_explain.trulens.nn.slices.InputCut.__init__","title":"<code>__init__(anchor='in', accessor=None)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>anchor</code> <code>str</code> <p>Determines whether input (<code>'in'</code>) or the output (<code>'out'</code>) tensor of the spcified layer should be used.</p> <code>'in'</code> <code>accessor</code> <code>Optional[Callable]</code> <p>An accessor function that operates on the layer, mapping the  tensor (or list thereof) corresponding to the layer's  input/output to another tensor (or list thereof). This can be  used to, e.g., extract a particular output from a layer that  produces a sequence of outputs. If <code>accessor</code> is <code>None</code>, the  following accessor function will be used:  <pre><code>lambda t: t[-1] if isinstance(t, list) else t\n</code></pre></p> <code>None</code> Source code in <code>trulens_explain/trulens/nn/slices.py</code> <pre><code>def __init__(self, anchor: str = 'in', accessor: Optional[Callable] = None):\n\"\"\"\n    Parameters:\n        anchor: \n            Determines whether input (`'in'`) or the output (`'out'`) tensor\n            of the spcified layer should be used.\n\n        accessor:\n            An accessor function that operates on the layer, mapping the \n            tensor (or list thereof) corresponding to the layer's \n            input/output to another tensor (or list thereof). This can be \n            used to, e.g., extract a particular output from a layer that \n            produces a sequence of outputs. If `accessor` is `None`, the \n            following accessor function will be used: \n            ```python\n            lambda t: t[-1] if isinstance(t, list) else t\n            ```\n    \"\"\"\n    super().__init__(None, anchor, accessor)\n</code></pre>"},{"location":"trulens_explain/api/slices/#trulens_explain.trulens.nn.slices.LogitCut","title":"<code>LogitCut</code>","text":"<p>         Bases: <code>Cut</code></p> <p>Special cut that selects the logit layer of a model. The logit layer must be named <code>'logits'</code> or otherwise specified by the user to the model wrapper.</p> Source code in <code>trulens_explain/trulens/nn/slices.py</code> <pre><code>class LogitCut(Cut):\n\"\"\"\n    Special cut that selects the logit layer of a model. The logit layer must be\n    named `'logits'` or otherwise specified by the user to the model wrapper.\n    \"\"\"\n\n    def __init__(\n        self, anchor: str = 'out', accessor: Optional[Callable] = None\n    ):\n\"\"\"\n        Parameters:\n            anchor: \n                Determines whether input (`'in'`) or the output (`'out'`) tensor\n                of the spcified layer should be used.\n\n            accessor:\n                An accessor function that operates on the layer, mapping the \n                tensor (or list thereof) corresponding to the layer's \n                input/output to another tensor (or list thereof). This can be \n                used to, e.g., extract a particular output from a layer that \n                produces a sequence of outputs. If `accessor` is `None`, the \n                following accessor function will be used: \n                ```python\n                lambda t: t[-1] if isinstance(t, list) else t\n                ```\n        \"\"\"\n        super(LogitCut, self).__init__(None, anchor, accessor)\n</code></pre>"},{"location":"trulens_explain/api/slices/#trulens_explain.trulens.nn.slices.LogitCut.__init__","title":"<code>__init__(anchor='out', accessor=None)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>anchor</code> <code>str</code> <p>Determines whether input (<code>'in'</code>) or the output (<code>'out'</code>) tensor of the spcified layer should be used.</p> <code>'out'</code> <code>accessor</code> <code>Optional[Callable]</code> <p>An accessor function that operates on the layer, mapping the  tensor (or list thereof) corresponding to the layer's  input/output to another tensor (or list thereof). This can be  used to, e.g., extract a particular output from a layer that  produces a sequence of outputs. If <code>accessor</code> is <code>None</code>, the  following accessor function will be used:  <pre><code>lambda t: t[-1] if isinstance(t, list) else t\n</code></pre></p> <code>None</code> Source code in <code>trulens_explain/trulens/nn/slices.py</code> <pre><code>def __init__(\n    self, anchor: str = 'out', accessor: Optional[Callable] = None\n):\n\"\"\"\n    Parameters:\n        anchor: \n            Determines whether input (`'in'`) or the output (`'out'`) tensor\n            of the spcified layer should be used.\n\n        accessor:\n            An accessor function that operates on the layer, mapping the \n            tensor (or list thereof) corresponding to the layer's \n            input/output to another tensor (or list thereof). This can be \n            used to, e.g., extract a particular output from a layer that \n            produces a sequence of outputs. If `accessor` is `None`, the \n            following accessor function will be used: \n            ```python\n            lambda t: t[-1] if isinstance(t, list) else t\n            ```\n    \"\"\"\n    super(LogitCut, self).__init__(None, anchor, accessor)\n</code></pre>"},{"location":"trulens_explain/api/slices/#trulens_explain.trulens.nn.slices.OutputCut","title":"<code>OutputCut</code>","text":"<p>         Bases: <code>Cut</code></p> <p>Special cut that selects the output(s) of a model.</p> Source code in <code>trulens_explain/trulens/nn/slices.py</code> <pre><code>class OutputCut(Cut):\n\"\"\"\n    Special cut that selects the output(s) of a model.\n    \"\"\"\n\n    def __init__(\n        self, anchor: str = 'out', accessor: Optional[Callable] = None\n    ):\n\"\"\"\n        Parameters:\n            anchor: \n                Determines whether input (`'in'`) or the output (`'out'`) tensor\n                of the spcified layer should be used.\n\n            accessor:\n                An accessor function that operates on the layer, mapping the \n                tensor (or list thereof) corresponding to the layer's \n                input/output to another tensor (or list thereof). This can be \n                used to, e.g., extract a particular output from a layer that \n                produces a sequence of outputs. If `accessor` is `None`, the \n                following accessor function will be used: \n                ```python\n                lambda t: t[-1] if isinstance(t, list) else t\n                ```\n        \"\"\"\n        super(OutputCut, self).__init__(None, anchor, accessor)\n</code></pre>"},{"location":"trulens_explain/api/slices/#trulens_explain.trulens.nn.slices.OutputCut.__init__","title":"<code>__init__(anchor='out', accessor=None)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>anchor</code> <code>str</code> <p>Determines whether input (<code>'in'</code>) or the output (<code>'out'</code>) tensor of the spcified layer should be used.</p> <code>'out'</code> <code>accessor</code> <code>Optional[Callable]</code> <p>An accessor function that operates on the layer, mapping the  tensor (or list thereof) corresponding to the layer's  input/output to another tensor (or list thereof). This can be  used to, e.g., extract a particular output from a layer that  produces a sequence of outputs. If <code>accessor</code> is <code>None</code>, the  following accessor function will be used:  <pre><code>lambda t: t[-1] if isinstance(t, list) else t\n</code></pre></p> <code>None</code> Source code in <code>trulens_explain/trulens/nn/slices.py</code> <pre><code>def __init__(\n    self, anchor: str = 'out', accessor: Optional[Callable] = None\n):\n\"\"\"\n    Parameters:\n        anchor: \n            Determines whether input (`'in'`) or the output (`'out'`) tensor\n            of the spcified layer should be used.\n\n        accessor:\n            An accessor function that operates on the layer, mapping the \n            tensor (or list thereof) corresponding to the layer's \n            input/output to another tensor (or list thereof). This can be \n            used to, e.g., extract a particular output from a layer that \n            produces a sequence of outputs. If `accessor` is `None`, the \n            following accessor function will be used: \n            ```python\n            lambda t: t[-1] if isinstance(t, list) else t\n            ```\n    \"\"\"\n    super(OutputCut, self).__init__(None, anchor, accessor)\n</code></pre>"},{"location":"trulens_explain/api/slices/#trulens_explain.trulens.nn.slices.Slice","title":"<code>Slice</code>","text":"<p>         Bases: <code>object</code></p> <p>Class representing a slice of a network. A network, \\(f\\), can be broken into a slice, \\(f = g \\circ h\\), where \\(h\\) can be thought of as a  pre-processor that computes features, and \\(g\\) can be thought of as a  sub-model that uses the features computed by \\(h\\).</p> <p>A <code>Slice</code> object represents a slice as two <code>Cut</code>s, <code>from_cut</code> and <code>to_cut</code>, which are the layers corresponding to the output of \\(h\\) and \\(g\\),  respectively.</p> Source code in <code>trulens_explain/trulens/nn/slices.py</code> <pre><code>class Slice(object):\n\"\"\"\n    Class representing a slice of a network. A network, $f$, can be broken\n    into a slice, $f = g \\\\circ h$, where $h$ can be thought of as a \n    pre-processor that computes features, and $g$ can be thought of as a \n    sub-model that uses the features computed by $h$.\n\n    A `Slice` object represents a slice as two `Cut`s, `from_cut` and `to_cut`,\n    which are the layers corresponding to the output of $h$ and $g$, \n    respectively.\n    \"\"\"\n\n    def __init__(self, from_cut: Cut, to_cut: Cut):\n\"\"\"\n        Parameters:\n            from_cut:\n                Cut representing the output of the preprocessing function, $h$,\n                in slice, $f = g \\\\circ h$.\n\n            to_cut:\n                Cut representing the output of the sub-model, $g$, in slice, \n                $f = g \\\\circ h$.\n        \"\"\"\n        self._from_cut = from_cut\n        self._to_cut = to_cut\n\n    @property\n    def from_cut(self) -&gt; Cut:\n\"\"\"\n        Cut representing the output of the preprocessing function, $h$, in \n        slice, $f = g \\\\circ h$.\n        \"\"\"\n        return self._from_cut\n\n    @property\n    def to_cut(self) -&gt; Cut:\n\"\"\"\n        Cut representing the output of the sub-model, $g$, in slice, \n        $f = g \\\\circ h$.\n        \"\"\"\n        return self._to_cut\n\n    @staticmethod\n    def full_network():\n\"\"\"\n        Returns\n        -------\n        Slice\n            A slice representing the entire model, i.e., :math:`f = g \\\\circ h`,\n            where :math:`h` is the identity function and :math:`g = f`.\n        \"\"\"\n        return Slice(InputCut(), OutputCut())\n</code></pre>"},{"location":"trulens_explain/api/slices/#trulens_explain.trulens.nn.slices.Slice.from_cut","title":"<code>from_cut: Cut</code>  <code>property</code>","text":"<p>Cut representing the output of the preprocessing function, \\(h\\), in  slice, \\(f = g \\circ h\\).</p>"},{"location":"trulens_explain/api/slices/#trulens_explain.trulens.nn.slices.Slice.to_cut","title":"<code>to_cut: Cut</code>  <code>property</code>","text":"<p>Cut representing the output of the sub-model, \\(g\\), in slice,  \\(f = g \\circ h\\).</p>"},{"location":"trulens_explain/api/slices/#trulens_explain.trulens.nn.slices.Slice.__init__","title":"<code>__init__(from_cut, to_cut)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>from_cut</code> <code>Cut</code> <p>Cut representing the output of the preprocessing function, \\(h\\), in slice, \\(f = g \\circ h\\).</p> required <code>to_cut</code> <code>Cut</code> <p>Cut representing the output of the sub-model, \\(g\\), in slice,  \\(f = g \\circ h\\).</p> required Source code in <code>trulens_explain/trulens/nn/slices.py</code> <pre><code>def __init__(self, from_cut: Cut, to_cut: Cut):\n\"\"\"\n    Parameters:\n        from_cut:\n            Cut representing the output of the preprocessing function, $h$,\n            in slice, $f = g \\\\circ h$.\n\n        to_cut:\n            Cut representing the output of the sub-model, $g$, in slice, \n            $f = g \\\\circ h$.\n    \"\"\"\n    self._from_cut = from_cut\n    self._to_cut = to_cut\n</code></pre>"},{"location":"trulens_explain/api/slices/#trulens_explain.trulens.nn.slices.Slice.full_network","title":"<code>full_network()</code>  <code>staticmethod</code>","text":""},{"location":"trulens_explain/api/slices/#trulens_explain.trulens.nn.slices.Slice.full_network--returns","title":"Returns","text":"<p>Slice     A slice representing the entire model, i.e., :math:<code>f = g \\circ h</code>,     where :math:<code>h</code> is the identity function and :math:<code>g = f</code>.</p> Source code in <code>trulens_explain/trulens/nn/slices.py</code> <pre><code>@staticmethod\ndef full_network():\n\"\"\"\n    Returns\n    -------\n    Slice\n        A slice representing the entire model, i.e., :math:`f = g \\\\circ h`,\n        where :math:`h` is the identity function and :math:`g = f`.\n    \"\"\"\n    return Slice(InputCut(), OutputCut())\n</code></pre>"},{"location":"trulens_explain/api/visualizations/","title":"Visualization Methods","text":"<p>One clear use case for measuring attributions is for human consumption. In order to be fully leveraged by humans, explanations need to be interpretable \u2014 a large vector of numbers doesn\u2019t in general make us more confident we understand what a network is doing. We therefore view an explanation as comprised of both an attribution measurement and an interpretation of what the attribution  values represent.</p> <p>One obvious way to interpret attributions, particularly in the image domain, is via visualization. This module provides several visualization methods for interpreting attributions as images.</p>"},{"location":"trulens_explain/api/visualizations/#trulens_explain.trulens.visualizations.ChannelMaskVisualizer","title":"<code>ChannelMaskVisualizer</code>","text":"<p>         Bases: <code>object</code></p> <p>Uses internal influence to visualize the pixels that are most salient towards a particular internal channel or neuron.</p> Source code in <code>trulens_explain/trulens/visualizations.py</code> <pre><code>class ChannelMaskVisualizer(object):\n\"\"\"\n    Uses internal influence to visualize the pixels that are most salient\n    towards a particular internal channel or neuron.\n    \"\"\"\n\n    def __init__(\n        self,\n        model,\n        layer,\n        channel,\n        channel_axis=None,\n        agg_fn=None,\n        doi=None,\n        blur=None,\n        threshold=0.5,\n        masked_opacity=0.2,\n        combine_channels: bool = True,\n        use_attr_as_opacity=None,\n        positive_only=None\n    ):\n\"\"\"\n        Configures the default parameters for the `__call__` method (these can \n        be overridden by passing in values to `__call__`).\n\n        Parameters:\n            model:\n                The wrapped model whose channel we're visualizing.\n\n            layer:\n                The identifier (either index or name) of the layer in which the \n                channel we're visualizing resides.\n\n            channel:\n                Index of the channel (for convolutional layers) or internal \n                neuron (for fully-connected layers) that we'd like to visualize.\n\n            channel_axis:\n                If different from the channel axis specified by the backend, the\n                supplied `channel_axis` will be used if operating on a \n                convolutional layer with 4-D image format.\n\n            agg_fn:\n                Function with which to aggregate the remaining dimensions \n                (except the batch dimension) in order to get a single scalar \n                value for each channel; If `None`, a sum over each neuron in the\n                channel will be taken. This argument is not used when the \n                channels are scalars, e.g., for dense layers.\n\n            doi:\n                The distribution of interest to use when computing the input\n                attributions towards the specified channel. If `None`, \n                `PointDoI` will be used.\n\n            blur:\n                Gives the radius of a Gaussian blur to be applied to the \n                attributions before visualizing. This can be used to help focus\n                on salient regions rather than specific salient pixels.\n\n            threshold:\n                Value in the range [0, 1]. Attribution values at or  below the \n                percentile given by `threshold` (after normalization, blurring,\n                etc.) will be masked.\n\n            masked_opacity: \n                Value in the range [0, 1] specifying the opacity for the parts\n                of the image that are masked.\n\n            combine_channels:\n                If `True`, the attributions will be averaged across the channel\n                dimension, resulting in a 1-channel attribution map.\n\n            use_attr_as_opacity:\n                If `True`, instead of using `threshold` and `masked_opacity`,\n                the opacity of each pixel is given by the 0-1-normalized \n                attribution value.\n\n            positive_only:\n                If `True`, only pixels with positive attribution will be \n                unmasked (or given nonzero opacity when `use_attr_as_opacity` is\n                true).\n        \"\"\"\n        B = get_backend()\n        if (B is not None and (channel_axis is None or channel_axis &lt; 0)):\n            channel_axis = B.channel_axis\n        elif (channel_axis is None or channel_axis &lt; 0):\n            channel_axis = 1\n\n        self.mask_visualizer = MaskVisualizer(\n            blur, threshold, masked_opacity, combine_channels,\n            use_attr_as_opacity, positive_only\n        )\n\n        self.infl_input = InternalInfluence(\n            model, (InputCut(), Cut(layer)),\n            InternalChannelQoI(channel, channel_axis, agg_fn),\n            PointDoi() if doi is None else doi\n        )\n\n    def __call__(\n        self,\n        x,\n        x_preprocessed=None,\n        output_file=None,\n        blur=None,\n        threshold=None,\n        masked_opacity=None,\n        combine_channels=None\n    ):\n\"\"\"\n        Visualizes the given attributions by overlaying an attribution heatmap \n        over the given image.\n\n        Parameters\n        ----------\n        attributions : numpy.ndarray\n            The attributions to visualize. Expected to be in 4-D image format.\n\n        x : numpy.ndarray\n            The original image(s) over which the attributions are calculated.\n            Must be the same shape as expected by the model used with this\n            visualizer.\n\n        x_preprocessed : numpy.ndarray, optional\n            If the model requires a preprocessed input (e.g., with the mean\n            subtracted) that is different from how the image should be \n            visualized, ``x_preprocessed`` should be specified. In this case \n            ``x`` will be used for visualization, and ``x_preprocessed`` will be\n            passed to the model when calculating attributions. Must be the same \n            shape as ``x``.\n\n        output_file : str, optional\n            If specified, the resulting visualization will be saved to a file\n            with the name given by ``output_file``.\n\n        blur : float, optional\n            If specified, gives the radius of a Gaussian blur to be applied to\n            the attributions before visualizing. This can be used to help focus\n            on salient regions rather than specific salient pixels. If None, \n            defaults to the value supplied to the constructor. Default None.\n\n        threshold : float\n            Value in the range [0, 1]. Attribution values at or  below the \n            percentile given by ``threshold`` will be masked. If None, defaults \n            to the value supplied to the constructor. Default None.\n\n        masked_opacity: float\n            Value in the range [0, 1] specifying the opacity for the parts of\n            the image that are masked. Default 0.2. If None, defaults to the \n            value supplied to the constructor. Default None.\n\n        combine_channels : bool\n            If True, the attributions will be averaged across the channel\n            dimension, resulting in a 1-channel attribution map. If None, \n            defaults to the value supplied to the constructor. Default None.\n        \"\"\"\n\n        attrs_input = self.infl_input.attributions(\n            x if x_preprocessed is None else x_preprocessed\n        )\n\n        return self.mask_visualizer(\n            attrs_input, x, output_file, blur, threshold, masked_opacity,\n            combine_channels\n        )\n</code></pre>"},{"location":"trulens_explain/api/visualizations/#trulens_explain.trulens.visualizations.ChannelMaskVisualizer.__call__","title":"<code>__call__(x, x_preprocessed=None, output_file=None, blur=None, threshold=None, masked_opacity=None, combine_channels=None)</code>","text":"<p>Visualizes the given attributions by overlaying an attribution heatmap  over the given image.</p>"},{"location":"trulens_explain/api/visualizations/#trulens_explain.trulens.visualizations.ChannelMaskVisualizer.__call__--parameters","title":"Parameters","text":"numpy.ndarray <p>The attributions to visualize. Expected to be in 4-D image format.</p> numpy.ndarray <p>The original image(s) over which the attributions are calculated. Must be the same shape as expected by the model used with this visualizer.</p> numpy.ndarray, optional <p>If the model requires a preprocessed input (e.g., with the mean subtracted) that is different from how the image should be  visualized, <code>x_preprocessed</code> should be specified. In this case  <code>x</code> will be used for visualization, and <code>x_preprocessed</code> will be passed to the model when calculating attributions. Must be the same  shape as <code>x</code>.</p> str, optional <p>If specified, the resulting visualization will be saved to a file with the name given by <code>output_file</code>.</p> float, optional <p>If specified, gives the radius of a Gaussian blur to be applied to the attributions before visualizing. This can be used to help focus on salient regions rather than specific salient pixels. If None,  defaults to the value supplied to the constructor. Default None.</p> float <p>Value in the range [0, 1]. Attribution values at or  below the  percentile given by <code>threshold</code> will be masked. If None, defaults  to the value supplied to the constructor. Default None.</p> float <p>Value in the range [0, 1] specifying the opacity for the parts of the image that are masked. Default 0.2. If None, defaults to the  value supplied to the constructor. Default None.</p> bool <p>If True, the attributions will be averaged across the channel dimension, resulting in a 1-channel attribution map. If None,  defaults to the value supplied to the constructor. Default None.</p> Source code in <code>trulens_explain/trulens/visualizations.py</code> <pre><code>def __call__(\n    self,\n    x,\n    x_preprocessed=None,\n    output_file=None,\n    blur=None,\n    threshold=None,\n    masked_opacity=None,\n    combine_channels=None\n):\n\"\"\"\n    Visualizes the given attributions by overlaying an attribution heatmap \n    over the given image.\n\n    Parameters\n    ----------\n    attributions : numpy.ndarray\n        The attributions to visualize. Expected to be in 4-D image format.\n\n    x : numpy.ndarray\n        The original image(s) over which the attributions are calculated.\n        Must be the same shape as expected by the model used with this\n        visualizer.\n\n    x_preprocessed : numpy.ndarray, optional\n        If the model requires a preprocessed input (e.g., with the mean\n        subtracted) that is different from how the image should be \n        visualized, ``x_preprocessed`` should be specified. In this case \n        ``x`` will be used for visualization, and ``x_preprocessed`` will be\n        passed to the model when calculating attributions. Must be the same \n        shape as ``x``.\n\n    output_file : str, optional\n        If specified, the resulting visualization will be saved to a file\n        with the name given by ``output_file``.\n\n    blur : float, optional\n        If specified, gives the radius of a Gaussian blur to be applied to\n        the attributions before visualizing. This can be used to help focus\n        on salient regions rather than specific salient pixels. If None, \n        defaults to the value supplied to the constructor. Default None.\n\n    threshold : float\n        Value in the range [0, 1]. Attribution values at or  below the \n        percentile given by ``threshold`` will be masked. If None, defaults \n        to the value supplied to the constructor. Default None.\n\n    masked_opacity: float\n        Value in the range [0, 1] specifying the opacity for the parts of\n        the image that are masked. Default 0.2. If None, defaults to the \n        value supplied to the constructor. Default None.\n\n    combine_channels : bool\n        If True, the attributions will be averaged across the channel\n        dimension, resulting in a 1-channel attribution map. If None, \n        defaults to the value supplied to the constructor. Default None.\n    \"\"\"\n\n    attrs_input = self.infl_input.attributions(\n        x if x_preprocessed is None else x_preprocessed\n    )\n\n    return self.mask_visualizer(\n        attrs_input, x, output_file, blur, threshold, masked_opacity,\n        combine_channels\n    )\n</code></pre>"},{"location":"trulens_explain/api/visualizations/#trulens_explain.trulens.visualizations.ChannelMaskVisualizer.__init__","title":"<code>__init__(model, layer, channel, channel_axis=None, agg_fn=None, doi=None, blur=None, threshold=0.5, masked_opacity=0.2, combine_channels=True, use_attr_as_opacity=None, positive_only=None)</code>","text":"<p>Configures the default parameters for the <code>__call__</code> method (these can  be overridden by passing in values to <code>__call__</code>).</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <p>The wrapped model whose channel we're visualizing.</p> required <code>layer</code> <p>The identifier (either index or name) of the layer in which the  channel we're visualizing resides.</p> required <code>channel</code> <p>Index of the channel (for convolutional layers) or internal  neuron (for fully-connected layers) that we'd like to visualize.</p> required <code>channel_axis</code> <p>If different from the channel axis specified by the backend, the supplied <code>channel_axis</code> will be used if operating on a  convolutional layer with 4-D image format.</p> <code>None</code> <code>agg_fn</code> <p>Function with which to aggregate the remaining dimensions  (except the batch dimension) in order to get a single scalar  value for each channel; If <code>None</code>, a sum over each neuron in the channel will be taken. This argument is not used when the  channels are scalars, e.g., for dense layers.</p> <code>None</code> <code>doi</code> <p>The distribution of interest to use when computing the input attributions towards the specified channel. If <code>None</code>,  <code>PointDoI</code> will be used.</p> <code>None</code> <code>blur</code> <p>Gives the radius of a Gaussian blur to be applied to the  attributions before visualizing. This can be used to help focus on salient regions rather than specific salient pixels.</p> <code>None</code> <code>threshold</code> <p>Value in the range [0, 1]. Attribution values at or  below the  percentile given by <code>threshold</code> (after normalization, blurring, etc.) will be masked.</p> <code>0.5</code> <code>masked_opacity</code> <p>Value in the range [0, 1] specifying the opacity for the parts of the image that are masked.</p> <code>0.2</code> <code>combine_channels</code> <code>bool</code> <p>If <code>True</code>, the attributions will be averaged across the channel dimension, resulting in a 1-channel attribution map.</p> <code>True</code> <code>use_attr_as_opacity</code> <p>If <code>True</code>, instead of using <code>threshold</code> and <code>masked_opacity</code>, the opacity of each pixel is given by the 0-1-normalized  attribution value.</p> <code>None</code> <code>positive_only</code> <p>If <code>True</code>, only pixels with positive attribution will be  unmasked (or given nonzero opacity when <code>use_attr_as_opacity</code> is true).</p> <code>None</code> Source code in <code>trulens_explain/trulens/visualizations.py</code> <pre><code>def __init__(\n    self,\n    model,\n    layer,\n    channel,\n    channel_axis=None,\n    agg_fn=None,\n    doi=None,\n    blur=None,\n    threshold=0.5,\n    masked_opacity=0.2,\n    combine_channels: bool = True,\n    use_attr_as_opacity=None,\n    positive_only=None\n):\n\"\"\"\n    Configures the default parameters for the `__call__` method (these can \n    be overridden by passing in values to `__call__`).\n\n    Parameters:\n        model:\n            The wrapped model whose channel we're visualizing.\n\n        layer:\n            The identifier (either index or name) of the layer in which the \n            channel we're visualizing resides.\n\n        channel:\n            Index of the channel (for convolutional layers) or internal \n            neuron (for fully-connected layers) that we'd like to visualize.\n\n        channel_axis:\n            If different from the channel axis specified by the backend, the\n            supplied `channel_axis` will be used if operating on a \n            convolutional layer with 4-D image format.\n\n        agg_fn:\n            Function with which to aggregate the remaining dimensions \n            (except the batch dimension) in order to get a single scalar \n            value for each channel; If `None`, a sum over each neuron in the\n            channel will be taken. This argument is not used when the \n            channels are scalars, e.g., for dense layers.\n\n        doi:\n            The distribution of interest to use when computing the input\n            attributions towards the specified channel. If `None`, \n            `PointDoI` will be used.\n\n        blur:\n            Gives the radius of a Gaussian blur to be applied to the \n            attributions before visualizing. This can be used to help focus\n            on salient regions rather than specific salient pixels.\n\n        threshold:\n            Value in the range [0, 1]. Attribution values at or  below the \n            percentile given by `threshold` (after normalization, blurring,\n            etc.) will be masked.\n\n        masked_opacity: \n            Value in the range [0, 1] specifying the opacity for the parts\n            of the image that are masked.\n\n        combine_channels:\n            If `True`, the attributions will be averaged across the channel\n            dimension, resulting in a 1-channel attribution map.\n\n        use_attr_as_opacity:\n            If `True`, instead of using `threshold` and `masked_opacity`,\n            the opacity of each pixel is given by the 0-1-normalized \n            attribution value.\n\n        positive_only:\n            If `True`, only pixels with positive attribution will be \n            unmasked (or given nonzero opacity when `use_attr_as_opacity` is\n            true).\n    \"\"\"\n    B = get_backend()\n    if (B is not None and (channel_axis is None or channel_axis &lt; 0)):\n        channel_axis = B.channel_axis\n    elif (channel_axis is None or channel_axis &lt; 0):\n        channel_axis = 1\n\n    self.mask_visualizer = MaskVisualizer(\n        blur, threshold, masked_opacity, combine_channels,\n        use_attr_as_opacity, positive_only\n    )\n\n    self.infl_input = InternalInfluence(\n        model, (InputCut(), Cut(layer)),\n        InternalChannelQoI(channel, channel_axis, agg_fn),\n        PointDoi() if doi is None else doi\n    )\n</code></pre>"},{"location":"trulens_explain/api/visualizations/#trulens_explain.trulens.visualizations.HTML","title":"<code>HTML</code>","text":"<p>         Bases: <code>Output</code></p> <p>HTML visualization output format.</p> Source code in <code>trulens_explain/trulens/visualizations.py</code> <pre><code>class HTML(Output):\n\"\"\"HTML visualization output format.\"\"\"\n\n    def __init__(self):\n        try:\n            self.m_html = importlib.import_module(\"html\")\n        except:\n            raise ImportError(\n                \"HTML output requires html python module. Try 'pip install html'.\"\n            )\n\n    def blank(self):\n        return \"\"\n\n    def space(self):\n        return \"&amp;nbsp;\"\n\n    def escape(self, s):\n        return self.m_html.escape(s)\n\n    def linebreak(self):\n        return \"&lt;br/&gt;\"\n\n    def line(self, s):\n        return f\"&lt;span style='padding: 2px; margin: 2px; background: gray; border-radius: 4px;'&gt;{s}&lt;/span&gt;\"\n\n    def magnitude_colored(self, s, mag):\n        red = 0.0\n        green = 0.0\n        if mag &gt; 0:\n            green = 1.0  # 0.5 + mag * 0.5\n            red = 1.0 - mag * 0.5\n        else:\n            red = 1.0\n            green = 1.0 + mag * 0.5\n            #red = 0.5 - mag * 0.5\n\n        blue = min(red, green)\n        # blue = 1.0 - max(red, green)\n\n        return f\"&lt;span title='{mag:0.3f}' style='margin: 1px; padding: 1px; border-radius: 4px; background: black; color: rgb({red*255}, {green*255}, {blue*255});'&gt;{s}&lt;/span&gt;\"\n\n    def append(self, *pieces):\n        return ''.join(pieces)\n\n    def render(self, s):\n        return s\n</code></pre>"},{"location":"trulens_explain/api/visualizations/#trulens_explain.trulens.visualizations.HeatmapVisualizer","title":"<code>HeatmapVisualizer</code>","text":"<p>         Bases: <code>Visualizer</code></p> <p>Visualizes attributions by overlaying an attribution heatmap over the original image, similar to how GradCAM visualizes attributions.</p> Source code in <code>trulens_explain/trulens/visualizations.py</code> <pre><code>class HeatmapVisualizer(Visualizer):\n\"\"\"\n    Visualizes attributions by overlaying an attribution heatmap over the\n    original image, similar to how GradCAM visualizes attributions.\n    \"\"\"\n\n    def __init__(\n        self,\n        overlay_opacity=0.5,\n        normalization_type=None,\n        blur=10.,\n        cmap='jet'\n    ):\n\"\"\"\n        Configures the default parameters for the `__call__` method (these can \n        be overridden by passing in values to `__call__`).\n\n        Parameters:\n            overlay_opacity: float\n                Value in the range [0, 1] specifying the opacity for the heatmap\n                overlay.\n\n            normalization_type:\n                Specifies one of the following configurations for normalizing\n                the attributions (each item is normalized separately):\n\n                - `'unsigned_max'`: normalizes the attributions to the range \n                  [-1, 1] by dividing the attributions by the maximum absolute \n                  attribution value.\n                - `'unsigned_max_positive_centered'`: same as above, but scales\n                  the values to the range [0, 1], with negative scores less than\n                  0.5 and positive scores greater than 0.5. \n                - `'magnitude_max'`: takes the absolute value of the \n                  attributions, then normalizes the attributions to the range \n                  [0, 1] by dividing by the maximum absolute attribution value.\n                - `'magnitude_sum'`: takes the absolute value of the \n                  attributions, then scales them such that they sum to 1. If \n                  this option is used, each channel is normalized separately, \n                  such that each channel sums to 1.\n                - `'signed_max'`: normalizes the attributions to the range \n                  [-1, 1] by dividing the positive values by the maximum \n                  positive attribution value and the negative values by the \n                  minimum negative attribution value.\n                - `'signed_max_positive_centered'`: same as above, but scales \n                  the values to the range [0, 1], with negative scores less than\n                  0.5 and positive scores greater than 0.5.\n                - `'signed_sum'`: scales the positive attributions such that \n                  they sum to 1 and the negative attributions such that they\n                  scale to -1. If this option is used, each channel is \n                  normalized separately.\n                - `'01'`: normalizes the attributions to the range [0, 1] by \n                  subtracting the minimum attribution value then dividing by the\n                  maximum attribution value.\n                - `'unnormalized'`: leaves the attributions unaffected.\n\n                If `None`, either `'unsigned_max'` (for single-channel data) or \n                `'unsigned_max_positive_centered'` (for multi-channel data) is\n                used.\n\n            blur:\n                Gives the radius of a Gaussian blur to be applied to the \n                attributions before visualizing. This can be used to help focus\n                on salient regions rather than specific salient pixels.\n\n            cmap: matplotlib.colors.Colormap | str, optional\n                Colormap or name of a Colormap to use for the visualization. If \n                `None`, the colormap will be chosen based on the normalization \n                type. This argument is only used for single-channel data\n                (including when `combine_channels` is True).\n        \"\"\"\n\n        super().__init__(\n            combine_channels=True,\n            normalization_type=normalization_type,\n            blur=blur,\n            cmap=cmap\n        )\n\n        self.default_overlay_opacity = overlay_opacity\n\n    def __call__(\n        self,\n        attributions,\n        x,\n        output_file=None,\n        imshow=True,\n        fig=None,\n        return_tiled=False,\n        overlay_opacity=None,\n        normalization_type=None,\n        blur=None,\n        cmap=None\n    ) -&gt; np.ndarray:\n\"\"\"\n        Visualizes the given attributions by overlaying an attribution heatmap \n        over the given image.\n\n        Parameters:\n            attributions:\n                A `np.ndarray` containing the attributions to be visualized.\n\n            x:\n                A `np.ndarray` of items in the same shape as `attributions`\n                corresponding to the records explained by the given \n                attributions. The visualization will be superimposed onto the\n                corresponding set of records.\n\n            output_file:\n                File name to save the visualization image to. If `None`, no\n                image will be saved, but the figure can still be displayed.\n\n            imshow:\n                If true, a the visualization will be displayed. Otherwise the\n                figure will not be displayed, but the figure can still be saved.\n\n            fig:\n                The `pyplot` figure to display the visualization in. If `None`,\n                a new figure will be created.\n\n            return_tiled:\n                If true, the returned array will be in the same shape as the\n                visualization, with no batch dimension and the samples in the\n                batch tiled along the width and height dimensions. If false, the\n                returned array will be reshaped to match `attributions`.\n\n            overlay_opacity: float\n                Value in the range [0, 1] specifying the opacity for the heatmap\n                overlay. If `None`, defaults to the value supplied to the \n                constructor.\n\n            normalization_type:\n                Specifies one of the following configurations for normalizing\n                the attributions (each item is normalized separately):\n\n                - `'unsigned_max'`: normalizes the attributions to the range \n                  [-1, 1] by dividing the attributions by the maximum absolute \n                  attribution value.\n                - `'unsigned_max_positive_centered'`: same as above, but scales\n                  the values to the range [0, 1], with negative scores less than\n                  0.5 and positive scores greater than 0.5. \n                - `'magnitude_max'`: takes the absolute value of the \n                  attributions, then normalizes the attributions to the range \n                  [0, 1] by dividing by the maximum absolute attribution value.\n                - `'magnitude_sum'`: takes the absolute value of the \n                  attributions, then scales them such that they sum to 1. If \n                  this option is used, each channel is normalized separately, \n                  such that each channel sums to 1.\n                - `'signed_max'`: normalizes the attributions to the range \n                  [-1, 1] by dividing the positive values by the maximum \n                  positive attribution value and the negative values by the \n                  minimum negative attribution value.\n                - `'signed_max_positive_centered'`: same as above, but scales \n                  the values to the range [0, 1], with negative scores less than\n                  0.5 and positive scores greater than 0.5.\n                - `'signed_sum'`: scales the positive attributions such that \n                  they sum to 1 and the negative attributions such that they\n                  scale to -1. If this option is used, each channel is \n                  normalized separately.\n                - `'01'`: normalizes the attributions to the range [0, 1] by \n                  subtracting the minimum attribution value then dividing by the\n                  maximum attribution value.\n                - `'unnormalized'`: leaves the attributions unaffected.\n\n                If `None`, defaults to the value supplied to the constructor.\n\n            blur:\n                Gives the radius of a Gaussian blur to be applied to the \n                attributions before visualizing. This can be used to help focus\n                on salient regions rather than specific salient pixels. If\n                `None`, defaults to the value supplied to the constructor.\n\n            cmap: matplotlib.colors.Colormap | str, optional\n                Colormap or name of a Colormap to use for the visualization. If\n                `None`, defaults to the value supplied to the constructor.\n\n        Returns:\n            A `np.ndarray` array of the numerical representation of the\n            attributions as modified for the visualization. This includes \n            normalization, blurring, etc.\n        \"\"\"\n        _, normalization_type, blur, cmap = self._check_args(\n            attributions, None, normalization_type, blur, cmap\n        )\n\n        # Combine the channels.\n        attributions = attributions.mean(\n            axis=get_backend().channel_axis, keepdims=True\n        )\n\n        # Blur the attributions so the explanation is smoother.\n        if blur:\n            attributions = self._blur(attributions, blur)\n\n        # Normalize the attributions.\n        attributions = self._normalize(attributions, normalization_type)\n\n        tiled_attributions = self.tiler.tile(attributions)\n\n        # Normalize the pixels to be in the range [0, 1].\n        x = self._normalize(x, '01')\n        tiled_x = self.tiler.tile(x)\n\n        if cmap is None:\n            cmap = self.default_cmap\n\n        if overlay_opacity is None:\n            overlay_opacity = self.default_overlay_opacity\n\n        # Display the figure:\n        _fig = plt.figure() if fig is None else fig\n\n        plt.axis('off')\n        plt.imshow(tiled_x)\n        plt.imshow(tiled_attributions, alpha=overlay_opacity, cmap=cmap)\n\n        if output_file:\n            plt.savefig(output_file, bbox_inches=0)\n\n        if imshow:\n            plt.show()\n\n        elif fig is None:\n            plt.close(_fig)\n\n        return tiled_attributions if return_tiled else attributions\n</code></pre>"},{"location":"trulens_explain/api/visualizations/#trulens_explain.trulens.visualizations.HeatmapVisualizer.__call__","title":"<code>__call__(attributions, x, output_file=None, imshow=True, fig=None, return_tiled=False, overlay_opacity=None, normalization_type=None, blur=None, cmap=None)</code>","text":"<p>Visualizes the given attributions by overlaying an attribution heatmap  over the given image.</p> <p>Parameters:</p> Name Type Description Default <code>attributions</code> <p>A <code>np.ndarray</code> containing the attributions to be visualized.</p> required <code>x</code> <p>A <code>np.ndarray</code> of items in the same shape as <code>attributions</code> corresponding to the records explained by the given  attributions. The visualization will be superimposed onto the corresponding set of records.</p> required <code>output_file</code> <p>File name to save the visualization image to. If <code>None</code>, no image will be saved, but the figure can still be displayed.</p> <code>None</code> <code>imshow</code> <p>If true, a the visualization will be displayed. Otherwise the figure will not be displayed, but the figure can still be saved.</p> <code>True</code> <code>fig</code> <p>The <code>pyplot</code> figure to display the visualization in. If <code>None</code>, a new figure will be created.</p> <code>None</code> <code>return_tiled</code> <p>If true, the returned array will be in the same shape as the visualization, with no batch dimension and the samples in the batch tiled along the width and height dimensions. If false, the returned array will be reshaped to match <code>attributions</code>.</p> <code>False</code> <code>overlay_opacity</code> <p>float Value in the range [0, 1] specifying the opacity for the heatmap overlay. If <code>None</code>, defaults to the value supplied to the  constructor.</p> <code>None</code> <code>normalization_type</code> <p>Specifies one of the following configurations for normalizing the attributions (each item is normalized separately):</p> <ul> <li><code>'unsigned_max'</code>: normalizes the attributions to the range    [-1, 1] by dividing the attributions by the maximum absolute    attribution value.</li> <li><code>'unsigned_max_positive_centered'</code>: same as above, but scales   the values to the range [0, 1], with negative scores less than   0.5 and positive scores greater than 0.5. </li> <li><code>'magnitude_max'</code>: takes the absolute value of the    attributions, then normalizes the attributions to the range    [0, 1] by dividing by the maximum absolute attribution value.</li> <li><code>'magnitude_sum'</code>: takes the absolute value of the    attributions, then scales them such that they sum to 1. If    this option is used, each channel is normalized separately,    such that each channel sums to 1.</li> <li><code>'signed_max'</code>: normalizes the attributions to the range    [-1, 1] by dividing the positive values by the maximum    positive attribution value and the negative values by the    minimum negative attribution value.</li> <li><code>'signed_max_positive_centered'</code>: same as above, but scales    the values to the range [0, 1], with negative scores less than   0.5 and positive scores greater than 0.5.</li> <li><code>'signed_sum'</code>: scales the positive attributions such that    they sum to 1 and the negative attributions such that they   scale to -1. If this option is used, each channel is    normalized separately.</li> <li><code>'01'</code>: normalizes the attributions to the range [0, 1] by    subtracting the minimum attribution value then dividing by the   maximum attribution value.</li> <li><code>'unnormalized'</code>: leaves the attributions unaffected.</li> </ul> <p>If <code>None</code>, defaults to the value supplied to the constructor.</p> <code>None</code> <code>blur</code> <p>Gives the radius of a Gaussian blur to be applied to the  attributions before visualizing. This can be used to help focus on salient regions rather than specific salient pixels. If <code>None</code>, defaults to the value supplied to the constructor.</p> <code>None</code> <code>cmap</code> <p>matplotlib.colors.Colormap | str, optional Colormap or name of a Colormap to use for the visualization. If <code>None</code>, defaults to the value supplied to the constructor.</p> <code>None</code> <p>Returns:</p> Type Description <code>np.ndarray</code> <p>A <code>np.ndarray</code> array of the numerical representation of the</p> <code>np.ndarray</code> <p>attributions as modified for the visualization. This includes </p> <code>np.ndarray</code> <p>normalization, blurring, etc.</p> Source code in <code>trulens_explain/trulens/visualizations.py</code> <pre><code>def __call__(\n    self,\n    attributions,\n    x,\n    output_file=None,\n    imshow=True,\n    fig=None,\n    return_tiled=False,\n    overlay_opacity=None,\n    normalization_type=None,\n    blur=None,\n    cmap=None\n) -&gt; np.ndarray:\n\"\"\"\n    Visualizes the given attributions by overlaying an attribution heatmap \n    over the given image.\n\n    Parameters:\n        attributions:\n            A `np.ndarray` containing the attributions to be visualized.\n\n        x:\n            A `np.ndarray` of items in the same shape as `attributions`\n            corresponding to the records explained by the given \n            attributions. The visualization will be superimposed onto the\n            corresponding set of records.\n\n        output_file:\n            File name to save the visualization image to. If `None`, no\n            image will be saved, but the figure can still be displayed.\n\n        imshow:\n            If true, a the visualization will be displayed. Otherwise the\n            figure will not be displayed, but the figure can still be saved.\n\n        fig:\n            The `pyplot` figure to display the visualization in. If `None`,\n            a new figure will be created.\n\n        return_tiled:\n            If true, the returned array will be in the same shape as the\n            visualization, with no batch dimension and the samples in the\n            batch tiled along the width and height dimensions. If false, the\n            returned array will be reshaped to match `attributions`.\n\n        overlay_opacity: float\n            Value in the range [0, 1] specifying the opacity for the heatmap\n            overlay. If `None`, defaults to the value supplied to the \n            constructor.\n\n        normalization_type:\n            Specifies one of the following configurations for normalizing\n            the attributions (each item is normalized separately):\n\n            - `'unsigned_max'`: normalizes the attributions to the range \n              [-1, 1] by dividing the attributions by the maximum absolute \n              attribution value.\n            - `'unsigned_max_positive_centered'`: same as above, but scales\n              the values to the range [0, 1], with negative scores less than\n              0.5 and positive scores greater than 0.5. \n            - `'magnitude_max'`: takes the absolute value of the \n              attributions, then normalizes the attributions to the range \n              [0, 1] by dividing by the maximum absolute attribution value.\n            - `'magnitude_sum'`: takes the absolute value of the \n              attributions, then scales them such that they sum to 1. If \n              this option is used, each channel is normalized separately, \n              such that each channel sums to 1.\n            - `'signed_max'`: normalizes the attributions to the range \n              [-1, 1] by dividing the positive values by the maximum \n              positive attribution value and the negative values by the \n              minimum negative attribution value.\n            - `'signed_max_positive_centered'`: same as above, but scales \n              the values to the range [0, 1], with negative scores less than\n              0.5 and positive scores greater than 0.5.\n            - `'signed_sum'`: scales the positive attributions such that \n              they sum to 1 and the negative attributions such that they\n              scale to -1. If this option is used, each channel is \n              normalized separately.\n            - `'01'`: normalizes the attributions to the range [0, 1] by \n              subtracting the minimum attribution value then dividing by the\n              maximum attribution value.\n            - `'unnormalized'`: leaves the attributions unaffected.\n\n            If `None`, defaults to the value supplied to the constructor.\n\n        blur:\n            Gives the radius of a Gaussian blur to be applied to the \n            attributions before visualizing. This can be used to help focus\n            on salient regions rather than specific salient pixels. If\n            `None`, defaults to the value supplied to the constructor.\n\n        cmap: matplotlib.colors.Colormap | str, optional\n            Colormap or name of a Colormap to use for the visualization. If\n            `None`, defaults to the value supplied to the constructor.\n\n    Returns:\n        A `np.ndarray` array of the numerical representation of the\n        attributions as modified for the visualization. This includes \n        normalization, blurring, etc.\n    \"\"\"\n    _, normalization_type, blur, cmap = self._check_args(\n        attributions, None, normalization_type, blur, cmap\n    )\n\n    # Combine the channels.\n    attributions = attributions.mean(\n        axis=get_backend().channel_axis, keepdims=True\n    )\n\n    # Blur the attributions so the explanation is smoother.\n    if blur:\n        attributions = self._blur(attributions, blur)\n\n    # Normalize the attributions.\n    attributions = self._normalize(attributions, normalization_type)\n\n    tiled_attributions = self.tiler.tile(attributions)\n\n    # Normalize the pixels to be in the range [0, 1].\n    x = self._normalize(x, '01')\n    tiled_x = self.tiler.tile(x)\n\n    if cmap is None:\n        cmap = self.default_cmap\n\n    if overlay_opacity is None:\n        overlay_opacity = self.default_overlay_opacity\n\n    # Display the figure:\n    _fig = plt.figure() if fig is None else fig\n\n    plt.axis('off')\n    plt.imshow(tiled_x)\n    plt.imshow(tiled_attributions, alpha=overlay_opacity, cmap=cmap)\n\n    if output_file:\n        plt.savefig(output_file, bbox_inches=0)\n\n    if imshow:\n        plt.show()\n\n    elif fig is None:\n        plt.close(_fig)\n\n    return tiled_attributions if return_tiled else attributions\n</code></pre>"},{"location":"trulens_explain/api/visualizations/#trulens_explain.trulens.visualizations.HeatmapVisualizer.__init__","title":"<code>__init__(overlay_opacity=0.5, normalization_type=None, blur=10.0, cmap='jet')</code>","text":"<p>Configures the default parameters for the <code>__call__</code> method (these can  be overridden by passing in values to <code>__call__</code>).</p> <p>Parameters:</p> Name Type Description Default <code>overlay_opacity</code> <p>float Value in the range [0, 1] specifying the opacity for the heatmap overlay.</p> <code>0.5</code> <code>normalization_type</code> <p>Specifies one of the following configurations for normalizing the attributions (each item is normalized separately):</p> <ul> <li><code>'unsigned_max'</code>: normalizes the attributions to the range    [-1, 1] by dividing the attributions by the maximum absolute    attribution value.</li> <li><code>'unsigned_max_positive_centered'</code>: same as above, but scales   the values to the range [0, 1], with negative scores less than   0.5 and positive scores greater than 0.5. </li> <li><code>'magnitude_max'</code>: takes the absolute value of the    attributions, then normalizes the attributions to the range    [0, 1] by dividing by the maximum absolute attribution value.</li> <li><code>'magnitude_sum'</code>: takes the absolute value of the    attributions, then scales them such that they sum to 1. If    this option is used, each channel is normalized separately,    such that each channel sums to 1.</li> <li><code>'signed_max'</code>: normalizes the attributions to the range    [-1, 1] by dividing the positive values by the maximum    positive attribution value and the negative values by the    minimum negative attribution value.</li> <li><code>'signed_max_positive_centered'</code>: same as above, but scales    the values to the range [0, 1], with negative scores less than   0.5 and positive scores greater than 0.5.</li> <li><code>'signed_sum'</code>: scales the positive attributions such that    they sum to 1 and the negative attributions such that they   scale to -1. If this option is used, each channel is    normalized separately.</li> <li><code>'01'</code>: normalizes the attributions to the range [0, 1] by    subtracting the minimum attribution value then dividing by the   maximum attribution value.</li> <li><code>'unnormalized'</code>: leaves the attributions unaffected.</li> </ul> <p>If <code>None</code>, either <code>'unsigned_max'</code> (for single-channel data) or  <code>'unsigned_max_positive_centered'</code> (for multi-channel data) is used.</p> <code>None</code> <code>blur</code> <p>Gives the radius of a Gaussian blur to be applied to the  attributions before visualizing. This can be used to help focus on salient regions rather than specific salient pixels.</p> <code>10.0</code> <code>cmap</code> <p>matplotlib.colors.Colormap | str, optional Colormap or name of a Colormap to use for the visualization. If  <code>None</code>, the colormap will be chosen based on the normalization  type. This argument is only used for single-channel data (including when <code>combine_channels</code> is True).</p> <code>'jet'</code> Source code in <code>trulens_explain/trulens/visualizations.py</code> <pre><code>def __init__(\n    self,\n    overlay_opacity=0.5,\n    normalization_type=None,\n    blur=10.,\n    cmap='jet'\n):\n\"\"\"\n    Configures the default parameters for the `__call__` method (these can \n    be overridden by passing in values to `__call__`).\n\n    Parameters:\n        overlay_opacity: float\n            Value in the range [0, 1] specifying the opacity for the heatmap\n            overlay.\n\n        normalization_type:\n            Specifies one of the following configurations for normalizing\n            the attributions (each item is normalized separately):\n\n            - `'unsigned_max'`: normalizes the attributions to the range \n              [-1, 1] by dividing the attributions by the maximum absolute \n              attribution value.\n            - `'unsigned_max_positive_centered'`: same as above, but scales\n              the values to the range [0, 1], with negative scores less than\n              0.5 and positive scores greater than 0.5. \n            - `'magnitude_max'`: takes the absolute value of the \n              attributions, then normalizes the attributions to the range \n              [0, 1] by dividing by the maximum absolute attribution value.\n            - `'magnitude_sum'`: takes the absolute value of the \n              attributions, then scales them such that they sum to 1. If \n              this option is used, each channel is normalized separately, \n              such that each channel sums to 1.\n            - `'signed_max'`: normalizes the attributions to the range \n              [-1, 1] by dividing the positive values by the maximum \n              positive attribution value and the negative values by the \n              minimum negative attribution value.\n            - `'signed_max_positive_centered'`: same as above, but scales \n              the values to the range [0, 1], with negative scores less than\n              0.5 and positive scores greater than 0.5.\n            - `'signed_sum'`: scales the positive attributions such that \n              they sum to 1 and the negative attributions such that they\n              scale to -1. If this option is used, each channel is \n              normalized separately.\n            - `'01'`: normalizes the attributions to the range [0, 1] by \n              subtracting the minimum attribution value then dividing by the\n              maximum attribution value.\n            - `'unnormalized'`: leaves the attributions unaffected.\n\n            If `None`, either `'unsigned_max'` (for single-channel data) or \n            `'unsigned_max_positive_centered'` (for multi-channel data) is\n            used.\n\n        blur:\n            Gives the radius of a Gaussian blur to be applied to the \n            attributions before visualizing. This can be used to help focus\n            on salient regions rather than specific salient pixels.\n\n        cmap: matplotlib.colors.Colormap | str, optional\n            Colormap or name of a Colormap to use for the visualization. If \n            `None`, the colormap will be chosen based on the normalization \n            type. This argument is only used for single-channel data\n            (including when `combine_channels` is True).\n    \"\"\"\n\n    super().__init__(\n        combine_channels=True,\n        normalization_type=normalization_type,\n        blur=blur,\n        cmap=cmap\n    )\n\n    self.default_overlay_opacity = overlay_opacity\n</code></pre>"},{"location":"trulens_explain/api/visualizations/#trulens_explain.trulens.visualizations.IPython","title":"<code>IPython</code>","text":"<p>         Bases: <code>HTML</code></p> <p>Interactive python visualization output format.</p> Source code in <code>trulens_explain/trulens/visualizations.py</code> <pre><code>class IPython(HTML):\n\"\"\"Interactive python visualization output format.\"\"\"\n\n    def __init__(self):\n        super(IPython, self).__init__()\n        try:\n            self.m_ipy = importlib.import_module(\"IPython\")\n        except:\n            raise ImportError(\n                \"Jupyter output requires IPython python module. Try 'pip install ipykernel'.\"\n            )\n\n    def render(self, s: str):\n        html = HTML.render(self, s)\n        return self.m_ipy.display.HTML(html)\n</code></pre>"},{"location":"trulens_explain/api/visualizations/#trulens_explain.trulens.visualizations.MaskVisualizer","title":"<code>MaskVisualizer</code>","text":"<p>         Bases: <code>object</code></p> <p>Visualizes attributions by masking the original image to highlight the regions with influence above a given threshold percentile. Intended  particularly for use with input-attributions.</p> Source code in <code>trulens_explain/trulens/visualizations.py</code> <pre><code>class MaskVisualizer(object):\n\"\"\"\n    Visualizes attributions by masking the original image to highlight the\n    regions with influence above a given threshold percentile. Intended \n    particularly for use with input-attributions.\n    \"\"\"\n\n    def __init__(\n        self,\n        blur=5.,\n        threshold=0.5,\n        masked_opacity=0.2,\n        combine_channels=True,\n        use_attr_as_opacity=False,\n        positive_only=True\n    ):\n\"\"\"\n        Configures the default parameters for the `__call__` method (these can \n        be overridden by passing in values to `__call__`).\n\n        Parameters:\n            blur:\n                Gives the radius of a Gaussian blur to be applied to the \n                attributions before visualizing. This can be used to help focus\n                on salient regions rather than specific salient pixels.\n\n            threshold:\n                Value in the range [0, 1]. Attribution values at or  below the \n                percentile given by `threshold` (after normalization, blurring,\n                etc.) will be masked.\n\n            masked_opacity: \n                Value in the range [0, 1] specifying the opacity for the parts\n                of the image that are masked.\n\n            combine_channels:\n                If `True`, the attributions will be averaged across the channel\n                dimension, resulting in a 1-channel attribution map.\n\n            use_attr_as_opacity:\n                If `True`, instead of using `threshold` and `masked_opacity`,\n                the opacity of each pixel is given by the 0-1-normalized \n                attribution value.\n\n            positive_only:\n                If `True`, only pixels with positive attribution will be \n                unmasked (or given nonzero opacity when `use_attr_as_opacity` is\n                true).\n        \"\"\"\n\n        self.default_blur = blur\n        self.default_thresh = threshold\n        self.default_masked_opacity = masked_opacity\n        self.default_combine_channels = combine_channels\n\n        # TODO(klas): in the future we can allow configuring of tiling settings\n        #   by allowing the user to specify the tiler.\n        self.tiler = Tiler()\n\n    def __call__(\n        self,\n        attributions,\n        x,\n        output_file=None,\n        imshow=True,\n        fig=None,\n        return_tiled=True,\n        blur=None,\n        threshold=None,\n        masked_opacity=None,\n        combine_channels=None,\n        use_attr_as_opacity=None,\n        positive_only=None\n    ):\n        channel_axis = get_backend().channel_axis\n        if attributions.shape != x.shape:\n            raise ValueError(\n                'Shape of `attributions` {} must match shape of `x` {}'.format(\n                    attributions.shape, x.shape\n                )\n            )\n\n        if blur is None:\n            blur = self.default_blur\n\n        if threshold is None:\n            threshold = self.default_thresh\n\n        if masked_opacity is None:\n            masked_opacity = self.default_masked_opacity\n\n        if combine_channels is None:\n            combine_channels = self.default_combine_channels\n\n        if len(attributions.shape) != 4:\n            raise ValueError(\n                '`MaskVisualizer` is inteded for 4-D image-format data. Given '\n                'input with dimension {}'.format(len(attributions.shape))\n            )\n\n        if combine_channels is None:\n            combine_channels = self.default_combine_channels\n\n        if combine_channels:\n            attributions = attributions.mean(axis=channel_axis, keepdims=True)\n\n        if x.shape[channel_axis] not in (1, 3, 4):\n            raise ValueError(\n                'To visualize, attributions must have either 1, 3, or 4 color '\n                'channels, but Visualizer got {} channels.\\n'\n                'If you are visualizing an internal layer, consider setting '\n                '`combine_channels` to True'.format(\n                    attributions.shape[channel_axis]\n                )\n            )\n\n        # Blur the attributions so the explanation is smoother.\n        if blur is not None:\n            attributions = [gaussian_filter(a, blur) for a in attributions]\n\n        # If `positive_only` clip attributions.\n        if positive_only:\n            attributions = np.maximum(attributions, 0)\n\n        # Normalize the attributions to be in the range [0, 1].\n        attributions = [a - a.min() for a in attributions]\n        attributions = [\n            0. * a if a.max() == 0. else a / a.max() for a in attributions\n        ]\n\n        # Normalize the pixels to be in the range [0, 1]\n        x = [xc - xc.min() for xc in x]\n        x = np.array([0. * xc if xc.max() == 0. else xc / xc.max() for xc in x])\n\n        # Threshold the attributions to create a mask.\n        if threshold is not None:\n            percentiles = [\n                np.percentile(a, 100 * threshold) for a in attributions\n            ]\n            masks = np.array(\n                [\n                    np.maximum(a &gt; p, masked_opacity)\n                    for a, p in zip(attributions, percentiles)\n                ]\n            )\n\n        else:\n            masks = np.array(attributions)\n\n        # Use the mask on the original image to visualize the explanation.\n        attributions = masks * x\n        tiled_attributions = self.tiler.tile(attributions)\n\n        if imshow:\n            plt.axis('off')\n            plt.imshow(tiled_attributions)\n\n            if output_file:\n                plt.savefig(output_file, bbox_inches=0)\n\n        return tiled_attributions if return_tiled else attributions\n</code></pre>"},{"location":"trulens_explain/api/visualizations/#trulens_explain.trulens.visualizations.MaskVisualizer.__init__","title":"<code>__init__(blur=5.0, threshold=0.5, masked_opacity=0.2, combine_channels=True, use_attr_as_opacity=False, positive_only=True)</code>","text":"<p>Configures the default parameters for the <code>__call__</code> method (these can  be overridden by passing in values to <code>__call__</code>).</p> <p>Parameters:</p> Name Type Description Default <code>blur</code> <p>Gives the radius of a Gaussian blur to be applied to the  attributions before visualizing. This can be used to help focus on salient regions rather than specific salient pixels.</p> <code>5.0</code> <code>threshold</code> <p>Value in the range [0, 1]. Attribution values at or  below the  percentile given by <code>threshold</code> (after normalization, blurring, etc.) will be masked.</p> <code>0.5</code> <code>masked_opacity</code> <p>Value in the range [0, 1] specifying the opacity for the parts of the image that are masked.</p> <code>0.2</code> <code>combine_channels</code> <p>If <code>True</code>, the attributions will be averaged across the channel dimension, resulting in a 1-channel attribution map.</p> <code>True</code> <code>use_attr_as_opacity</code> <p>If <code>True</code>, instead of using <code>threshold</code> and <code>masked_opacity</code>, the opacity of each pixel is given by the 0-1-normalized  attribution value.</p> <code>False</code> <code>positive_only</code> <p>If <code>True</code>, only pixels with positive attribution will be  unmasked (or given nonzero opacity when <code>use_attr_as_opacity</code> is true).</p> <code>True</code> Source code in <code>trulens_explain/trulens/visualizations.py</code> <pre><code>def __init__(\n    self,\n    blur=5.,\n    threshold=0.5,\n    masked_opacity=0.2,\n    combine_channels=True,\n    use_attr_as_opacity=False,\n    positive_only=True\n):\n\"\"\"\n    Configures the default parameters for the `__call__` method (these can \n    be overridden by passing in values to `__call__`).\n\n    Parameters:\n        blur:\n            Gives the radius of a Gaussian blur to be applied to the \n            attributions before visualizing. This can be used to help focus\n            on salient regions rather than specific salient pixels.\n\n        threshold:\n            Value in the range [0, 1]. Attribution values at or  below the \n            percentile given by `threshold` (after normalization, blurring,\n            etc.) will be masked.\n\n        masked_opacity: \n            Value in the range [0, 1] specifying the opacity for the parts\n            of the image that are masked.\n\n        combine_channels:\n            If `True`, the attributions will be averaged across the channel\n            dimension, resulting in a 1-channel attribution map.\n\n        use_attr_as_opacity:\n            If `True`, instead of using `threshold` and `masked_opacity`,\n            the opacity of each pixel is given by the 0-1-normalized \n            attribution value.\n\n        positive_only:\n            If `True`, only pixels with positive attribution will be \n            unmasked (or given nonzero opacity when `use_attr_as_opacity` is\n            true).\n    \"\"\"\n\n    self.default_blur = blur\n    self.default_thresh = threshold\n    self.default_masked_opacity = masked_opacity\n    self.default_combine_channels = combine_channels\n\n    # TODO(klas): in the future we can allow configuring of tiling settings\n    #   by allowing the user to specify the tiler.\n    self.tiler = Tiler()\n</code></pre>"},{"location":"trulens_explain/api/visualizations/#trulens_explain.trulens.visualizations.NLP","title":"<code>NLP</code>","text":"<p>         Bases: <code>object</code></p> <p>NLP Visualization tools.</p> Source code in <code>trulens_explain/trulens/visualizations.py</code> <pre><code>class NLP(object):\n\"\"\"NLP Visualization tools.\"\"\"\n\n    # Batches of text inputs not yet tokenized.\n    TextBatch = TypeVar(\"TextBatch\")\n\n    # Inputs that are directly accepted by wrapped models, tokenized.\n    # TODO(piotrm): Reuse other typevars/aliases from elsewhere.\n    ModelInput = TypeVar(\"ModelInput\")\n\n    # Outputs produced by wrapped models.\n    # TODO(piotrm): Reuse other typevars/aliases from elsewhere.\n    ModelOutput = TypeVar(\"ModelOutput\")\n\n    def __init__(\n        self,\n        wrapper: ModelWrapper,\n        output: Optional[Output] = None,\n        labels: Optional[Iterable[str]] = None,\n        tokenize: Optional[Callable[[TextBatch], ModelInputs]] = None,\n        decode: Optional[Callable[[Tensor], str]] = None,\n        input_accessor: Optional[Callable[[ModelInputs],\n                                          Iterable[Tensor]]] = None,\n        output_accessor: Optional[Callable[[ModelOutput],\n                                           Iterable[Tensor]]] = None,\n        attr_aggregate: Optional[Callable[[Tensor], Tensor]] = None,\n        hidden_tokens: Optional[Set[int]] = set()\n    ):\n\"\"\"Initializate NLP visualization tools for a given environment.\n\n        Parameters:\n            wrapper: ModelWrapper\n                The wrapped model whose channel we're visualizing.\n\n            output: Output, optional\n                Visualization output format. Defaults to PlainText unless\n                ipython is detected and in which case defaults to IPython\n                format.\n\n            labels: Iterable[str], optional\n                Names of prediction classes for classification models.\n\n            tokenize: Callable[[TextBatch], ModelInput], optional\n                Method to tokenize an instance.\n\n            decode: Callable[[Tensor], str], optional\n                Method to invert/decode the tokenization.\n\n            input_accessor: Callable[[ModelInputs], Iterable[Tensor]], optional\n                Method to extract input/token ids from model inputs (tokenize\n                output) if needed.\n\n            output_accessor: Callable[[ModelOutput], Iterable[Tensor]], optional\n                Method to extract outout logits from output structures if\n                needed.\n\n            attr_aggregate: Callable[[Tensor], Tensor], optional\n                Method to aggregate attribution for embedding into a single\n                value. Defaults to sum.\n\n            hidden_tokens: Set[int], optional\n                For token-based visualizations, which tokens to hide.\n        \"\"\"\n        if output is None:\n            try:\n                # check if running in interactive python (jupyer, colab, etc) to\n                # use appropriate output format\n                get_ipython()\n                output = IPython()\n\n            except NameError:\n                output = PlainText()\n                tru_logger(\n                    \"WARNING: could not guess preferred visualization output format, using PlainText\"\n                )\n\n        # TODO: automatic inference of various parameters for common repositories like huggingface, tfhub.\n\n        self.output = output\n        self.labels = labels\n        self.tokenize = tokenize\n        self.decode = decode\n        self.wrapper = wrapper\n\n        self.input_accessor = input_accessor  # could be inferred\n        self.output_accessor = output_accessor  # could be inferred\n\n        B = get_backend()\n\n        if attr_aggregate is None:\n            attr_aggregate = B.sum\n\n        self.attr_aggregate = attr_aggregate\n\n        self.hidden_tokens = hidden_tokens\n\n    def token_attribution(self, texts: Iterable[str], attr: AttributionMethod):\n\"\"\"Visualize a token-based input attribution on given `texts` inputs via the attribution method `attr`.\n\n        Parameters:\n            texts: Iterable[str]\n                The input texts to visualize.\n\n            attr: AttributionMethod\n                The attribution method to generate the token importances with.\n\n        Returns: Any\n            The visualization in the format specified by this class's `output` parameter.\n        \"\"\"\n\n        B = get_backend()\n\n        if self.tokenize is None:\n            return ValueError(\"tokenize not provided to NLP visualizer.\")\n\n        inputs = self.tokenize(texts)\n\n        outputs = inputs.call_on(self.wrapper._model)\n        attrs = inputs.call_on(attr.attributions)\n\n        content = self.output.blank()\n\n        input_ids = inputs\n        if self.input_accessor is not None:\n            input_ids = self.input_accessor(inputs)\n\n        if (not isinstance(input_ids, Iterable)) or isinstance(input_ids, dict):\n            raise ValueError(\n                f\"Inputs ({input_ids.__class__.__name__}) need to be iterable over instances. You might need to set input_accessor.\"\n            )\n\n        output_logits = outputs\n        if self.output_accessor is not None:\n            output_logits = self.output_accessor(outputs)\n\n        if (not isinstance(output_logits, Iterable)) or isinstance(\n                output_logits, dict):\n            raise ValueError(\n                f\"Outputs ({output_logits.__class__.__name__}) need to be iterable over instances. You might need to set output_accessor.\"\n            )\n\n        for i, (sentence_word_id, attr,\n                logits) in enumerate(zip(input_ids, attrs, output_logits)):\n\n            logits = logits.to('cpu').detach().numpy()\n            pred = logits.argmax()\n\n            if self.labels is not None:\n                pred_name = self.labels[pred]\n            else:\n                pred_name = str(pred)\n\n            sent = self.output.append(\n                self.output.escape(pred_name), \":\", self.output.space()\n            )\n\n            for word_id, attr in zip(sentence_word_id, attr):\n                word_id = int(B.as_array(word_id))\n\n                if word_id in self.hidden_tokens:\n                    continue\n\n                if self.decode is not None:\n                    word = self.decode(word_id)\n                else:\n                    word = str(word_id)\n\n                mag = self.attr_aggregate(attr)\n\n                if word[0] == ' ':\n                    word = word[1:]\n                    sent = self.output.append(sent, self.output.space())\n\n                sent = self.output.append(\n                    sent,\n                    self.output.magnitude_colored(\n                        self.output.escape(word), mag\n                    )\n                )\n\n            content = self.output.append(\n                content, self.output.line(sent), self.output.linebreak(),\n                self.output.linebreak()\n            )\n\n        return self.output.render(content)\n</code></pre>"},{"location":"trulens_explain/api/visualizations/#trulens_explain.trulens.visualizations.NLP.__init__","title":"<code>__init__(wrapper, output=None, labels=None, tokenize=None, decode=None, input_accessor=None, output_accessor=None, attr_aggregate=None, hidden_tokens=set())</code>","text":"<p>Initializate NLP visualization tools for a given environment.</p> <p>Parameters:</p> Name Type Description Default <code>wrapper</code> <code>ModelWrapper</code> <p>ModelWrapper The wrapped model whose channel we're visualizing.</p> required <code>output</code> <code>Optional[Output]</code> <p>Output, optional Visualization output format. Defaults to PlainText unless ipython is detected and in which case defaults to IPython format.</p> <code>None</code> <code>labels</code> <code>Optional[Iterable[str]]</code> <p>Iterable[str], optional Names of prediction classes for classification models.</p> <code>None</code> <code>tokenize</code> <code>Optional[Callable[[TextBatch], ModelInputs]]</code> <p>Callable[[TextBatch], ModelInput], optional Method to tokenize an instance.</p> <code>None</code> <code>decode</code> <code>Optional[Callable[[Tensor], str]]</code> <p>Callable[[Tensor], str], optional Method to invert/decode the tokenization.</p> <code>None</code> <code>input_accessor</code> <code>Optional[Callable[[ModelInputs], Iterable[Tensor]]]</code> <p>Callable[[ModelInputs], Iterable[Tensor]], optional Method to extract input/token ids from model inputs (tokenize output) if needed.</p> <code>None</code> <code>output_accessor</code> <code>Optional[Callable[[ModelOutput], Iterable[Tensor]]]</code> <p>Callable[[ModelOutput], Iterable[Tensor]], optional Method to extract outout logits from output structures if needed.</p> <code>None</code> <code>attr_aggregate</code> <code>Optional[Callable[[Tensor], Tensor]]</code> <p>Callable[[Tensor], Tensor], optional Method to aggregate attribution for embedding into a single value. Defaults to sum.</p> <code>None</code> <code>hidden_tokens</code> <code>Optional[Set[int]]</code> <p>Set[int], optional For token-based visualizations, which tokens to hide.</p> <code>set()</code> Source code in <code>trulens_explain/trulens/visualizations.py</code> <pre><code>def __init__(\n    self,\n    wrapper: ModelWrapper,\n    output: Optional[Output] = None,\n    labels: Optional[Iterable[str]] = None,\n    tokenize: Optional[Callable[[TextBatch], ModelInputs]] = None,\n    decode: Optional[Callable[[Tensor], str]] = None,\n    input_accessor: Optional[Callable[[ModelInputs],\n                                      Iterable[Tensor]]] = None,\n    output_accessor: Optional[Callable[[ModelOutput],\n                                       Iterable[Tensor]]] = None,\n    attr_aggregate: Optional[Callable[[Tensor], Tensor]] = None,\n    hidden_tokens: Optional[Set[int]] = set()\n):\n\"\"\"Initializate NLP visualization tools for a given environment.\n\n    Parameters:\n        wrapper: ModelWrapper\n            The wrapped model whose channel we're visualizing.\n\n        output: Output, optional\n            Visualization output format. Defaults to PlainText unless\n            ipython is detected and in which case defaults to IPython\n            format.\n\n        labels: Iterable[str], optional\n            Names of prediction classes for classification models.\n\n        tokenize: Callable[[TextBatch], ModelInput], optional\n            Method to tokenize an instance.\n\n        decode: Callable[[Tensor], str], optional\n            Method to invert/decode the tokenization.\n\n        input_accessor: Callable[[ModelInputs], Iterable[Tensor]], optional\n            Method to extract input/token ids from model inputs (tokenize\n            output) if needed.\n\n        output_accessor: Callable[[ModelOutput], Iterable[Tensor]], optional\n            Method to extract outout logits from output structures if\n            needed.\n\n        attr_aggregate: Callable[[Tensor], Tensor], optional\n            Method to aggregate attribution for embedding into a single\n            value. Defaults to sum.\n\n        hidden_tokens: Set[int], optional\n            For token-based visualizations, which tokens to hide.\n    \"\"\"\n    if output is None:\n        try:\n            # check if running in interactive python (jupyer, colab, etc) to\n            # use appropriate output format\n            get_ipython()\n            output = IPython()\n\n        except NameError:\n            output = PlainText()\n            tru_logger(\n                \"WARNING: could not guess preferred visualization output format, using PlainText\"\n            )\n\n    # TODO: automatic inference of various parameters for common repositories like huggingface, tfhub.\n\n    self.output = output\n    self.labels = labels\n    self.tokenize = tokenize\n    self.decode = decode\n    self.wrapper = wrapper\n\n    self.input_accessor = input_accessor  # could be inferred\n    self.output_accessor = output_accessor  # could be inferred\n\n    B = get_backend()\n\n    if attr_aggregate is None:\n        attr_aggregate = B.sum\n\n    self.attr_aggregate = attr_aggregate\n\n    self.hidden_tokens = hidden_tokens\n</code></pre>"},{"location":"trulens_explain/api/visualizations/#trulens_explain.trulens.visualizations.NLP.token_attribution","title":"<code>token_attribution(texts, attr)</code>","text":"<p>Visualize a token-based input attribution on given <code>texts</code> inputs via the attribution method <code>attr</code>.</p> <p>Parameters:</p> Name Type Description Default <code>texts</code> <code>Iterable[str]</code> <p>Iterable[str] The input texts to visualize.</p> required <code>attr</code> <code>AttributionMethod</code> <p>AttributionMethod The attribution method to generate the token importances with.</p> required <p>Any</p> Type Description <p>The visualization in the format specified by this class's <code>output</code> parameter.</p> Source code in <code>trulens_explain/trulens/visualizations.py</code> <pre><code>def token_attribution(self, texts: Iterable[str], attr: AttributionMethod):\n\"\"\"Visualize a token-based input attribution on given `texts` inputs via the attribution method `attr`.\n\n    Parameters:\n        texts: Iterable[str]\n            The input texts to visualize.\n\n        attr: AttributionMethod\n            The attribution method to generate the token importances with.\n\n    Returns: Any\n        The visualization in the format specified by this class's `output` parameter.\n    \"\"\"\n\n    B = get_backend()\n\n    if self.tokenize is None:\n        return ValueError(\"tokenize not provided to NLP visualizer.\")\n\n    inputs = self.tokenize(texts)\n\n    outputs = inputs.call_on(self.wrapper._model)\n    attrs = inputs.call_on(attr.attributions)\n\n    content = self.output.blank()\n\n    input_ids = inputs\n    if self.input_accessor is not None:\n        input_ids = self.input_accessor(inputs)\n\n    if (not isinstance(input_ids, Iterable)) or isinstance(input_ids, dict):\n        raise ValueError(\n            f\"Inputs ({input_ids.__class__.__name__}) need to be iterable over instances. You might need to set input_accessor.\"\n        )\n\n    output_logits = outputs\n    if self.output_accessor is not None:\n        output_logits = self.output_accessor(outputs)\n\n    if (not isinstance(output_logits, Iterable)) or isinstance(\n            output_logits, dict):\n        raise ValueError(\n            f\"Outputs ({output_logits.__class__.__name__}) need to be iterable over instances. You might need to set output_accessor.\"\n        )\n\n    for i, (sentence_word_id, attr,\n            logits) in enumerate(zip(input_ids, attrs, output_logits)):\n\n        logits = logits.to('cpu').detach().numpy()\n        pred = logits.argmax()\n\n        if self.labels is not None:\n            pred_name = self.labels[pred]\n        else:\n            pred_name = str(pred)\n\n        sent = self.output.append(\n            self.output.escape(pred_name), \":\", self.output.space()\n        )\n\n        for word_id, attr in zip(sentence_word_id, attr):\n            word_id = int(B.as_array(word_id))\n\n            if word_id in self.hidden_tokens:\n                continue\n\n            if self.decode is not None:\n                word = self.decode(word_id)\n            else:\n                word = str(word_id)\n\n            mag = self.attr_aggregate(attr)\n\n            if word[0] == ' ':\n                word = word[1:]\n                sent = self.output.append(sent, self.output.space())\n\n            sent = self.output.append(\n                sent,\n                self.output.magnitude_colored(\n                    self.output.escape(word), mag\n                )\n            )\n\n        content = self.output.append(\n            content, self.output.line(sent), self.output.linebreak(),\n            self.output.linebreak()\n        )\n\n    return self.output.render(content)\n</code></pre>"},{"location":"trulens_explain/api/visualizations/#trulens_explain.trulens.visualizations.Output","title":"<code>Output</code>","text":"<p>         Bases: <code>ABC</code></p> <p>Base class for visualization output formats.</p> Source code in <code>trulens_explain/trulens/visualizations.py</code> <pre><code>class Output(ABC):\n\"\"\"Base class for visualization output formats.\"\"\"\n\n    @abstractmethod\n    def blank(self) -&gt; str:\n        ...\n\n    @abstractmethod\n    def space(self) -&gt; str:\n        ...\n\n    @abstractmethod\n    def escape(self, s: str) -&gt; str:\n        ...\n\n    @abstractmethod\n    def line(self, s: str) -&gt; str:\n        ...\n\n    @abstractmethod\n    def magnitude_colored(self, s: str, mag: float) -&gt; str:\n        ...\n\n    @abstractmethod\n    def append(self, *parts: Iterable[str]) -&gt; str:\n        ...\n\n    @abstractmethod\n    def render(self, s: str) -&gt; str:\n        ...\n</code></pre>"},{"location":"trulens_explain/api/visualizations/#trulens_explain.trulens.visualizations.PlainText","title":"<code>PlainText</code>","text":"<p>         Bases: <code>Output</code></p> <p>Plain text visualization output format.</p> Source code in <code>trulens_explain/trulens/visualizations.py</code> <pre><code>class PlainText(Output):\n\"\"\"Plain text visualization output format.\"\"\"\n\n    def blank(self):\n        return \"\"\n\n    def space(self):\n        return \" \"\n\n    def escape(self, s):\n        return s\n\n    def line(self, s):\n        return s\n\n    def magnitude_colored(self, s, mag):\n        return f\"{s}({mag:0.3f})\"\n\n    def append(self, *parts):\n        return ''.join(parts)\n\n    def render(self, s):\n        return s\n</code></pre>"},{"location":"trulens_explain/api/visualizations/#trulens_explain.trulens.visualizations.Tiler","title":"<code>Tiler</code>","text":"<p>         Bases: <code>object</code></p> <p>Used to tile batched images or attributions.</p> Source code in <code>trulens_explain/trulens/visualizations.py</code> <pre><code>class Tiler(object):\n\"\"\"\n    Used to tile batched images or attributions.\n    \"\"\"\n\n    def tile(self, a: np.ndarray) -&gt; np.ndarray:\n\"\"\"\n        Tiles the given array into a grid that is as square as possible.\n\n        Parameters:\n            a:\n                An array of 4D batched image data.\n\n        Returns:\n            A tiled array of the images from `a`. The resulting array has rank\n            3 for color images, and 2 for grayscale images (the batch dimension\n            is removed, as well as the channel dimension for grayscale images).\n            The resulting array has its color channel dimension ordered last to\n            fit the requirements of the `matplotlib` library.\n        \"\"\"\n\n        # `pyplot` expects the channels to come last.\n        if get_backend().dim_order == 'channels_first':\n            a = a.transpose((0, 2, 3, 1))\n\n        n, h, w, c = a.shape\n\n        rows = int(np.sqrt(n))\n        cols = int(np.ceil(float(n) / rows))\n\n        new_a = np.zeros((h * rows, w * cols, c))\n\n        for i, x in enumerate(a):\n            row = i // cols\n            col = i % cols\n            new_a[row * h:(row + 1) * h, col * w:(col + 1) * w] = x\n\n        return np.squeeze(new_a)\n</code></pre>"},{"location":"trulens_explain/api/visualizations/#trulens_explain.trulens.visualizations.Tiler.tile","title":"<code>tile(a)</code>","text":"<p>Tiles the given array into a grid that is as square as possible.</p> <p>Parameters:</p> Name Type Description Default <code>a</code> <code>np.ndarray</code> <p>An array of 4D batched image data.</p> required <p>Returns:</p> Type Description <code>np.ndarray</code> <p>A tiled array of the images from <code>a</code>. The resulting array has rank</p> <code>np.ndarray</code> <p>3 for color images, and 2 for grayscale images (the batch dimension</p> <code>np.ndarray</code> <p>is removed, as well as the channel dimension for grayscale images).</p> <code>np.ndarray</code> <p>The resulting array has its color channel dimension ordered last to</p> <code>np.ndarray</code> <p>fit the requirements of the <code>matplotlib</code> library.</p> Source code in <code>trulens_explain/trulens/visualizations.py</code> <pre><code>def tile(self, a: np.ndarray) -&gt; np.ndarray:\n\"\"\"\n    Tiles the given array into a grid that is as square as possible.\n\n    Parameters:\n        a:\n            An array of 4D batched image data.\n\n    Returns:\n        A tiled array of the images from `a`. The resulting array has rank\n        3 for color images, and 2 for grayscale images (the batch dimension\n        is removed, as well as the channel dimension for grayscale images).\n        The resulting array has its color channel dimension ordered last to\n        fit the requirements of the `matplotlib` library.\n    \"\"\"\n\n    # `pyplot` expects the channels to come last.\n    if get_backend().dim_order == 'channels_first':\n        a = a.transpose((0, 2, 3, 1))\n\n    n, h, w, c = a.shape\n\n    rows = int(np.sqrt(n))\n    cols = int(np.ceil(float(n) / rows))\n\n    new_a = np.zeros((h * rows, w * cols, c))\n\n    for i, x in enumerate(a):\n        row = i // cols\n        col = i % cols\n        new_a[row * h:(row + 1) * h, col * w:(col + 1) * w] = x\n\n    return np.squeeze(new_a)\n</code></pre>"},{"location":"trulens_explain/api/visualizations/#trulens_explain.trulens.visualizations.Visualizer","title":"<code>Visualizer</code>","text":"<p>         Bases: <code>object</code></p> <p>Visualizes attributions directly as a color image. Intended particularly for use with input-attributions.</p> <p>This can also be used for viewing images (rather than attributions).</p> Source code in <code>trulens_explain/trulens/visualizations.py</code> <pre><code>class Visualizer(object):\n\"\"\"\n    Visualizes attributions directly as a color image. Intended particularly for\n    use with input-attributions.\n\n    This can also be used for viewing images (rather than attributions).\n    \"\"\"\n\n    def __init__(\n        self,\n        combine_channels: bool = False,\n        normalization_type: str = None,\n        blur: float = 0.,\n        cmap: Colormap = None\n    ):\n\"\"\"\n        Configures the default parameters for the `__call__` method (these can \n        be overridden by passing in values to `__call__`).\n\n        Parameters:\n            combine_channels:\n                If `True`, the attributions will be averaged across the channel\n                dimension, resulting in a 1-channel attribution map.\n\n            normalization_type:\n                Specifies one of the following configurations for normalizing\n                the attributions (each item is normalized separately):\n\n                - `'unsigned_max'`: normalizes the attributions to the range \n                  [-1, 1] by dividing the attributions by the maximum absolute \n                  attribution value.\n                - `'unsigned_max_positive_centered'`: same as above, but scales\n                  the values to the range [0, 1], with negative scores less than\n                  0.5 and positive scores greater than 0.5. \n                - `'magnitude_max'`: takes the absolute value of the \n                  attributions, then normalizes the attributions to the range \n                  [0, 1] by dividing by the maximum absolute attribution value.\n                - `'magnitude_sum'`: takes the absolute value of the \n                  attributions, then scales them such that they sum to 1. If \n                  this option is used, each channel is normalized separately, \n                  such that each channel sums to 1.\n                - `'signed_max'`: normalizes the attributions to the range \n                  [-1, 1] by dividing the positive values by the maximum \n                  positive attribution value and the negative values by the \n                  minimum negative attribution value.\n                - `'signed_max_positive_centered'`: same as above, but scales \n                  the values to the range [0, 1], with negative scores less than\n                  0.5 and positive scores greater than 0.5.\n                - `'signed_sum'`: scales the positive attributions such that \n                  they sum to 1 and the negative attributions such that they\n                  scale to -1. If this option is used, each channel is \n                  normalized separately.\n                - `'01'`: normalizes the attributions to the range [0, 1] by \n                  subtracting the minimum attribution value then dividing by the\n                  maximum attribution value.\n                - `'unnormalized'`: leaves the attributions unaffected.\n\n                If `None`, either `'unsigned_max'` (for single-channel data) or \n                `'unsigned_max_positive_centered'` (for multi-channel data) is\n                used.\n\n            blur:\n                Gives the radius of a Gaussian blur to be applied to the \n                attributions before visualizing. This can be used to help focus\n                on salient regions rather than specific salient pixels.\n\n            cmap: matplotlib.colors.Colormap | str, optional\n                Colormap or name of a Colormap to use for the visualization. If \n                `None`, the colormap will be chosen based on the normalization \n                type. This argument is only used for single-channel data\n                (including when `combine_channels` is True).\n        \"\"\"\n        self.default_combine_channels = combine_channels\n        self.default_normalization_type = normalization_type\n        self.default_blur = blur\n        self.default_cmap = cmap if cmap is not None else self._get_hotcold()\n\n        # TODO(klas): in the future we can allow configuring of tiling settings\n        #   by allowing the user to specify the tiler.\n        self.tiler = Tiler()\n\n    def __call__(\n        self,\n        attributions,\n        output_file=None,\n        imshow=True,\n        fig=None,\n        return_tiled=False,\n        combine_channels=None,\n        normalization_type=None,\n        blur=None,\n        cmap=None\n    ) -&gt; np.ndarray:\n\"\"\"\n        Visualizes the given attributions.\n\n        Parameters:\n            attributions:\n                A `np.ndarray` containing the attributions to be visualized.\n\n            output_file:\n                File name to save the visualization image to. If `None`, no\n                image will be saved, but the figure can still be displayed.\n\n            imshow:\n                If true, a the visualization will be displayed. Otherwise the\n                figure will not be displayed, but the figure can still be saved.\n\n            fig:\n                The `pyplot` figure to display the visualization in. If `None`,\n                a new figure will be created.\n\n            return_tiled:\n                If true, the returned array will be in the same shape as the\n                visualization, with no batch dimension and the samples in the\n                batch tiled along the width and height dimensions. If false, the\n                returned array will be reshaped to match `attributions`.\n\n            combine_channels:\n                If `True`, the attributions will be averaged across the channel\n                dimension, resulting in a 1-channel attribution map. If `None`,\n                defaults to the value supplied to the constructor.\n\n            normalization_type:\n                Specifies one of the following configurations for normalizing\n                the attributions (each item is normalized separately):\n\n                - `'unsigned_max'`: normalizes the attributions to the range \n                  [-1, 1] by dividing the attributions by the maximum absolute \n                  attribution value.\n                - `'unsigned_max_positive_centered'`: same as above, but scales\n                  the values to the range [0, 1], with negative scores less than\n                  0.5 and positive scores greater than 0.5. \n                - `'magnitude_max'`: takes the absolute value of the \n                  attributions, then normalizes the attributions to the range \n                  [0, 1] by dividing by the maximum absolute attribution value.\n                - `'magnitude_sum'`: takes the absolute value of the \n                  attributions, then scales them such that they sum to 1. If \n                  this option is used, each channel is normalized separately, \n                  such that each channel sums to 1.\n                - `'signed_max'`: normalizes the attributions to the range \n                  [-1, 1] by dividing the positive values by the maximum \n                  positive attribution value and the negative values by the \n                  minimum negative attribution value.\n                - `'signed_max_positive_centered'`: same as above, but scales \n                  the values to the range [0, 1], with negative scores less than\n                  0.5 and positive scores greater than 0.5.\n                - `'signed_sum'`: scales the positive attributions such that \n                  they sum to 1 and the negative attributions such that they\n                  scale to -1. If this option is used, each channel is \n                  normalized separately.\n                - `'01'`: normalizes the attributions to the range [0, 1] by \n                  subtracting the minimum attribution value then dividing by the\n                  maximum attribution value.\n                - `'unnormalized'`: leaves the attributions unaffected.\n\n                If `None`, defaults to the value supplied to the constructor.\n\n            blur:\n                Gives the radius of a Gaussian blur to be applied to the \n                attributions before visualizing. This can be used to help focus\n                on salient regions rather than specific salient pixels. If\n                `None`, defaults to the value supplied to the constructor.\n\n            cmap: matplotlib.colors.Colormap | str, optional\n                Colormap or name of a Colormap to use for the visualization. If\n                `None`, defaults to the value supplied to the constructor.\n\n        Returns:\n            A `np.ndarray` array of the numerical representation of the\n            attributions as modified for the visualization. This includes \n            normalization, blurring, etc.\n        \"\"\"\n        combine_channels, normalization_type, blur, cmap = self._check_args(\n            attributions, combine_channels, normalization_type, blur, cmap\n        )\n\n        # Combine the channels if specified.\n        if combine_channels:\n            attributions = attributions.mean(\n                axis=get_backend().channel_axis, keepdims=True\n            )\n\n        # Blur the attributions so the explanation is smoother.\n        if blur:\n            attributions = self._blur(attributions, blur)\n\n        # Normalize the attributions.\n        attributions = self._normalize(attributions, normalization_type)\n\n        tiled_attributions = self.tiler.tile(attributions)\n\n        # Display the figure:\n        _fig = plt.figure() if fig is None else fig\n\n        plt.axis('off')\n        plt.imshow(tiled_attributions, cmap=cmap)\n\n        if output_file:\n            plt.savefig(output_file, bbox_inches=0)\n\n        if imshow:\n            plt.show()\n\n        elif fig is None:\n            plt.close(_fig)\n\n        return tiled_attributions if return_tiled else attributions\n\n    def _check_args(\n        self, attributions, combine_channels, normalization_type, blur, cmap\n    ):\n\"\"\"\n        Validates the arguments, and sets them to their default values if they\n        are not specified.\n        \"\"\"\n        if attributions.ndim != 4:\n            raise ValueError(\n                '`Visualizer` is inteded for 4-D image-format data. Given '\n                'input with dimension {}'.format(attributions.ndim)\n            )\n\n        if combine_channels is None:\n            combine_channels = self.default_combine_channels\n\n        channel_axis = get_backend().channel_axis\n        if not (attributions.shape[channel_axis] in (1, 3, 4) or\n                combine_channels):\n\n            raise ValueError(\n                'To visualize, attributions must have either 1, 3, or 4 color '\n                'channels, but `Visualizer` got {} channels.\\n'\n                'If you are visualizing an internal layer, consider setting '\n                '`combine_channels` to True'.format(\n                    attributions.shape[channel_axis]\n                )\n            )\n\n        if normalization_type is None:\n            normalization_type = self.default_normalization_type\n\n            if normalization_type is None:\n                if combine_channels or attributions.shape[channel_axis] == 1:\n                    normalization_type = 'unsigned_max'\n\n                else:\n                    normalization_type = 'unsigned_max_positive_centered'\n\n        valid_normalization_types = [\n            'unsigned_max',\n            'unsigned_max_positive_centered',\n            'magnitude_max',\n            'magnitude_sum',\n            'signed_max',\n            'signed_max_positive_centered',\n            'signed_sum',\n            '01',\n            'unnormalized',\n        ]\n        if normalization_type not in valid_normalization_types:\n            raise ValueError(\n                '`norm` must be None or one of the following options:' +\n                ','.join(\n                    [\n                        '\\'{}\\''.form(norm_type)\n                        for norm_type in valid_normalization_types\n                    ]\n                )\n            )\n\n        if blur is None:\n            blur = self.default_blur\n\n        if cmap is None:\n            cmap = self.default_cmap\n\n        return combine_channels, normalization_type, blur, cmap\n\n    def _normalize(self, attributions, normalization_type, eps=1e-20):\n        channel_axis = get_backend().channel_axis\n        if normalization_type == 'unnormalized':\n            return attributions\n\n        split_by_channel = normalization_type.endswith('sum')\n\n        channel_split = [attributions] if split_by_channel else np.split(\n            attributions, attributions.shape[channel_axis], axis=channel_axis\n        )\n\n        normalized_attributions = []\n        for c_map in channel_split:\n            if normalization_type == 'magnitude_max':\n                c_map = np.abs(c_map) / (\n                    np.abs(c_map).max(axis=(1, 2, 3), keepdims=True) + eps\n                )\n\n            elif normalization_type == 'magnitude_sum':\n                c_map = np.abs(c_map) / (\n                    np.abs(c_map).sum(axis=(1, 2, 3), keepdims=True) + eps\n                )\n\n            elif normalization_type.startswith('signed_max'):\n                postive_max = c_map.max(axis=(1, 2, 3), keepdims=True)\n                negative_max = (-c_map).max(axis=(1, 2, 3), keepdims=True)\n\n                # Normalize the postive socres to [0, 1] and negative socresn to\n                # [-1, 0].\n                normalization_factor = np.where(\n                    c_map &gt;= 0, postive_max, negative_max\n                )\n                c_map = c_map / (normalization_factor + eps)\n\n                # If positive-centered, normalize so that all scores are in the\n                # range [0, 1], with negative scores less than 0.5 and positive\n                # scores greater than 0.5.\n                if normalization_type.endswith('positive_centered'):\n                    c_map = c_map / 2. + 0.5\n\n            elif normalization_type == 'signed_sum':\n                postive_max = np.maximum(c_map, 0).sum(\n                    axis=(1, 2, 3), keepdims=True\n                )\n                negative_max = np.maximum(-c_map, 0).sum(\n                    axis=(1, 2, 3), keepdims=True\n                )\n\n                # Normalize the postive socres to ensure they sum to 1 and the\n                # negative scores to ensure they sum to -1.\n                normalization_factor = np.where(\n                    c_map &gt;= 0, postive_max, negative_max\n                )\n                c_map = c_map / (normalization_factor + eps)\n\n            elif normalization_type.startswith('unsigned_max'):\n                c_map = c_map / (\n                    np.abs(c_map).max(axis=(1, 2, 3), keepdims=True) + eps\n                )\n\n                # If positive-centered, normalize so that all scores are in the\n                # range [0, 1], with negative scores less than 0.5 and positive\n                # scores greater than 0.5.\n                if normalization_type.endswith('positive_centered'):\n                    c_map = c_map / 2. + 0.5\n\n            elif normalization_type == '01':\n                c_map = c_map - c_map.min(axis=(1, 2, 3), keepdims=True)\n                c_map = c_map / (c_map.max(axis=(1, 2, 3), keepdims=True) + eps)\n\n            normalized_attributions.append(c_map)\n\n        return np.concatenate(normalized_attributions, axis=channel_axis)\n\n    def _blur(self, attributions, blur):\n        for i in range(attributions.shape[0]):\n            attributions[i] = gaussian_filter(attributions[i], blur)\n\n        return attributions\n\n    def _get_hotcold(self):\n        hot = cm.get_cmap('hot', 128)\n        cool = cm.get_cmap('cool', 128)\n        binary = cm.get_cmap('binary', 128)\n        hotcold = np.vstack(\n            (\n                binary(np.linspace(0, 1, 128)) * cool(np.linspace(0, 1, 128)),\n                hot(np.linspace(0, 1, 128))\n            )\n        )\n\n        return ListedColormap(hotcold, name='hotcold')\n</code></pre>"},{"location":"trulens_explain/api/visualizations/#trulens_explain.trulens.visualizations.Visualizer.__call__","title":"<code>__call__(attributions, output_file=None, imshow=True, fig=None, return_tiled=False, combine_channels=None, normalization_type=None, blur=None, cmap=None)</code>","text":"<p>Visualizes the given attributions.</p> <p>Parameters:</p> Name Type Description Default <code>attributions</code> <p>A <code>np.ndarray</code> containing the attributions to be visualized.</p> required <code>output_file</code> <p>File name to save the visualization image to. If <code>None</code>, no image will be saved, but the figure can still be displayed.</p> <code>None</code> <code>imshow</code> <p>If true, a the visualization will be displayed. Otherwise the figure will not be displayed, but the figure can still be saved.</p> <code>True</code> <code>fig</code> <p>The <code>pyplot</code> figure to display the visualization in. If <code>None</code>, a new figure will be created.</p> <code>None</code> <code>return_tiled</code> <p>If true, the returned array will be in the same shape as the visualization, with no batch dimension and the samples in the batch tiled along the width and height dimensions. If false, the returned array will be reshaped to match <code>attributions</code>.</p> <code>False</code> <code>combine_channels</code> <p>If <code>True</code>, the attributions will be averaged across the channel dimension, resulting in a 1-channel attribution map. If <code>None</code>, defaults to the value supplied to the constructor.</p> <code>None</code> <code>normalization_type</code> <p>Specifies one of the following configurations for normalizing the attributions (each item is normalized separately):</p> <ul> <li><code>'unsigned_max'</code>: normalizes the attributions to the range    [-1, 1] by dividing the attributions by the maximum absolute    attribution value.</li> <li><code>'unsigned_max_positive_centered'</code>: same as above, but scales   the values to the range [0, 1], with negative scores less than   0.5 and positive scores greater than 0.5. </li> <li><code>'magnitude_max'</code>: takes the absolute value of the    attributions, then normalizes the attributions to the range    [0, 1] by dividing by the maximum absolute attribution value.</li> <li><code>'magnitude_sum'</code>: takes the absolute value of the    attributions, then scales them such that they sum to 1. If    this option is used, each channel is normalized separately,    such that each channel sums to 1.</li> <li><code>'signed_max'</code>: normalizes the attributions to the range    [-1, 1] by dividing the positive values by the maximum    positive attribution value and the negative values by the    minimum negative attribution value.</li> <li><code>'signed_max_positive_centered'</code>: same as above, but scales    the values to the range [0, 1], with negative scores less than   0.5 and positive scores greater than 0.5.</li> <li><code>'signed_sum'</code>: scales the positive attributions such that    they sum to 1 and the negative attributions such that they   scale to -1. If this option is used, each channel is    normalized separately.</li> <li><code>'01'</code>: normalizes the attributions to the range [0, 1] by    subtracting the minimum attribution value then dividing by the   maximum attribution value.</li> <li><code>'unnormalized'</code>: leaves the attributions unaffected.</li> </ul> <p>If <code>None</code>, defaults to the value supplied to the constructor.</p> <code>None</code> <code>blur</code> <p>Gives the radius of a Gaussian blur to be applied to the  attributions before visualizing. This can be used to help focus on salient regions rather than specific salient pixels. If <code>None</code>, defaults to the value supplied to the constructor.</p> <code>None</code> <code>cmap</code> <p>matplotlib.colors.Colormap | str, optional Colormap or name of a Colormap to use for the visualization. If <code>None</code>, defaults to the value supplied to the constructor.</p> <code>None</code> <p>Returns:</p> Type Description <code>np.ndarray</code> <p>A <code>np.ndarray</code> array of the numerical representation of the</p> <code>np.ndarray</code> <p>attributions as modified for the visualization. This includes </p> <code>np.ndarray</code> <p>normalization, blurring, etc.</p> Source code in <code>trulens_explain/trulens/visualizations.py</code> <pre><code>def __call__(\n    self,\n    attributions,\n    output_file=None,\n    imshow=True,\n    fig=None,\n    return_tiled=False,\n    combine_channels=None,\n    normalization_type=None,\n    blur=None,\n    cmap=None\n) -&gt; np.ndarray:\n\"\"\"\n    Visualizes the given attributions.\n\n    Parameters:\n        attributions:\n            A `np.ndarray` containing the attributions to be visualized.\n\n        output_file:\n            File name to save the visualization image to. If `None`, no\n            image will be saved, but the figure can still be displayed.\n\n        imshow:\n            If true, a the visualization will be displayed. Otherwise the\n            figure will not be displayed, but the figure can still be saved.\n\n        fig:\n            The `pyplot` figure to display the visualization in. If `None`,\n            a new figure will be created.\n\n        return_tiled:\n            If true, the returned array will be in the same shape as the\n            visualization, with no batch dimension and the samples in the\n            batch tiled along the width and height dimensions. If false, the\n            returned array will be reshaped to match `attributions`.\n\n        combine_channels:\n            If `True`, the attributions will be averaged across the channel\n            dimension, resulting in a 1-channel attribution map. If `None`,\n            defaults to the value supplied to the constructor.\n\n        normalization_type:\n            Specifies one of the following configurations for normalizing\n            the attributions (each item is normalized separately):\n\n            - `'unsigned_max'`: normalizes the attributions to the range \n              [-1, 1] by dividing the attributions by the maximum absolute \n              attribution value.\n            - `'unsigned_max_positive_centered'`: same as above, but scales\n              the values to the range [0, 1], with negative scores less than\n              0.5 and positive scores greater than 0.5. \n            - `'magnitude_max'`: takes the absolute value of the \n              attributions, then normalizes the attributions to the range \n              [0, 1] by dividing by the maximum absolute attribution value.\n            - `'magnitude_sum'`: takes the absolute value of the \n              attributions, then scales them such that they sum to 1. If \n              this option is used, each channel is normalized separately, \n              such that each channel sums to 1.\n            - `'signed_max'`: normalizes the attributions to the range \n              [-1, 1] by dividing the positive values by the maximum \n              positive attribution value and the negative values by the \n              minimum negative attribution value.\n            - `'signed_max_positive_centered'`: same as above, but scales \n              the values to the range [0, 1], with negative scores less than\n              0.5 and positive scores greater than 0.5.\n            - `'signed_sum'`: scales the positive attributions such that \n              they sum to 1 and the negative attributions such that they\n              scale to -1. If this option is used, each channel is \n              normalized separately.\n            - `'01'`: normalizes the attributions to the range [0, 1] by \n              subtracting the minimum attribution value then dividing by the\n              maximum attribution value.\n            - `'unnormalized'`: leaves the attributions unaffected.\n\n            If `None`, defaults to the value supplied to the constructor.\n\n        blur:\n            Gives the radius of a Gaussian blur to be applied to the \n            attributions before visualizing. This can be used to help focus\n            on salient regions rather than specific salient pixels. If\n            `None`, defaults to the value supplied to the constructor.\n\n        cmap: matplotlib.colors.Colormap | str, optional\n            Colormap or name of a Colormap to use for the visualization. If\n            `None`, defaults to the value supplied to the constructor.\n\n    Returns:\n        A `np.ndarray` array of the numerical representation of the\n        attributions as modified for the visualization. This includes \n        normalization, blurring, etc.\n    \"\"\"\n    combine_channels, normalization_type, blur, cmap = self._check_args(\n        attributions, combine_channels, normalization_type, blur, cmap\n    )\n\n    # Combine the channels if specified.\n    if combine_channels:\n        attributions = attributions.mean(\n            axis=get_backend().channel_axis, keepdims=True\n        )\n\n    # Blur the attributions so the explanation is smoother.\n    if blur:\n        attributions = self._blur(attributions, blur)\n\n    # Normalize the attributions.\n    attributions = self._normalize(attributions, normalization_type)\n\n    tiled_attributions = self.tiler.tile(attributions)\n\n    # Display the figure:\n    _fig = plt.figure() if fig is None else fig\n\n    plt.axis('off')\n    plt.imshow(tiled_attributions, cmap=cmap)\n\n    if output_file:\n        plt.savefig(output_file, bbox_inches=0)\n\n    if imshow:\n        plt.show()\n\n    elif fig is None:\n        plt.close(_fig)\n\n    return tiled_attributions if return_tiled else attributions\n</code></pre>"},{"location":"trulens_explain/api/visualizations/#trulens_explain.trulens.visualizations.Visualizer.__init__","title":"<code>__init__(combine_channels=False, normalization_type=None, blur=0.0, cmap=None)</code>","text":"<p>Configures the default parameters for the <code>__call__</code> method (these can  be overridden by passing in values to <code>__call__</code>).</p> <p>Parameters:</p> Name Type Description Default <code>combine_channels</code> <code>bool</code> <p>If <code>True</code>, the attributions will be averaged across the channel dimension, resulting in a 1-channel attribution map.</p> <code>False</code> <code>normalization_type</code> <code>str</code> <p>Specifies one of the following configurations for normalizing the attributions (each item is normalized separately):</p> <ul> <li><code>'unsigned_max'</code>: normalizes the attributions to the range    [-1, 1] by dividing the attributions by the maximum absolute    attribution value.</li> <li><code>'unsigned_max_positive_centered'</code>: same as above, but scales   the values to the range [0, 1], with negative scores less than   0.5 and positive scores greater than 0.5. </li> <li><code>'magnitude_max'</code>: takes the absolute value of the    attributions, then normalizes the attributions to the range    [0, 1] by dividing by the maximum absolute attribution value.</li> <li><code>'magnitude_sum'</code>: takes the absolute value of the    attributions, then scales them such that they sum to 1. If    this option is used, each channel is normalized separately,    such that each channel sums to 1.</li> <li><code>'signed_max'</code>: normalizes the attributions to the range    [-1, 1] by dividing the positive values by the maximum    positive attribution value and the negative values by the    minimum negative attribution value.</li> <li><code>'signed_max_positive_centered'</code>: same as above, but scales    the values to the range [0, 1], with negative scores less than   0.5 and positive scores greater than 0.5.</li> <li><code>'signed_sum'</code>: scales the positive attributions such that    they sum to 1 and the negative attributions such that they   scale to -1. If this option is used, each channel is    normalized separately.</li> <li><code>'01'</code>: normalizes the attributions to the range [0, 1] by    subtracting the minimum attribution value then dividing by the   maximum attribution value.</li> <li><code>'unnormalized'</code>: leaves the attributions unaffected.</li> </ul> <p>If <code>None</code>, either <code>'unsigned_max'</code> (for single-channel data) or  <code>'unsigned_max_positive_centered'</code> (for multi-channel data) is used.</p> <code>None</code> <code>blur</code> <code>float</code> <p>Gives the radius of a Gaussian blur to be applied to the  attributions before visualizing. This can be used to help focus on salient regions rather than specific salient pixels.</p> <code>0.0</code> <code>cmap</code> <code>Colormap</code> <p>matplotlib.colors.Colormap | str, optional Colormap or name of a Colormap to use for the visualization. If  <code>None</code>, the colormap will be chosen based on the normalization  type. This argument is only used for single-channel data (including when <code>combine_channels</code> is True).</p> <code>None</code> Source code in <code>trulens_explain/trulens/visualizations.py</code> <pre><code>def __init__(\n    self,\n    combine_channels: bool = False,\n    normalization_type: str = None,\n    blur: float = 0.,\n    cmap: Colormap = None\n):\n\"\"\"\n    Configures the default parameters for the `__call__` method (these can \n    be overridden by passing in values to `__call__`).\n\n    Parameters:\n        combine_channels:\n            If `True`, the attributions will be averaged across the channel\n            dimension, resulting in a 1-channel attribution map.\n\n        normalization_type:\n            Specifies one of the following configurations for normalizing\n            the attributions (each item is normalized separately):\n\n            - `'unsigned_max'`: normalizes the attributions to the range \n              [-1, 1] by dividing the attributions by the maximum absolute \n              attribution value.\n            - `'unsigned_max_positive_centered'`: same as above, but scales\n              the values to the range [0, 1], with negative scores less than\n              0.5 and positive scores greater than 0.5. \n            - `'magnitude_max'`: takes the absolute value of the \n              attributions, then normalizes the attributions to the range \n              [0, 1] by dividing by the maximum absolute attribution value.\n            - `'magnitude_sum'`: takes the absolute value of the \n              attributions, then scales them such that they sum to 1. If \n              this option is used, each channel is normalized separately, \n              such that each channel sums to 1.\n            - `'signed_max'`: normalizes the attributions to the range \n              [-1, 1] by dividing the positive values by the maximum \n              positive attribution value and the negative values by the \n              minimum negative attribution value.\n            - `'signed_max_positive_centered'`: same as above, but scales \n              the values to the range [0, 1], with negative scores less than\n              0.5 and positive scores greater than 0.5.\n            - `'signed_sum'`: scales the positive attributions such that \n              they sum to 1 and the negative attributions such that they\n              scale to -1. If this option is used, each channel is \n              normalized separately.\n            - `'01'`: normalizes the attributions to the range [0, 1] by \n              subtracting the minimum attribution value then dividing by the\n              maximum attribution value.\n            - `'unnormalized'`: leaves the attributions unaffected.\n\n            If `None`, either `'unsigned_max'` (for single-channel data) or \n            `'unsigned_max_positive_centered'` (for multi-channel data) is\n            used.\n\n        blur:\n            Gives the radius of a Gaussian blur to be applied to the \n            attributions before visualizing. This can be used to help focus\n            on salient regions rather than specific salient pixels.\n\n        cmap: matplotlib.colors.Colormap | str, optional\n            Colormap or name of a Colormap to use for the visualization. If \n            `None`, the colormap will be chosen based on the normalization \n            type. This argument is only used for single-channel data\n            (including when `combine_channels` is True).\n    \"\"\"\n    self.default_combine_channels = combine_channels\n    self.default_normalization_type = normalization_type\n    self.default_blur = blur\n    self.default_cmap = cmap if cmap is not None else self._get_hotcold()\n\n    # TODO(klas): in the future we can allow configuring of tiling settings\n    #   by allowing the user to specify the tiler.\n    self.tiler = Tiler()\n</code></pre>"}]}