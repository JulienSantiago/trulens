{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"","title":"Home"},{"location":"attribution_parameterization/","text":"Attribution Parameterization \u00b6 Attributions for different models and use cases can range from simple to more complex. This will help see how to set the various parameters to get what you need. Basic Definitions and Terminology \u00b6 What is a tensor? A tensor is a multidimensional object that can be model inputs, or layer activations. What is a layer? A layer is a set of neurons that can be thought of as a function on input tensors. Layer inputs are tensors. Layer outputs are modified tensors. What are anchors? Anchors are ways of specifying which tensors you want. You may want the input tensor of a layer, or the output tensor of a layer. Eg: Say you have a concat layer and you want to explain the 2 concatenated tensors. The concat operation is not usually a layer tracked by the model. If you try the 'in' anchor of the layer after the operation, you get a single tensor with all the information you need. What is a Quantity of Interest (QoI)? A QoI is a scalar number that is being explained. Eg: With saliency maps, you get dx/dy (i.e the effect of input on output). y in this case is the QoI scalar. It is usually the output of a neuron, but could be a sum of multiple neurons. What is an attribution? An attribution is a numerical value associated with every element in a tensor that explains a QoI. Eg: With saliency maps, you get dx/dy . x is the associated tensor. The entirety of dx/dy is the explanation. What are cuts? Cuts are tensors that cut a network into two parts. They are composed of a layer and an anchor. What are slices? Slices are two cuts leaving a slice of the network. The attribution will be on the first cut, explaining the QoI on the second cut of the slice. Eg: With saliency maps, the trulens slice would be AttributionCut: Cut(x) to QoICut: Cut(y) denoted by Slice(Cut(x),Cut(y)) . How to use Trulens? \u00b6 This section will cover different use cases from the most basic to the most complex. Case 1: Input-Output cut (Basic configuration) \u00b6 Use case: Explain the input given the output. Cuts needed: Trulens Defaults. Attribution Cut (The tensor we would like to assign importance) \u2192 InputCut aka model args/kwargs QoI Cut (The tensor that we are interested to explain) \u2192 OutputCut Case 2: The QoI Cut \u00b6 Now suppose you want to explain some internal layer\u2019s output (intermediate output). i.e how the input is affecting the output at some intermediate layer. Use case: Explain something that isn't the default model output. Eg: If you want to explain a logit layer instead of the probit layer (the final layer). Cuts needed: As you want to explain something different than the default output, you need to change the QoI from the default to the layer that you are interested. Attribution Cut \u2192 InputCut QoI Cut \u2192 Your logit layer, anchor:'out' Case 3: The Attribution Cut \u00b6 Now suppose you want to know the attribution of some internal layer on the final output. Use cases: As a preprocessing step, you drop a feature, so do not need attributions on that. For torch models, model inputs are not tensors. so you'd want the 'in' anchor of the first layer. Cuts needed: As you want to know the affect of some other layer rather than the input layer, you need to customize the attribution cut. Model inputs \u2192 InputCut Attribution Cut \u2192 Your attribution layer (The layer you want to assign importance/attributions with respect to output), anchor:'in' QoI Cut \u2192 OutputCut Advanced Use Cases \u00b6 For the following use cases, it may help to see the Advanced Definitions Case 4: The Distribution of Interest (DoI) Cut / Explanation flexibility \u00b6 Usually, we explain the output with respect to each point in the input. All cases up to now were using a default called PointDoI. Now, suppose you want to explain using an aggregate over samples of points. Use case: You want to do integrated gradients, gradcam, shapley values instead of saliency maps. These only differ by sampling strategies. Eg: Integrated gradients is a sample from a straight line from a baseline to a value. Cuts needed: Define a DoI that samples from the default attribution cut. Model inputs \u2192 InputCut DoI/Attribution Cut \u2192 Your baseline/DoI/attribution layer, anchor:'in' QoI Cut \u2192 OutputCut Case 5: Internal explanations \u00b6 Use case: You want to explain an internal layer. Things like integrated gradients are a DoI on the baseline to the value, but it is on the layer the baseline is defined. If you want to explain an internal layer, you do not move the DoI layer. Cuts needed: Attribution layer different from DoI. Model inputs \u2192 InputCut DoI Cut \u2192 Your baseline/DoI layer, anchor:'in' Attribution Cut \u2192 Your internal attribution layer, anchor:'out' or 'in' QoI Cut \u2192 OutputCut Case 6: Your baseline happens at a different layer than your sampling. \u00b6 Use Case: in NLP, baselines are tokens, but the interpolation is on the embedding layer. Cuts needed: Baseline different from DoI. Model inputs \u2192 InputCut Baseline Cut \u2192 tokens, anchor:'out' DoI/Attribution Cut \u2192 embeddings, anchor:'out' QoI Cut \u2192 OutputCut Case 7: Putting it together - The most complex one one we can do with our Trulens \u00b6 Use Case: Internal layer explanations of NLP, on the logit layer of a model with probit outputs. Model inputs \u2192 InputCut Baseline Cut \u2192 tokens, anchor:'out' DoI Cut \u2192 embeddings, anchor:'out' Attribution Cut \u2192 Internal layer, anchor:'out' QoI Cut \u2192 logit layer, anchor:'out' Summary \u00b6 InputCut is model args / kwargs. OutputCut is the model output. Baseline Cut is the tensor associated with the Integrated gradients baseline. Can be the InputCut or later. DoI Cut is the tensor associated with explanation sampling. Can be the BaselineCut or later. Attribution Cut is the tensor that should be explained. Can be the DoICut or later. QoI Cut is what is being explained with a QoI. Must be after the AttributionCut. Advanced Definitions \u00b6 What is a Distribution of Interest (DoI)? The distribution of interest is a concept of aggregating attributions over a sample or distribution. GradCam does this over a guassian distribution of inputs. Shapley values do this over different background data. Integrated gradients does this over an interpolation from a baseline to the input. How does this relate to the Attribution Cut? The sample or distributions are taken at a place that is humanly considered the input, even if this differs from the programatic model input. For attributions, all parts of a network can have have an attribution towards the QoI. It is that the most common use case is to explain the tensors that are also the humanly considered input (which is where the DoI occurs). How does this relate to the Baseline Cut? The Baseline Cut is only applicable to the Integrated Gradients method. It is also only needed when there is no mathematical way to interpolate the baseline to the input. Eg. if the input is 'Hello', but the baseline is a '[MASK]' token, we cannot interpolate that. We define the baseline at the token layer, but interpolate on a numeric layer like the embeddings.","title":"Attributions for Different Use Cases"},{"location":"attribution_parameterization/#attribution-parameterization","text":"Attributions for different models and use cases can range from simple to more complex. This will help see how to set the various parameters to get what you need.","title":"Attribution Parameterization"},{"location":"attribution_parameterization/#basic-definitions-and-terminology","text":"What is a tensor? A tensor is a multidimensional object that can be model inputs, or layer activations. What is a layer? A layer is a set of neurons that can be thought of as a function on input tensors. Layer inputs are tensors. Layer outputs are modified tensors. What are anchors? Anchors are ways of specifying which tensors you want. You may want the input tensor of a layer, or the output tensor of a layer. Eg: Say you have a concat layer and you want to explain the 2 concatenated tensors. The concat operation is not usually a layer tracked by the model. If you try the 'in' anchor of the layer after the operation, you get a single tensor with all the information you need. What is a Quantity of Interest (QoI)? A QoI is a scalar number that is being explained. Eg: With saliency maps, you get dx/dy (i.e the effect of input on output). y in this case is the QoI scalar. It is usually the output of a neuron, but could be a sum of multiple neurons. What is an attribution? An attribution is a numerical value associated with every element in a tensor that explains a QoI. Eg: With saliency maps, you get dx/dy . x is the associated tensor. The entirety of dx/dy is the explanation. What are cuts? Cuts are tensors that cut a network into two parts. They are composed of a layer and an anchor. What are slices? Slices are two cuts leaving a slice of the network. The attribution will be on the first cut, explaining the QoI on the second cut of the slice. Eg: With saliency maps, the trulens slice would be AttributionCut: Cut(x) to QoICut: Cut(y) denoted by Slice(Cut(x),Cut(y)) .","title":"Basic Definitions and Terminology"},{"location":"attribution_parameterization/#how-to-use-trulens","text":"This section will cover different use cases from the most basic to the most complex.","title":"How to use Trulens?"},{"location":"attribution_parameterization/#case-1-input-output-cut-basic-configuration","text":"Use case: Explain the input given the output. Cuts needed: Trulens Defaults. Attribution Cut (The tensor we would like to assign importance) \u2192 InputCut aka model args/kwargs QoI Cut (The tensor that we are interested to explain) \u2192 OutputCut","title":"Case 1: Input-Output cut (Basic configuration)"},{"location":"attribution_parameterization/#case-2-the-qoi-cut","text":"Now suppose you want to explain some internal layer\u2019s output (intermediate output). i.e how the input is affecting the output at some intermediate layer. Use case: Explain something that isn't the default model output. Eg: If you want to explain a logit layer instead of the probit layer (the final layer). Cuts needed: As you want to explain something different than the default output, you need to change the QoI from the default to the layer that you are interested. Attribution Cut \u2192 InputCut QoI Cut \u2192 Your logit layer, anchor:'out'","title":"Case 2: The QoI Cut"},{"location":"attribution_parameterization/#case-3-the-attribution-cut","text":"Now suppose you want to know the attribution of some internal layer on the final output. Use cases: As a preprocessing step, you drop a feature, so do not need attributions on that. For torch models, model inputs are not tensors. so you'd want the 'in' anchor of the first layer. Cuts needed: As you want to know the affect of some other layer rather than the input layer, you need to customize the attribution cut. Model inputs \u2192 InputCut Attribution Cut \u2192 Your attribution layer (The layer you want to assign importance/attributions with respect to output), anchor:'in' QoI Cut \u2192 OutputCut","title":"Case 3: The Attribution Cut"},{"location":"attribution_parameterization/#advanced-use-cases","text":"For the following use cases, it may help to see the Advanced Definitions","title":"Advanced Use Cases"},{"location":"attribution_parameterization/#case-4-the-distribution-of-interest-doi-cut-explanation-flexibility","text":"Usually, we explain the output with respect to each point in the input. All cases up to now were using a default called PointDoI. Now, suppose you want to explain using an aggregate over samples of points. Use case: You want to do integrated gradients, gradcam, shapley values instead of saliency maps. These only differ by sampling strategies. Eg: Integrated gradients is a sample from a straight line from a baseline to a value. Cuts needed: Define a DoI that samples from the default attribution cut. Model inputs \u2192 InputCut DoI/Attribution Cut \u2192 Your baseline/DoI/attribution layer, anchor:'in' QoI Cut \u2192 OutputCut","title":"Case 4: The Distribution of Interest (DoI) Cut / Explanation flexibility"},{"location":"attribution_parameterization/#case-5-internal-explanations","text":"Use case: You want to explain an internal layer. Things like integrated gradients are a DoI on the baseline to the value, but it is on the layer the baseline is defined. If you want to explain an internal layer, you do not move the DoI layer. Cuts needed: Attribution layer different from DoI. Model inputs \u2192 InputCut DoI Cut \u2192 Your baseline/DoI layer, anchor:'in' Attribution Cut \u2192 Your internal attribution layer, anchor:'out' or 'in' QoI Cut \u2192 OutputCut","title":"Case 5: Internal explanations"},{"location":"attribution_parameterization/#case-6-your-baseline-happens-at-a-different-layer-than-your-sampling","text":"Use Case: in NLP, baselines are tokens, but the interpolation is on the embedding layer. Cuts needed: Baseline different from DoI. Model inputs \u2192 InputCut Baseline Cut \u2192 tokens, anchor:'out' DoI/Attribution Cut \u2192 embeddings, anchor:'out' QoI Cut \u2192 OutputCut","title":"Case 6: Your baseline happens at a different layer than your sampling."},{"location":"attribution_parameterization/#case-7-putting-it-together-the-most-complex-one-one-we-can-do-with-our-trulens","text":"Use Case: Internal layer explanations of NLP, on the logit layer of a model with probit outputs. Model inputs \u2192 InputCut Baseline Cut \u2192 tokens, anchor:'out' DoI Cut \u2192 embeddings, anchor:'out' Attribution Cut \u2192 Internal layer, anchor:'out' QoI Cut \u2192 logit layer, anchor:'out'","title":"Case 7: Putting it together - The most complex one one we can do with our Trulens"},{"location":"attribution_parameterization/#summary","text":"InputCut is model args / kwargs. OutputCut is the model output. Baseline Cut is the tensor associated with the Integrated gradients baseline. Can be the InputCut or later. DoI Cut is the tensor associated with explanation sampling. Can be the BaselineCut or later. Attribution Cut is the tensor that should be explained. Can be the DoICut or later. QoI Cut is what is being explained with a QoI. Must be after the AttributionCut.","title":"Summary"},{"location":"attribution_parameterization/#advanced-definitions","text":"What is a Distribution of Interest (DoI)? The distribution of interest is a concept of aggregating attributions over a sample or distribution. GradCam does this over a guassian distribution of inputs. Shapley values do this over different background data. Integrated gradients does this over an interpolation from a baseline to the input. How does this relate to the Attribution Cut? The sample or distributions are taken at a place that is humanly considered the input, even if this differs from the programatic model input. For attributions, all parts of a network can have have an attribution towards the QoI. It is that the most common use case is to explain the tensors that are also the humanly considered input (which is where the DoI occurs). How does this relate to the Baseline Cut? The Baseline Cut is only applicable to the Integrated Gradients method. It is also only needed when there is no mathematical way to interpolate the baseline to the input. Eg. if the input is 'Hello', but the baseline is a '[MASK]' token, we cannot interpolate that. We define the baseline at the token layer, but interpolate on a numeric layer like the embeddings.","title":"Advanced Definitions"},{"location":"install/","text":"Getting access to TruLens \u00b6 These installation instructions assume that you have conda installed and added to your path. Create a virtual environment (or modify an existing one). conda create -n \"<my_name>\" python=3.7 # Skip if using existing environment. conda activate <my_name> Install dependencies. conda install tensorflow-gpu=1 # Or whatever backend you're using. conda install keras # Or whatever backend you're using. conda install matplotlib # For visualizations. [Pip installation] Install the trulens pip package. pip install trulens [Local installation] If you would like to develop or modify trulens, you can download the source code by cloning the trulens repo. git clone https://github.com/truera/trulens.git [Locall installation] Install the trulens repo. cd trulens pip install -e .","title":"Installation"},{"location":"install/#getting-access-to-trulens","text":"These installation instructions assume that you have conda installed and added to your path. Create a virtual environment (or modify an existing one). conda create -n \"<my_name>\" python=3.7 # Skip if using existing environment. conda activate <my_name> Install dependencies. conda install tensorflow-gpu=1 # Or whatever backend you're using. conda install keras # Or whatever backend you're using. conda install matplotlib # For visualizations. [Pip installation] Install the trulens pip package. pip install trulens [Local installation] If you would like to develop or modify trulens, you can download the source code by cloning the trulens repo. git clone https://github.com/truera/trulens.git [Locall installation] Install the trulens repo. cd trulens pip install -e .","title":"Getting access to TruLens"},{"location":"quickstart/","text":"Quickstart \u00b6 Playground \u00b6 To quickly play around with the TruLens library, check out the following CoLab notebooks: PyTorch: Tensorflow 2 / Keras: Install & Use \u00b6 Check out the Installation instructions for information on how to install the library, use it, and contribute.","title":"Quickstart"},{"location":"quickstart/#quickstart","text":"","title":"Quickstart"},{"location":"quickstart/#playground","text":"To quickly play around with the TruLens library, check out the following CoLab notebooks: PyTorch: Tensorflow 2 / Keras:","title":"Playground"},{"location":"quickstart/#install-use","text":"Check out the Installation instructions for information on how to install the library, use it, and contribute.","title":"Install &amp; Use"},{"location":"welcome/","text":"Welcome to TruLens! \u00b6 TruLens is a cross-framework library for deep learning explainability. It provides a uniform abstraction over a number of different frameworks. It provides a uniform abstraction layer over TensorFlow, Pytorch, and Keras and allows input and internal explanations. This paper is an introduction to the theoretical foundations of the library. We\u2019ve been using TruLens at TruEra across a wide range of real-world use cases to explain deep learning models ranging from time-series RNNs to image and NLP models, and wanted to share the awesomeness with the world. Documentation Quick Usage \u00b6 To quickly play around with the TruLens library, check out the following CoLab notebooks: PyTorch: Tensorflow 2 / Keras: NLP with PyTorch: NLP with Tensorflow 2 / Keras: Installation \u00b6 These installation instructions assume that you have conda installed and added to your path. Create a virtual environment (or modify an existing one). conda create -n \"<my_name>\" python=3.7 # Skip if using existing environment. conda activate <my_name> Install dependencies. conda install tensorflow-gpu=1 # Or whatever backend you're using. conda install keras # Or whatever backend you're using. conda install matplotlib # For visualizations. Install the trulens package pip install trulens Overview \u00b6 Attributions \u00b6 Model Wrappers \u00b6 In order to support a wide variety of backends with different interfaces for their respective models, TruLens uses its own ModelWrapper class which provides a general model interface to simplify the implementation of the API functions. To get the model wrapper, use the get_model_wrapper method in trulens.nn.models . A model wrapper class exists for each backend that converts a model in the respective backend's format to the general TruLens ModelWrapper interface. The wrappers are found in the models module, and any model defined using Keras, Pytorch, or Tensorflow should be wrapped before being used with the other API functions that require a model -- all other TruLens functionalities expect models to be an instance of trulens.nn.models.ModelWrapper . For example, from trulens.nn.models import get_model_wrapper wrapped_model = get_model_wrapper ( model_defined_via_keras ) Attribution Methods \u00b6 Attribution methods, in the most general sense, allow us to quantify the contribution of particular variables in a model towards a particular behavior of the model. In many cases, for example, this may simply measure the effect each input variable has on the output of the network. Attribution methods extend the AttributionMethod class, and many concrete instances are found in the trulens.nn.attribution module. Once an attribution method has been instantiated, its main function is its attributions method, which returns an np.Array of batched items, where each item matches the shape of the input to the model the attribution method was instantiated with. See the method comparison demo for further information on the different types of attribution methods, their uses, and their relationships with one another. Slices, Quantities, and Distributions \u00b6 In order to obtain a high degree of flexibility in the types of attributions that can be produced, we implement Internal Influence , which is parameterized by a slice , quantity of interest , and distribution of interest , explained below. The slice essentially defines a layer to use for internal attributions. The slice for the InternalInfluence method can be specified by an instance of the Slice class in the trulens.nn.slices module. A Slice object specifies two layers: (1) the layer of the variables that we are calculating attribution for (e.g., the input layer), and (2) the layer whose output defines our quantity of interest (e.g., the output layer, see below for more on quantities of interest). The quantity of interest (QoI) essentially defines the model behavior we would like to explain using attributions. The QoI is a function of the model's output at some layer. For example, it may select the confidence score for a particular class. In its most general form, the QoI can be pecified by an implementation of the QoI class in the trulens.nn.quantities module. Several common default implementations are provided in this module as well. The distribution of interest (DoI) essentially specifies for which points surrounding each record the calculated attribution should be valid. The distribution can be specified via an implementation of the DoI class in the trulens.nn.distributions module, which is a function taking an input record and producing a list of sample input points to aggregate attribution over. A few common default distributions implementing the DoI class can be found in the trulens.nn.distributions module. See Attributions for Different Use Cases for further explanations of the purpose of these parameters and examples of their usage. Visualizations \u00b6 In order to interpret the attributions produced by an AttributionMethod , a few useful visualizers are provided in the trulens.visualizations module. While the interface of each visualizer varies slightly, in general, the visualizers are a function taking an np.Array representing the attributions returned from an AttributionMethod and producing an image that can be used to interpret the attributions. Contact Us \u00b6 To communicate with other trulens developers, join our Slack ! Citation \u00b6 To cite this repository: curl -LH \"Accept: application/x-bibtex\" https://doi.org/10.5281/zenodo.4495856","title":"Welcome to TruLens!"},{"location":"welcome/#welcome-to-trulens","text":"TruLens is a cross-framework library for deep learning explainability. It provides a uniform abstraction over a number of different frameworks. It provides a uniform abstraction layer over TensorFlow, Pytorch, and Keras and allows input and internal explanations. This paper is an introduction to the theoretical foundations of the library. We\u2019ve been using TruLens at TruEra across a wide range of real-world use cases to explain deep learning models ranging from time-series RNNs to image and NLP models, and wanted to share the awesomeness with the world. Documentation","title":"Welcome to TruLens!"},{"location":"welcome/#quick-usage","text":"To quickly play around with the TruLens library, check out the following CoLab notebooks: PyTorch: Tensorflow 2 / Keras: NLP with PyTorch: NLP with Tensorflow 2 / Keras:","title":"Quick Usage"},{"location":"welcome/#installation","text":"These installation instructions assume that you have conda installed and added to your path. Create a virtual environment (or modify an existing one). conda create -n \"<my_name>\" python=3.7 # Skip if using existing environment. conda activate <my_name> Install dependencies. conda install tensorflow-gpu=1 # Or whatever backend you're using. conda install keras # Or whatever backend you're using. conda install matplotlib # For visualizations. Install the trulens package pip install trulens","title":"Installation"},{"location":"welcome/#overview","text":"","title":"Overview"},{"location":"welcome/#attributions","text":"","title":"Attributions"},{"location":"welcome/#model-wrappers","text":"In order to support a wide variety of backends with different interfaces for their respective models, TruLens uses its own ModelWrapper class which provides a general model interface to simplify the implementation of the API functions. To get the model wrapper, use the get_model_wrapper method in trulens.nn.models . A model wrapper class exists for each backend that converts a model in the respective backend's format to the general TruLens ModelWrapper interface. The wrappers are found in the models module, and any model defined using Keras, Pytorch, or Tensorflow should be wrapped before being used with the other API functions that require a model -- all other TruLens functionalities expect models to be an instance of trulens.nn.models.ModelWrapper . For example, from trulens.nn.models import get_model_wrapper wrapped_model = get_model_wrapper ( model_defined_via_keras )","title":"Model Wrappers"},{"location":"welcome/#attribution-methods","text":"Attribution methods, in the most general sense, allow us to quantify the contribution of particular variables in a model towards a particular behavior of the model. In many cases, for example, this may simply measure the effect each input variable has on the output of the network. Attribution methods extend the AttributionMethod class, and many concrete instances are found in the trulens.nn.attribution module. Once an attribution method has been instantiated, its main function is its attributions method, which returns an np.Array of batched items, where each item matches the shape of the input to the model the attribution method was instantiated with. See the method comparison demo for further information on the different types of attribution methods, their uses, and their relationships with one another.","title":"Attribution Methods"},{"location":"welcome/#slices-quantities-and-distributions","text":"In order to obtain a high degree of flexibility in the types of attributions that can be produced, we implement Internal Influence , which is parameterized by a slice , quantity of interest , and distribution of interest , explained below. The slice essentially defines a layer to use for internal attributions. The slice for the InternalInfluence method can be specified by an instance of the Slice class in the trulens.nn.slices module. A Slice object specifies two layers: (1) the layer of the variables that we are calculating attribution for (e.g., the input layer), and (2) the layer whose output defines our quantity of interest (e.g., the output layer, see below for more on quantities of interest). The quantity of interest (QoI) essentially defines the model behavior we would like to explain using attributions. The QoI is a function of the model's output at some layer. For example, it may select the confidence score for a particular class. In its most general form, the QoI can be pecified by an implementation of the QoI class in the trulens.nn.quantities module. Several common default implementations are provided in this module as well. The distribution of interest (DoI) essentially specifies for which points surrounding each record the calculated attribution should be valid. The distribution can be specified via an implementation of the DoI class in the trulens.nn.distributions module, which is a function taking an input record and producing a list of sample input points to aggregate attribution over. A few common default distributions implementing the DoI class can be found in the trulens.nn.distributions module. See Attributions for Different Use Cases for further explanations of the purpose of these parameters and examples of their usage.","title":"Slices, Quantities, and Distributions"},{"location":"welcome/#visualizations","text":"In order to interpret the attributions produced by an AttributionMethod , a few useful visualizers are provided in the trulens.visualizations module. While the interface of each visualizer varies slightly, in general, the visualizers are a function taking an np.Array representing the attributions returned from an AttributionMethod and producing an image that can be used to interpret the attributions.","title":"Visualizations"},{"location":"welcome/#contact-us","text":"To communicate with other trulens developers, join our Slack !","title":"Contact Us"},{"location":"welcome/#citation","text":"To cite this repository: curl -LH \"Accept: application/x-bibtex\" https://doi.org/10.5281/zenodo.4495856","title":"Citation"},{"location":"api/attribution/","text":"Attribution Methods \u00b6 Attribution methods quantitatively measure the contribution of each of a function's individual inputs to its output. Gradient-based attribution methods compute the gradient of a model with respect to its inputs to describe how important each input is towards the output prediction. These methods can be applied to assist in explaining deep networks. TruLens provides implementations of several such techniques, found in this package. AttributionMethod ( ABC ) \u00b6 Interface used by all attribution methods. An attribution method takes a neural network model and provides the ability to assign values to the variables of the network that specify the importance of each variable towards particular predictions. Source code in trulens/nn/attribution.py class AttributionMethod ( AbstractBaseClass ): \"\"\" Interface used by all attribution methods. An attribution method takes a neural network model and provides the ability to assign values to the variables of the network that specify the importance of each variable towards particular predictions. \"\"\" @abstractmethod def __init__ ( self , model : ModelWrapper , rebatch_size : int = None , * args , ** kwargs ): \"\"\" Abstract constructor. Parameters: model: ModelWrapper Model for which attributions are calculated. rebatch_size: int (optional) Will rebatch instances to this size if given. This may be required for GPU usage if using a DoI which produces multiple instances per user-provided instance. Many valued DoIs will expand the tensors sent to each layer to original_batch_size * doi_size. The rebatch size will break up original_batch_size * doi_size into rebatch_size chunks to send to model. \"\"\" self . _model = model self . rebatch_size = rebatch_size @property def model ( self ) -> ModelWrapper : \"\"\" Model for which attributions are calculated. \"\"\" return self . _model @abstractmethod def _attributions ( self , model_inputs : ModelInputs ) -> AttributionResult : \"\"\" For attributions that have options to return multiple things depending on configuration, wrap those multiple things in the AttributionResult tuple. \"\"\" ... def attributions ( self , * model_args : ArgsLike , ** model_kwargs : KwargsLike ) -> Union [ TensorLike , ArgsLike [ TensorLike ], ArgsLike [ ArgsLike [ TensorLike ]]]: \"\"\" Returns attributions for the given input. Attributions are in the same shape as the layer that attributions are being generated for. The numeric scale of the attributions will depend on the specific implementations of the Distribution of Interest and Quantity of Interest. However it is generally related to the scale of gradients on the Quantity of Interest. For example, Integrated Gradients uses the linear interpolation Distribution of Interest which subsumes the completeness axiom which ensures the sum of all attributions of a record equals the output determined by the Quantity of Interest on the same record. The Point Distribution of Interest will be determined by the gradient at a single point, thus being a good measure of model sensitivity. Parameters: model_args: ArgsLike, model_kwargs: KwargsLike The args and kwargs given to the call method of a model. This should represent the records to obtain attributions for, assumed to be a *batched* input. if `self.model` supports evaluation on *data tensors*, the appropriate tensor type may be used (e.g., Pytorch models may accept Pytorch tensors in addition to `np.ndarray`s). The shape of the inputs must match the input shape of `self.model`. Returns - np.ndarray when single attribution_cut input, single qoi output - or ArgsLike[np.ndarray] when single input, multiple output (or vice versa) - or ArgsLike[ArgsLike[np.ndarray]] when multiple output (outer), multiple input (inner) An array of attributions, matching the shape and type of `from_cut` of the slice. Each entry in the returned array represents the degree to which the corresponding feature affected the model's outcome on the corresponding point. If attributing to a component with multiple inputs, a list for each will be returned. If the quantity of interest features multiple outputs, a list for each will be returned. \"\"\" # Calls like: attributions([arg1, arg2]) will get read as model_args = # ([arg1, arg2],), that is, a tuple with a single element containing the # model args. Test below checks for this. TODO: Disallow such # invocations? They should be given as attributions(arg1, arg2). if isinstance ( model_args , tuple ) and len ( model_args ) == 1 and isinstance ( model_args [ 0 ], DATA_CONTAINER_TYPE ): model_args = model_args [ 0 ] model_inputs = ModelInputs ( args = many_of_om ( model_args ), kwargs = model_kwargs ) # Will cast results to this data container type. return_type = type ( model_inputs . first ()) pieces = self . _attributions ( model_inputs ) # Format attributions into the public structure which throws out output # lists and input lists if there is only one output or only one input. # Also cast to whatever the input type was. attributions : Outputs [ Inputs [ np . ndarray ]] = nested_cast ( backend = get_backend (), astype = return_type , args = pieces . attributions ) attributions : Outputs [ OM [ Inputs , np . ndarray ] ] = [ om_of_many ( attr ) for attr in attributions ] attributions : OM [ Outputs , OM [ Inputs , np . ndarray ]] = om_of_many ( attributions ) if pieces . gradients is not None or pieces . interventions is not None : tru_logger . warning ( \"AttributionMethod configured to return gradients or interventions. \" \"Use the internal _attribution call to retrieve those.\" ) return attributions model : ModelWrapper property readonly \u00b6 Model for which attributions are calculated. __init__ ( self , model , rebatch_size = None , * args , ** kwargs ) special \u00b6 Abstract constructor. Parameters: Name Type Description Default model ModelWrapper ModelWrapper Model for which attributions are calculated. required rebatch_size int int (optional) Will rebatch instances to this size if given. This may be required for GPU usage if using a DoI which produces multiple instances per user-provided instance. Many valued DoIs will expand the tensors sent to each layer to original_batch_size * doi_size. The rebatch size will break up original_batch_size * doi_size into rebatch_size chunks to send to model. None Source code in trulens/nn/attribution.py @abstractmethod def __init__ ( self , model : ModelWrapper , rebatch_size : int = None , * args , ** kwargs ): \"\"\" Abstract constructor. Parameters: model: ModelWrapper Model for which attributions are calculated. rebatch_size: int (optional) Will rebatch instances to this size if given. This may be required for GPU usage if using a DoI which produces multiple instances per user-provided instance. Many valued DoIs will expand the tensors sent to each layer to original_batch_size * doi_size. The rebatch size will break up original_batch_size * doi_size into rebatch_size chunks to send to model. \"\"\" self . _model = model self . rebatch_size = rebatch_size attributions ( self , * model_args , ** model_kwargs ) \u00b6 Returns attributions for the given input. Attributions are in the same shape as the layer that attributions are being generated for. The numeric scale of the attributions will depend on the specific implementations of the Distribution of Interest and Quantity of Interest. However it is generally related to the scale of gradients on the Quantity of Interest. For example, Integrated Gradients uses the linear interpolation Distribution of Interest which subsumes the completeness axiom which ensures the sum of all attributions of a record equals the output determined by the Quantity of Interest on the same record. The Point Distribution of Interest will be determined by the gradient at a single point, thus being a good measure of model sensitivity. Parameters: Name Type Description Default model_args ArgsLike ArgsLike, model_kwargs: KwargsLike The args and kwargs given to the call method of a model. This should represent the records to obtain attributions for, assumed to be a batched input. if self.model supports evaluation on data tensors , the appropriate tensor type may be used (e.g., Pytorch models may accept Pytorch tensors in addition to np.ndarray s). The shape of the inputs must match the input shape of self.model . () Returns - np.ndarray when single attribution_cut input, single qoi output - or ArgsLike[np.ndarray] when single input, multiple output (or vice versa) - or ArgsLike[ArgsLike[np.ndarray]] when multiple output (outer), multiple input (inner) An array of attributions, matching the shape and type of `from_cut` of the slice. Each entry in the returned array represents the degree to which the corresponding feature affected the model's outcome on the corresponding point. If attributing to a component with multiple inputs, a list for each will be returned. If the quantity of interest features multiple outputs, a list for each will be returned. Source code in trulens/nn/attribution.py def attributions ( self , * model_args : ArgsLike , ** model_kwargs : KwargsLike ) -> Union [ TensorLike , ArgsLike [ TensorLike ], ArgsLike [ ArgsLike [ TensorLike ]]]: \"\"\" Returns attributions for the given input. Attributions are in the same shape as the layer that attributions are being generated for. The numeric scale of the attributions will depend on the specific implementations of the Distribution of Interest and Quantity of Interest. However it is generally related to the scale of gradients on the Quantity of Interest. For example, Integrated Gradients uses the linear interpolation Distribution of Interest which subsumes the completeness axiom which ensures the sum of all attributions of a record equals the output determined by the Quantity of Interest on the same record. The Point Distribution of Interest will be determined by the gradient at a single point, thus being a good measure of model sensitivity. Parameters: model_args: ArgsLike, model_kwargs: KwargsLike The args and kwargs given to the call method of a model. This should represent the records to obtain attributions for, assumed to be a *batched* input. if `self.model` supports evaluation on *data tensors*, the appropriate tensor type may be used (e.g., Pytorch models may accept Pytorch tensors in addition to `np.ndarray`s). The shape of the inputs must match the input shape of `self.model`. Returns - np.ndarray when single attribution_cut input, single qoi output - or ArgsLike[np.ndarray] when single input, multiple output (or vice versa) - or ArgsLike[ArgsLike[np.ndarray]] when multiple output (outer), multiple input (inner) An array of attributions, matching the shape and type of `from_cut` of the slice. Each entry in the returned array represents the degree to which the corresponding feature affected the model's outcome on the corresponding point. If attributing to a component with multiple inputs, a list for each will be returned. If the quantity of interest features multiple outputs, a list for each will be returned. \"\"\" # Calls like: attributions([arg1, arg2]) will get read as model_args = # ([arg1, arg2],), that is, a tuple with a single element containing the # model args. Test below checks for this. TODO: Disallow such # invocations? They should be given as attributions(arg1, arg2). if isinstance ( model_args , tuple ) and len ( model_args ) == 1 and isinstance ( model_args [ 0 ], DATA_CONTAINER_TYPE ): model_args = model_args [ 0 ] model_inputs = ModelInputs ( args = many_of_om ( model_args ), kwargs = model_kwargs ) # Will cast results to this data container type. return_type = type ( model_inputs . first ()) pieces = self . _attributions ( model_inputs ) # Format attributions into the public structure which throws out output # lists and input lists if there is only one output or only one input. # Also cast to whatever the input type was. attributions : Outputs [ Inputs [ np . ndarray ]] = nested_cast ( backend = get_backend (), astype = return_type , args = pieces . attributions ) attributions : Outputs [ OM [ Inputs , np . ndarray ] ] = [ om_of_many ( attr ) for attr in attributions ] attributions : OM [ Outputs , OM [ Inputs , np . ndarray ]] = om_of_many ( attributions ) if pieces . gradients is not None or pieces . interventions is not None : tru_logger . warning ( \"AttributionMethod configured to return gradients or interventions. \" \"Use the internal _attribution call to retrieve those.\" ) return attributions AttributionResult dataclass \u00b6 _attribution method output container. Source code in trulens/nn/attribution.py @dataclass class AttributionResult : \"\"\" _attribution method output container. \"\"\" attributions : Outputs [ Inputs [ TensorLike ]] = None gradients : Outputs [ Inputs [ Uniform [ TensorLike ]]] = None interventions : Inputs [ Uniform [ TensorLike ]] = None InputAttribution ( InternalInfluence ) \u00b6 Attributions of input features on either internal or output quantities. This is essentially an alias for InternalInfluence ( model , ( trulens . nn . slices . InputCut (), cut ), qoi , doi , multiply_activation ) Source code in trulens/nn/attribution.py class InputAttribution ( InternalInfluence ): \"\"\" Attributions of input features on either internal or output quantities. This is essentially an alias for ```python InternalInfluence( model, (trulens.nn.slices.InputCut(), cut), qoi, doi, multiply_activation) ``` \"\"\" def __init__ ( self , model : ModelWrapper , qoi_cut : CutLike = None , # see WARNING-LOAD-INIT qoi : QoiLike = 'max' , doi_cut : CutLike = None , # see WARNING-LOAD-INIT doi : DoiLike = 'point' , multiply_activation : bool = True , * args , ** kwargs ): \"\"\" Parameters: model : Model for which attributions are calculated. qoi_cut : The cut determining the layer from which the QoI is derived. Expects a `Cut` object, or a related type that can be interpreted as a `Cut`, as documented below. If an `int` is given, it represents the index of a layer in `model`. If a `str` is given, it represents the name of a layer in `model`. `None` is an alternative for `slices.OutputCut()`. qoi : quantities.QoI | int | tuple | str Quantity of interest to attribute. Expects a `QoI` object, or a related type that can be interpreted as a `QoI`, as documented below. If an `int` is given, the quantity of interest is taken to be the slice output for the class/neuron/channel specified by the given integer, i.e., ```python quantities.InternalChannelQoI(qoi) ``` If a tuple or list of two integers is given, then the quantity of interest is taken to be the comparative quantity for the class given by the first integer against the class given by the second integer, i.e., ```python quantities.ComparativeQoI(*qoi) ``` If a callable is given, it is interpreted as a function representing the QoI, i.e., ```python quantities.LambdaQoI(qoi) ``` If the string, `'max'`, is given, the quantity of interest is taken to be the output for the class with the maximum score, i.e., ```python quantities.MaxClassQoI() ``` doi_cut : For models which have non-differentiable pre-processing at the start of the model, specify the cut of the initial differentiable input form. For NLP models, for example, this could point to the embedding layer. If not provided, InputCut is assumed. doi : distributions.DoI | str Distribution of interest over inputs. Expects a `DoI` object, or a related type that can be interpreted as a `DoI`, as documented below. If the string, `'point'`, is given, the distribution is taken to be the single point passed to `attributions`, i.e., ```python distributions.PointDoi() ``` If the string, `'linear'`, is given, the distribution is taken to be the linear interpolation from the zero input to the point passed to `attributions`, i.e., ```python distributions.LinearDoi() ``` multiply_activation : bool, optional Whether to multiply the gradient result by its corresponding activation, thus converting from \"*influence space*\" to \"*attribution space*.\" \"\"\" if doi_cut is None : # WARNING-LOAD-INIT: Do not put this as a default arg in the def # line. That would cause an instantiation of InputCut when this # class is loaded and before it is used. Because get_backend gets # called in Cut.__init__, it may fail if this class is loaded before # trulens.nn.models.get_model_wrapper is called on some model. doi_cut = InputCut () super () . __init__ ( model , ( doi_cut , qoi_cut ), qoi , doi , multiply_activation = multiply_activation , * args , ** kwargs ) __init__ ( self , model , qoi_cut = None , qoi = 'max' , doi_cut = None , doi = 'point' , multiply_activation = True , * args , ** kwargs ) special \u00b6 Parameters: Name Type Description Default model Model for which attributions are calculated. required qoi_cut The cut determining the layer from which the QoI is derived. Expects a Cut object, or a related type that can be interpreted as a Cut , as documented below. If an int is given, it represents the index of a layer in model . If a str is given, it represents the name of a layer in model . None is an alternative for slices.OutputCut() . None qoi quantities.QoI | int | tuple | str Quantity of interest to attribute. Expects a QoI object, or a related type that can be interpreted as a QoI , as documented below. If an int is given, the quantity of interest is taken to be the slice output for the class/neuron/channel specified by the given integer, i.e., python quantities.InternalChannelQoI(qoi) If a tuple or list of two integers is given, then the quantity of interest is taken to be the comparative quantity for the class given by the first integer against the class given by the second integer, i.e., ```python quantities.ComparativeQoI(*qoi) If a callable is given, it is interpreted as a function representing the QoI, i.e., ```python quantities.LambdaQoI(qoi) If the string, 'max' , is given, the quantity of interest is taken to be the output for the class with the maximum score, i.e., python quantities.MaxClassQoI() 'max' doi_cut For models which have non-differentiable pre-processing at the start of the model, specify the cut of the initial differentiable input form. For NLP models, for example, this could point to the embedding layer. If not provided, InputCut is assumed. None doi distributions.DoI | str Distribution of interest over inputs. Expects a DoI object, or a related type that can be interpreted as a DoI , as documented below. If the string, 'point' , is given, the distribution is taken to be the single point passed to attributions , i.e., python distributions.PointDoi() If the string, 'linear' , is given, the distribution is taken to be the linear interpolation from the zero input to the point passed to attributions , i.e., python distributions.LinearDoi() 'point' multiply_activation bool, optional Whether to multiply the gradient result by its corresponding activation, thus converting from \" influence space \" to \" attribution space .\" True Source code in trulens/nn/attribution.py def __init__ ( self , model : ModelWrapper , qoi_cut : CutLike = None , # see WARNING-LOAD-INIT qoi : QoiLike = 'max' , doi_cut : CutLike = None , # see WARNING-LOAD-INIT doi : DoiLike = 'point' , multiply_activation : bool = True , * args , ** kwargs ): \"\"\" Parameters: model : Model for which attributions are calculated. qoi_cut : The cut determining the layer from which the QoI is derived. Expects a `Cut` object, or a related type that can be interpreted as a `Cut`, as documented below. If an `int` is given, it represents the index of a layer in `model`. If a `str` is given, it represents the name of a layer in `model`. `None` is an alternative for `slices.OutputCut()`. qoi : quantities.QoI | int | tuple | str Quantity of interest to attribute. Expects a `QoI` object, or a related type that can be interpreted as a `QoI`, as documented below. If an `int` is given, the quantity of interest is taken to be the slice output for the class/neuron/channel specified by the given integer, i.e., ```python quantities.InternalChannelQoI(qoi) ``` If a tuple or list of two integers is given, then the quantity of interest is taken to be the comparative quantity for the class given by the first integer against the class given by the second integer, i.e., ```python quantities.ComparativeQoI(*qoi) ``` If a callable is given, it is interpreted as a function representing the QoI, i.e., ```python quantities.LambdaQoI(qoi) ``` If the string, `'max'`, is given, the quantity of interest is taken to be the output for the class with the maximum score, i.e., ```python quantities.MaxClassQoI() ``` doi_cut : For models which have non-differentiable pre-processing at the start of the model, specify the cut of the initial differentiable input form. For NLP models, for example, this could point to the embedding layer. If not provided, InputCut is assumed. doi : distributions.DoI | str Distribution of interest over inputs. Expects a `DoI` object, or a related type that can be interpreted as a `DoI`, as documented below. If the string, `'point'`, is given, the distribution is taken to be the single point passed to `attributions`, i.e., ```python distributions.PointDoi() ``` If the string, `'linear'`, is given, the distribution is taken to be the linear interpolation from the zero input to the point passed to `attributions`, i.e., ```python distributions.LinearDoi() ``` multiply_activation : bool, optional Whether to multiply the gradient result by its corresponding activation, thus converting from \"*influence space*\" to \"*attribution space*.\" \"\"\" if doi_cut is None : # WARNING-LOAD-INIT: Do not put this as a default arg in the def # line. That would cause an instantiation of InputCut when this # class is loaded and before it is used. Because get_backend gets # called in Cut.__init__, it may fail if this class is loaded before # trulens.nn.models.get_model_wrapper is called on some model. doi_cut = InputCut () super () . __init__ ( model , ( doi_cut , qoi_cut ), qoi , doi , multiply_activation = multiply_activation , * args , ** kwargs ) IntegratedGradients ( InputAttribution ) \u00b6 Implementation for the Integrated Gradients method from the following paper: Axiomatic Attribution for Deep Networks This should be cited using: @INPROCEEDINGS { sundararajan17axiomatic , author = {Mukund Sundararajan and Ankur Taly, and Qiqi Yan} , title = {Axiomatic Attribution for Deep Networks} , booktitle = {International Conference on Machine Learning (ICML)} , year = {2017} , } This is essentially an alias for InternalInfluence ( model , ( trulens . nn . slices . InputCut (), trulens . nn . slices . OutputCut ()), 'max' , trulens . nn . distributions . LinearDoi ( baseline , resolution ), multiply_activation = True ) Source code in trulens/nn/attribution.py class IntegratedGradients ( InputAttribution ): \"\"\" Implementation for the Integrated Gradients method from the following paper: [Axiomatic Attribution for Deep Networks]( https://arxiv.org/pdf/1703.01365) This should be cited using: ```bibtex @INPROCEEDINGS{ sundararajan17axiomatic, author={Mukund Sundararajan and Ankur Taly, and Qiqi Yan}, title={Axiomatic Attribution for Deep Networks}, booktitle={International Conference on Machine Learning (ICML)}, year={2017}, } ``` This is essentially an alias for ```python InternalInfluence( model, (trulens.nn.slices.InputCut(), trulens.nn.slices.OutputCut()), 'max', trulens.nn.distributions.LinearDoi(baseline, resolution), multiply_activation=True) ``` \"\"\" def __init__ ( self , model : ModelWrapper , baseline = None , resolution : int = 50 , doi_cut = None , # see WARNING-LOAD-INIT qoi = 'max' , qoi_cut = None , # see WARNING-LOAD-INIT * args , ** kwargs ): \"\"\" Parameters: model: Model for which attributions are calculated. baseline: The baseline to interpolate from. Must be same shape as the input. If `None` is given, the zero vector in the appropriate shape will be used. resolution: Number of points to use in the approximation. A higher resolution is more computationally expensive, but gives a better approximation of the mathematical formula this attribution method represents. \"\"\" if doi_cut is None : doi_cut = InputCut () if qoi_cut is None : qoi_cut = OutputCut () super () . __init__ ( model = model , qoi_cut = qoi_cut , qoi = qoi , doi_cut = doi_cut , doi = LinearDoi ( baseline , resolution , cut = doi_cut ), multiply_activation = True , * args , ** kwargs ) __init__ ( self , model , baseline = None , resolution = 50 , doi_cut = None , qoi = 'max' , qoi_cut = None , * args , ** kwargs ) special \u00b6 Parameters: Name Type Description Default model ModelWrapper Model for which attributions are calculated. required baseline The baseline to interpolate from. Must be same shape as the input. If None is given, the zero vector in the appropriate shape will be used. None resolution int Number of points to use in the approximation. A higher resolution is more computationally expensive, but gives a better approximation of the mathematical formula this attribution method represents. 50 Source code in trulens/nn/attribution.py def __init__ ( self , model : ModelWrapper , baseline = None , resolution : int = 50 , doi_cut = None , # see WARNING-LOAD-INIT qoi = 'max' , qoi_cut = None , # see WARNING-LOAD-INIT * args , ** kwargs ): \"\"\" Parameters: model: Model for which attributions are calculated. baseline: The baseline to interpolate from. Must be same shape as the input. If `None` is given, the zero vector in the appropriate shape will be used. resolution: Number of points to use in the approximation. A higher resolution is more computationally expensive, but gives a better approximation of the mathematical formula this attribution method represents. \"\"\" if doi_cut is None : doi_cut = InputCut () if qoi_cut is None : qoi_cut = OutputCut () super () . __init__ ( model = model , qoi_cut = qoi_cut , qoi = qoi , doi_cut = doi_cut , doi = LinearDoi ( baseline , resolution , cut = doi_cut ), multiply_activation = True , * args , ** kwargs ) InternalInfluence ( AttributionMethod ) \u00b6 Internal attributions parameterized by a slice, quantity of interest, and distribution of interest. The slice specifies the layers at which the internals of the model are to be exposed; it is represented by two cuts , which specify the layer the attributions are assigned to and the layer from which the quantity of interest is derived. The Quantity of Interest (QoI) is a function of the output specified by the slice that determines the network output behavior that the attributions are to describe. The Distribution of Interest (DoI) specifies the records over which the attributions are aggregated. More information can be found in the following paper: Influence-Directed Explanations for Deep Convolutional Networks This should be cited using: @INPROCEEDINGS { leino18influence , author = { Klas Leino and Shayak Sen and Anupam Datta and Matt Fredrikson and Linyi Li} , title = { Influence-Directed Explanations for Deep Convolutional Networks} , booktitle = {IEEE International Test Conference (ITC)} , year = {2018} , } Source code in trulens/nn/attribution.py class InternalInfluence ( AttributionMethod ): \"\"\"Internal attributions parameterized by a slice, quantity of interest, and distribution of interest. The *slice* specifies the layers at which the internals of the model are to be exposed; it is represented by two *cuts*, which specify the layer the attributions are assigned to and the layer from which the quantity of interest is derived. The *Quantity of Interest* (QoI) is a function of the output specified by the slice that determines the network output behavior that the attributions are to describe. The *Distribution of Interest* (DoI) specifies the records over which the attributions are aggregated. More information can be found in the following paper: [Influence-Directed Explanations for Deep Convolutional Networks]( https://arxiv.org/pdf/1802.03788.pdf) This should be cited using: ```bibtex @INPROCEEDINGS{ leino18influence, author={ Klas Leino and Shayak Sen and Anupam Datta and Matt Fredrikson and Linyi Li}, title={ Influence-Directed Explanations for Deep Convolutional Networks}, booktitle={IEEE International Test Conference (ITC)}, year={2018}, } ``` \"\"\" def __init__ ( self , model : ModelWrapper , cuts : SliceLike , qoi : QoiLike , doi : DoiLike , multiply_activation : bool = True , return_grads : bool = False , return_doi : bool = False , * args , ** kwargs ): \"\"\" Parameters: model: Model for which attributions are calculated. cuts: The slice to use when computing the attributions. The slice keeps track of the layer whose output attributions are calculated and the layer for which the quantity of interest is computed. Expects a `Slice` object, or a related type that can be interpreted as a `Slice`, as documented below. If a single `Cut` object is given, it is assumed to be the cut representing the layer for which attributions are calculated (i.e., `from_cut` in `Slice`) and the layer for the quantity of interest (i.e., `to_cut` in `slices.Slice`) is taken to be the output of the network. If a tuple or list of two `Cut`s is given, they are assumed to be `from_cut` and `to_cut`, respectively. A cut (or the cuts within the tuple) can also be represented as an `int`, `str`, or `None`. If an `int` is given, it represents the index of a layer in `model`. If a `str` is given, it represents the name of a layer in `model`. `None` is an alternative for `slices.InputCut`. qoi: Quantity of interest to attribute. Expects a `QoI` object, or a related type that can be interpreted as a `QoI`, as documented below. If an `int` is given, the quantity of interest is taken to be the slice output for the class/neuron/channel specified by the given integer, i.e., ```python quantities.InternalChannelQoI(qoi) ``` If a tuple or list of two integers is given, then the quantity of interest is taken to be the comparative quantity for the class given by the first integer against the class given by the second integer, i.e., ```python quantities.ComparativeQoI(*qoi) ``` If a callable is given, it is interpreted as a function representing the QoI, i.e., ```python quantities.LambdaQoI(qoi) ``` If the string, `'max'`, is given, the quantity of interest is taken to be the output for the class with the maximum score, i.e., ```python quantities.MaxClassQoI() ``` doi: Distribution of interest over inputs. Expects a `DoI` object, or a related type that can be interpreted as a `DoI`, as documented below. If the string, `'point'`, is given, the distribution is taken to be the single point passed to `attributions`, i.e., ```python distributions.PointDoi() ``` If the string, `'linear'`, is given, the distribution is taken to be the linear interpolation from the zero input to the point passed to `attributions`, i.e., ```python distributions.LinearDoi() ``` multiply_activation: Whether to multiply the gradient result by its corresponding activation, thus converting from \"*influence space*\" to \"*attribution space*.\" \"\"\" super () . __init__ ( model , * args , ** kwargs ) self . slice = InternalInfluence . __get_slice ( cuts ) self . qoi = InternalInfluence . __get_qoi ( qoi ) self . doi = InternalInfluence . __get_doi ( doi , cut = self . slice . from_cut ) self . _do_multiply = multiply_activation self . _return_grads = return_grads self . _return_doi = return_doi def _attributions ( self , model_inputs : ModelInputs ) -> AttributionResult : # NOTE: not symbolic B = get_backend () results = AttributionResult () # Create a message for out-of-memory errors regarding float and batch size. if len ( list ( model_inputs . values ())) == 0 : batch_size = 1 else : batch_size = model_inputs . first () . shape [ 0 ] param_msgs = [ f \"float size = { B . floatX_size } ( { B . floatX } ); consider changing to a smaller type.\" , f \"batch size = { batch_size } ; consider reducing the size of the batch you send to the attributions method.\" ] doi_cut = self . doi . cut () if self . doi . cut () else InputCut () with memory_suggestions ( * param_msgs ): # Handles out-of-memory messages. doi_val : List [ B . Tensor ] = self . model . _fprop ( model_inputs = model_inputs , to_cut = doi_cut , doi_cut = InputCut (), attribution_cut = None , # InputCut(), intervention = model_inputs )[ 0 ] doi_val = nested_map ( doi_val , B . as_array ) D = self . doi . _wrap_public_call ( doi_val , model_inputs = model_inputs ) if self . _return_doi : results . interventions = D # : Inputs[Uniform[TensorLike]] n_doi = len ( D [ 0 ]) D = self . __concatenate_doi ( D ) rebatch_size = self . rebatch_size if rebatch_size is None : rebatch_size = len ( D [ 0 ]) intervention = TensorArgs ( args = D ) model_inputs_expanded = tile ( what = model_inputs , onto = intervention ) # Create a message for out-of-memory errors regarding doi_size. # TODO: Generalize this message to doi other than LinearDoI: doi_size_msg = f \"distribution of interest size = { n_doi } ; consider reducing intervention resolution.\" combined_batch_size = n_doi * batch_size combined_batch_msg = f \"combined batch size = { combined_batch_size } ; consider reducing batch size, intervention size\" rebatch_size_msg = f \"rebatch_size = { rebatch_size } ; consider reducing this AttributionMethod constructor parameter (default is same as combined batch size).\" # Calculate the gradient of each of the points in the DoI. with memory_suggestions ( param_msgs + [ doi_size_msg , combined_batch_msg , rebatch_size_msg ] ): # Handles out-of-memory messages. qoi_grads_expanded : List [ Outputs [ Inputs [ TensorLike ]]] = [] for inputs_batch , intervention_batch in rebatch ( model_inputs_expanded , intervention , batch_size = rebatch_size ): qoi_grads_expanded_batch : Outputs [ Inputs [ TensorLike ]] = self . model . _qoi_bprop ( qoi = self . qoi , model_inputs = inputs_batch , attribution_cut = self . slice . from_cut , to_cut = self . slice . to_cut , intervention = intervention_batch , doi_cut = doi_cut ) # important to cast to numpy inside loop: qoi_grads_expanded . append ( nested_map ( qoi_grads_expanded_batch , B . as_array ) ) num_outputs = len ( qoi_grads_expanded [ 0 ]) num_inputs = len ( qoi_grads_expanded [ 0 ][ 0 ]) transpose = [ [[] for _ in range ( num_inputs )] for _ in range ( num_outputs ) ] for o in range ( num_outputs ): for i in range ( num_inputs ): for qoi_grads_batch in qoi_grads_expanded : transpose [ o ][ i ] . append ( qoi_grads_batch [ o ][ i ]) qoi_grads_expanded : Outputs [ Inputs [ np . ndarray ]] = nested_map ( transpose , np . concatenate , nest = 2 ) qoi_grads_expanded : Outputs [ Inputs [ np . ndarray ]] = nested_map ( qoi_grads_expanded , lambda grad : np . reshape ( grad , ( n_doi , - 1 ) + grad . shape [ 1 :]), nest = 2 ) if self . _return_grads : results . gradients = qoi_grads_expanded # : Outputs[Inputs[Uniform[TensorLike]]] # TODO: Does this need to be done in numpy? attrs : Outputs [ Inputs [ TensorLike ]] = nested_map ( qoi_grads_expanded , lambda grad : np . mean ( grad , axis = 0 ), nest = 2 ) # Multiply by the activation multiplier if specified. if self . _do_multiply : with memory_suggestions ( param_msgs ): z_val = self . model . _fprop ( model_inputs = model_inputs , doi_cut = InputCut (), attribution_cut = None , to_cut = self . slice . from_cut , intervention = model_inputs # intentional )[ 0 ] mults : Inputs [ TensorLike ] = self . doi . _wrap_public_get_activation_multiplier ( z_val , model_inputs = model_inputs ) mults : Inputs [ np . ndarray ] = nested_cast ( backend = B , args = mults , astype = np . ndarray ) attrs = [ [ att * mult for att , mult in zip ( attr , mults ) # Inputs ] for attr in attrs # Outputs ] results . attributions = attrs # : Outputs[Inputs[TensorLike]] return results @staticmethod def __get_qoi ( qoi_arg ): \"\"\" Helper function to get a `QoI` object from more user-friendly primitive arguments. \"\"\" # TODO(klas): we could potentially do some basic error catching here, # for example, making sure the index for a given channel is in range. if isinstance ( qoi_arg , QoI ): # We were already given a QoI, so return it. return qoi_arg elif callable ( qoi_arg ): # If we were given a callable, treat that function as a QoI. return LambdaQoI ( qoi_arg ) elif isinstance ( qoi_arg , int ): # If we receive an int, we take it to be the class/channel index # (whether it's a class or channel depends on the layer the quantity # is for, but `InternalChannelQoI` generalizes to both). return InternalChannelQoI ( qoi_arg ) elif isinstance ( qoi_arg , DATA_CONTAINER_TYPE ): # If we receive a DATA_CONTAINER_TYPE, we take it to be two classes # for which we are performing a comparative quantity of interest. if len ( qoi_arg ) == 2 : return ComparativeQoI ( * qoi_arg ) else : raise ValueError ( 'Tuple or list argument for `qoi` must have length 2' ) elif isinstance ( qoi_arg , str ): # We can specify `MaxClassQoI` via the string 'max'. if qoi_arg == 'max' : return MaxClassQoI () else : raise ValueError ( 'String argument for `qoi` must be one of the following: \\n ' ' - \"max\"' ) else : raise ValueError ( 'Unrecognized argument type for `qoi`' ) @staticmethod def __get_doi ( doi_arg , cut = None ): \"\"\" Helper function to get a `DoI` object from more user-friendly primitive arguments. \"\"\" if isinstance ( doi_arg , DoI ): # We were already given a DoI, so return it. return doi_arg elif isinstance ( doi_arg , str ): # We can specify `PointDoi` via the string 'point', or `LinearDoi` # via the string 'linear'. if doi_arg == 'point' : return PointDoi ( cut = cut ) elif doi_arg == 'linear' : return LinearDoi ( cut = cut ) else : raise ValueError ( 'String argument for `doi` must be one of the following: \\n ' ' - \"point\" \\n ' ' - \"linear\"' ) else : raise ValueError ( 'Unrecognized argument type for `doi`' ) @staticmethod def __get_slice ( slice_arg ): \"\"\" Helper function to get a `Slice` object from more user-friendly primitive arguments. \"\"\" if isinstance ( slice_arg , Slice ): # We are already given a Slice, so return it. return slice_arg elif ( isinstance ( slice_arg , Cut ) or isinstance ( slice_arg , int ) or isinstance ( slice_arg , str ) or slice_arg is None or slice_arg == 0 ): # If we receive a Cut, we take it to be the Cut of the start layer. return Slice ( InternalInfluence . __get_cut ( slice_arg ), OutputCut ()) elif isinstance ( slice_arg , DATA_CONTAINER_TYPE ): # If we receive a DATA_CONTAINER_TYPE, we take it to be the start # and end layer of the slice. if len ( slice_arg ) == 2 : if slice_arg [ 1 ] is None : return Slice ( InternalInfluence . __get_cut ( slice_arg [ 0 ]), OutputCut () ) else : return Slice ( InternalInfluence . __get_cut ( slice_arg [ 0 ]), InternalInfluence . __get_cut ( slice_arg [ 1 ]) ) else : raise ValueError ( 'Tuple or list argument for `cuts` must have length 2' ) else : raise ValueError ( 'Unrecognized argument type for `cuts`' ) @staticmethod def __get_cut ( cut_arg ): \"\"\" Helper function to get a `Cut` object from more user-friendly primitive arguments. \"\"\" if isinstance ( cut_arg , Cut ): # We are already given a Cut, so return it. return cut_arg elif cut_arg is None or cut_arg == 0 : # If we receive None or zero, we take it to be the input cut. return InputCut () # TODO(klas): may want a bit more validation here. elif isinstance ( cut_arg , int ) or isinstance ( cut_arg , str ): return Cut ( cut_arg ) else : raise ValueError ( 'Unrecognized argument type for cut' ) @staticmethod def __concatenate_doi ( D : Inputs [ Uniform [ TensorLike ]]) -> Inputs [ TensorLike ]: # Returns one TensorLike for each model input. if len ( D [ 0 ]) == 0 : raise ValueError ( 'Got empty distribution of interest. `DoI` must return at ' 'least one point.' ) # TODO: should this always be done in numpy or can we do it in backend? D = nested_cast ( backend = get_backend (), args = D , astype = np . ndarray ) return [ np . concatenate ( Di ) for Di in D ] __init__ ( self , model , cuts , qoi , doi , multiply_activation = True , return_grads = False , return_doi = False , * args , ** kwargs ) special \u00b6 Parameters: Name Type Description Default model ModelWrapper Model for which attributions are calculated. required cuts SliceLike The slice to use when computing the attributions. The slice keeps track of the layer whose output attributions are calculated and the layer for which the quantity of interest is computed. Expects a Slice object, or a related type that can be interpreted as a Slice , as documented below. If a single Cut object is given, it is assumed to be the cut representing the layer for which attributions are calculated (i.e., from_cut in Slice ) and the layer for the quantity of interest (i.e., to_cut in slices.Slice ) is taken to be the output of the network. If a tuple or list of two Cut s is given, they are assumed to be from_cut and to_cut , respectively. A cut (or the cuts within the tuple) can also be represented as an int , str , or None . If an int is given, it represents the index of a layer in model . If a str is given, it represents the name of a layer in model . None is an alternative for slices.InputCut . required qoi QoiLike Quantity of interest to attribute. Expects a QoI object, or a related type that can be interpreted as a QoI , as documented below. If an int is given, the quantity of interest is taken to be the slice output for the class/neuron/channel specified by the given integer, i.e., quantities . InternalChannelQoI ( qoi ) If a tuple or list of two integers is given, then the quantity of interest is taken to be the comparative quantity for the class given by the first integer against the class given by the second integer, i.e., quantities . ComparativeQoI ( * qoi ) If a callable is given, it is interpreted as a function representing the QoI, i.e., quantities . LambdaQoI ( qoi ) If the string, 'max' , is given, the quantity of interest is taken to be the output for the class with the maximum score, i.e., quantities . MaxClassQoI () required doi DoiLike Distribution of interest over inputs. Expects a DoI object, or a related type that can be interpreted as a DoI , as documented below. If the string, 'point' , is given, the distribution is taken to be the single point passed to attributions , i.e., distributions . PointDoi () If the string, 'linear' , is given, the distribution is taken to be the linear interpolation from the zero input to the point passed to attributions , i.e., distributions . LinearDoi () required multiply_activation bool Whether to multiply the gradient result by its corresponding activation, thus converting from \" influence space \" to \" attribution space .\" True Source code in trulens/nn/attribution.py def __init__ ( self , model : ModelWrapper , cuts : SliceLike , qoi : QoiLike , doi : DoiLike , multiply_activation : bool = True , return_grads : bool = False , return_doi : bool = False , * args , ** kwargs ): \"\"\" Parameters: model: Model for which attributions are calculated. cuts: The slice to use when computing the attributions. The slice keeps track of the layer whose output attributions are calculated and the layer for which the quantity of interest is computed. Expects a `Slice` object, or a related type that can be interpreted as a `Slice`, as documented below. If a single `Cut` object is given, it is assumed to be the cut representing the layer for which attributions are calculated (i.e., `from_cut` in `Slice`) and the layer for the quantity of interest (i.e., `to_cut` in `slices.Slice`) is taken to be the output of the network. If a tuple or list of two `Cut`s is given, they are assumed to be `from_cut` and `to_cut`, respectively. A cut (or the cuts within the tuple) can also be represented as an `int`, `str`, or `None`. If an `int` is given, it represents the index of a layer in `model`. If a `str` is given, it represents the name of a layer in `model`. `None` is an alternative for `slices.InputCut`. qoi: Quantity of interest to attribute. Expects a `QoI` object, or a related type that can be interpreted as a `QoI`, as documented below. If an `int` is given, the quantity of interest is taken to be the slice output for the class/neuron/channel specified by the given integer, i.e., ```python quantities.InternalChannelQoI(qoi) ``` If a tuple or list of two integers is given, then the quantity of interest is taken to be the comparative quantity for the class given by the first integer against the class given by the second integer, i.e., ```python quantities.ComparativeQoI(*qoi) ``` If a callable is given, it is interpreted as a function representing the QoI, i.e., ```python quantities.LambdaQoI(qoi) ``` If the string, `'max'`, is given, the quantity of interest is taken to be the output for the class with the maximum score, i.e., ```python quantities.MaxClassQoI() ``` doi: Distribution of interest over inputs. Expects a `DoI` object, or a related type that can be interpreted as a `DoI`, as documented below. If the string, `'point'`, is given, the distribution is taken to be the single point passed to `attributions`, i.e., ```python distributions.PointDoi() ``` If the string, `'linear'`, is given, the distribution is taken to be the linear interpolation from the zero input to the point passed to `attributions`, i.e., ```python distributions.LinearDoi() ``` multiply_activation: Whether to multiply the gradient result by its corresponding activation, thus converting from \"*influence space*\" to \"*attribution space*.\" \"\"\" super () . __init__ ( model , * args , ** kwargs ) self . slice = InternalInfluence . __get_slice ( cuts ) self . qoi = InternalInfluence . __get_qoi ( qoi ) self . doi = InternalInfluence . __get_doi ( doi , cut = self . slice . from_cut ) self . _do_multiply = multiply_activation self . _return_grads = return_grads self . _return_doi = return_doi","title":"Attribution"},{"location":"api/attribution/#attribution-methods","text":"Attribution methods quantitatively measure the contribution of each of a function's individual inputs to its output. Gradient-based attribution methods compute the gradient of a model with respect to its inputs to describe how important each input is towards the output prediction. These methods can be applied to assist in explaining deep networks. TruLens provides implementations of several such techniques, found in this package.","title":"Attribution Methods"},{"location":"api/attribution/#trulens.nn.attribution.AttributionMethod","text":"Interface used by all attribution methods. An attribution method takes a neural network model and provides the ability to assign values to the variables of the network that specify the importance of each variable towards particular predictions. Source code in trulens/nn/attribution.py class AttributionMethod ( AbstractBaseClass ): \"\"\" Interface used by all attribution methods. An attribution method takes a neural network model and provides the ability to assign values to the variables of the network that specify the importance of each variable towards particular predictions. \"\"\" @abstractmethod def __init__ ( self , model : ModelWrapper , rebatch_size : int = None , * args , ** kwargs ): \"\"\" Abstract constructor. Parameters: model: ModelWrapper Model for which attributions are calculated. rebatch_size: int (optional) Will rebatch instances to this size if given. This may be required for GPU usage if using a DoI which produces multiple instances per user-provided instance. Many valued DoIs will expand the tensors sent to each layer to original_batch_size * doi_size. The rebatch size will break up original_batch_size * doi_size into rebatch_size chunks to send to model. \"\"\" self . _model = model self . rebatch_size = rebatch_size @property def model ( self ) -> ModelWrapper : \"\"\" Model for which attributions are calculated. \"\"\" return self . _model @abstractmethod def _attributions ( self , model_inputs : ModelInputs ) -> AttributionResult : \"\"\" For attributions that have options to return multiple things depending on configuration, wrap those multiple things in the AttributionResult tuple. \"\"\" ... def attributions ( self , * model_args : ArgsLike , ** model_kwargs : KwargsLike ) -> Union [ TensorLike , ArgsLike [ TensorLike ], ArgsLike [ ArgsLike [ TensorLike ]]]: \"\"\" Returns attributions for the given input. Attributions are in the same shape as the layer that attributions are being generated for. The numeric scale of the attributions will depend on the specific implementations of the Distribution of Interest and Quantity of Interest. However it is generally related to the scale of gradients on the Quantity of Interest. For example, Integrated Gradients uses the linear interpolation Distribution of Interest which subsumes the completeness axiom which ensures the sum of all attributions of a record equals the output determined by the Quantity of Interest on the same record. The Point Distribution of Interest will be determined by the gradient at a single point, thus being a good measure of model sensitivity. Parameters: model_args: ArgsLike, model_kwargs: KwargsLike The args and kwargs given to the call method of a model. This should represent the records to obtain attributions for, assumed to be a *batched* input. if `self.model` supports evaluation on *data tensors*, the appropriate tensor type may be used (e.g., Pytorch models may accept Pytorch tensors in addition to `np.ndarray`s). The shape of the inputs must match the input shape of `self.model`. Returns - np.ndarray when single attribution_cut input, single qoi output - or ArgsLike[np.ndarray] when single input, multiple output (or vice versa) - or ArgsLike[ArgsLike[np.ndarray]] when multiple output (outer), multiple input (inner) An array of attributions, matching the shape and type of `from_cut` of the slice. Each entry in the returned array represents the degree to which the corresponding feature affected the model's outcome on the corresponding point. If attributing to a component with multiple inputs, a list for each will be returned. If the quantity of interest features multiple outputs, a list for each will be returned. \"\"\" # Calls like: attributions([arg1, arg2]) will get read as model_args = # ([arg1, arg2],), that is, a tuple with a single element containing the # model args. Test below checks for this. TODO: Disallow such # invocations? They should be given as attributions(arg1, arg2). if isinstance ( model_args , tuple ) and len ( model_args ) == 1 and isinstance ( model_args [ 0 ], DATA_CONTAINER_TYPE ): model_args = model_args [ 0 ] model_inputs = ModelInputs ( args = many_of_om ( model_args ), kwargs = model_kwargs ) # Will cast results to this data container type. return_type = type ( model_inputs . first ()) pieces = self . _attributions ( model_inputs ) # Format attributions into the public structure which throws out output # lists and input lists if there is only one output or only one input. # Also cast to whatever the input type was. attributions : Outputs [ Inputs [ np . ndarray ]] = nested_cast ( backend = get_backend (), astype = return_type , args = pieces . attributions ) attributions : Outputs [ OM [ Inputs , np . ndarray ] ] = [ om_of_many ( attr ) for attr in attributions ] attributions : OM [ Outputs , OM [ Inputs , np . ndarray ]] = om_of_many ( attributions ) if pieces . gradients is not None or pieces . interventions is not None : tru_logger . warning ( \"AttributionMethod configured to return gradients or interventions. \" \"Use the internal _attribution call to retrieve those.\" ) return attributions","title":"AttributionMethod"},{"location":"api/attribution/#trulens.nn.attribution.AttributionMethod.model","text":"Model for which attributions are calculated.","title":"model"},{"location":"api/attribution/#trulens.nn.attribution.AttributionMethod.__init__","text":"Abstract constructor. Parameters: Name Type Description Default model ModelWrapper ModelWrapper Model for which attributions are calculated. required rebatch_size int int (optional) Will rebatch instances to this size if given. This may be required for GPU usage if using a DoI which produces multiple instances per user-provided instance. Many valued DoIs will expand the tensors sent to each layer to original_batch_size * doi_size. The rebatch size will break up original_batch_size * doi_size into rebatch_size chunks to send to model. None Source code in trulens/nn/attribution.py @abstractmethod def __init__ ( self , model : ModelWrapper , rebatch_size : int = None , * args , ** kwargs ): \"\"\" Abstract constructor. Parameters: model: ModelWrapper Model for which attributions are calculated. rebatch_size: int (optional) Will rebatch instances to this size if given. This may be required for GPU usage if using a DoI which produces multiple instances per user-provided instance. Many valued DoIs will expand the tensors sent to each layer to original_batch_size * doi_size. The rebatch size will break up original_batch_size * doi_size into rebatch_size chunks to send to model. \"\"\" self . _model = model self . rebatch_size = rebatch_size","title":"__init__()"},{"location":"api/attribution/#trulens.nn.attribution.AttributionMethod.attributions","text":"Returns attributions for the given input. Attributions are in the same shape as the layer that attributions are being generated for. The numeric scale of the attributions will depend on the specific implementations of the Distribution of Interest and Quantity of Interest. However it is generally related to the scale of gradients on the Quantity of Interest. For example, Integrated Gradients uses the linear interpolation Distribution of Interest which subsumes the completeness axiom which ensures the sum of all attributions of a record equals the output determined by the Quantity of Interest on the same record. The Point Distribution of Interest will be determined by the gradient at a single point, thus being a good measure of model sensitivity. Parameters: Name Type Description Default model_args ArgsLike ArgsLike, model_kwargs: KwargsLike The args and kwargs given to the call method of a model. This should represent the records to obtain attributions for, assumed to be a batched input. if self.model supports evaluation on data tensors , the appropriate tensor type may be used (e.g., Pytorch models may accept Pytorch tensors in addition to np.ndarray s). The shape of the inputs must match the input shape of self.model . () Returns - np.ndarray when single attribution_cut input, single qoi output - or ArgsLike[np.ndarray] when single input, multiple output (or vice versa) - or ArgsLike[ArgsLike[np.ndarray]] when multiple output (outer), multiple input (inner) An array of attributions, matching the shape and type of `from_cut` of the slice. Each entry in the returned array represents the degree to which the corresponding feature affected the model's outcome on the corresponding point. If attributing to a component with multiple inputs, a list for each will be returned. If the quantity of interest features multiple outputs, a list for each will be returned. Source code in trulens/nn/attribution.py def attributions ( self , * model_args : ArgsLike , ** model_kwargs : KwargsLike ) -> Union [ TensorLike , ArgsLike [ TensorLike ], ArgsLike [ ArgsLike [ TensorLike ]]]: \"\"\" Returns attributions for the given input. Attributions are in the same shape as the layer that attributions are being generated for. The numeric scale of the attributions will depend on the specific implementations of the Distribution of Interest and Quantity of Interest. However it is generally related to the scale of gradients on the Quantity of Interest. For example, Integrated Gradients uses the linear interpolation Distribution of Interest which subsumes the completeness axiom which ensures the sum of all attributions of a record equals the output determined by the Quantity of Interest on the same record. The Point Distribution of Interest will be determined by the gradient at a single point, thus being a good measure of model sensitivity. Parameters: model_args: ArgsLike, model_kwargs: KwargsLike The args and kwargs given to the call method of a model. This should represent the records to obtain attributions for, assumed to be a *batched* input. if `self.model` supports evaluation on *data tensors*, the appropriate tensor type may be used (e.g., Pytorch models may accept Pytorch tensors in addition to `np.ndarray`s). The shape of the inputs must match the input shape of `self.model`. Returns - np.ndarray when single attribution_cut input, single qoi output - or ArgsLike[np.ndarray] when single input, multiple output (or vice versa) - or ArgsLike[ArgsLike[np.ndarray]] when multiple output (outer), multiple input (inner) An array of attributions, matching the shape and type of `from_cut` of the slice. Each entry in the returned array represents the degree to which the corresponding feature affected the model's outcome on the corresponding point. If attributing to a component with multiple inputs, a list for each will be returned. If the quantity of interest features multiple outputs, a list for each will be returned. \"\"\" # Calls like: attributions([arg1, arg2]) will get read as model_args = # ([arg1, arg2],), that is, a tuple with a single element containing the # model args. Test below checks for this. TODO: Disallow such # invocations? They should be given as attributions(arg1, arg2). if isinstance ( model_args , tuple ) and len ( model_args ) == 1 and isinstance ( model_args [ 0 ], DATA_CONTAINER_TYPE ): model_args = model_args [ 0 ] model_inputs = ModelInputs ( args = many_of_om ( model_args ), kwargs = model_kwargs ) # Will cast results to this data container type. return_type = type ( model_inputs . first ()) pieces = self . _attributions ( model_inputs ) # Format attributions into the public structure which throws out output # lists and input lists if there is only one output or only one input. # Also cast to whatever the input type was. attributions : Outputs [ Inputs [ np . ndarray ]] = nested_cast ( backend = get_backend (), astype = return_type , args = pieces . attributions ) attributions : Outputs [ OM [ Inputs , np . ndarray ] ] = [ om_of_many ( attr ) for attr in attributions ] attributions : OM [ Outputs , OM [ Inputs , np . ndarray ]] = om_of_many ( attributions ) if pieces . gradients is not None or pieces . interventions is not None : tru_logger . warning ( \"AttributionMethod configured to return gradients or interventions. \" \"Use the internal _attribution call to retrieve those.\" ) return attributions","title":"attributions()"},{"location":"api/attribution/#trulens.nn.attribution.AttributionResult","text":"_attribution method output container. Source code in trulens/nn/attribution.py @dataclass class AttributionResult : \"\"\" _attribution method output container. \"\"\" attributions : Outputs [ Inputs [ TensorLike ]] = None gradients : Outputs [ Inputs [ Uniform [ TensorLike ]]] = None interventions : Inputs [ Uniform [ TensorLike ]] = None","title":"AttributionResult"},{"location":"api/attribution/#trulens.nn.attribution.InputAttribution","text":"Attributions of input features on either internal or output quantities. This is essentially an alias for InternalInfluence ( model , ( trulens . nn . slices . InputCut (), cut ), qoi , doi , multiply_activation ) Source code in trulens/nn/attribution.py class InputAttribution ( InternalInfluence ): \"\"\" Attributions of input features on either internal or output quantities. This is essentially an alias for ```python InternalInfluence( model, (trulens.nn.slices.InputCut(), cut), qoi, doi, multiply_activation) ``` \"\"\" def __init__ ( self , model : ModelWrapper , qoi_cut : CutLike = None , # see WARNING-LOAD-INIT qoi : QoiLike = 'max' , doi_cut : CutLike = None , # see WARNING-LOAD-INIT doi : DoiLike = 'point' , multiply_activation : bool = True , * args , ** kwargs ): \"\"\" Parameters: model : Model for which attributions are calculated. qoi_cut : The cut determining the layer from which the QoI is derived. Expects a `Cut` object, or a related type that can be interpreted as a `Cut`, as documented below. If an `int` is given, it represents the index of a layer in `model`. If a `str` is given, it represents the name of a layer in `model`. `None` is an alternative for `slices.OutputCut()`. qoi : quantities.QoI | int | tuple | str Quantity of interest to attribute. Expects a `QoI` object, or a related type that can be interpreted as a `QoI`, as documented below. If an `int` is given, the quantity of interest is taken to be the slice output for the class/neuron/channel specified by the given integer, i.e., ```python quantities.InternalChannelQoI(qoi) ``` If a tuple or list of two integers is given, then the quantity of interest is taken to be the comparative quantity for the class given by the first integer against the class given by the second integer, i.e., ```python quantities.ComparativeQoI(*qoi) ``` If a callable is given, it is interpreted as a function representing the QoI, i.e., ```python quantities.LambdaQoI(qoi) ``` If the string, `'max'`, is given, the quantity of interest is taken to be the output for the class with the maximum score, i.e., ```python quantities.MaxClassQoI() ``` doi_cut : For models which have non-differentiable pre-processing at the start of the model, specify the cut of the initial differentiable input form. For NLP models, for example, this could point to the embedding layer. If not provided, InputCut is assumed. doi : distributions.DoI | str Distribution of interest over inputs. Expects a `DoI` object, or a related type that can be interpreted as a `DoI`, as documented below. If the string, `'point'`, is given, the distribution is taken to be the single point passed to `attributions`, i.e., ```python distributions.PointDoi() ``` If the string, `'linear'`, is given, the distribution is taken to be the linear interpolation from the zero input to the point passed to `attributions`, i.e., ```python distributions.LinearDoi() ``` multiply_activation : bool, optional Whether to multiply the gradient result by its corresponding activation, thus converting from \"*influence space*\" to \"*attribution space*.\" \"\"\" if doi_cut is None : # WARNING-LOAD-INIT: Do not put this as a default arg in the def # line. That would cause an instantiation of InputCut when this # class is loaded and before it is used. Because get_backend gets # called in Cut.__init__, it may fail if this class is loaded before # trulens.nn.models.get_model_wrapper is called on some model. doi_cut = InputCut () super () . __init__ ( model , ( doi_cut , qoi_cut ), qoi , doi , multiply_activation = multiply_activation , * args , ** kwargs )","title":"InputAttribution"},{"location":"api/attribution/#trulens.nn.attribution.InputAttribution.__init__","text":"Parameters: Name Type Description Default model Model for which attributions are calculated. required qoi_cut The cut determining the layer from which the QoI is derived. Expects a Cut object, or a related type that can be interpreted as a Cut , as documented below. If an int is given, it represents the index of a layer in model . If a str is given, it represents the name of a layer in model . None is an alternative for slices.OutputCut() . None qoi quantities.QoI | int | tuple | str Quantity of interest to attribute. Expects a QoI object, or a related type that can be interpreted as a QoI , as documented below. If an int is given, the quantity of interest is taken to be the slice output for the class/neuron/channel specified by the given integer, i.e., python quantities.InternalChannelQoI(qoi) If a tuple or list of two integers is given, then the quantity of interest is taken to be the comparative quantity for the class given by the first integer against the class given by the second integer, i.e., ```python quantities.ComparativeQoI(*qoi) If a callable is given, it is interpreted as a function representing the QoI, i.e., ```python quantities.LambdaQoI(qoi) If the string, 'max' , is given, the quantity of interest is taken to be the output for the class with the maximum score, i.e., python quantities.MaxClassQoI() 'max' doi_cut For models which have non-differentiable pre-processing at the start of the model, specify the cut of the initial differentiable input form. For NLP models, for example, this could point to the embedding layer. If not provided, InputCut is assumed. None doi distributions.DoI | str Distribution of interest over inputs. Expects a DoI object, or a related type that can be interpreted as a DoI , as documented below. If the string, 'point' , is given, the distribution is taken to be the single point passed to attributions , i.e., python distributions.PointDoi() If the string, 'linear' , is given, the distribution is taken to be the linear interpolation from the zero input to the point passed to attributions , i.e., python distributions.LinearDoi() 'point' multiply_activation bool, optional Whether to multiply the gradient result by its corresponding activation, thus converting from \" influence space \" to \" attribution space .\" True Source code in trulens/nn/attribution.py def __init__ ( self , model : ModelWrapper , qoi_cut : CutLike = None , # see WARNING-LOAD-INIT qoi : QoiLike = 'max' , doi_cut : CutLike = None , # see WARNING-LOAD-INIT doi : DoiLike = 'point' , multiply_activation : bool = True , * args , ** kwargs ): \"\"\" Parameters: model : Model for which attributions are calculated. qoi_cut : The cut determining the layer from which the QoI is derived. Expects a `Cut` object, or a related type that can be interpreted as a `Cut`, as documented below. If an `int` is given, it represents the index of a layer in `model`. If a `str` is given, it represents the name of a layer in `model`. `None` is an alternative for `slices.OutputCut()`. qoi : quantities.QoI | int | tuple | str Quantity of interest to attribute. Expects a `QoI` object, or a related type that can be interpreted as a `QoI`, as documented below. If an `int` is given, the quantity of interest is taken to be the slice output for the class/neuron/channel specified by the given integer, i.e., ```python quantities.InternalChannelQoI(qoi) ``` If a tuple or list of two integers is given, then the quantity of interest is taken to be the comparative quantity for the class given by the first integer against the class given by the second integer, i.e., ```python quantities.ComparativeQoI(*qoi) ``` If a callable is given, it is interpreted as a function representing the QoI, i.e., ```python quantities.LambdaQoI(qoi) ``` If the string, `'max'`, is given, the quantity of interest is taken to be the output for the class with the maximum score, i.e., ```python quantities.MaxClassQoI() ``` doi_cut : For models which have non-differentiable pre-processing at the start of the model, specify the cut of the initial differentiable input form. For NLP models, for example, this could point to the embedding layer. If not provided, InputCut is assumed. doi : distributions.DoI | str Distribution of interest over inputs. Expects a `DoI` object, or a related type that can be interpreted as a `DoI`, as documented below. If the string, `'point'`, is given, the distribution is taken to be the single point passed to `attributions`, i.e., ```python distributions.PointDoi() ``` If the string, `'linear'`, is given, the distribution is taken to be the linear interpolation from the zero input to the point passed to `attributions`, i.e., ```python distributions.LinearDoi() ``` multiply_activation : bool, optional Whether to multiply the gradient result by its corresponding activation, thus converting from \"*influence space*\" to \"*attribution space*.\" \"\"\" if doi_cut is None : # WARNING-LOAD-INIT: Do not put this as a default arg in the def # line. That would cause an instantiation of InputCut when this # class is loaded and before it is used. Because get_backend gets # called in Cut.__init__, it may fail if this class is loaded before # trulens.nn.models.get_model_wrapper is called on some model. doi_cut = InputCut () super () . __init__ ( model , ( doi_cut , qoi_cut ), qoi , doi , multiply_activation = multiply_activation , * args , ** kwargs )","title":"__init__()"},{"location":"api/attribution/#trulens.nn.attribution.IntegratedGradients","text":"Implementation for the Integrated Gradients method from the following paper: Axiomatic Attribution for Deep Networks This should be cited using: @INPROCEEDINGS { sundararajan17axiomatic , author = {Mukund Sundararajan and Ankur Taly, and Qiqi Yan} , title = {Axiomatic Attribution for Deep Networks} , booktitle = {International Conference on Machine Learning (ICML)} , year = {2017} , } This is essentially an alias for InternalInfluence ( model , ( trulens . nn . slices . InputCut (), trulens . nn . slices . OutputCut ()), 'max' , trulens . nn . distributions . LinearDoi ( baseline , resolution ), multiply_activation = True ) Source code in trulens/nn/attribution.py class IntegratedGradients ( InputAttribution ): \"\"\" Implementation for the Integrated Gradients method from the following paper: [Axiomatic Attribution for Deep Networks]( https://arxiv.org/pdf/1703.01365) This should be cited using: ```bibtex @INPROCEEDINGS{ sundararajan17axiomatic, author={Mukund Sundararajan and Ankur Taly, and Qiqi Yan}, title={Axiomatic Attribution for Deep Networks}, booktitle={International Conference on Machine Learning (ICML)}, year={2017}, } ``` This is essentially an alias for ```python InternalInfluence( model, (trulens.nn.slices.InputCut(), trulens.nn.slices.OutputCut()), 'max', trulens.nn.distributions.LinearDoi(baseline, resolution), multiply_activation=True) ``` \"\"\" def __init__ ( self , model : ModelWrapper , baseline = None , resolution : int = 50 , doi_cut = None , # see WARNING-LOAD-INIT qoi = 'max' , qoi_cut = None , # see WARNING-LOAD-INIT * args , ** kwargs ): \"\"\" Parameters: model: Model for which attributions are calculated. baseline: The baseline to interpolate from. Must be same shape as the input. If `None` is given, the zero vector in the appropriate shape will be used. resolution: Number of points to use in the approximation. A higher resolution is more computationally expensive, but gives a better approximation of the mathematical formula this attribution method represents. \"\"\" if doi_cut is None : doi_cut = InputCut () if qoi_cut is None : qoi_cut = OutputCut () super () . __init__ ( model = model , qoi_cut = qoi_cut , qoi = qoi , doi_cut = doi_cut , doi = LinearDoi ( baseline , resolution , cut = doi_cut ), multiply_activation = True , * args , ** kwargs )","title":"IntegratedGradients"},{"location":"api/attribution/#trulens.nn.attribution.IntegratedGradients.__init__","text":"Parameters: Name Type Description Default model ModelWrapper Model for which attributions are calculated. required baseline The baseline to interpolate from. Must be same shape as the input. If None is given, the zero vector in the appropriate shape will be used. None resolution int Number of points to use in the approximation. A higher resolution is more computationally expensive, but gives a better approximation of the mathematical formula this attribution method represents. 50 Source code in trulens/nn/attribution.py def __init__ ( self , model : ModelWrapper , baseline = None , resolution : int = 50 , doi_cut = None , # see WARNING-LOAD-INIT qoi = 'max' , qoi_cut = None , # see WARNING-LOAD-INIT * args , ** kwargs ): \"\"\" Parameters: model: Model for which attributions are calculated. baseline: The baseline to interpolate from. Must be same shape as the input. If `None` is given, the zero vector in the appropriate shape will be used. resolution: Number of points to use in the approximation. A higher resolution is more computationally expensive, but gives a better approximation of the mathematical formula this attribution method represents. \"\"\" if doi_cut is None : doi_cut = InputCut () if qoi_cut is None : qoi_cut = OutputCut () super () . __init__ ( model = model , qoi_cut = qoi_cut , qoi = qoi , doi_cut = doi_cut , doi = LinearDoi ( baseline , resolution , cut = doi_cut ), multiply_activation = True , * args , ** kwargs )","title":"__init__()"},{"location":"api/attribution/#trulens.nn.attribution.InternalInfluence","text":"Internal attributions parameterized by a slice, quantity of interest, and distribution of interest. The slice specifies the layers at which the internals of the model are to be exposed; it is represented by two cuts , which specify the layer the attributions are assigned to and the layer from which the quantity of interest is derived. The Quantity of Interest (QoI) is a function of the output specified by the slice that determines the network output behavior that the attributions are to describe. The Distribution of Interest (DoI) specifies the records over which the attributions are aggregated. More information can be found in the following paper: Influence-Directed Explanations for Deep Convolutional Networks This should be cited using: @INPROCEEDINGS { leino18influence , author = { Klas Leino and Shayak Sen and Anupam Datta and Matt Fredrikson and Linyi Li} , title = { Influence-Directed Explanations for Deep Convolutional Networks} , booktitle = {IEEE International Test Conference (ITC)} , year = {2018} , } Source code in trulens/nn/attribution.py class InternalInfluence ( AttributionMethod ): \"\"\"Internal attributions parameterized by a slice, quantity of interest, and distribution of interest. The *slice* specifies the layers at which the internals of the model are to be exposed; it is represented by two *cuts*, which specify the layer the attributions are assigned to and the layer from which the quantity of interest is derived. The *Quantity of Interest* (QoI) is a function of the output specified by the slice that determines the network output behavior that the attributions are to describe. The *Distribution of Interest* (DoI) specifies the records over which the attributions are aggregated. More information can be found in the following paper: [Influence-Directed Explanations for Deep Convolutional Networks]( https://arxiv.org/pdf/1802.03788.pdf) This should be cited using: ```bibtex @INPROCEEDINGS{ leino18influence, author={ Klas Leino and Shayak Sen and Anupam Datta and Matt Fredrikson and Linyi Li}, title={ Influence-Directed Explanations for Deep Convolutional Networks}, booktitle={IEEE International Test Conference (ITC)}, year={2018}, } ``` \"\"\" def __init__ ( self , model : ModelWrapper , cuts : SliceLike , qoi : QoiLike , doi : DoiLike , multiply_activation : bool = True , return_grads : bool = False , return_doi : bool = False , * args , ** kwargs ): \"\"\" Parameters: model: Model for which attributions are calculated. cuts: The slice to use when computing the attributions. The slice keeps track of the layer whose output attributions are calculated and the layer for which the quantity of interest is computed. Expects a `Slice` object, or a related type that can be interpreted as a `Slice`, as documented below. If a single `Cut` object is given, it is assumed to be the cut representing the layer for which attributions are calculated (i.e., `from_cut` in `Slice`) and the layer for the quantity of interest (i.e., `to_cut` in `slices.Slice`) is taken to be the output of the network. If a tuple or list of two `Cut`s is given, they are assumed to be `from_cut` and `to_cut`, respectively. A cut (or the cuts within the tuple) can also be represented as an `int`, `str`, or `None`. If an `int` is given, it represents the index of a layer in `model`. If a `str` is given, it represents the name of a layer in `model`. `None` is an alternative for `slices.InputCut`. qoi: Quantity of interest to attribute. Expects a `QoI` object, or a related type that can be interpreted as a `QoI`, as documented below. If an `int` is given, the quantity of interest is taken to be the slice output for the class/neuron/channel specified by the given integer, i.e., ```python quantities.InternalChannelQoI(qoi) ``` If a tuple or list of two integers is given, then the quantity of interest is taken to be the comparative quantity for the class given by the first integer against the class given by the second integer, i.e., ```python quantities.ComparativeQoI(*qoi) ``` If a callable is given, it is interpreted as a function representing the QoI, i.e., ```python quantities.LambdaQoI(qoi) ``` If the string, `'max'`, is given, the quantity of interest is taken to be the output for the class with the maximum score, i.e., ```python quantities.MaxClassQoI() ``` doi: Distribution of interest over inputs. Expects a `DoI` object, or a related type that can be interpreted as a `DoI`, as documented below. If the string, `'point'`, is given, the distribution is taken to be the single point passed to `attributions`, i.e., ```python distributions.PointDoi() ``` If the string, `'linear'`, is given, the distribution is taken to be the linear interpolation from the zero input to the point passed to `attributions`, i.e., ```python distributions.LinearDoi() ``` multiply_activation: Whether to multiply the gradient result by its corresponding activation, thus converting from \"*influence space*\" to \"*attribution space*.\" \"\"\" super () . __init__ ( model , * args , ** kwargs ) self . slice = InternalInfluence . __get_slice ( cuts ) self . qoi = InternalInfluence . __get_qoi ( qoi ) self . doi = InternalInfluence . __get_doi ( doi , cut = self . slice . from_cut ) self . _do_multiply = multiply_activation self . _return_grads = return_grads self . _return_doi = return_doi def _attributions ( self , model_inputs : ModelInputs ) -> AttributionResult : # NOTE: not symbolic B = get_backend () results = AttributionResult () # Create a message for out-of-memory errors regarding float and batch size. if len ( list ( model_inputs . values ())) == 0 : batch_size = 1 else : batch_size = model_inputs . first () . shape [ 0 ] param_msgs = [ f \"float size = { B . floatX_size } ( { B . floatX } ); consider changing to a smaller type.\" , f \"batch size = { batch_size } ; consider reducing the size of the batch you send to the attributions method.\" ] doi_cut = self . doi . cut () if self . doi . cut () else InputCut () with memory_suggestions ( * param_msgs ): # Handles out-of-memory messages. doi_val : List [ B . Tensor ] = self . model . _fprop ( model_inputs = model_inputs , to_cut = doi_cut , doi_cut = InputCut (), attribution_cut = None , # InputCut(), intervention = model_inputs )[ 0 ] doi_val = nested_map ( doi_val , B . as_array ) D = self . doi . _wrap_public_call ( doi_val , model_inputs = model_inputs ) if self . _return_doi : results . interventions = D # : Inputs[Uniform[TensorLike]] n_doi = len ( D [ 0 ]) D = self . __concatenate_doi ( D ) rebatch_size = self . rebatch_size if rebatch_size is None : rebatch_size = len ( D [ 0 ]) intervention = TensorArgs ( args = D ) model_inputs_expanded = tile ( what = model_inputs , onto = intervention ) # Create a message for out-of-memory errors regarding doi_size. # TODO: Generalize this message to doi other than LinearDoI: doi_size_msg = f \"distribution of interest size = { n_doi } ; consider reducing intervention resolution.\" combined_batch_size = n_doi * batch_size combined_batch_msg = f \"combined batch size = { combined_batch_size } ; consider reducing batch size, intervention size\" rebatch_size_msg = f \"rebatch_size = { rebatch_size } ; consider reducing this AttributionMethod constructor parameter (default is same as combined batch size).\" # Calculate the gradient of each of the points in the DoI. with memory_suggestions ( param_msgs + [ doi_size_msg , combined_batch_msg , rebatch_size_msg ] ): # Handles out-of-memory messages. qoi_grads_expanded : List [ Outputs [ Inputs [ TensorLike ]]] = [] for inputs_batch , intervention_batch in rebatch ( model_inputs_expanded , intervention , batch_size = rebatch_size ): qoi_grads_expanded_batch : Outputs [ Inputs [ TensorLike ]] = self . model . _qoi_bprop ( qoi = self . qoi , model_inputs = inputs_batch , attribution_cut = self . slice . from_cut , to_cut = self . slice . to_cut , intervention = intervention_batch , doi_cut = doi_cut ) # important to cast to numpy inside loop: qoi_grads_expanded . append ( nested_map ( qoi_grads_expanded_batch , B . as_array ) ) num_outputs = len ( qoi_grads_expanded [ 0 ]) num_inputs = len ( qoi_grads_expanded [ 0 ][ 0 ]) transpose = [ [[] for _ in range ( num_inputs )] for _ in range ( num_outputs ) ] for o in range ( num_outputs ): for i in range ( num_inputs ): for qoi_grads_batch in qoi_grads_expanded : transpose [ o ][ i ] . append ( qoi_grads_batch [ o ][ i ]) qoi_grads_expanded : Outputs [ Inputs [ np . ndarray ]] = nested_map ( transpose , np . concatenate , nest = 2 ) qoi_grads_expanded : Outputs [ Inputs [ np . ndarray ]] = nested_map ( qoi_grads_expanded , lambda grad : np . reshape ( grad , ( n_doi , - 1 ) + grad . shape [ 1 :]), nest = 2 ) if self . _return_grads : results . gradients = qoi_grads_expanded # : Outputs[Inputs[Uniform[TensorLike]]] # TODO: Does this need to be done in numpy? attrs : Outputs [ Inputs [ TensorLike ]] = nested_map ( qoi_grads_expanded , lambda grad : np . mean ( grad , axis = 0 ), nest = 2 ) # Multiply by the activation multiplier if specified. if self . _do_multiply : with memory_suggestions ( param_msgs ): z_val = self . model . _fprop ( model_inputs = model_inputs , doi_cut = InputCut (), attribution_cut = None , to_cut = self . slice . from_cut , intervention = model_inputs # intentional )[ 0 ] mults : Inputs [ TensorLike ] = self . doi . _wrap_public_get_activation_multiplier ( z_val , model_inputs = model_inputs ) mults : Inputs [ np . ndarray ] = nested_cast ( backend = B , args = mults , astype = np . ndarray ) attrs = [ [ att * mult for att , mult in zip ( attr , mults ) # Inputs ] for attr in attrs # Outputs ] results . attributions = attrs # : Outputs[Inputs[TensorLike]] return results @staticmethod def __get_qoi ( qoi_arg ): \"\"\" Helper function to get a `QoI` object from more user-friendly primitive arguments. \"\"\" # TODO(klas): we could potentially do some basic error catching here, # for example, making sure the index for a given channel is in range. if isinstance ( qoi_arg , QoI ): # We were already given a QoI, so return it. return qoi_arg elif callable ( qoi_arg ): # If we were given a callable, treat that function as a QoI. return LambdaQoI ( qoi_arg ) elif isinstance ( qoi_arg , int ): # If we receive an int, we take it to be the class/channel index # (whether it's a class or channel depends on the layer the quantity # is for, but `InternalChannelQoI` generalizes to both). return InternalChannelQoI ( qoi_arg ) elif isinstance ( qoi_arg , DATA_CONTAINER_TYPE ): # If we receive a DATA_CONTAINER_TYPE, we take it to be two classes # for which we are performing a comparative quantity of interest. if len ( qoi_arg ) == 2 : return ComparativeQoI ( * qoi_arg ) else : raise ValueError ( 'Tuple or list argument for `qoi` must have length 2' ) elif isinstance ( qoi_arg , str ): # We can specify `MaxClassQoI` via the string 'max'. if qoi_arg == 'max' : return MaxClassQoI () else : raise ValueError ( 'String argument for `qoi` must be one of the following: \\n ' ' - \"max\"' ) else : raise ValueError ( 'Unrecognized argument type for `qoi`' ) @staticmethod def __get_doi ( doi_arg , cut = None ): \"\"\" Helper function to get a `DoI` object from more user-friendly primitive arguments. \"\"\" if isinstance ( doi_arg , DoI ): # We were already given a DoI, so return it. return doi_arg elif isinstance ( doi_arg , str ): # We can specify `PointDoi` via the string 'point', or `LinearDoi` # via the string 'linear'. if doi_arg == 'point' : return PointDoi ( cut = cut ) elif doi_arg == 'linear' : return LinearDoi ( cut = cut ) else : raise ValueError ( 'String argument for `doi` must be one of the following: \\n ' ' - \"point\" \\n ' ' - \"linear\"' ) else : raise ValueError ( 'Unrecognized argument type for `doi`' ) @staticmethod def __get_slice ( slice_arg ): \"\"\" Helper function to get a `Slice` object from more user-friendly primitive arguments. \"\"\" if isinstance ( slice_arg , Slice ): # We are already given a Slice, so return it. return slice_arg elif ( isinstance ( slice_arg , Cut ) or isinstance ( slice_arg , int ) or isinstance ( slice_arg , str ) or slice_arg is None or slice_arg == 0 ): # If we receive a Cut, we take it to be the Cut of the start layer. return Slice ( InternalInfluence . __get_cut ( slice_arg ), OutputCut ()) elif isinstance ( slice_arg , DATA_CONTAINER_TYPE ): # If we receive a DATA_CONTAINER_TYPE, we take it to be the start # and end layer of the slice. if len ( slice_arg ) == 2 : if slice_arg [ 1 ] is None : return Slice ( InternalInfluence . __get_cut ( slice_arg [ 0 ]), OutputCut () ) else : return Slice ( InternalInfluence . __get_cut ( slice_arg [ 0 ]), InternalInfluence . __get_cut ( slice_arg [ 1 ]) ) else : raise ValueError ( 'Tuple or list argument for `cuts` must have length 2' ) else : raise ValueError ( 'Unrecognized argument type for `cuts`' ) @staticmethod def __get_cut ( cut_arg ): \"\"\" Helper function to get a `Cut` object from more user-friendly primitive arguments. \"\"\" if isinstance ( cut_arg , Cut ): # We are already given a Cut, so return it. return cut_arg elif cut_arg is None or cut_arg == 0 : # If we receive None or zero, we take it to be the input cut. return InputCut () # TODO(klas): may want a bit more validation here. elif isinstance ( cut_arg , int ) or isinstance ( cut_arg , str ): return Cut ( cut_arg ) else : raise ValueError ( 'Unrecognized argument type for cut' ) @staticmethod def __concatenate_doi ( D : Inputs [ Uniform [ TensorLike ]]) -> Inputs [ TensorLike ]: # Returns one TensorLike for each model input. if len ( D [ 0 ]) == 0 : raise ValueError ( 'Got empty distribution of interest. `DoI` must return at ' 'least one point.' ) # TODO: should this always be done in numpy or can we do it in backend? D = nested_cast ( backend = get_backend (), args = D , astype = np . ndarray ) return [ np . concatenate ( Di ) for Di in D ]","title":"InternalInfluence"},{"location":"api/attribution/#trulens.nn.attribution.InternalInfluence.__init__","text":"Parameters: Name Type Description Default model ModelWrapper Model for which attributions are calculated. required cuts SliceLike The slice to use when computing the attributions. The slice keeps track of the layer whose output attributions are calculated and the layer for which the quantity of interest is computed. Expects a Slice object, or a related type that can be interpreted as a Slice , as documented below. If a single Cut object is given, it is assumed to be the cut representing the layer for which attributions are calculated (i.e., from_cut in Slice ) and the layer for the quantity of interest (i.e., to_cut in slices.Slice ) is taken to be the output of the network. If a tuple or list of two Cut s is given, they are assumed to be from_cut and to_cut , respectively. A cut (or the cuts within the tuple) can also be represented as an int , str , or None . If an int is given, it represents the index of a layer in model . If a str is given, it represents the name of a layer in model . None is an alternative for slices.InputCut . required qoi QoiLike Quantity of interest to attribute. Expects a QoI object, or a related type that can be interpreted as a QoI , as documented below. If an int is given, the quantity of interest is taken to be the slice output for the class/neuron/channel specified by the given integer, i.e., quantities . InternalChannelQoI ( qoi ) If a tuple or list of two integers is given, then the quantity of interest is taken to be the comparative quantity for the class given by the first integer against the class given by the second integer, i.e., quantities . ComparativeQoI ( * qoi ) If a callable is given, it is interpreted as a function representing the QoI, i.e., quantities . LambdaQoI ( qoi ) If the string, 'max' , is given, the quantity of interest is taken to be the output for the class with the maximum score, i.e., quantities . MaxClassQoI () required doi DoiLike Distribution of interest over inputs. Expects a DoI object, or a related type that can be interpreted as a DoI , as documented below. If the string, 'point' , is given, the distribution is taken to be the single point passed to attributions , i.e., distributions . PointDoi () If the string, 'linear' , is given, the distribution is taken to be the linear interpolation from the zero input to the point passed to attributions , i.e., distributions . LinearDoi () required multiply_activation bool Whether to multiply the gradient result by its corresponding activation, thus converting from \" influence space \" to \" attribution space .\" True Source code in trulens/nn/attribution.py def __init__ ( self , model : ModelWrapper , cuts : SliceLike , qoi : QoiLike , doi : DoiLike , multiply_activation : bool = True , return_grads : bool = False , return_doi : bool = False , * args , ** kwargs ): \"\"\" Parameters: model: Model for which attributions are calculated. cuts: The slice to use when computing the attributions. The slice keeps track of the layer whose output attributions are calculated and the layer for which the quantity of interest is computed. Expects a `Slice` object, or a related type that can be interpreted as a `Slice`, as documented below. If a single `Cut` object is given, it is assumed to be the cut representing the layer for which attributions are calculated (i.e., `from_cut` in `Slice`) and the layer for the quantity of interest (i.e., `to_cut` in `slices.Slice`) is taken to be the output of the network. If a tuple or list of two `Cut`s is given, they are assumed to be `from_cut` and `to_cut`, respectively. A cut (or the cuts within the tuple) can also be represented as an `int`, `str`, or `None`. If an `int` is given, it represents the index of a layer in `model`. If a `str` is given, it represents the name of a layer in `model`. `None` is an alternative for `slices.InputCut`. qoi: Quantity of interest to attribute. Expects a `QoI` object, or a related type that can be interpreted as a `QoI`, as documented below. If an `int` is given, the quantity of interest is taken to be the slice output for the class/neuron/channel specified by the given integer, i.e., ```python quantities.InternalChannelQoI(qoi) ``` If a tuple or list of two integers is given, then the quantity of interest is taken to be the comparative quantity for the class given by the first integer against the class given by the second integer, i.e., ```python quantities.ComparativeQoI(*qoi) ``` If a callable is given, it is interpreted as a function representing the QoI, i.e., ```python quantities.LambdaQoI(qoi) ``` If the string, `'max'`, is given, the quantity of interest is taken to be the output for the class with the maximum score, i.e., ```python quantities.MaxClassQoI() ``` doi: Distribution of interest over inputs. Expects a `DoI` object, or a related type that can be interpreted as a `DoI`, as documented below. If the string, `'point'`, is given, the distribution is taken to be the single point passed to `attributions`, i.e., ```python distributions.PointDoi() ``` If the string, `'linear'`, is given, the distribution is taken to be the linear interpolation from the zero input to the point passed to `attributions`, i.e., ```python distributions.LinearDoi() ``` multiply_activation: Whether to multiply the gradient result by its corresponding activation, thus converting from \"*influence space*\" to \"*attribution space*.\" \"\"\" super () . __init__ ( model , * args , ** kwargs ) self . slice = InternalInfluence . __get_slice ( cuts ) self . qoi = InternalInfluence . __get_qoi ( qoi ) self . doi = InternalInfluence . __get_doi ( doi , cut = self . slice . from_cut ) self . _do_multiply = multiply_activation self . _return_grads = return_grads self . _return_doi = return_doi","title":"__init__()"},{"location":"api/distributions/","text":"Distributions of Interest \u00b6 The distribution of interest lets us specify the set of samples over which we want our explanations to be faithful. In some cases, we may want to explain the model\u2019s behavior on a particular record, whereas other times we may be interested in a more general behavior over a distribution of samples. DoI ( ABC ) \u00b6 Interface for distributions of interest. The Distribution of Interest (DoI) specifies the samples over which an attribution method is aggregated. Source code in trulens/nn/distributions.py class DoI ( AbstractBaseClass ): \"\"\" Interface for distributions of interest. The *Distribution of Interest* (DoI) specifies the samples over which an attribution method is aggregated. \"\"\" def __init__ ( self , cut : Cut = None ): \"\"\"\"Initialize DoI Parameters: cut (Cut, optional): The Cut in which the DoI will be applied. If `None`, the DoI will be applied to the input. otherwise, the distribution should be applied to the latent space defined by the cut. \"\"\" self . _cut = cut def _wrap_public_call ( self , z : Inputs [ TensorLike ], * , model_inputs : ModelInputs ) -> Inputs [ Uniform [ TensorLike ]]: \"\"\"Same as __call__ but input and output types are more specific and less permissive. Formats the inputs for special cases that might be more convenient for the user's __call__ implementation and formats its return back to the consistent type.\"\"\" z : Inputs [ TensorLike ] = om_of_many ( z ) if accepts_model_inputs ( self . __call__ ): ret = self . __call__ ( z , model_inputs = model_inputs ) else : ret = self . __call__ ( z ) # Wrap the public doi generator with appropriate type aliases. if isinstance ( ret [ 0 ], DATA_CONTAINER_TYPE ): ret = Inputs ( Uniform ( x ) for x in ret ) else : ret = Uniform ( ret ) ret : Inputs [ Uniform [ TensorLike ]] = many_of_om ( ret , innertype = Uniform ) return ret @abstractmethod def __call__ ( self , z : OM [ Inputs , TensorLike ], * , model_inputs : Optional [ ModelInputs ] = None ) -> OM [ Inputs , Uniform [ TensorLike ]]: \"\"\" Computes the distribution of interest from an initial point. If z: TensorLike is given, we assume there is only 1 input to the DoI layer. If z: List[TensorLike] is given, it provides all of the inputs to the DoI layer. Either way, we always return List[List[TensorLike]] (alias Inputs[Uniform[TensorLike]]) with outer list spanning layer inputs, and inner list spanning a distribution's instance. Parameters: z: Input point from which the distribution is derived. If list/tuple, the point is defined by multiple tensors. model_inputs: Optional wrapped model input arguments that produce value z at cut. Returns: List of points which are all assigned equal probability mass in the distribution of interest, i.e., the distribution of interest is a discrete, uniform distribution over the list of returned points. If z is multi-input, returns a distribution for each input. \"\"\" raise NotImplementedError # @property def cut ( self ) -> Cut : \"\"\" Returns: The Cut in which the DoI will be applied. If `None`, the DoI will be applied to the input. otherwise, the distribution should be applied to the latent space defined by the cut. \"\"\" return self . _cut def _wrap_public_get_activation_multiplier ( self , activation : Inputs [ TensorLike ], * , model_inputs : ModelInputs ) -> Inputs [ TensorLike ]: \"\"\"Same as get_activation_multiplier but without \"one-or-more\". \"\"\" activations : OM [ Inputs , TensorLike ] = om_of_many ( activation ) # get_activation_multiplier is public if accepts_model_inputs ( self . get_activation_multiplier ): ret : OM [ Inputs , TensorLike ] = self . get_activation_multiplier ( activations , model_inputs = model_inputs ) else : ret : OM [ Inputs , TensorLike ] = self . get_activation_multiplier ( activations ) ret : Inputs [ TensorLike ] = many_of_om ( ret ) return ret def get_activation_multiplier ( self , activation : OM [ Inputs , TensorLike ], * , model_inputs : Optional [ ModelInputs ] = None ) -> OM [ Inputs , TensorLike ]: \"\"\" Returns a term to multiply the gradient by to convert from \"*influence space*\" to \"*attribution space*\". Conceptually, \"influence space\" corresponds to the potential effect of a slight increase in each feature, while \"attribution space\" corresponds to an approximation of the net marginal contribution to the quantity of interest of each feature. Parameters: activation: The activation of the layer the DoI is applied to. DoI may be multi-input in which case activation will be a list. model_inputs: Optional wrapped model input arguments that produce activation at cut. Returns: An array with the same shape as ``activation`` that will be multiplied by the gradient to obtain the attribution. The default implementation of this method simply returns ``activation``. If activation is multi-input, returns one multiplier for each. \"\"\" return om_of_many ( activation ) def _assert_cut_contains_only_one_tensor ( self , x ): if isinstance ( x , list ) and len ( x ) == 1 : x = x [ 0 ] if isinstance ( x , list ): raise DoiCutSupportError ( ' \\n\\n ' 'Cut provided to distribution of interest was comprised of ' 'multiple tensors, but ` {} ` is only defined for cuts comprised ' 'of a single tensor (received a list of {} tensors). \\n ' ' \\n ' 'Either (1) select a slice where the `to_cut` corresponds to a ' 'single tensor, or (2) implement/use a `DoI` object that ' 'supports lists of tensors, i.e., where the parameter, `z`, to ' '`__call__` is expected/allowed to be a list of {} tensors.' . format ( self . __class__ . __name__ , len ( x ), len ( x )) ) elif not ( isinstance ( x , np . ndarray ) or get_backend () . is_tensor ( x )): raise ValueError ( '` {} ` expected to receive an instance of `Tensor` or ' '`np.ndarray`, but received an instance of {} ' . format ( self . __class__ . __name__ , type ( x ) ) ) __call__ ( self , z , * , model_inputs = None ) special \u00b6 Computes the distribution of interest from an initial point. If z: TensorLike is given, we assume there is only 1 input to the DoI layer. If z: List[TensorLike] is given, it provides all of the inputs to the DoI layer. Either way, we always return List[List[TensorLike]] (alias Inputs[Uniform[TensorLike]]) with outer list spanning layer inputs, and inner list spanning a distribution's instance. Parameters: Name Type Description Default z OM[Inputs, TensorLike] Input point from which the distribution is derived. If list/tuple, the point is defined by multiple tensors. required model_inputs Optional[ModelInputs] Optional wrapped model input arguments that produce value z at cut. None Returns: Type Description OM[Inputs, Uniform[TensorLike]] List of points which are all assigned equal probability mass in the distribution of interest, i.e., the distribution of interest is a discrete, uniform distribution over the list of returned points. If z is multi-input, returns a distribution for each input. Source code in trulens/nn/distributions.py @abstractmethod def __call__ ( self , z : OM [ Inputs , TensorLike ], * , model_inputs : Optional [ ModelInputs ] = None ) -> OM [ Inputs , Uniform [ TensorLike ]]: \"\"\" Computes the distribution of interest from an initial point. If z: TensorLike is given, we assume there is only 1 input to the DoI layer. If z: List[TensorLike] is given, it provides all of the inputs to the DoI layer. Either way, we always return List[List[TensorLike]] (alias Inputs[Uniform[TensorLike]]) with outer list spanning layer inputs, and inner list spanning a distribution's instance. Parameters: z: Input point from which the distribution is derived. If list/tuple, the point is defined by multiple tensors. model_inputs: Optional wrapped model input arguments that produce value z at cut. Returns: List of points which are all assigned equal probability mass in the distribution of interest, i.e., the distribution of interest is a discrete, uniform distribution over the list of returned points. If z is multi-input, returns a distribution for each input. \"\"\" raise NotImplementedError __init__ ( self , cut = None ) special \u00b6 \"Initialize DoI Parameters: Name Type Description Default cut Cut The Cut in which the DoI will be applied. If None , the DoI will be applied to the input. otherwise, the distribution should be applied to the latent space defined by the cut. None Source code in trulens/nn/distributions.py def __init__ ( self , cut : Cut = None ): \"\"\"\"Initialize DoI Parameters: cut (Cut, optional): The Cut in which the DoI will be applied. If `None`, the DoI will be applied to the input. otherwise, the distribution should be applied to the latent space defined by the cut. \"\"\" self . _cut = cut cut ( self ) \u00b6 Returns: Type Description Cut The Cut in which the DoI will be applied. If None , the DoI will be applied to the input. otherwise, the distribution should be applied to the latent space defined by the cut. Source code in trulens/nn/distributions.py def cut ( self ) -> Cut : \"\"\" Returns: The Cut in which the DoI will be applied. If `None`, the DoI will be applied to the input. otherwise, the distribution should be applied to the latent space defined by the cut. \"\"\" return self . _cut get_activation_multiplier ( self , activation , * , model_inputs = None ) \u00b6 Returns a term to multiply the gradient by to convert from \" influence space \" to \" attribution space \". Conceptually, \"influence space\" corresponds to the potential effect of a slight increase in each feature, while \"attribution space\" corresponds to an approximation of the net marginal contribution to the quantity of interest of each feature. Parameters: Name Type Description Default activation OM[Inputs, TensorLike] The activation of the layer the DoI is applied to. DoI may be multi-input in which case activation will be a list. required model_inputs Optional[ModelInputs] Optional wrapped model input arguments that produce activation at cut. None Returns: Type Description OM[Inputs, TensorLike] An array with the same shape as activation that will be multiplied by the gradient to obtain the attribution. The default implementation of this method simply returns activation . If activation is multi-input, returns one multiplier for each. Source code in trulens/nn/distributions.py def get_activation_multiplier ( self , activation : OM [ Inputs , TensorLike ], * , model_inputs : Optional [ ModelInputs ] = None ) -> OM [ Inputs , TensorLike ]: \"\"\" Returns a term to multiply the gradient by to convert from \"*influence space*\" to \"*attribution space*\". Conceptually, \"influence space\" corresponds to the potential effect of a slight increase in each feature, while \"attribution space\" corresponds to an approximation of the net marginal contribution to the quantity of interest of each feature. Parameters: activation: The activation of the layer the DoI is applied to. DoI may be multi-input in which case activation will be a list. model_inputs: Optional wrapped model input arguments that produce activation at cut. Returns: An array with the same shape as ``activation`` that will be multiplied by the gradient to obtain the attribution. The default implementation of this method simply returns ``activation``. If activation is multi-input, returns one multiplier for each. \"\"\" return om_of_many ( activation ) DoiCutSupportError ( ValueError ) \u00b6 Exception raised if the distribution of interest is called on a cut whose output is not supported by the distribution of interest. Source code in trulens/nn/distributions.py class DoiCutSupportError ( ValueError ): \"\"\" Exception raised if the distribution of interest is called on a cut whose output is not supported by the distribution of interest. \"\"\" pass GaussianDoi ( DoI ) \u00b6 Distribution representing a Gaussian ball around the point. Used by Smooth Gradients. Source code in trulens/nn/distributions.py class GaussianDoi ( DoI ): \"\"\" Distribution representing a Gaussian ball around the point. Used by Smooth Gradients. \"\"\" def __init__ ( self , var : float , resolution : int , cut : Cut = None ): \"\"\" Parameters: var: The variance of the Gaussian noise to be added around the point. resolution: Number of samples returned by each call to this DoI. cut (Cut, optional): The Cut in which the DoI will be applied. If `None`, the DoI will be applied to the input. otherwise, the distribution should be applied to the latent space defined by the cut. \"\"\" super ( GaussianDoi , self ) . __init__ ( cut ) self . _var = var self . _resolution = resolution def __call__ ( self , z : OM [ Inputs , TensorLike ]) -> OM [ Inputs , Uniform [ TensorLike ]]: # Public interface. B = get_backend () self . _assert_cut_contains_only_one_tensor ( z ) def gauss_of_input ( z : TensorLike ) -> Uniform [ TensorLike ]: # TODO: make a pytorch backend with the same interface to use in places like these. if B . is_tensor ( z ): # Tensor implementation. return [ z + B . random_normal_like ( z , var = self . _var ) for _ in range ( self . _resolution ) ] # Uniform else : # Array implementation. return [ z + np . random . normal ( 0. , np . sqrt ( self . _var ), z . shape ) for _ in range ( self . _resolution ) ] # Uniform z : Inputs [ TensorLike ] = many_of_om ( z ) return om_of_many ( list ( map ( gauss_of_input , z ))) __init__ ( self , var , resolution , cut = None ) special \u00b6 Parameters: Name Type Description Default var float The variance of the Gaussian noise to be added around the point. required resolution int Number of samples returned by each call to this DoI. required cut Cut The Cut in which the DoI will be applied. If None , the DoI will be applied to the input. otherwise, the distribution should be applied to the latent space defined by the cut. None Source code in trulens/nn/distributions.py def __init__ ( self , var : float , resolution : int , cut : Cut = None ): \"\"\" Parameters: var: The variance of the Gaussian noise to be added around the point. resolution: Number of samples returned by each call to this DoI. cut (Cut, optional): The Cut in which the DoI will be applied. If `None`, the DoI will be applied to the input. otherwise, the distribution should be applied to the latent space defined by the cut. \"\"\" super ( GaussianDoi , self ) . __init__ ( cut ) self . _var = var self . _resolution = resolution LinearDoi ( DoI ) \u00b6 Distribution representing the linear interpolation between a baseline and the given point. Used by Integrated Gradients. Source code in trulens/nn/distributions.py class LinearDoi ( DoI ): \"\"\" Distribution representing the linear interpolation between a baseline and the given point. Used by Integrated Gradients. \"\"\" def __init__ ( self , baseline : BaselineLike = None , resolution : int = 10 , * , cut : Cut = None , ): \"\"\" The DoI for point, `z`, will be a uniform distribution over the points on the line segment connecting `z` to `baseline`, approximated by a sample of `resolution` points equally spaced along this segment. Parameters: cut (Cut, optional, from DoI): The Cut in which the DoI will be applied. If `None`, the DoI will be applied to the input. otherwise, the distribution should be applied to the latent space defined by the cut. baseline (BaselineLike, optional): The baseline to interpolate from. Must be same shape as the space the distribution acts over, i.e., the shape of the points, `z`, eventually passed to `__call__`. If `cut` is `None`, this must be the same shape as the input, otherwise this must be the same shape as the latent space defined by the cut. If `None` is given, `baseline` will be the zero vector in the appropriate shape. If the baseline is callable, it is expected to return the `baseline`, given `z` and optional model arguments. resolution (int): Number of points returned by each call to this DoI. A higher resolution is more computationally expensive, but gives a better approximation of the DoI this object mathematically represents. \"\"\" super ( LinearDoi , self ) . __init__ ( cut ) self . _baseline = baseline self . _resolution = resolution @property def baseline ( self ) -> BaselineLike : return self . _baseline @property def resolution ( self ) -> int : return self . _resolution def __call__ ( self , z : OM [ Inputs , TensorLike ], * , model_inputs : Optional [ ModelInputs ] = None ) -> OM [ Inputs , Uniform [ TensorLike ]]: self . _assert_cut_contains_only_one_tensor ( z ) z : Inputs [ TensorLike ] = many_of_om ( z ) baseline = self . _compute_baseline ( z , model_inputs = model_inputs ) r = 1. if self . _resolution == 1 else self . _resolution - 1. return om_of_many ([ # Inputs [ # Uniform ( 1. - i / r ) * z_ + i / r * b_ for i in range ( self . _resolution ) ] for z_ , b_ in zip ( z , baseline ) ]) def get_activation_multiplier ( self , activation : OM [ Inputs , TensorLike ], * , model_inputs : Optional [ ModelInputs ] = None ) -> Inputs [ TensorLike ]: \"\"\" Returns a term to multiply the gradient by to convert from \"*influence space*\" to \"*attribution space*\". Conceptually, \"influence space\" corresponds to the potential effect of a slight increase in each feature, while \"attribution space\" corresponds to an approximation of the net marginal contribution to the quantity of interest of each feature. Parameters: activation: The activation of the layer the DoI is applied to. Returns: The activation adjusted by the baseline passed to the constructor. \"\"\" activation : Inputs [ TensorLike ] = many_of_om ( activation ) baseline : Inputs [ TensorLike ] = self . _compute_baseline ( activation , model_inputs = model_inputs ) if baseline is None : return activation return [ a - b for a , b in zip ( activation , baseline )] def _compute_baseline ( self , z : Inputs [ TensorLike ], * , model_inputs : Optional [ ModelInputs ] = None ) -> Inputs [ TensorLike ]: B = get_backend () _baseline : BaselineLike = self . baseline # user-provided if isinstance ( _baseline , Callable ): if accepts_model_inputs ( _baseline ): _baseline : OM [ Inputs , TensorLike ] = many_of_om ( _baseline ( om_of_many ( z ), model_inputs = model_inputs ) ) else : _baseline : OM [ Inputs , TensorLike ] = many_of_om ( _baseline ( om_of_many ( z )) ) else : _baseline : OM [ Inputs , TensorLike ] if _baseline is None : _baseline : Inputs [ TensorLike ] = [ B . zeros_like ( z_ ) for z_ in z ] else : _baseline : Inputs [ TensorLike ] = many_of_om ( _baseline ) # Came from user; could have been single or multiple inputs. # Cast to either Tensor or numpy.ndarray to match what was given in z. return nested_cast ( backend = B , args = _baseline , astype = type ( z [ 0 ])) __init__ ( self , baseline = None , resolution = 10 , * , cut = None ) special \u00b6 The DoI for point, z , will be a uniform distribution over the points on the line segment connecting z to baseline , approximated by a sample of resolution points equally spaced along this segment. Parameters: Name Type Description Default cut Cut, optional, from DoI The Cut in which the DoI will be applied. If None , the DoI will be applied to the input. otherwise, the distribution should be applied to the latent space defined by the cut. None baseline BaselineLike The baseline to interpolate from. Must be same shape as the space the distribution acts over, i.e., the shape of the points, z , eventually passed to __call__ . If cut is None , this must be the same shape as the input, otherwise this must be the same shape as the latent space defined by the cut. If None is given, baseline will be the zero vector in the appropriate shape. If the baseline is callable, it is expected to return the baseline , given z and optional model arguments. None resolution int Number of points returned by each call to this DoI. A higher resolution is more computationally expensive, but gives a better approximation of the DoI this object mathematically represents. 10 Source code in trulens/nn/distributions.py def __init__ ( self , baseline : BaselineLike = None , resolution : int = 10 , * , cut : Cut = None , ): \"\"\" The DoI for point, `z`, will be a uniform distribution over the points on the line segment connecting `z` to `baseline`, approximated by a sample of `resolution` points equally spaced along this segment. Parameters: cut (Cut, optional, from DoI): The Cut in which the DoI will be applied. If `None`, the DoI will be applied to the input. otherwise, the distribution should be applied to the latent space defined by the cut. baseline (BaselineLike, optional): The baseline to interpolate from. Must be same shape as the space the distribution acts over, i.e., the shape of the points, `z`, eventually passed to `__call__`. If `cut` is `None`, this must be the same shape as the input, otherwise this must be the same shape as the latent space defined by the cut. If `None` is given, `baseline` will be the zero vector in the appropriate shape. If the baseline is callable, it is expected to return the `baseline`, given `z` and optional model arguments. resolution (int): Number of points returned by each call to this DoI. A higher resolution is more computationally expensive, but gives a better approximation of the DoI this object mathematically represents. \"\"\" super ( LinearDoi , self ) . __init__ ( cut ) self . _baseline = baseline self . _resolution = resolution get_activation_multiplier ( self , activation , * , model_inputs = None ) \u00b6 Returns a term to multiply the gradient by to convert from \" influence space \" to \" attribution space \". Conceptually, \"influence space\" corresponds to the potential effect of a slight increase in each feature, while \"attribution space\" corresponds to an approximation of the net marginal contribution to the quantity of interest of each feature. Parameters: Name Type Description Default activation OM[Inputs, TensorLike] The activation of the layer the DoI is applied to. required Returns: Type Description Inputs[TensorLike] The activation adjusted by the baseline passed to the constructor. Source code in trulens/nn/distributions.py def get_activation_multiplier ( self , activation : OM [ Inputs , TensorLike ], * , model_inputs : Optional [ ModelInputs ] = None ) -> Inputs [ TensorLike ]: \"\"\" Returns a term to multiply the gradient by to convert from \"*influence space*\" to \"*attribution space*\". Conceptually, \"influence space\" corresponds to the potential effect of a slight increase in each feature, while \"attribution space\" corresponds to an approximation of the net marginal contribution to the quantity of interest of each feature. Parameters: activation: The activation of the layer the DoI is applied to. Returns: The activation adjusted by the baseline passed to the constructor. \"\"\" activation : Inputs [ TensorLike ] = many_of_om ( activation ) baseline : Inputs [ TensorLike ] = self . _compute_baseline ( activation , model_inputs = model_inputs ) if baseline is None : return activation return [ a - b for a , b in zip ( activation , baseline )] PointDoi ( DoI ) \u00b6 Distribution that puts all probability mass on a single point. Source code in trulens/nn/distributions.py class PointDoi ( DoI ): \"\"\" Distribution that puts all probability mass on a single point. \"\"\" def __init__ ( self , cut : Cut = None ): \"\"\"\"Initialize PointDoI Parameters: cut (Cut, optional): The Cut in which the DoI will be applied. If `None`, the DoI will be applied to the input. otherwise, the distribution should be applied to the latent space defined by the cut. \"\"\" super ( PointDoi , self ) . __init__ ( cut ) def __call__ ( self , z : OM [ Inputs , TensorLike ], * , model_inputs : Optional [ ModelInputs ] = None ) -> OM [ Inputs , Uniform [ TensorLike ]]: z : Inputs [ TensorLike ] = many_of_om ( z ) return om_of_many ([ [ z_ ] # a point Uniform for z_ in z ]) __init__ ( self , cut = None ) special \u00b6 \"Initialize PointDoI Parameters: Name Type Description Default cut Cut The Cut in which the DoI will be applied. If None , the DoI will be applied to the input. otherwise, the distribution should be applied to the latent space defined by the cut. None Source code in trulens/nn/distributions.py def __init__ ( self , cut : Cut = None ): \"\"\"\"Initialize PointDoI Parameters: cut (Cut, optional): The Cut in which the DoI will be applied. If `None`, the DoI will be applied to the input. otherwise, the distribution should be applied to the latent space defined by the cut. \"\"\" super ( PointDoi , self ) . __init__ ( cut )","title":"Distributions"},{"location":"api/distributions/#distributions-of-interest","text":"The distribution of interest lets us specify the set of samples over which we want our explanations to be faithful. In some cases, we may want to explain the model\u2019s behavior on a particular record, whereas other times we may be interested in a more general behavior over a distribution of samples.","title":"Distributions of Interest"},{"location":"api/distributions/#trulens.nn.distributions.DoI","text":"Interface for distributions of interest. The Distribution of Interest (DoI) specifies the samples over which an attribution method is aggregated. Source code in trulens/nn/distributions.py class DoI ( AbstractBaseClass ): \"\"\" Interface for distributions of interest. The *Distribution of Interest* (DoI) specifies the samples over which an attribution method is aggregated. \"\"\" def __init__ ( self , cut : Cut = None ): \"\"\"\"Initialize DoI Parameters: cut (Cut, optional): The Cut in which the DoI will be applied. If `None`, the DoI will be applied to the input. otherwise, the distribution should be applied to the latent space defined by the cut. \"\"\" self . _cut = cut def _wrap_public_call ( self , z : Inputs [ TensorLike ], * , model_inputs : ModelInputs ) -> Inputs [ Uniform [ TensorLike ]]: \"\"\"Same as __call__ but input and output types are more specific and less permissive. Formats the inputs for special cases that might be more convenient for the user's __call__ implementation and formats its return back to the consistent type.\"\"\" z : Inputs [ TensorLike ] = om_of_many ( z ) if accepts_model_inputs ( self . __call__ ): ret = self . __call__ ( z , model_inputs = model_inputs ) else : ret = self . __call__ ( z ) # Wrap the public doi generator with appropriate type aliases. if isinstance ( ret [ 0 ], DATA_CONTAINER_TYPE ): ret = Inputs ( Uniform ( x ) for x in ret ) else : ret = Uniform ( ret ) ret : Inputs [ Uniform [ TensorLike ]] = many_of_om ( ret , innertype = Uniform ) return ret @abstractmethod def __call__ ( self , z : OM [ Inputs , TensorLike ], * , model_inputs : Optional [ ModelInputs ] = None ) -> OM [ Inputs , Uniform [ TensorLike ]]: \"\"\" Computes the distribution of interest from an initial point. If z: TensorLike is given, we assume there is only 1 input to the DoI layer. If z: List[TensorLike] is given, it provides all of the inputs to the DoI layer. Either way, we always return List[List[TensorLike]] (alias Inputs[Uniform[TensorLike]]) with outer list spanning layer inputs, and inner list spanning a distribution's instance. Parameters: z: Input point from which the distribution is derived. If list/tuple, the point is defined by multiple tensors. model_inputs: Optional wrapped model input arguments that produce value z at cut. Returns: List of points which are all assigned equal probability mass in the distribution of interest, i.e., the distribution of interest is a discrete, uniform distribution over the list of returned points. If z is multi-input, returns a distribution for each input. \"\"\" raise NotImplementedError # @property def cut ( self ) -> Cut : \"\"\" Returns: The Cut in which the DoI will be applied. If `None`, the DoI will be applied to the input. otherwise, the distribution should be applied to the latent space defined by the cut. \"\"\" return self . _cut def _wrap_public_get_activation_multiplier ( self , activation : Inputs [ TensorLike ], * , model_inputs : ModelInputs ) -> Inputs [ TensorLike ]: \"\"\"Same as get_activation_multiplier but without \"one-or-more\". \"\"\" activations : OM [ Inputs , TensorLike ] = om_of_many ( activation ) # get_activation_multiplier is public if accepts_model_inputs ( self . get_activation_multiplier ): ret : OM [ Inputs , TensorLike ] = self . get_activation_multiplier ( activations , model_inputs = model_inputs ) else : ret : OM [ Inputs , TensorLike ] = self . get_activation_multiplier ( activations ) ret : Inputs [ TensorLike ] = many_of_om ( ret ) return ret def get_activation_multiplier ( self , activation : OM [ Inputs , TensorLike ], * , model_inputs : Optional [ ModelInputs ] = None ) -> OM [ Inputs , TensorLike ]: \"\"\" Returns a term to multiply the gradient by to convert from \"*influence space*\" to \"*attribution space*\". Conceptually, \"influence space\" corresponds to the potential effect of a slight increase in each feature, while \"attribution space\" corresponds to an approximation of the net marginal contribution to the quantity of interest of each feature. Parameters: activation: The activation of the layer the DoI is applied to. DoI may be multi-input in which case activation will be a list. model_inputs: Optional wrapped model input arguments that produce activation at cut. Returns: An array with the same shape as ``activation`` that will be multiplied by the gradient to obtain the attribution. The default implementation of this method simply returns ``activation``. If activation is multi-input, returns one multiplier for each. \"\"\" return om_of_many ( activation ) def _assert_cut_contains_only_one_tensor ( self , x ): if isinstance ( x , list ) and len ( x ) == 1 : x = x [ 0 ] if isinstance ( x , list ): raise DoiCutSupportError ( ' \\n\\n ' 'Cut provided to distribution of interest was comprised of ' 'multiple tensors, but ` {} ` is only defined for cuts comprised ' 'of a single tensor (received a list of {} tensors). \\n ' ' \\n ' 'Either (1) select a slice where the `to_cut` corresponds to a ' 'single tensor, or (2) implement/use a `DoI` object that ' 'supports lists of tensors, i.e., where the parameter, `z`, to ' '`__call__` is expected/allowed to be a list of {} tensors.' . format ( self . __class__ . __name__ , len ( x ), len ( x )) ) elif not ( isinstance ( x , np . ndarray ) or get_backend () . is_tensor ( x )): raise ValueError ( '` {} ` expected to receive an instance of `Tensor` or ' '`np.ndarray`, but received an instance of {} ' . format ( self . __class__ . __name__ , type ( x ) ) )","title":"DoI"},{"location":"api/distributions/#trulens.nn.distributions.DoI.__call__","text":"Computes the distribution of interest from an initial point. If z: TensorLike is given, we assume there is only 1 input to the DoI layer. If z: List[TensorLike] is given, it provides all of the inputs to the DoI layer. Either way, we always return List[List[TensorLike]] (alias Inputs[Uniform[TensorLike]]) with outer list spanning layer inputs, and inner list spanning a distribution's instance. Parameters: Name Type Description Default z OM[Inputs, TensorLike] Input point from which the distribution is derived. If list/tuple, the point is defined by multiple tensors. required model_inputs Optional[ModelInputs] Optional wrapped model input arguments that produce value z at cut. None Returns: Type Description OM[Inputs, Uniform[TensorLike]] List of points which are all assigned equal probability mass in the distribution of interest, i.e., the distribution of interest is a discrete, uniform distribution over the list of returned points. If z is multi-input, returns a distribution for each input. Source code in trulens/nn/distributions.py @abstractmethod def __call__ ( self , z : OM [ Inputs , TensorLike ], * , model_inputs : Optional [ ModelInputs ] = None ) -> OM [ Inputs , Uniform [ TensorLike ]]: \"\"\" Computes the distribution of interest from an initial point. If z: TensorLike is given, we assume there is only 1 input to the DoI layer. If z: List[TensorLike] is given, it provides all of the inputs to the DoI layer. Either way, we always return List[List[TensorLike]] (alias Inputs[Uniform[TensorLike]]) with outer list spanning layer inputs, and inner list spanning a distribution's instance. Parameters: z: Input point from which the distribution is derived. If list/tuple, the point is defined by multiple tensors. model_inputs: Optional wrapped model input arguments that produce value z at cut. Returns: List of points which are all assigned equal probability mass in the distribution of interest, i.e., the distribution of interest is a discrete, uniform distribution over the list of returned points. If z is multi-input, returns a distribution for each input. \"\"\" raise NotImplementedError","title":"__call__()"},{"location":"api/distributions/#trulens.nn.distributions.DoI.__init__","text":"\"Initialize DoI Parameters: Name Type Description Default cut Cut The Cut in which the DoI will be applied. If None , the DoI will be applied to the input. otherwise, the distribution should be applied to the latent space defined by the cut. None Source code in trulens/nn/distributions.py def __init__ ( self , cut : Cut = None ): \"\"\"\"Initialize DoI Parameters: cut (Cut, optional): The Cut in which the DoI will be applied. If `None`, the DoI will be applied to the input. otherwise, the distribution should be applied to the latent space defined by the cut. \"\"\" self . _cut = cut","title":"__init__()"},{"location":"api/distributions/#trulens.nn.distributions.DoI.cut","text":"Returns: Type Description Cut The Cut in which the DoI will be applied. If None , the DoI will be applied to the input. otherwise, the distribution should be applied to the latent space defined by the cut. Source code in trulens/nn/distributions.py def cut ( self ) -> Cut : \"\"\" Returns: The Cut in which the DoI will be applied. If `None`, the DoI will be applied to the input. otherwise, the distribution should be applied to the latent space defined by the cut. \"\"\" return self . _cut","title":"cut()"},{"location":"api/distributions/#trulens.nn.distributions.DoI.get_activation_multiplier","text":"Returns a term to multiply the gradient by to convert from \" influence space \" to \" attribution space \". Conceptually, \"influence space\" corresponds to the potential effect of a slight increase in each feature, while \"attribution space\" corresponds to an approximation of the net marginal contribution to the quantity of interest of each feature. Parameters: Name Type Description Default activation OM[Inputs, TensorLike] The activation of the layer the DoI is applied to. DoI may be multi-input in which case activation will be a list. required model_inputs Optional[ModelInputs] Optional wrapped model input arguments that produce activation at cut. None Returns: Type Description OM[Inputs, TensorLike] An array with the same shape as activation that will be multiplied by the gradient to obtain the attribution. The default implementation of this method simply returns activation . If activation is multi-input, returns one multiplier for each. Source code in trulens/nn/distributions.py def get_activation_multiplier ( self , activation : OM [ Inputs , TensorLike ], * , model_inputs : Optional [ ModelInputs ] = None ) -> OM [ Inputs , TensorLike ]: \"\"\" Returns a term to multiply the gradient by to convert from \"*influence space*\" to \"*attribution space*\". Conceptually, \"influence space\" corresponds to the potential effect of a slight increase in each feature, while \"attribution space\" corresponds to an approximation of the net marginal contribution to the quantity of interest of each feature. Parameters: activation: The activation of the layer the DoI is applied to. DoI may be multi-input in which case activation will be a list. model_inputs: Optional wrapped model input arguments that produce activation at cut. Returns: An array with the same shape as ``activation`` that will be multiplied by the gradient to obtain the attribution. The default implementation of this method simply returns ``activation``. If activation is multi-input, returns one multiplier for each. \"\"\" return om_of_many ( activation )","title":"get_activation_multiplier()"},{"location":"api/distributions/#trulens.nn.distributions.DoiCutSupportError","text":"Exception raised if the distribution of interest is called on a cut whose output is not supported by the distribution of interest. Source code in trulens/nn/distributions.py class DoiCutSupportError ( ValueError ): \"\"\" Exception raised if the distribution of interest is called on a cut whose output is not supported by the distribution of interest. \"\"\" pass","title":"DoiCutSupportError"},{"location":"api/distributions/#trulens.nn.distributions.GaussianDoi","text":"Distribution representing a Gaussian ball around the point. Used by Smooth Gradients. Source code in trulens/nn/distributions.py class GaussianDoi ( DoI ): \"\"\" Distribution representing a Gaussian ball around the point. Used by Smooth Gradients. \"\"\" def __init__ ( self , var : float , resolution : int , cut : Cut = None ): \"\"\" Parameters: var: The variance of the Gaussian noise to be added around the point. resolution: Number of samples returned by each call to this DoI. cut (Cut, optional): The Cut in which the DoI will be applied. If `None`, the DoI will be applied to the input. otherwise, the distribution should be applied to the latent space defined by the cut. \"\"\" super ( GaussianDoi , self ) . __init__ ( cut ) self . _var = var self . _resolution = resolution def __call__ ( self , z : OM [ Inputs , TensorLike ]) -> OM [ Inputs , Uniform [ TensorLike ]]: # Public interface. B = get_backend () self . _assert_cut_contains_only_one_tensor ( z ) def gauss_of_input ( z : TensorLike ) -> Uniform [ TensorLike ]: # TODO: make a pytorch backend with the same interface to use in places like these. if B . is_tensor ( z ): # Tensor implementation. return [ z + B . random_normal_like ( z , var = self . _var ) for _ in range ( self . _resolution ) ] # Uniform else : # Array implementation. return [ z + np . random . normal ( 0. , np . sqrt ( self . _var ), z . shape ) for _ in range ( self . _resolution ) ] # Uniform z : Inputs [ TensorLike ] = many_of_om ( z ) return om_of_many ( list ( map ( gauss_of_input , z )))","title":"GaussianDoi"},{"location":"api/distributions/#trulens.nn.distributions.GaussianDoi.__init__","text":"Parameters: Name Type Description Default var float The variance of the Gaussian noise to be added around the point. required resolution int Number of samples returned by each call to this DoI. required cut Cut The Cut in which the DoI will be applied. If None , the DoI will be applied to the input. otherwise, the distribution should be applied to the latent space defined by the cut. None Source code in trulens/nn/distributions.py def __init__ ( self , var : float , resolution : int , cut : Cut = None ): \"\"\" Parameters: var: The variance of the Gaussian noise to be added around the point. resolution: Number of samples returned by each call to this DoI. cut (Cut, optional): The Cut in which the DoI will be applied. If `None`, the DoI will be applied to the input. otherwise, the distribution should be applied to the latent space defined by the cut. \"\"\" super ( GaussianDoi , self ) . __init__ ( cut ) self . _var = var self . _resolution = resolution","title":"__init__()"},{"location":"api/distributions/#trulens.nn.distributions.LinearDoi","text":"Distribution representing the linear interpolation between a baseline and the given point. Used by Integrated Gradients. Source code in trulens/nn/distributions.py class LinearDoi ( DoI ): \"\"\" Distribution representing the linear interpolation between a baseline and the given point. Used by Integrated Gradients. \"\"\" def __init__ ( self , baseline : BaselineLike = None , resolution : int = 10 , * , cut : Cut = None , ): \"\"\" The DoI for point, `z`, will be a uniform distribution over the points on the line segment connecting `z` to `baseline`, approximated by a sample of `resolution` points equally spaced along this segment. Parameters: cut (Cut, optional, from DoI): The Cut in which the DoI will be applied. If `None`, the DoI will be applied to the input. otherwise, the distribution should be applied to the latent space defined by the cut. baseline (BaselineLike, optional): The baseline to interpolate from. Must be same shape as the space the distribution acts over, i.e., the shape of the points, `z`, eventually passed to `__call__`. If `cut` is `None`, this must be the same shape as the input, otherwise this must be the same shape as the latent space defined by the cut. If `None` is given, `baseline` will be the zero vector in the appropriate shape. If the baseline is callable, it is expected to return the `baseline`, given `z` and optional model arguments. resolution (int): Number of points returned by each call to this DoI. A higher resolution is more computationally expensive, but gives a better approximation of the DoI this object mathematically represents. \"\"\" super ( LinearDoi , self ) . __init__ ( cut ) self . _baseline = baseline self . _resolution = resolution @property def baseline ( self ) -> BaselineLike : return self . _baseline @property def resolution ( self ) -> int : return self . _resolution def __call__ ( self , z : OM [ Inputs , TensorLike ], * , model_inputs : Optional [ ModelInputs ] = None ) -> OM [ Inputs , Uniform [ TensorLike ]]: self . _assert_cut_contains_only_one_tensor ( z ) z : Inputs [ TensorLike ] = many_of_om ( z ) baseline = self . _compute_baseline ( z , model_inputs = model_inputs ) r = 1. if self . _resolution == 1 else self . _resolution - 1. return om_of_many ([ # Inputs [ # Uniform ( 1. - i / r ) * z_ + i / r * b_ for i in range ( self . _resolution ) ] for z_ , b_ in zip ( z , baseline ) ]) def get_activation_multiplier ( self , activation : OM [ Inputs , TensorLike ], * , model_inputs : Optional [ ModelInputs ] = None ) -> Inputs [ TensorLike ]: \"\"\" Returns a term to multiply the gradient by to convert from \"*influence space*\" to \"*attribution space*\". Conceptually, \"influence space\" corresponds to the potential effect of a slight increase in each feature, while \"attribution space\" corresponds to an approximation of the net marginal contribution to the quantity of interest of each feature. Parameters: activation: The activation of the layer the DoI is applied to. Returns: The activation adjusted by the baseline passed to the constructor. \"\"\" activation : Inputs [ TensorLike ] = many_of_om ( activation ) baseline : Inputs [ TensorLike ] = self . _compute_baseline ( activation , model_inputs = model_inputs ) if baseline is None : return activation return [ a - b for a , b in zip ( activation , baseline )] def _compute_baseline ( self , z : Inputs [ TensorLike ], * , model_inputs : Optional [ ModelInputs ] = None ) -> Inputs [ TensorLike ]: B = get_backend () _baseline : BaselineLike = self . baseline # user-provided if isinstance ( _baseline , Callable ): if accepts_model_inputs ( _baseline ): _baseline : OM [ Inputs , TensorLike ] = many_of_om ( _baseline ( om_of_many ( z ), model_inputs = model_inputs ) ) else : _baseline : OM [ Inputs , TensorLike ] = many_of_om ( _baseline ( om_of_many ( z )) ) else : _baseline : OM [ Inputs , TensorLike ] if _baseline is None : _baseline : Inputs [ TensorLike ] = [ B . zeros_like ( z_ ) for z_ in z ] else : _baseline : Inputs [ TensorLike ] = many_of_om ( _baseline ) # Came from user; could have been single or multiple inputs. # Cast to either Tensor or numpy.ndarray to match what was given in z. return nested_cast ( backend = B , args = _baseline , astype = type ( z [ 0 ]))","title":"LinearDoi"},{"location":"api/distributions/#trulens.nn.distributions.LinearDoi.__init__","text":"The DoI for point, z , will be a uniform distribution over the points on the line segment connecting z to baseline , approximated by a sample of resolution points equally spaced along this segment. Parameters: Name Type Description Default cut Cut, optional, from DoI The Cut in which the DoI will be applied. If None , the DoI will be applied to the input. otherwise, the distribution should be applied to the latent space defined by the cut. None baseline BaselineLike The baseline to interpolate from. Must be same shape as the space the distribution acts over, i.e., the shape of the points, z , eventually passed to __call__ . If cut is None , this must be the same shape as the input, otherwise this must be the same shape as the latent space defined by the cut. If None is given, baseline will be the zero vector in the appropriate shape. If the baseline is callable, it is expected to return the baseline , given z and optional model arguments. None resolution int Number of points returned by each call to this DoI. A higher resolution is more computationally expensive, but gives a better approximation of the DoI this object mathematically represents. 10 Source code in trulens/nn/distributions.py def __init__ ( self , baseline : BaselineLike = None , resolution : int = 10 , * , cut : Cut = None , ): \"\"\" The DoI for point, `z`, will be a uniform distribution over the points on the line segment connecting `z` to `baseline`, approximated by a sample of `resolution` points equally spaced along this segment. Parameters: cut (Cut, optional, from DoI): The Cut in which the DoI will be applied. If `None`, the DoI will be applied to the input. otherwise, the distribution should be applied to the latent space defined by the cut. baseline (BaselineLike, optional): The baseline to interpolate from. Must be same shape as the space the distribution acts over, i.e., the shape of the points, `z`, eventually passed to `__call__`. If `cut` is `None`, this must be the same shape as the input, otherwise this must be the same shape as the latent space defined by the cut. If `None` is given, `baseline` will be the zero vector in the appropriate shape. If the baseline is callable, it is expected to return the `baseline`, given `z` and optional model arguments. resolution (int): Number of points returned by each call to this DoI. A higher resolution is more computationally expensive, but gives a better approximation of the DoI this object mathematically represents. \"\"\" super ( LinearDoi , self ) . __init__ ( cut ) self . _baseline = baseline self . _resolution = resolution","title":"__init__()"},{"location":"api/distributions/#trulens.nn.distributions.LinearDoi.get_activation_multiplier","text":"Returns a term to multiply the gradient by to convert from \" influence space \" to \" attribution space \". Conceptually, \"influence space\" corresponds to the potential effect of a slight increase in each feature, while \"attribution space\" corresponds to an approximation of the net marginal contribution to the quantity of interest of each feature. Parameters: Name Type Description Default activation OM[Inputs, TensorLike] The activation of the layer the DoI is applied to. required Returns: Type Description Inputs[TensorLike] The activation adjusted by the baseline passed to the constructor. Source code in trulens/nn/distributions.py def get_activation_multiplier ( self , activation : OM [ Inputs , TensorLike ], * , model_inputs : Optional [ ModelInputs ] = None ) -> Inputs [ TensorLike ]: \"\"\" Returns a term to multiply the gradient by to convert from \"*influence space*\" to \"*attribution space*\". Conceptually, \"influence space\" corresponds to the potential effect of a slight increase in each feature, while \"attribution space\" corresponds to an approximation of the net marginal contribution to the quantity of interest of each feature. Parameters: activation: The activation of the layer the DoI is applied to. Returns: The activation adjusted by the baseline passed to the constructor. \"\"\" activation : Inputs [ TensorLike ] = many_of_om ( activation ) baseline : Inputs [ TensorLike ] = self . _compute_baseline ( activation , model_inputs = model_inputs ) if baseline is None : return activation return [ a - b for a , b in zip ( activation , baseline )]","title":"get_activation_multiplier()"},{"location":"api/distributions/#trulens.nn.distributions.PointDoi","text":"Distribution that puts all probability mass on a single point. Source code in trulens/nn/distributions.py class PointDoi ( DoI ): \"\"\" Distribution that puts all probability mass on a single point. \"\"\" def __init__ ( self , cut : Cut = None ): \"\"\"\"Initialize PointDoI Parameters: cut (Cut, optional): The Cut in which the DoI will be applied. If `None`, the DoI will be applied to the input. otherwise, the distribution should be applied to the latent space defined by the cut. \"\"\" super ( PointDoi , self ) . __init__ ( cut ) def __call__ ( self , z : OM [ Inputs , TensorLike ], * , model_inputs : Optional [ ModelInputs ] = None ) -> OM [ Inputs , Uniform [ TensorLike ]]: z : Inputs [ TensorLike ] = many_of_om ( z ) return om_of_many ([ [ z_ ] # a point Uniform for z_ in z ])","title":"PointDoi"},{"location":"api/distributions/#trulens.nn.distributions.PointDoi.__init__","text":"\"Initialize PointDoI Parameters: Name Type Description Default cut Cut The Cut in which the DoI will be applied. If None , the DoI will be applied to the input. otherwise, the distribution should be applied to the latent space defined by the cut. None Source code in trulens/nn/distributions.py def __init__ ( self , cut : Cut = None ): \"\"\"\"Initialize PointDoI Parameters: cut (Cut, optional): The Cut in which the DoI will be applied. If `None`, the DoI will be applied to the input. otherwise, the distribution should be applied to the latent space defined by the cut. \"\"\" super ( PointDoi , self ) . __init__ ( cut )","title":"__init__()"},{"location":"api/model_wrappers/","text":"Model Wrappers \u00b6 The TruLens library is designed to support models implemented via a variety of different popular python neural network frameworks: Keras (with TensorFlow or Theano backend), TensorFlow, and Pytorch. Models developed with different frameworks implement things (e.g., gradient computations) a number of different ways. We define framework specific ModelWrapper instances to create a unified model API, providing the same functionality to models that are implemented in disparate frameworks. In order to compute attributions for a model, we provide a trulens.nn.models.get_model_wrapper function that will return an appropriate ModelWrapper instance. Some parameters are exclusively utilized for specific frameworks and are outlined in the parameter descriptions. get_model_wrapper ( model , * , logit_layer = None , replace_softmax = False , softmax_layer =- 1 , custom_objects = None , device = None , input_tensors = None , output_tensors = None , internal_tensor_dict = None , default_feed_dict = None , session = None , backend = None , force_eval = True , ** kwargs ) \u00b6 Returns a ModelWrapper implementation that exposes the components needed for computing attributions. Parameters: Name Type Description Default model Union[tf.Graph, keras.Model, tensorflow.keras.Model, torch.nn.Module] The model to wrap. If using the TensorFlow 1 backend, this is expected to be a graph object. required logit_layer Supported for Keras and Pytorch models. Specifies the name or index of the layer that produces the logit predictions. None replace_softmax bool Supported for Keras models only. If true, the activation function in the softmax layer (specified by softmax_layer ) will be changed to a 'linear' activation. False softmax_layer Supported for Keras models only. Specifies the layer that performs the softmax. This layer should have an activation attribute. Only used when replace_softmax is true. -1 custom_objects Optional, for use with Keras models only. A dictionary of custom objects used by the Keras model. None device str Optional, for use with Pytorch models only. A string specifying the device to run the model on. None input_tensors Required for use with TensorFlow 1 graph models only. A list of tensors representing the input to the model graph. None output_tensors Required for use with TensorFlow 1 graph models only. A list of tensors representing the output to the model graph. None internal_tensor_dict Optional, for use with TensorFlow 1 graph models only. A dictionary mapping user-selected layer names to the internal tensors in the model graph that the user would like to expose. This is provided to give more human-readable names to the layers if desired. Internal tensors can also be accessed via the name given to them by tensorflow. None default_feed_dict Optional, for use with TensorFlow 1 graph models only. A dictionary of default values to give to tensors in the model graph. None session Optional, for use with TensorFlow 1 graph models only. A tf.Session object to run the model graph in. If None , a new temporary session will be generated every time the model is run. None backend Optional, for forcing a specific backend. String values recognized are pytorch, tensorflow, keras, or tf.keras. None force_eval _Optional, True will force a model.eval() call for PyTorch models. False will retain current model state True Returns: ModelWrapper Source code in trulens/nn/models/__init__.py def get_model_wrapper ( model : ModelLike , * , logit_layer = None , replace_softmax : bool = False , softmax_layer =- 1 , custom_objects = None , device : str = None , input_tensors = None , output_tensors = None , internal_tensor_dict = None , default_feed_dict = None , session = None , backend = None , force_eval = True , ** kwargs ): \"\"\" Returns a ModelWrapper implementation that exposes the components needed for computing attributions. Parameters: model: The model to wrap. If using the TensorFlow 1 backend, this is expected to be a graph object. logit_layer: _Supported for Keras and Pytorch models._ Specifies the name or index of the layer that produces the logit predictions. replace_softmax: _Supported for Keras models only._ If true, the activation function in the softmax layer (specified by `softmax_layer`) will be changed to a `'linear'` activation. softmax_layer: _Supported for Keras models only._ Specifies the layer that performs the softmax. This layer should have an `activation` attribute. Only used when `replace_softmax` is true. custom_objects: _Optional, for use with Keras models only._ A dictionary of custom objects used by the Keras model. device: _Optional, for use with Pytorch models only._ A string specifying the device to run the model on. input_tensors: _Required for use with TensorFlow 1 graph models only._ A list of tensors representing the input to the model graph. output_tensors: _Required for use with TensorFlow 1 graph models only._ A list of tensors representing the output to the model graph. internal_tensor_dict: _Optional, for use with TensorFlow 1 graph models only._ A dictionary mapping user-selected layer names to the internal tensors in the model graph that the user would like to expose. This is provided to give more human-readable names to the layers if desired. Internal tensors can also be accessed via the name given to them by tensorflow. default_feed_dict: _Optional, for use with TensorFlow 1 graph models only._ A dictionary of default values to give to tensors in the model graph. session: _Optional, for use with TensorFlow 1 graph models only._ A `tf.Session` object to run the model graph in. If `None`, a new temporary session will be generated every time the model is run. backend: _Optional, for forcing a specific backend._ String values recognized are pytorch, tensorflow, keras, or tf.keras. force_eval: _Optional, True will force a model.eval() call for PyTorch models. False will retain current model state Returns: ModelWrapper \"\"\" if 'input_shape' in kwargs : tru_logger . deprecate ( f \"get_model_wrapper: input_shape parameter is no longer used and will be removed in the future\" ) del kwargs [ 'input_shape' ] if 'input_dtype' in kwargs : tru_logger . deprecate ( f \"get_model_wrapper: input_dtype parameter is no longer used and will be removed in the future\" ) del kwargs [ 'input_dtype' ] # get existing backend B = get_backend ( suppress_warnings = True ) if backend is None : backend = discern_backend ( model ) tru_logger . info ( \"Detected {} backend for {} .\" . format ( backend . name . lower (), type ( model ) ) ) else : backend = Backend . from_name ( backend ) if B is None or ( backend is not Backend . UNKNOWN and B . backend != backend ): tru_logger . info ( \"Changing backend from {} to {} .\" . format ( None if B is None else B . backend , backend ) ) os . environ [ 'TRULENS_BACKEND' ] = backend . name . lower () B = get_backend () else : tru_logger . info ( \"Using backend {} .\" . format ( B . backend )) tru_logger . info ( \"If this seems incorrect, you can force the correct backend by passing the `backend` parameter directly into your get_model_wrapper call.\" ) if B . backend . is_keras_derivative (): from trulens.nn.models.keras import KerasModelWrapper return KerasModelWrapper ( model , logit_layer = logit_layer , replace_softmax = replace_softmax , softmax_layer = softmax_layer , custom_objects = custom_objects ) elif B . backend == Backend . PYTORCH : from trulens.nn.models.pytorch import PytorchModelWrapper return PytorchModelWrapper ( model , logit_layer = logit_layer , device = device , force_eval = force_eval ) elif B . backend == Backend . TENSORFLOW : import tensorflow as tf if tf . __version__ . startswith ( '2' ): from trulens.nn.models.tensorflow_v2 import Tensorflow2ModelWrapper return Tensorflow2ModelWrapper ( model , logit_layer = logit_layer , replace_softmax = replace_softmax , softmax_layer = softmax_layer , custom_objects = custom_objects ) else : from trulens.nn.models.tensorflow_v1 import TensorflowModelWrapper if input_tensors is None : tru_logger . error ( 'tensorflow1 model must pass parameter: input_tensors' ) if output_tensors is None : tru_logger . error ( 'tensorflow1 model must pass parameter: output_tensors' ) return TensorflowModelWrapper ( model , input_tensors = input_tensors , output_tensors = output_tensors , internal_tensor_dict = internal_tensor_dict , session = session )","title":"Models"},{"location":"api/model_wrappers/#model-wrappers","text":"The TruLens library is designed to support models implemented via a variety of different popular python neural network frameworks: Keras (with TensorFlow or Theano backend), TensorFlow, and Pytorch. Models developed with different frameworks implement things (e.g., gradient computations) a number of different ways. We define framework specific ModelWrapper instances to create a unified model API, providing the same functionality to models that are implemented in disparate frameworks. In order to compute attributions for a model, we provide a trulens.nn.models.get_model_wrapper function that will return an appropriate ModelWrapper instance. Some parameters are exclusively utilized for specific frameworks and are outlined in the parameter descriptions.","title":"Model Wrappers"},{"location":"api/model_wrappers/#trulens.nn.models.__init__.get_model_wrapper","text":"Returns a ModelWrapper implementation that exposes the components needed for computing attributions. Parameters: Name Type Description Default model Union[tf.Graph, keras.Model, tensorflow.keras.Model, torch.nn.Module] The model to wrap. If using the TensorFlow 1 backend, this is expected to be a graph object. required logit_layer Supported for Keras and Pytorch models. Specifies the name or index of the layer that produces the logit predictions. None replace_softmax bool Supported for Keras models only. If true, the activation function in the softmax layer (specified by softmax_layer ) will be changed to a 'linear' activation. False softmax_layer Supported for Keras models only. Specifies the layer that performs the softmax. This layer should have an activation attribute. Only used when replace_softmax is true. -1 custom_objects Optional, for use with Keras models only. A dictionary of custom objects used by the Keras model. None device str Optional, for use with Pytorch models only. A string specifying the device to run the model on. None input_tensors Required for use with TensorFlow 1 graph models only. A list of tensors representing the input to the model graph. None output_tensors Required for use with TensorFlow 1 graph models only. A list of tensors representing the output to the model graph. None internal_tensor_dict Optional, for use with TensorFlow 1 graph models only. A dictionary mapping user-selected layer names to the internal tensors in the model graph that the user would like to expose. This is provided to give more human-readable names to the layers if desired. Internal tensors can also be accessed via the name given to them by tensorflow. None default_feed_dict Optional, for use with TensorFlow 1 graph models only. A dictionary of default values to give to tensors in the model graph. None session Optional, for use with TensorFlow 1 graph models only. A tf.Session object to run the model graph in. If None , a new temporary session will be generated every time the model is run. None backend Optional, for forcing a specific backend. String values recognized are pytorch, tensorflow, keras, or tf.keras. None force_eval _Optional, True will force a model.eval() call for PyTorch models. False will retain current model state True Returns: ModelWrapper Source code in trulens/nn/models/__init__.py def get_model_wrapper ( model : ModelLike , * , logit_layer = None , replace_softmax : bool = False , softmax_layer =- 1 , custom_objects = None , device : str = None , input_tensors = None , output_tensors = None , internal_tensor_dict = None , default_feed_dict = None , session = None , backend = None , force_eval = True , ** kwargs ): \"\"\" Returns a ModelWrapper implementation that exposes the components needed for computing attributions. Parameters: model: The model to wrap. If using the TensorFlow 1 backend, this is expected to be a graph object. logit_layer: _Supported for Keras and Pytorch models._ Specifies the name or index of the layer that produces the logit predictions. replace_softmax: _Supported for Keras models only._ If true, the activation function in the softmax layer (specified by `softmax_layer`) will be changed to a `'linear'` activation. softmax_layer: _Supported for Keras models only._ Specifies the layer that performs the softmax. This layer should have an `activation` attribute. Only used when `replace_softmax` is true. custom_objects: _Optional, for use with Keras models only._ A dictionary of custom objects used by the Keras model. device: _Optional, for use with Pytorch models only._ A string specifying the device to run the model on. input_tensors: _Required for use with TensorFlow 1 graph models only._ A list of tensors representing the input to the model graph. output_tensors: _Required for use with TensorFlow 1 graph models only._ A list of tensors representing the output to the model graph. internal_tensor_dict: _Optional, for use with TensorFlow 1 graph models only._ A dictionary mapping user-selected layer names to the internal tensors in the model graph that the user would like to expose. This is provided to give more human-readable names to the layers if desired. Internal tensors can also be accessed via the name given to them by tensorflow. default_feed_dict: _Optional, for use with TensorFlow 1 graph models only._ A dictionary of default values to give to tensors in the model graph. session: _Optional, for use with TensorFlow 1 graph models only._ A `tf.Session` object to run the model graph in. If `None`, a new temporary session will be generated every time the model is run. backend: _Optional, for forcing a specific backend._ String values recognized are pytorch, tensorflow, keras, or tf.keras. force_eval: _Optional, True will force a model.eval() call for PyTorch models. False will retain current model state Returns: ModelWrapper \"\"\" if 'input_shape' in kwargs : tru_logger . deprecate ( f \"get_model_wrapper: input_shape parameter is no longer used and will be removed in the future\" ) del kwargs [ 'input_shape' ] if 'input_dtype' in kwargs : tru_logger . deprecate ( f \"get_model_wrapper: input_dtype parameter is no longer used and will be removed in the future\" ) del kwargs [ 'input_dtype' ] # get existing backend B = get_backend ( suppress_warnings = True ) if backend is None : backend = discern_backend ( model ) tru_logger . info ( \"Detected {} backend for {} .\" . format ( backend . name . lower (), type ( model ) ) ) else : backend = Backend . from_name ( backend ) if B is None or ( backend is not Backend . UNKNOWN and B . backend != backend ): tru_logger . info ( \"Changing backend from {} to {} .\" . format ( None if B is None else B . backend , backend ) ) os . environ [ 'TRULENS_BACKEND' ] = backend . name . lower () B = get_backend () else : tru_logger . info ( \"Using backend {} .\" . format ( B . backend )) tru_logger . info ( \"If this seems incorrect, you can force the correct backend by passing the `backend` parameter directly into your get_model_wrapper call.\" ) if B . backend . is_keras_derivative (): from trulens.nn.models.keras import KerasModelWrapper return KerasModelWrapper ( model , logit_layer = logit_layer , replace_softmax = replace_softmax , softmax_layer = softmax_layer , custom_objects = custom_objects ) elif B . backend == Backend . PYTORCH : from trulens.nn.models.pytorch import PytorchModelWrapper return PytorchModelWrapper ( model , logit_layer = logit_layer , device = device , force_eval = force_eval ) elif B . backend == Backend . TENSORFLOW : import tensorflow as tf if tf . __version__ . startswith ( '2' ): from trulens.nn.models.tensorflow_v2 import Tensorflow2ModelWrapper return Tensorflow2ModelWrapper ( model , logit_layer = logit_layer , replace_softmax = replace_softmax , softmax_layer = softmax_layer , custom_objects = custom_objects ) else : from trulens.nn.models.tensorflow_v1 import TensorflowModelWrapper if input_tensors is None : tru_logger . error ( 'tensorflow1 model must pass parameter: input_tensors' ) if output_tensors is None : tru_logger . error ( 'tensorflow1 model must pass parameter: output_tensors' ) return TensorflowModelWrapper ( model , input_tensors = input_tensors , output_tensors = output_tensors , internal_tensor_dict = internal_tensor_dict , session = session )","title":"get_model_wrapper()"},{"location":"api/quantities/","text":"Quantities of Interest \u00b6 A Quantity of Interest (QoI) is a function of the output that determines the network output behavior that the attributions describe. The quantity of interest lets us specify what we want to explain. Often, this is the output of the network corresponding to a particular class, addressing, e.g., \"Why did the model classify a given image as a car?\" However, we could also consider various combinations of outputs, allowing us to ask more specific questions, such as, \"Why did the model classify a given image as a sedan and not a convertible ?\" The former may highlight general \u201ccar features,\u201d such as tires, while the latter (called a comparative explanation) might focus on the roof of the car, a \u201ccar feature\u201d not shared by convertibles. ClassQoI ( QoI ) \u00b6 Quantity of interest for attributing output towards a specified class. Source code in trulens/nn/quantities.py class ClassQoI ( QoI ): \"\"\" Quantity of interest for attributing output towards a specified class. \"\"\" def __init__ ( self , cl : int ): \"\"\" Parameters: cl: The index of the class the QoI is for. \"\"\" self . cl = cl def __call__ ( self , y : TensorLike ) -> TensorLike : self . _assert_cut_contains_only_one_tensor ( y ) return y [:, self . cl ] __init__ ( self , cl ) special \u00b6 Parameters: Name Type Description Default cl int The index of the class the QoI is for. required Source code in trulens/nn/quantities.py def __init__ ( self , cl : int ): \"\"\" Parameters: cl: The index of the class the QoI is for. \"\"\" self . cl = cl ClassSeqQoI ( QoI ) \u00b6 Quantity of interest for attributing output towards a sequence of classes for each input. Source code in trulens/nn/quantities.py class ClassSeqQoI ( QoI ): \"\"\" Quantity of interest for attributing output towards a sequence of classes for each input. \"\"\" def __init__ ( self , seq_labels : List [ int ]): \"\"\" Parameters: seq_labels: A sequence of classes corresponding to each input. \"\"\" self . seq_labels = seq_labels def __call__ ( self , y ): self . _assert_cut_contains_only_one_tensor ( y ) assert get_backend () . shape ( y )[ 0 ] == len ( self . seq_labels ) return y [:, self . seq_labels ] __init__ ( self , seq_labels ) special \u00b6 Parameters: Name Type Description Default seq_labels List[int] A sequence of classes corresponding to each input. required Source code in trulens/nn/quantities.py def __init__ ( self , seq_labels : List [ int ]): \"\"\" Parameters: seq_labels: A sequence of classes corresponding to each input. \"\"\" self . seq_labels = seq_labels ComparativeQoI ( QoI ) \u00b6 Quantity of interest for attributing network output towards a given class, relative to another. Source code in trulens/nn/quantities.py class ComparativeQoI ( QoI ): \"\"\" Quantity of interest for attributing network output towards a given class, relative to another. \"\"\" def __init__ ( self , cl1 : int , cl2 : int ): \"\"\" Parameters: cl1: The index of the class the QoI is for. cl2: The index of the class to compare against. \"\"\" self . cl1 = cl1 self . cl2 = cl2 def __call__ ( self , y : TensorLike ) -> TensorLike : self . _assert_cut_contains_only_one_tensor ( y ) return y [:, self . cl1 ] - y [:, self . cl2 ] __init__ ( self , cl1 , cl2 ) special \u00b6 Parameters: Name Type Description Default cl1 int The index of the class the QoI is for. required cl2 int The index of the class to compare against. required Source code in trulens/nn/quantities.py def __init__ ( self , cl1 : int , cl2 : int ): \"\"\" Parameters: cl1: The index of the class the QoI is for. cl2: The index of the class to compare against. \"\"\" self . cl1 = cl1 self . cl2 = cl2 InternalChannelQoI ( QoI ) \u00b6 Quantity of interest for attributing output towards the output of an internal convolutional layer channel, aggregating using a specified operation. Also works for non-convolutional dense layers, where the given neuron's activation is returned. Source code in trulens/nn/quantities.py class InternalChannelQoI ( QoI ): \"\"\" Quantity of interest for attributing output towards the output of an internal convolutional layer channel, aggregating using a specified operation. Also works for non-convolutional dense layers, where the given neuron's activation is returned. \"\"\" @staticmethod def _batch_sum ( x ): \"\"\" Sums batched 2D channels, leaving the batch dimension unchanged. \"\"\" return get_backend () . sum ( x , axis = ( 1 , 2 )) def __init__ ( self , channel : Union [ int , List [ int ]], channel_axis : Optional [ int ] = None , agg_fn : Optional [ Callable ] = None ): \"\"\" Parameters: channel: Channel to return. If a list is provided, then the quantity sums over each of the channels in the list. channel_axis: Channel dimension index, if relevant, e.g., for 2D convolutional layers. If `channel_axis` is `None`, then the channel axis of the relevant backend will be used. This argument is not used when the channels are scalars, e.g., for dense layers. agg_fn: Function with which to aggregate the remaining dimensions (except the batch dimension) in order to get a single scalar value for each channel. If `agg_fn` is `None` then a sum over each neuron in the channel will be taken. This argument is not used when the channels are scalars, e.g., for dense layers. \"\"\" if channel_axis is None : channel_axis = get_backend () . channel_axis if agg_fn is None : agg_fn = InternalChannelQoI . _batch_sum self . _channel_ax = channel_axis self . _agg_fn = agg_fn self . _channels = channel if isinstance ( channel , list ) else [ channel ] def __call__ ( self , y : TensorLike ) -> TensorLike : B = get_backend () self . _assert_cut_contains_only_one_tensor ( y ) if len ( B . int_shape ( y )) == 2 : return sum ([ y [:, ch ] for ch in self . _channels ]) elif len ( B . int_shape ( y )) == 3 : return sum ([ self . _agg_fn ( y [:, :, ch ]) for ch in self . _channel ]) elif len ( B . int_shape ( y )) == 4 : if self . _channel_ax == 1 : return sum ([ self . _agg_fn ( y [:, ch ]) for ch in self . _channels ]) elif self . _channel_ax == 3 : return sum ( [ self . _agg_fn ( y [:, :, :, ch ]) for ch in self . _channels ] ) else : raise ValueError ( 'Unsupported channel axis for convolutional layer: {} ' . format ( self . _channel_ax ) ) else : raise QoiCutSupportError ( 'Unsupported tensor rank for `InternalChannelQoI`: {} ' . format ( len ( B . int_shape ( y )) ) ) __init__ ( self , channel , channel_axis = None , agg_fn = None ) special \u00b6 Parameters: Name Type Description Default channel Union[int, List[int]] Channel to return. If a list is provided, then the quantity sums over each of the channels in the list. required channel_axis Optional[int] Channel dimension index, if relevant, e.g., for 2D convolutional layers. If channel_axis is None , then the channel axis of the relevant backend will be used. This argument is not used when the channels are scalars, e.g., for dense layers. None agg_fn Optional[Callable] Function with which to aggregate the remaining dimensions (except the batch dimension) in order to get a single scalar value for each channel. If agg_fn is None then a sum over each neuron in the channel will be taken. This argument is not used when the channels are scalars, e.g., for dense layers. None Source code in trulens/nn/quantities.py def __init__ ( self , channel : Union [ int , List [ int ]], channel_axis : Optional [ int ] = None , agg_fn : Optional [ Callable ] = None ): \"\"\" Parameters: channel: Channel to return. If a list is provided, then the quantity sums over each of the channels in the list. channel_axis: Channel dimension index, if relevant, e.g., for 2D convolutional layers. If `channel_axis` is `None`, then the channel axis of the relevant backend will be used. This argument is not used when the channels are scalars, e.g., for dense layers. agg_fn: Function with which to aggregate the remaining dimensions (except the batch dimension) in order to get a single scalar value for each channel. If `agg_fn` is `None` then a sum over each neuron in the channel will be taken. This argument is not used when the channels are scalars, e.g., for dense layers. \"\"\" if channel_axis is None : channel_axis = get_backend () . channel_axis if agg_fn is None : agg_fn = InternalChannelQoI . _batch_sum self . _channel_ax = channel_axis self . _agg_fn = agg_fn self . _channels = channel if isinstance ( channel , list ) else [ channel ] LambdaQoI ( QoI ) \u00b6 Generic quantity of interest allowing the user to specify a function of the model's output as the QoI. Source code in trulens/nn/quantities.py class LambdaQoI ( QoI ): \"\"\" Generic quantity of interest allowing the user to specify a function of the model's output as the QoI. \"\"\" def __init__ ( self , function : Callable ): \"\"\" Parameters: function: A callable that takes a single argument representing the model's tensor output and returns a differentiable batched scalar tensor representing the QoI. \"\"\" if len ( signature ( function ) . parameters ) != 1 : raise ValueError ( 'QoI function must take exactly 1 argument, but provided ' 'function takes {} arguments' . format ( len ( signature ( function ) . parameters ) ) ) self . function = function def __call__ ( self , y : TensorLike ) -> TensorLike : return self . function ( y ) __init__ ( self , function ) special \u00b6 Parameters: Name Type Description Default function Callable A callable that takes a single argument representing the model's tensor output and returns a differentiable batched scalar tensor representing the QoI. required Source code in trulens/nn/quantities.py def __init__ ( self , function : Callable ): \"\"\" Parameters: function: A callable that takes a single argument representing the model's tensor output and returns a differentiable batched scalar tensor representing the QoI. \"\"\" if len ( signature ( function ) . parameters ) != 1 : raise ValueError ( 'QoI function must take exactly 1 argument, but provided ' 'function takes {} arguments' . format ( len ( signature ( function ) . parameters ) ) ) self . function = function MaxClassQoI ( QoI ) \u00b6 Quantity of interest for attributing output towards the maximum-predicted class. Source code in trulens/nn/quantities.py class MaxClassQoI ( QoI ): \"\"\" Quantity of interest for attributing output towards the maximum-predicted class. \"\"\" def __init__ ( self , axis : int = 1 , activation : Union [ Callable , str , None ] = None ): \"\"\" Parameters: axis: Output dimension over which max operation is taken. activation: Activation function to be applied to the output before taking the max. If `activation` is a string, use the corresponding named activation function implemented by the backend. The following strings are currently supported as shorthands for the respective standard activation functions: - `'sigmoid'` - `'softmax'` If `activation` is `None`, no activation function is applied to the input. \"\"\" self . _axis = axis self . activation = activation def __call__ ( self , y : TensorLike ) -> TensorLike : self . _assert_cut_contains_only_one_tensor ( y ) if self . activation is not None : if isinstance ( self . activation , str ): self . activation = self . activation . lower () if self . activation in [ 'sigmoid' , 'softmax' ]: y = getattr ( get_backend (), self . activation )( y ) else : raise NotImplementedError ( 'This activation function is not currently supported ' 'by the backend' ) else : y = self . activation ( y ) return get_backend () . max ( y , axis = self . _axis ) __init__ ( self , axis = 1 , activation = None ) special \u00b6 Parameters: Name Type Description Default axis int Output dimension over which max operation is taken. 1 activation Union[Callable, str, None] Activation function to be applied to the output before taking the max. If activation is a string, use the corresponding named activation function implemented by the backend. The following strings are currently supported as shorthands for the respective standard activation functions: 'sigmoid' 'softmax' If activation is None , no activation function is applied to the input. None Source code in trulens/nn/quantities.py def __init__ ( self , axis : int = 1 , activation : Union [ Callable , str , None ] = None ): \"\"\" Parameters: axis: Output dimension over which max operation is taken. activation: Activation function to be applied to the output before taking the max. If `activation` is a string, use the corresponding named activation function implemented by the backend. The following strings are currently supported as shorthands for the respective standard activation functions: - `'sigmoid'` - `'softmax'` If `activation` is `None`, no activation function is applied to the input. \"\"\" self . _axis = axis self . activation = activation QoI ( ABC ) \u00b6 Interface for quantities of interest. The Quantity of Interest (QoI) is a function of the output specified by the slice that determines the network output behavior that the attributions describe. Source code in trulens/nn/quantities.py class QoI ( AbstractBaseClass ): \"\"\" Interface for quantities of interest. The *Quantity of Interest* (QoI) is a function of the output specified by the slice that determines the network output behavior that the attributions describe. \"\"\" def _wrap_public_call ( self , y : Outputs [ Tensor ]) -> Outputs [ Tensor ]: \"\"\" Wrap a public call that may result one or more tensors. Signature of this class is not specific while public calls are flexible. \"\"\" return many_of_om ( self . __call__ ( om_of_many ( y ))) @abstractmethod def __call__ ( self , y : OM [ Outputs , Tensor ]) -> OM [ Outputs , Tensor ]: \"\"\" Computes the distribution of interest from an initial point. Parameters: y: Output point from which the quantity is derived. Must be a differentiable tensor. Returns: A differentiable batched scalar tensor representing the QoI. \"\"\" raise NotImplementedError def _assert_cut_contains_only_one_tensor ( self , x ): if isinstance ( x , DATA_CONTAINER_TYPE ): raise QoiCutSupportError ( 'Cut provided to quantity of interest was comprised of ' 'multiple tensors, but ` {} ` is only defined for cuts comprised ' 'of a single tensor (received a list of {} tensors). \\n ' ' \\n ' 'Either (1) select a slice where the `to_cut` corresponds to a ' 'single tensor, or (2) implement/use a `QoI` object that ' 'supports lists of tensors, i.e., where the parameter, `x`, to ' '`__call__` is expected/allowed to be a list of {} tensors.' . format ( self . __class__ . __name__ , len ( x ), len ( x )) ) elif not get_backend () . is_tensor ( x ): raise ValueError ( '` {} ` expected to receive an instance of `Tensor`, but ' 'received an instance of {} ' . format ( self . __class__ . __name__ , type ( x ) ) ) __call__ ( self , y ) special \u00b6 Computes the distribution of interest from an initial point. Parameters: Name Type Description Default y OM[Outputs, Tensor] Output point from which the quantity is derived. Must be a differentiable tensor. required Returns: Type Description OM[Outputs, Tensor] A differentiable batched scalar tensor representing the QoI. Source code in trulens/nn/quantities.py @abstractmethod def __call__ ( self , y : OM [ Outputs , Tensor ]) -> OM [ Outputs , Tensor ]: \"\"\" Computes the distribution of interest from an initial point. Parameters: y: Output point from which the quantity is derived. Must be a differentiable tensor. Returns: A differentiable batched scalar tensor representing the QoI. \"\"\" raise NotImplementedError QoiCutSupportError ( ValueError ) \u00b6 Exception raised if the quantity of interest is called on a cut whose output is not supported by the quantity of interest. Source code in trulens/nn/quantities.py class QoiCutSupportError ( ValueError ): \"\"\" Exception raised if the quantity of interest is called on a cut whose output is not supported by the quantity of interest. \"\"\" pass ThresholdQoI ( QoI ) \u00b6 Quantity of interest for attributing network output toward the difference between two regions seperated by a given threshold. I.e., the quantity of interest is the \"high\" elements minus the \"low\" elements, where the high elements have activations above the threshold and the low elements have activations below the threshold. Use case: bianry segmentation. Source code in trulens/nn/quantities.py class ThresholdQoI ( QoI ): \"\"\" Quantity of interest for attributing network output toward the difference between two regions seperated by a given threshold. I.e., the quantity of interest is the \"high\" elements minus the \"low\" elements, where the high elements have activations above the threshold and the low elements have activations below the threshold. Use case: bianry segmentation. \"\"\" def __init__ ( self , threshold : float , low_minus_high : bool = False , activation : Union [ Callable , str , None ] = None ): \"\"\" Parameters: threshold: A threshold to determine the element-wise sign of the input tensor. The elements with activations higher than the threshold will retain their sign, while the elements with activations lower than the threshold will have their sign flipped (or vice versa if `low_minus_high` is set to `True`). low_minus_high: If `True`, substract the output with activations above the threshold from the output with activations below the threshold. If `False`, substract the output with activations below the threshold from the output with activations above the threshold. activation: str or function, optional Activation function to be applied to the quantity before taking the threshold. If `activation` is a string, use the corresponding activation function implemented by the backend (currently supported: `'sigmoid'` and `'softmax'`). Otherwise, if `activation` is not `None`, it will be treated as a callable. If `activation` is `None`, do not apply an activation function to the quantity. \"\"\" # TODO(klas):should this support an aggregation function? By default # this is a sum, but it could, for example, subtract the greatest # positive element from the least negative element. self . threshold = threshold self . low_minus_high = low_minus_high self . activation = activation def __call__ ( self , x : TensorLike ) -> TensorLike : B = get_backend () self . _assert_cut_contains_only_one_tensor ( x ) if self . activation is not None : if isinstance ( self . activation , str ): self . activation = self . activation . lower () if self . activation in [ 'sigmoid' , 'softmax' ]: x = getattr ( B , self . activation )( x ) else : raise NotImplementedError ( 'This activation function is not currently supported ' 'by the backend' ) else : x = self . activation ( x ) # TODO(klas): is the `clone` necessary here? Not sure why it was # included. mask = B . sign ( B . clone ( x ) - self . threshold ) if self . low_minus_high : mask = - mask non_batch_dimensions = tuple ( range ( len ( B . int_shape ( x )))[ 1 :]) return B . sum ( mask * x , axis = non_batch_dimensions ) __init__ ( self , threshold , low_minus_high = False , activation = None ) special \u00b6 Parameters: Name Type Description Default threshold float A threshold to determine the element-wise sign of the input tensor. The elements with activations higher than the threshold will retain their sign, while the elements with activations lower than the threshold will have their sign flipped (or vice versa if low_minus_high is set to True ). required low_minus_high bool If True , substract the output with activations above the threshold from the output with activations below the threshold. If False , substract the output with activations below the threshold from the output with activations above the threshold. False activation Union[Callable, str, None] str or function, optional Activation function to be applied to the quantity before taking the threshold. If activation is a string, use the corresponding activation function implemented by the backend (currently supported: 'sigmoid' and 'softmax' ). Otherwise, if activation is not None , it will be treated as a callable. If activation is None , do not apply an activation function to the quantity. None Source code in trulens/nn/quantities.py def __init__ ( self , threshold : float , low_minus_high : bool = False , activation : Union [ Callable , str , None ] = None ): \"\"\" Parameters: threshold: A threshold to determine the element-wise sign of the input tensor. The elements with activations higher than the threshold will retain their sign, while the elements with activations lower than the threshold will have their sign flipped (or vice versa if `low_minus_high` is set to `True`). low_minus_high: If `True`, substract the output with activations above the threshold from the output with activations below the threshold. If `False`, substract the output with activations below the threshold from the output with activations above the threshold. activation: str or function, optional Activation function to be applied to the quantity before taking the threshold. If `activation` is a string, use the corresponding activation function implemented by the backend (currently supported: `'sigmoid'` and `'softmax'`). Otherwise, if `activation` is not `None`, it will be treated as a callable. If `activation` is `None`, do not apply an activation function to the quantity. \"\"\" # TODO(klas):should this support an aggregation function? By default # this is a sum, but it could, for example, subtract the greatest # positive element from the least negative element. self . threshold = threshold self . low_minus_high = low_minus_high self . activation = activation","title":"Quantities"},{"location":"api/quantities/#quantities-of-interest","text":"A Quantity of Interest (QoI) is a function of the output that determines the network output behavior that the attributions describe. The quantity of interest lets us specify what we want to explain. Often, this is the output of the network corresponding to a particular class, addressing, e.g., \"Why did the model classify a given image as a car?\" However, we could also consider various combinations of outputs, allowing us to ask more specific questions, such as, \"Why did the model classify a given image as a sedan and not a convertible ?\" The former may highlight general \u201ccar features,\u201d such as tires, while the latter (called a comparative explanation) might focus on the roof of the car, a \u201ccar feature\u201d not shared by convertibles.","title":"Quantities of Interest"},{"location":"api/quantities/#trulens.nn.quantities.ClassQoI","text":"Quantity of interest for attributing output towards a specified class. Source code in trulens/nn/quantities.py class ClassQoI ( QoI ): \"\"\" Quantity of interest for attributing output towards a specified class. \"\"\" def __init__ ( self , cl : int ): \"\"\" Parameters: cl: The index of the class the QoI is for. \"\"\" self . cl = cl def __call__ ( self , y : TensorLike ) -> TensorLike : self . _assert_cut_contains_only_one_tensor ( y ) return y [:, self . cl ]","title":"ClassQoI"},{"location":"api/quantities/#trulens.nn.quantities.ClassQoI.__init__","text":"Parameters: Name Type Description Default cl int The index of the class the QoI is for. required Source code in trulens/nn/quantities.py def __init__ ( self , cl : int ): \"\"\" Parameters: cl: The index of the class the QoI is for. \"\"\" self . cl = cl","title":"__init__()"},{"location":"api/quantities/#trulens.nn.quantities.ClassSeqQoI","text":"Quantity of interest for attributing output towards a sequence of classes for each input. Source code in trulens/nn/quantities.py class ClassSeqQoI ( QoI ): \"\"\" Quantity of interest for attributing output towards a sequence of classes for each input. \"\"\" def __init__ ( self , seq_labels : List [ int ]): \"\"\" Parameters: seq_labels: A sequence of classes corresponding to each input. \"\"\" self . seq_labels = seq_labels def __call__ ( self , y ): self . _assert_cut_contains_only_one_tensor ( y ) assert get_backend () . shape ( y )[ 0 ] == len ( self . seq_labels ) return y [:, self . seq_labels ]","title":"ClassSeqQoI"},{"location":"api/quantities/#trulens.nn.quantities.ClassSeqQoI.__init__","text":"Parameters: Name Type Description Default seq_labels List[int] A sequence of classes corresponding to each input. required Source code in trulens/nn/quantities.py def __init__ ( self , seq_labels : List [ int ]): \"\"\" Parameters: seq_labels: A sequence of classes corresponding to each input. \"\"\" self . seq_labels = seq_labels","title":"__init__()"},{"location":"api/quantities/#trulens.nn.quantities.ComparativeQoI","text":"Quantity of interest for attributing network output towards a given class, relative to another. Source code in trulens/nn/quantities.py class ComparativeQoI ( QoI ): \"\"\" Quantity of interest for attributing network output towards a given class, relative to another. \"\"\" def __init__ ( self , cl1 : int , cl2 : int ): \"\"\" Parameters: cl1: The index of the class the QoI is for. cl2: The index of the class to compare against. \"\"\" self . cl1 = cl1 self . cl2 = cl2 def __call__ ( self , y : TensorLike ) -> TensorLike : self . _assert_cut_contains_only_one_tensor ( y ) return y [:, self . cl1 ] - y [:, self . cl2 ]","title":"ComparativeQoI"},{"location":"api/quantities/#trulens.nn.quantities.ComparativeQoI.__init__","text":"Parameters: Name Type Description Default cl1 int The index of the class the QoI is for. required cl2 int The index of the class to compare against. required Source code in trulens/nn/quantities.py def __init__ ( self , cl1 : int , cl2 : int ): \"\"\" Parameters: cl1: The index of the class the QoI is for. cl2: The index of the class to compare against. \"\"\" self . cl1 = cl1 self . cl2 = cl2","title":"__init__()"},{"location":"api/quantities/#trulens.nn.quantities.InternalChannelQoI","text":"Quantity of interest for attributing output towards the output of an internal convolutional layer channel, aggregating using a specified operation. Also works for non-convolutional dense layers, where the given neuron's activation is returned. Source code in trulens/nn/quantities.py class InternalChannelQoI ( QoI ): \"\"\" Quantity of interest for attributing output towards the output of an internal convolutional layer channel, aggregating using a specified operation. Also works for non-convolutional dense layers, where the given neuron's activation is returned. \"\"\" @staticmethod def _batch_sum ( x ): \"\"\" Sums batched 2D channels, leaving the batch dimension unchanged. \"\"\" return get_backend () . sum ( x , axis = ( 1 , 2 )) def __init__ ( self , channel : Union [ int , List [ int ]], channel_axis : Optional [ int ] = None , agg_fn : Optional [ Callable ] = None ): \"\"\" Parameters: channel: Channel to return. If a list is provided, then the quantity sums over each of the channels in the list. channel_axis: Channel dimension index, if relevant, e.g., for 2D convolutional layers. If `channel_axis` is `None`, then the channel axis of the relevant backend will be used. This argument is not used when the channels are scalars, e.g., for dense layers. agg_fn: Function with which to aggregate the remaining dimensions (except the batch dimension) in order to get a single scalar value for each channel. If `agg_fn` is `None` then a sum over each neuron in the channel will be taken. This argument is not used when the channels are scalars, e.g., for dense layers. \"\"\" if channel_axis is None : channel_axis = get_backend () . channel_axis if agg_fn is None : agg_fn = InternalChannelQoI . _batch_sum self . _channel_ax = channel_axis self . _agg_fn = agg_fn self . _channels = channel if isinstance ( channel , list ) else [ channel ] def __call__ ( self , y : TensorLike ) -> TensorLike : B = get_backend () self . _assert_cut_contains_only_one_tensor ( y ) if len ( B . int_shape ( y )) == 2 : return sum ([ y [:, ch ] for ch in self . _channels ]) elif len ( B . int_shape ( y )) == 3 : return sum ([ self . _agg_fn ( y [:, :, ch ]) for ch in self . _channel ]) elif len ( B . int_shape ( y )) == 4 : if self . _channel_ax == 1 : return sum ([ self . _agg_fn ( y [:, ch ]) for ch in self . _channels ]) elif self . _channel_ax == 3 : return sum ( [ self . _agg_fn ( y [:, :, :, ch ]) for ch in self . _channels ] ) else : raise ValueError ( 'Unsupported channel axis for convolutional layer: {} ' . format ( self . _channel_ax ) ) else : raise QoiCutSupportError ( 'Unsupported tensor rank for `InternalChannelQoI`: {} ' . format ( len ( B . int_shape ( y )) ) )","title":"InternalChannelQoI"},{"location":"api/quantities/#trulens.nn.quantities.InternalChannelQoI.__init__","text":"Parameters: Name Type Description Default channel Union[int, List[int]] Channel to return. If a list is provided, then the quantity sums over each of the channels in the list. required channel_axis Optional[int] Channel dimension index, if relevant, e.g., for 2D convolutional layers. If channel_axis is None , then the channel axis of the relevant backend will be used. This argument is not used when the channels are scalars, e.g., for dense layers. None agg_fn Optional[Callable] Function with which to aggregate the remaining dimensions (except the batch dimension) in order to get a single scalar value for each channel. If agg_fn is None then a sum over each neuron in the channel will be taken. This argument is not used when the channels are scalars, e.g., for dense layers. None Source code in trulens/nn/quantities.py def __init__ ( self , channel : Union [ int , List [ int ]], channel_axis : Optional [ int ] = None , agg_fn : Optional [ Callable ] = None ): \"\"\" Parameters: channel: Channel to return. If a list is provided, then the quantity sums over each of the channels in the list. channel_axis: Channel dimension index, if relevant, e.g., for 2D convolutional layers. If `channel_axis` is `None`, then the channel axis of the relevant backend will be used. This argument is not used when the channels are scalars, e.g., for dense layers. agg_fn: Function with which to aggregate the remaining dimensions (except the batch dimension) in order to get a single scalar value for each channel. If `agg_fn` is `None` then a sum over each neuron in the channel will be taken. This argument is not used when the channels are scalars, e.g., for dense layers. \"\"\" if channel_axis is None : channel_axis = get_backend () . channel_axis if agg_fn is None : agg_fn = InternalChannelQoI . _batch_sum self . _channel_ax = channel_axis self . _agg_fn = agg_fn self . _channels = channel if isinstance ( channel , list ) else [ channel ]","title":"__init__()"},{"location":"api/quantities/#trulens.nn.quantities.LambdaQoI","text":"Generic quantity of interest allowing the user to specify a function of the model's output as the QoI. Source code in trulens/nn/quantities.py class LambdaQoI ( QoI ): \"\"\" Generic quantity of interest allowing the user to specify a function of the model's output as the QoI. \"\"\" def __init__ ( self , function : Callable ): \"\"\" Parameters: function: A callable that takes a single argument representing the model's tensor output and returns a differentiable batched scalar tensor representing the QoI. \"\"\" if len ( signature ( function ) . parameters ) != 1 : raise ValueError ( 'QoI function must take exactly 1 argument, but provided ' 'function takes {} arguments' . format ( len ( signature ( function ) . parameters ) ) ) self . function = function def __call__ ( self , y : TensorLike ) -> TensorLike : return self . function ( y )","title":"LambdaQoI"},{"location":"api/quantities/#trulens.nn.quantities.LambdaQoI.__init__","text":"Parameters: Name Type Description Default function Callable A callable that takes a single argument representing the model's tensor output and returns a differentiable batched scalar tensor representing the QoI. required Source code in trulens/nn/quantities.py def __init__ ( self , function : Callable ): \"\"\" Parameters: function: A callable that takes a single argument representing the model's tensor output and returns a differentiable batched scalar tensor representing the QoI. \"\"\" if len ( signature ( function ) . parameters ) != 1 : raise ValueError ( 'QoI function must take exactly 1 argument, but provided ' 'function takes {} arguments' . format ( len ( signature ( function ) . parameters ) ) ) self . function = function","title":"__init__()"},{"location":"api/quantities/#trulens.nn.quantities.MaxClassQoI","text":"Quantity of interest for attributing output towards the maximum-predicted class. Source code in trulens/nn/quantities.py class MaxClassQoI ( QoI ): \"\"\" Quantity of interest for attributing output towards the maximum-predicted class. \"\"\" def __init__ ( self , axis : int = 1 , activation : Union [ Callable , str , None ] = None ): \"\"\" Parameters: axis: Output dimension over which max operation is taken. activation: Activation function to be applied to the output before taking the max. If `activation` is a string, use the corresponding named activation function implemented by the backend. The following strings are currently supported as shorthands for the respective standard activation functions: - `'sigmoid'` - `'softmax'` If `activation` is `None`, no activation function is applied to the input. \"\"\" self . _axis = axis self . activation = activation def __call__ ( self , y : TensorLike ) -> TensorLike : self . _assert_cut_contains_only_one_tensor ( y ) if self . activation is not None : if isinstance ( self . activation , str ): self . activation = self . activation . lower () if self . activation in [ 'sigmoid' , 'softmax' ]: y = getattr ( get_backend (), self . activation )( y ) else : raise NotImplementedError ( 'This activation function is not currently supported ' 'by the backend' ) else : y = self . activation ( y ) return get_backend () . max ( y , axis = self . _axis )","title":"MaxClassQoI"},{"location":"api/quantities/#trulens.nn.quantities.MaxClassQoI.__init__","text":"Parameters: Name Type Description Default axis int Output dimension over which max operation is taken. 1 activation Union[Callable, str, None] Activation function to be applied to the output before taking the max. If activation is a string, use the corresponding named activation function implemented by the backend. The following strings are currently supported as shorthands for the respective standard activation functions: 'sigmoid' 'softmax' If activation is None , no activation function is applied to the input. None Source code in trulens/nn/quantities.py def __init__ ( self , axis : int = 1 , activation : Union [ Callable , str , None ] = None ): \"\"\" Parameters: axis: Output dimension over which max operation is taken. activation: Activation function to be applied to the output before taking the max. If `activation` is a string, use the corresponding named activation function implemented by the backend. The following strings are currently supported as shorthands for the respective standard activation functions: - `'sigmoid'` - `'softmax'` If `activation` is `None`, no activation function is applied to the input. \"\"\" self . _axis = axis self . activation = activation","title":"__init__()"},{"location":"api/quantities/#trulens.nn.quantities.QoI","text":"Interface for quantities of interest. The Quantity of Interest (QoI) is a function of the output specified by the slice that determines the network output behavior that the attributions describe. Source code in trulens/nn/quantities.py class QoI ( AbstractBaseClass ): \"\"\" Interface for quantities of interest. The *Quantity of Interest* (QoI) is a function of the output specified by the slice that determines the network output behavior that the attributions describe. \"\"\" def _wrap_public_call ( self , y : Outputs [ Tensor ]) -> Outputs [ Tensor ]: \"\"\" Wrap a public call that may result one or more tensors. Signature of this class is not specific while public calls are flexible. \"\"\" return many_of_om ( self . __call__ ( om_of_many ( y ))) @abstractmethod def __call__ ( self , y : OM [ Outputs , Tensor ]) -> OM [ Outputs , Tensor ]: \"\"\" Computes the distribution of interest from an initial point. Parameters: y: Output point from which the quantity is derived. Must be a differentiable tensor. Returns: A differentiable batched scalar tensor representing the QoI. \"\"\" raise NotImplementedError def _assert_cut_contains_only_one_tensor ( self , x ): if isinstance ( x , DATA_CONTAINER_TYPE ): raise QoiCutSupportError ( 'Cut provided to quantity of interest was comprised of ' 'multiple tensors, but ` {} ` is only defined for cuts comprised ' 'of a single tensor (received a list of {} tensors). \\n ' ' \\n ' 'Either (1) select a slice where the `to_cut` corresponds to a ' 'single tensor, or (2) implement/use a `QoI` object that ' 'supports lists of tensors, i.e., where the parameter, `x`, to ' '`__call__` is expected/allowed to be a list of {} tensors.' . format ( self . __class__ . __name__ , len ( x ), len ( x )) ) elif not get_backend () . is_tensor ( x ): raise ValueError ( '` {} ` expected to receive an instance of `Tensor`, but ' 'received an instance of {} ' . format ( self . __class__ . __name__ , type ( x ) ) )","title":"QoI"},{"location":"api/quantities/#trulens.nn.quantities.QoI.__call__","text":"Computes the distribution of interest from an initial point. Parameters: Name Type Description Default y OM[Outputs, Tensor] Output point from which the quantity is derived. Must be a differentiable tensor. required Returns: Type Description OM[Outputs, Tensor] A differentiable batched scalar tensor representing the QoI. Source code in trulens/nn/quantities.py @abstractmethod def __call__ ( self , y : OM [ Outputs , Tensor ]) -> OM [ Outputs , Tensor ]: \"\"\" Computes the distribution of interest from an initial point. Parameters: y: Output point from which the quantity is derived. Must be a differentiable tensor. Returns: A differentiable batched scalar tensor representing the QoI. \"\"\" raise NotImplementedError","title":"__call__()"},{"location":"api/quantities/#trulens.nn.quantities.QoiCutSupportError","text":"Exception raised if the quantity of interest is called on a cut whose output is not supported by the quantity of interest. Source code in trulens/nn/quantities.py class QoiCutSupportError ( ValueError ): \"\"\" Exception raised if the quantity of interest is called on a cut whose output is not supported by the quantity of interest. \"\"\" pass","title":"QoiCutSupportError"},{"location":"api/quantities/#trulens.nn.quantities.ThresholdQoI","text":"Quantity of interest for attributing network output toward the difference between two regions seperated by a given threshold. I.e., the quantity of interest is the \"high\" elements minus the \"low\" elements, where the high elements have activations above the threshold and the low elements have activations below the threshold. Use case: bianry segmentation. Source code in trulens/nn/quantities.py class ThresholdQoI ( QoI ): \"\"\" Quantity of interest for attributing network output toward the difference between two regions seperated by a given threshold. I.e., the quantity of interest is the \"high\" elements minus the \"low\" elements, where the high elements have activations above the threshold and the low elements have activations below the threshold. Use case: bianry segmentation. \"\"\" def __init__ ( self , threshold : float , low_minus_high : bool = False , activation : Union [ Callable , str , None ] = None ): \"\"\" Parameters: threshold: A threshold to determine the element-wise sign of the input tensor. The elements with activations higher than the threshold will retain their sign, while the elements with activations lower than the threshold will have their sign flipped (or vice versa if `low_minus_high` is set to `True`). low_minus_high: If `True`, substract the output with activations above the threshold from the output with activations below the threshold. If `False`, substract the output with activations below the threshold from the output with activations above the threshold. activation: str or function, optional Activation function to be applied to the quantity before taking the threshold. If `activation` is a string, use the corresponding activation function implemented by the backend (currently supported: `'sigmoid'` and `'softmax'`). Otherwise, if `activation` is not `None`, it will be treated as a callable. If `activation` is `None`, do not apply an activation function to the quantity. \"\"\" # TODO(klas):should this support an aggregation function? By default # this is a sum, but it could, for example, subtract the greatest # positive element from the least negative element. self . threshold = threshold self . low_minus_high = low_minus_high self . activation = activation def __call__ ( self , x : TensorLike ) -> TensorLike : B = get_backend () self . _assert_cut_contains_only_one_tensor ( x ) if self . activation is not None : if isinstance ( self . activation , str ): self . activation = self . activation . lower () if self . activation in [ 'sigmoid' , 'softmax' ]: x = getattr ( B , self . activation )( x ) else : raise NotImplementedError ( 'This activation function is not currently supported ' 'by the backend' ) else : x = self . activation ( x ) # TODO(klas): is the `clone` necessary here? Not sure why it was # included. mask = B . sign ( B . clone ( x ) - self . threshold ) if self . low_minus_high : mask = - mask non_batch_dimensions = tuple ( range ( len ( B . int_shape ( x )))[ 1 :]) return B . sum ( mask * x , axis = non_batch_dimensions )","title":"ThresholdQoI"},{"location":"api/quantities/#trulens.nn.quantities.ThresholdQoI.__init__","text":"Parameters: Name Type Description Default threshold float A threshold to determine the element-wise sign of the input tensor. The elements with activations higher than the threshold will retain their sign, while the elements with activations lower than the threshold will have their sign flipped (or vice versa if low_minus_high is set to True ). required low_minus_high bool If True , substract the output with activations above the threshold from the output with activations below the threshold. If False , substract the output with activations below the threshold from the output with activations above the threshold. False activation Union[Callable, str, None] str or function, optional Activation function to be applied to the quantity before taking the threshold. If activation is a string, use the corresponding activation function implemented by the backend (currently supported: 'sigmoid' and 'softmax' ). Otherwise, if activation is not None , it will be treated as a callable. If activation is None , do not apply an activation function to the quantity. None Source code in trulens/nn/quantities.py def __init__ ( self , threshold : float , low_minus_high : bool = False , activation : Union [ Callable , str , None ] = None ): \"\"\" Parameters: threshold: A threshold to determine the element-wise sign of the input tensor. The elements with activations higher than the threshold will retain their sign, while the elements with activations lower than the threshold will have their sign flipped (or vice versa if `low_minus_high` is set to `True`). low_minus_high: If `True`, substract the output with activations above the threshold from the output with activations below the threshold. If `False`, substract the output with activations below the threshold from the output with activations above the threshold. activation: str or function, optional Activation function to be applied to the quantity before taking the threshold. If `activation` is a string, use the corresponding activation function implemented by the backend (currently supported: `'sigmoid'` and `'softmax'`). Otherwise, if `activation` is not `None`, it will be treated as a callable. If `activation` is `None`, do not apply an activation function to the quantity. \"\"\" # TODO(klas):should this support an aggregation function? By default # this is a sum, but it could, for example, subtract the greatest # positive element from the least negative element. self . threshold = threshold self . low_minus_high = low_minus_high self . activation = activation","title":"__init__()"},{"location":"api/slices/","text":"Slices \u00b6 The slice , or layer, of the network provides flexibility over the level of abstraction for the explanation. In a low layer, an explanation may highlight the edges that were most important in identifying an object like a face, while in a higher layer, the explanation might highlight high-level features such as a nose or mouth. By raising the level of abstraction, explanations that generalize over larger sets of samples are possible. Formally, A network, \\(f\\) , can be broken into a slice, \\(f = g \\circ h\\) , where \\(h\\) can be thought of as a pre-processor that computes features, and \\(g\\) can be thought of as a sub-model that uses the features computed by \\(h\\) . Cut \u00b6 A cut is the primary building block for a slice. It determines an internal component of a network to expose. A slice if formed by two cuts. Source code in trulens/nn/slices.py class Cut ( object ): \"\"\" A cut is the primary building block for a slice. It determines an internal component of a network to expose. A slice if formed by two cuts. \"\"\" def __init__ ( self , name : LayerIdentifier , anchor : str = 'out' , accessor : Optional [ Callable ] = None ): \"\"\" Parameters: name: The name or index of a layer in the model, or a list containing the names/indices of mutliple layers. anchor: Determines whether input (`'in'`) or the output (`'out'`) tensor of the spcified layer should be used. accessor: An accessor function that operates on the layer, mapping the tensor (or list thereof) corresponding to the layer's input/output to another tensor (or list thereof). This can be used to, e.g., extract a particular output from a layer that produces a sequence of outputs. If `accessor` is `None`, the following accessor function will be used: ```python lambda t: t[-1] if isinstance(t, list) else t ``` \"\"\" assert name is None or isinstance ( name , ( list , int , str ) ), \"Cut.name must be one of: layer index, layer name, or list of names/indices of multiple layers\" if isinstance ( name , list ): for n in name : assert isinstance ( n , ( int , str ) ), f \"Elements in Cut.name must be layer names (str) or indices (int). Got type { type ( n ) } \" anchor = str ( anchor ) assert anchor in [ 'in' , 'out' ], \"Cut.anchor must be one of ('in', 'out')\" assert accessor is None or isinstance ( accessor , Callable ), \"Cut.accessor must be callable or None\" if get_backend () . backend == 'pytorch' : if ( isinstance ( name , int ) or ( isinstance ( name , list ) and isinstance ( name [ 0 ], int ))): tru_logger . warning ( ' \\n\\n Pytorch does not have native support for indexed ' 'layers. Using layer indices is not recommended. \\n ' ) self . name = name self . accessor = accessor self . anchor = anchor def access_layer ( self , layer : TensorLike ) -> TensorLike : \"\"\" Applies `self.accessor` to the result of collecting the relevant tensor(s) associated with a layer's output. Parameters: layer: The tensor output (or input, if so specified by the anchor) of the layer(s) specified by this cut. Returns: The result of applying `self.accessor` to the given layer. \"\"\" if self . accessor is None : return layer [ - 1 ] if isinstance ( layer , list ) else layer else : layer = ( layer [ 0 ] if isinstance ( layer , list ) and len ( layer ) == 1 else layer ) return self . accessor ( layer ) __init__ ( self , name , anchor = 'out' , accessor = None ) special \u00b6 Parameters: Name Type Description Default name LayerIdentifier The name or index of a layer in the model, or a list containing the names/indices of mutliple layers. required anchor str Determines whether input ( 'in' ) or the output ( 'out' ) tensor of the spcified layer should be used. 'out' accessor Optional[Callable] An accessor function that operates on the layer, mapping the tensor (or list thereof) corresponding to the layer's input/output to another tensor (or list thereof). This can be used to, e.g., extract a particular output from a layer that produces a sequence of outputs. If accessor is None , the following accessor function will be used: lambda t : t [ - 1 ] if isinstance ( t , list ) else t None Source code in trulens/nn/slices.py def __init__ ( self , name : LayerIdentifier , anchor : str = 'out' , accessor : Optional [ Callable ] = None ): \"\"\" Parameters: name: The name or index of a layer in the model, or a list containing the names/indices of mutliple layers. anchor: Determines whether input (`'in'`) or the output (`'out'`) tensor of the spcified layer should be used. accessor: An accessor function that operates on the layer, mapping the tensor (or list thereof) corresponding to the layer's input/output to another tensor (or list thereof). This can be used to, e.g., extract a particular output from a layer that produces a sequence of outputs. If `accessor` is `None`, the following accessor function will be used: ```python lambda t: t[-1] if isinstance(t, list) else t ``` \"\"\" assert name is None or isinstance ( name , ( list , int , str ) ), \"Cut.name must be one of: layer index, layer name, or list of names/indices of multiple layers\" if isinstance ( name , list ): for n in name : assert isinstance ( n , ( int , str ) ), f \"Elements in Cut.name must be layer names (str) or indices (int). Got type { type ( n ) } \" anchor = str ( anchor ) assert anchor in [ 'in' , 'out' ], \"Cut.anchor must be one of ('in', 'out')\" assert accessor is None or isinstance ( accessor , Callable ), \"Cut.accessor must be callable or None\" if get_backend () . backend == 'pytorch' : if ( isinstance ( name , int ) or ( isinstance ( name , list ) and isinstance ( name [ 0 ], int ))): tru_logger . warning ( ' \\n\\n Pytorch does not have native support for indexed ' 'layers. Using layer indices is not recommended. \\n ' ) self . name = name self . accessor = accessor self . anchor = anchor access_layer ( self , layer ) \u00b6 Applies self.accessor to the result of collecting the relevant tensor(s) associated with a layer's output. Parameters: Name Type Description Default layer TensorLike The tensor output (or input, if so specified by the anchor) of the layer(s) specified by this cut. required Returns: Type Description TensorLike The result of applying self.accessor to the given layer. Source code in trulens/nn/slices.py def access_layer ( self , layer : TensorLike ) -> TensorLike : \"\"\" Applies `self.accessor` to the result of collecting the relevant tensor(s) associated with a layer's output. Parameters: layer: The tensor output (or input, if so specified by the anchor) of the layer(s) specified by this cut. Returns: The result of applying `self.accessor` to the given layer. \"\"\" if self . accessor is None : return layer [ - 1 ] if isinstance ( layer , list ) else layer else : layer = ( layer [ 0 ] if isinstance ( layer , list ) and len ( layer ) == 1 else layer ) return self . accessor ( layer ) InputCut ( Cut ) \u00b6 Special cut that selects the input(s) of a model. Source code in trulens/nn/slices.py class InputCut ( Cut ): \"\"\" Special cut that selects the input(s) of a model. \"\"\" def __init__ ( self , anchor : str = 'in' , accessor : Optional [ Callable ] = None ): \"\"\" Parameters: anchor: Determines whether input (`'in'`) or the output (`'out'`) tensor of the spcified layer should be used. accessor: An accessor function that operates on the layer, mapping the tensor (or list thereof) corresponding to the layer's input/output to another tensor (or list thereof). This can be used to, e.g., extract a particular output from a layer that produces a sequence of outputs. If `accessor` is `None`, the following accessor function will be used: ```python lambda t: t[-1] if isinstance(t, list) else t ``` \"\"\" super () . __init__ ( None , anchor , accessor ) __init__ ( self , anchor = 'in' , accessor = None ) special \u00b6 Parameters: Name Type Description Default anchor str Determines whether input ( 'in' ) or the output ( 'out' ) tensor of the spcified layer should be used. 'in' accessor Optional[Callable] An accessor function that operates on the layer, mapping the tensor (or list thereof) corresponding to the layer's input/output to another tensor (or list thereof). This can be used to, e.g., extract a particular output from a layer that produces a sequence of outputs. If accessor is None , the following accessor function will be used: lambda t : t [ - 1 ] if isinstance ( t , list ) else t None Source code in trulens/nn/slices.py def __init__ ( self , anchor : str = 'in' , accessor : Optional [ Callable ] = None ): \"\"\" Parameters: anchor: Determines whether input (`'in'`) or the output (`'out'`) tensor of the spcified layer should be used. accessor: An accessor function that operates on the layer, mapping the tensor (or list thereof) corresponding to the layer's input/output to another tensor (or list thereof). This can be used to, e.g., extract a particular output from a layer that produces a sequence of outputs. If `accessor` is `None`, the following accessor function will be used: ```python lambda t: t[-1] if isinstance(t, list) else t ``` \"\"\" super () . __init__ ( None , anchor , accessor ) LogitCut ( Cut ) \u00b6 Special cut that selects the logit layer of a model. The logit layer must be named 'logits' or otherwise specified by the user to the model wrapper. Source code in trulens/nn/slices.py class LogitCut ( Cut ): \"\"\" Special cut that selects the logit layer of a model. The logit layer must be named `'logits'` or otherwise specified by the user to the model wrapper. \"\"\" def __init__ ( self , anchor : str = 'out' , accessor : Optional [ Callable ] = None ): \"\"\" Parameters: anchor: Determines whether input (`'in'`) or the output (`'out'`) tensor of the spcified layer should be used. accessor: An accessor function that operates on the layer, mapping the tensor (or list thereof) corresponding to the layer's input/output to another tensor (or list thereof). This can be used to, e.g., extract a particular output from a layer that produces a sequence of outputs. If `accessor` is `None`, the following accessor function will be used: ```python lambda t: t[-1] if isinstance(t, list) else t ``` \"\"\" super ( LogitCut , self ) . __init__ ( None , anchor , accessor ) __init__ ( self , anchor = 'out' , accessor = None ) special \u00b6 Parameters: Name Type Description Default anchor str Determines whether input ( 'in' ) or the output ( 'out' ) tensor of the spcified layer should be used. 'out' accessor Optional[Callable] An accessor function that operates on the layer, mapping the tensor (or list thereof) corresponding to the layer's input/output to another tensor (or list thereof). This can be used to, e.g., extract a particular output from a layer that produces a sequence of outputs. If accessor is None , the following accessor function will be used: lambda t : t [ - 1 ] if isinstance ( t , list ) else t None Source code in trulens/nn/slices.py def __init__ ( self , anchor : str = 'out' , accessor : Optional [ Callable ] = None ): \"\"\" Parameters: anchor: Determines whether input (`'in'`) or the output (`'out'`) tensor of the spcified layer should be used. accessor: An accessor function that operates on the layer, mapping the tensor (or list thereof) corresponding to the layer's input/output to another tensor (or list thereof). This can be used to, e.g., extract a particular output from a layer that produces a sequence of outputs. If `accessor` is `None`, the following accessor function will be used: ```python lambda t: t[-1] if isinstance(t, list) else t ``` \"\"\" super ( LogitCut , self ) . __init__ ( None , anchor , accessor ) OutputCut ( Cut ) \u00b6 Special cut that selects the output(s) of a model. Source code in trulens/nn/slices.py class OutputCut ( Cut ): \"\"\" Special cut that selects the output(s) of a model. \"\"\" def __init__ ( self , anchor : str = 'out' , accessor : Optional [ Callable ] = None ): \"\"\" Parameters: anchor: Determines whether input (`'in'`) or the output (`'out'`) tensor of the spcified layer should be used. accessor: An accessor function that operates on the layer, mapping the tensor (or list thereof) corresponding to the layer's input/output to another tensor (or list thereof). This can be used to, e.g., extract a particular output from a layer that produces a sequence of outputs. If `accessor` is `None`, the following accessor function will be used: ```python lambda t: t[-1] if isinstance(t, list) else t ``` \"\"\" super ( OutputCut , self ) . __init__ ( None , anchor , accessor ) __init__ ( self , anchor = 'out' , accessor = None ) special \u00b6 Parameters: Name Type Description Default anchor str Determines whether input ( 'in' ) or the output ( 'out' ) tensor of the spcified layer should be used. 'out' accessor Optional[Callable] An accessor function that operates on the layer, mapping the tensor (or list thereof) corresponding to the layer's input/output to another tensor (or list thereof). This can be used to, e.g., extract a particular output from a layer that produces a sequence of outputs. If accessor is None , the following accessor function will be used: lambda t : t [ - 1 ] if isinstance ( t , list ) else t None Source code in trulens/nn/slices.py def __init__ ( self , anchor : str = 'out' , accessor : Optional [ Callable ] = None ): \"\"\" Parameters: anchor: Determines whether input (`'in'`) or the output (`'out'`) tensor of the spcified layer should be used. accessor: An accessor function that operates on the layer, mapping the tensor (or list thereof) corresponding to the layer's input/output to another tensor (or list thereof). This can be used to, e.g., extract a particular output from a layer that produces a sequence of outputs. If `accessor` is `None`, the following accessor function will be used: ```python lambda t: t[-1] if isinstance(t, list) else t ``` \"\"\" super ( OutputCut , self ) . __init__ ( None , anchor , accessor ) Slice \u00b6 Class representing a slice of a network. A network, \\(f\\) , can be broken into a slice, \\(f = g \\circ h\\) , where \\(h\\) can be thought of as a pre-processor that computes features, and \\(g\\) can be thought of as a sub-model that uses the features computed by \\(h\\) . A Slice object represents a slice as two Cut s, from_cut and to_cut , which are the layers corresponding to the output of \\(h\\) and \\(g\\) , respectively. Source code in trulens/nn/slices.py class Slice ( object ): \"\"\" Class representing a slice of a network. A network, $f$, can be broken into a slice, $f = g \\\\circ h$, where $h$ can be thought of as a pre-processor that computes features, and $g$ can be thought of as a sub-model that uses the features computed by $h$. A `Slice` object represents a slice as two `Cut`s, `from_cut` and `to_cut`, which are the layers corresponding to the output of $h$ and $g$, respectively. \"\"\" def __init__ ( self , from_cut : Cut , to_cut : Cut ): \"\"\" Parameters: from_cut: Cut representing the output of the preprocessing function, $h$, in slice, $f = g \\\\circ h$. to_cut: Cut representing the output of the sub-model, $g$, in slice, $f = g \\\\circ h$. \"\"\" self . _from_cut = from_cut self . _to_cut = to_cut @property def from_cut ( self ) -> Cut : \"\"\" Cut representing the output of the preprocessing function, $h$, in slice, $f = g \\\\circ h$. \"\"\" return self . _from_cut @property def to_cut ( self ) -> Cut : \"\"\" Cut representing the output of the sub-model, $g$, in slice, $f = g \\\\circ h$. \"\"\" return self . _to_cut @staticmethod def full_network (): \"\"\" Returns ------- Slice A slice representing the entire model, i.e., :math:`f = g \\\\circ h`, where :math:`h` is the identity function and :math:`g = f`. \"\"\" return Slice ( InputCut (), OutputCut ()) from_cut : Cut property readonly \u00b6 Cut representing the output of the preprocessing function, \\(h\\) , in slice, \\(f = g \\circ h\\) . to_cut : Cut property readonly \u00b6 Cut representing the output of the sub-model, \\(g\\) , in slice, \\(f = g \\circ h\\) . __init__ ( self , from_cut , to_cut ) special \u00b6 Parameters: Name Type Description Default from_cut Cut Cut representing the output of the preprocessing function, \\(h\\) , in slice, \\(f = g \\circ h\\) . required to_cut Cut Cut representing the output of the sub-model, \\(g\\) , in slice, \\(f = g \\circ h\\) . required Source code in trulens/nn/slices.py def __init__ ( self , from_cut : Cut , to_cut : Cut ): \"\"\" Parameters: from_cut: Cut representing the output of the preprocessing function, $h$, in slice, $f = g \\\\circ h$. to_cut: Cut representing the output of the sub-model, $g$, in slice, $f = g \\\\circ h$. \"\"\" self . _from_cut = from_cut self . _to_cut = to_cut full_network () staticmethod \u00b6 Returns \u00b6 Slice A slice representing the entire model, i.e., :math: f = g \\circ h , where :math: h is the identity function and :math: g = f . Source code in trulens/nn/slices.py @staticmethod def full_network (): \"\"\" Returns ------- Slice A slice representing the entire model, i.e., :math:`f = g \\\\circ h`, where :math:`h` is the identity function and :math:`g = f`. \"\"\" return Slice ( InputCut (), OutputCut ())","title":"Slices"},{"location":"api/slices/#slices","text":"The slice , or layer, of the network provides flexibility over the level of abstraction for the explanation. In a low layer, an explanation may highlight the edges that were most important in identifying an object like a face, while in a higher layer, the explanation might highlight high-level features such as a nose or mouth. By raising the level of abstraction, explanations that generalize over larger sets of samples are possible. Formally, A network, \\(f\\) , can be broken into a slice, \\(f = g \\circ h\\) , where \\(h\\) can be thought of as a pre-processor that computes features, and \\(g\\) can be thought of as a sub-model that uses the features computed by \\(h\\) .","title":"Slices"},{"location":"api/slices/#trulens.nn.slices.Cut","text":"A cut is the primary building block for a slice. It determines an internal component of a network to expose. A slice if formed by two cuts. Source code in trulens/nn/slices.py class Cut ( object ): \"\"\" A cut is the primary building block for a slice. It determines an internal component of a network to expose. A slice if formed by two cuts. \"\"\" def __init__ ( self , name : LayerIdentifier , anchor : str = 'out' , accessor : Optional [ Callable ] = None ): \"\"\" Parameters: name: The name or index of a layer in the model, or a list containing the names/indices of mutliple layers. anchor: Determines whether input (`'in'`) or the output (`'out'`) tensor of the spcified layer should be used. accessor: An accessor function that operates on the layer, mapping the tensor (or list thereof) corresponding to the layer's input/output to another tensor (or list thereof). This can be used to, e.g., extract a particular output from a layer that produces a sequence of outputs. If `accessor` is `None`, the following accessor function will be used: ```python lambda t: t[-1] if isinstance(t, list) else t ``` \"\"\" assert name is None or isinstance ( name , ( list , int , str ) ), \"Cut.name must be one of: layer index, layer name, or list of names/indices of multiple layers\" if isinstance ( name , list ): for n in name : assert isinstance ( n , ( int , str ) ), f \"Elements in Cut.name must be layer names (str) or indices (int). Got type { type ( n ) } \" anchor = str ( anchor ) assert anchor in [ 'in' , 'out' ], \"Cut.anchor must be one of ('in', 'out')\" assert accessor is None or isinstance ( accessor , Callable ), \"Cut.accessor must be callable or None\" if get_backend () . backend == 'pytorch' : if ( isinstance ( name , int ) or ( isinstance ( name , list ) and isinstance ( name [ 0 ], int ))): tru_logger . warning ( ' \\n\\n Pytorch does not have native support for indexed ' 'layers. Using layer indices is not recommended. \\n ' ) self . name = name self . accessor = accessor self . anchor = anchor def access_layer ( self , layer : TensorLike ) -> TensorLike : \"\"\" Applies `self.accessor` to the result of collecting the relevant tensor(s) associated with a layer's output. Parameters: layer: The tensor output (or input, if so specified by the anchor) of the layer(s) specified by this cut. Returns: The result of applying `self.accessor` to the given layer. \"\"\" if self . accessor is None : return layer [ - 1 ] if isinstance ( layer , list ) else layer else : layer = ( layer [ 0 ] if isinstance ( layer , list ) and len ( layer ) == 1 else layer ) return self . accessor ( layer )","title":"Cut"},{"location":"api/slices/#trulens.nn.slices.Cut.__init__","text":"Parameters: Name Type Description Default name LayerIdentifier The name or index of a layer in the model, or a list containing the names/indices of mutliple layers. required anchor str Determines whether input ( 'in' ) or the output ( 'out' ) tensor of the spcified layer should be used. 'out' accessor Optional[Callable] An accessor function that operates on the layer, mapping the tensor (or list thereof) corresponding to the layer's input/output to another tensor (or list thereof). This can be used to, e.g., extract a particular output from a layer that produces a sequence of outputs. If accessor is None , the following accessor function will be used: lambda t : t [ - 1 ] if isinstance ( t , list ) else t None Source code in trulens/nn/slices.py def __init__ ( self , name : LayerIdentifier , anchor : str = 'out' , accessor : Optional [ Callable ] = None ): \"\"\" Parameters: name: The name or index of a layer in the model, or a list containing the names/indices of mutliple layers. anchor: Determines whether input (`'in'`) or the output (`'out'`) tensor of the spcified layer should be used. accessor: An accessor function that operates on the layer, mapping the tensor (or list thereof) corresponding to the layer's input/output to another tensor (or list thereof). This can be used to, e.g., extract a particular output from a layer that produces a sequence of outputs. If `accessor` is `None`, the following accessor function will be used: ```python lambda t: t[-1] if isinstance(t, list) else t ``` \"\"\" assert name is None or isinstance ( name , ( list , int , str ) ), \"Cut.name must be one of: layer index, layer name, or list of names/indices of multiple layers\" if isinstance ( name , list ): for n in name : assert isinstance ( n , ( int , str ) ), f \"Elements in Cut.name must be layer names (str) or indices (int). Got type { type ( n ) } \" anchor = str ( anchor ) assert anchor in [ 'in' , 'out' ], \"Cut.anchor must be one of ('in', 'out')\" assert accessor is None or isinstance ( accessor , Callable ), \"Cut.accessor must be callable or None\" if get_backend () . backend == 'pytorch' : if ( isinstance ( name , int ) or ( isinstance ( name , list ) and isinstance ( name [ 0 ], int ))): tru_logger . warning ( ' \\n\\n Pytorch does not have native support for indexed ' 'layers. Using layer indices is not recommended. \\n ' ) self . name = name self . accessor = accessor self . anchor = anchor","title":"__init__()"},{"location":"api/slices/#trulens.nn.slices.Cut.access_layer","text":"Applies self.accessor to the result of collecting the relevant tensor(s) associated with a layer's output. Parameters: Name Type Description Default layer TensorLike The tensor output (or input, if so specified by the anchor) of the layer(s) specified by this cut. required Returns: Type Description TensorLike The result of applying self.accessor to the given layer. Source code in trulens/nn/slices.py def access_layer ( self , layer : TensorLike ) -> TensorLike : \"\"\" Applies `self.accessor` to the result of collecting the relevant tensor(s) associated with a layer's output. Parameters: layer: The tensor output (or input, if so specified by the anchor) of the layer(s) specified by this cut. Returns: The result of applying `self.accessor` to the given layer. \"\"\" if self . accessor is None : return layer [ - 1 ] if isinstance ( layer , list ) else layer else : layer = ( layer [ 0 ] if isinstance ( layer , list ) and len ( layer ) == 1 else layer ) return self . accessor ( layer )","title":"access_layer()"},{"location":"api/slices/#trulens.nn.slices.InputCut","text":"Special cut that selects the input(s) of a model. Source code in trulens/nn/slices.py class InputCut ( Cut ): \"\"\" Special cut that selects the input(s) of a model. \"\"\" def __init__ ( self , anchor : str = 'in' , accessor : Optional [ Callable ] = None ): \"\"\" Parameters: anchor: Determines whether input (`'in'`) or the output (`'out'`) tensor of the spcified layer should be used. accessor: An accessor function that operates on the layer, mapping the tensor (or list thereof) corresponding to the layer's input/output to another tensor (or list thereof). This can be used to, e.g., extract a particular output from a layer that produces a sequence of outputs. If `accessor` is `None`, the following accessor function will be used: ```python lambda t: t[-1] if isinstance(t, list) else t ``` \"\"\" super () . __init__ ( None , anchor , accessor )","title":"InputCut"},{"location":"api/slices/#trulens.nn.slices.InputCut.__init__","text":"Parameters: Name Type Description Default anchor str Determines whether input ( 'in' ) or the output ( 'out' ) tensor of the spcified layer should be used. 'in' accessor Optional[Callable] An accessor function that operates on the layer, mapping the tensor (or list thereof) corresponding to the layer's input/output to another tensor (or list thereof). This can be used to, e.g., extract a particular output from a layer that produces a sequence of outputs. If accessor is None , the following accessor function will be used: lambda t : t [ - 1 ] if isinstance ( t , list ) else t None Source code in trulens/nn/slices.py def __init__ ( self , anchor : str = 'in' , accessor : Optional [ Callable ] = None ): \"\"\" Parameters: anchor: Determines whether input (`'in'`) or the output (`'out'`) tensor of the spcified layer should be used. accessor: An accessor function that operates on the layer, mapping the tensor (or list thereof) corresponding to the layer's input/output to another tensor (or list thereof). This can be used to, e.g., extract a particular output from a layer that produces a sequence of outputs. If `accessor` is `None`, the following accessor function will be used: ```python lambda t: t[-1] if isinstance(t, list) else t ``` \"\"\" super () . __init__ ( None , anchor , accessor )","title":"__init__()"},{"location":"api/slices/#trulens.nn.slices.LogitCut","text":"Special cut that selects the logit layer of a model. The logit layer must be named 'logits' or otherwise specified by the user to the model wrapper. Source code in trulens/nn/slices.py class LogitCut ( Cut ): \"\"\" Special cut that selects the logit layer of a model. The logit layer must be named `'logits'` or otherwise specified by the user to the model wrapper. \"\"\" def __init__ ( self , anchor : str = 'out' , accessor : Optional [ Callable ] = None ): \"\"\" Parameters: anchor: Determines whether input (`'in'`) or the output (`'out'`) tensor of the spcified layer should be used. accessor: An accessor function that operates on the layer, mapping the tensor (or list thereof) corresponding to the layer's input/output to another tensor (or list thereof). This can be used to, e.g., extract a particular output from a layer that produces a sequence of outputs. If `accessor` is `None`, the following accessor function will be used: ```python lambda t: t[-1] if isinstance(t, list) else t ``` \"\"\" super ( LogitCut , self ) . __init__ ( None , anchor , accessor )","title":"LogitCut"},{"location":"api/slices/#trulens.nn.slices.LogitCut.__init__","text":"Parameters: Name Type Description Default anchor str Determines whether input ( 'in' ) or the output ( 'out' ) tensor of the spcified layer should be used. 'out' accessor Optional[Callable] An accessor function that operates on the layer, mapping the tensor (or list thereof) corresponding to the layer's input/output to another tensor (or list thereof). This can be used to, e.g., extract a particular output from a layer that produces a sequence of outputs. If accessor is None , the following accessor function will be used: lambda t : t [ - 1 ] if isinstance ( t , list ) else t None Source code in trulens/nn/slices.py def __init__ ( self , anchor : str = 'out' , accessor : Optional [ Callable ] = None ): \"\"\" Parameters: anchor: Determines whether input (`'in'`) or the output (`'out'`) tensor of the spcified layer should be used. accessor: An accessor function that operates on the layer, mapping the tensor (or list thereof) corresponding to the layer's input/output to another tensor (or list thereof). This can be used to, e.g., extract a particular output from a layer that produces a sequence of outputs. If `accessor` is `None`, the following accessor function will be used: ```python lambda t: t[-1] if isinstance(t, list) else t ``` \"\"\" super ( LogitCut , self ) . __init__ ( None , anchor , accessor )","title":"__init__()"},{"location":"api/slices/#trulens.nn.slices.OutputCut","text":"Special cut that selects the output(s) of a model. Source code in trulens/nn/slices.py class OutputCut ( Cut ): \"\"\" Special cut that selects the output(s) of a model. \"\"\" def __init__ ( self , anchor : str = 'out' , accessor : Optional [ Callable ] = None ): \"\"\" Parameters: anchor: Determines whether input (`'in'`) or the output (`'out'`) tensor of the spcified layer should be used. accessor: An accessor function that operates on the layer, mapping the tensor (or list thereof) corresponding to the layer's input/output to another tensor (or list thereof). This can be used to, e.g., extract a particular output from a layer that produces a sequence of outputs. If `accessor` is `None`, the following accessor function will be used: ```python lambda t: t[-1] if isinstance(t, list) else t ``` \"\"\" super ( OutputCut , self ) . __init__ ( None , anchor , accessor )","title":"OutputCut"},{"location":"api/slices/#trulens.nn.slices.OutputCut.__init__","text":"Parameters: Name Type Description Default anchor str Determines whether input ( 'in' ) or the output ( 'out' ) tensor of the spcified layer should be used. 'out' accessor Optional[Callable] An accessor function that operates on the layer, mapping the tensor (or list thereof) corresponding to the layer's input/output to another tensor (or list thereof). This can be used to, e.g., extract a particular output from a layer that produces a sequence of outputs. If accessor is None , the following accessor function will be used: lambda t : t [ - 1 ] if isinstance ( t , list ) else t None Source code in trulens/nn/slices.py def __init__ ( self , anchor : str = 'out' , accessor : Optional [ Callable ] = None ): \"\"\" Parameters: anchor: Determines whether input (`'in'`) or the output (`'out'`) tensor of the spcified layer should be used. accessor: An accessor function that operates on the layer, mapping the tensor (or list thereof) corresponding to the layer's input/output to another tensor (or list thereof). This can be used to, e.g., extract a particular output from a layer that produces a sequence of outputs. If `accessor` is `None`, the following accessor function will be used: ```python lambda t: t[-1] if isinstance(t, list) else t ``` \"\"\" super ( OutputCut , self ) . __init__ ( None , anchor , accessor )","title":"__init__()"},{"location":"api/slices/#trulens.nn.slices.Slice","text":"Class representing a slice of a network. A network, \\(f\\) , can be broken into a slice, \\(f = g \\circ h\\) , where \\(h\\) can be thought of as a pre-processor that computes features, and \\(g\\) can be thought of as a sub-model that uses the features computed by \\(h\\) . A Slice object represents a slice as two Cut s, from_cut and to_cut , which are the layers corresponding to the output of \\(h\\) and \\(g\\) , respectively. Source code in trulens/nn/slices.py class Slice ( object ): \"\"\" Class representing a slice of a network. A network, $f$, can be broken into a slice, $f = g \\\\circ h$, where $h$ can be thought of as a pre-processor that computes features, and $g$ can be thought of as a sub-model that uses the features computed by $h$. A `Slice` object represents a slice as two `Cut`s, `from_cut` and `to_cut`, which are the layers corresponding to the output of $h$ and $g$, respectively. \"\"\" def __init__ ( self , from_cut : Cut , to_cut : Cut ): \"\"\" Parameters: from_cut: Cut representing the output of the preprocessing function, $h$, in slice, $f = g \\\\circ h$. to_cut: Cut representing the output of the sub-model, $g$, in slice, $f = g \\\\circ h$. \"\"\" self . _from_cut = from_cut self . _to_cut = to_cut @property def from_cut ( self ) -> Cut : \"\"\" Cut representing the output of the preprocessing function, $h$, in slice, $f = g \\\\circ h$. \"\"\" return self . _from_cut @property def to_cut ( self ) -> Cut : \"\"\" Cut representing the output of the sub-model, $g$, in slice, $f = g \\\\circ h$. \"\"\" return self . _to_cut @staticmethod def full_network (): \"\"\" Returns ------- Slice A slice representing the entire model, i.e., :math:`f = g \\\\circ h`, where :math:`h` is the identity function and :math:`g = f`. \"\"\" return Slice ( InputCut (), OutputCut ())","title":"Slice"},{"location":"api/slices/#trulens.nn.slices.Slice.from_cut","text":"Cut representing the output of the preprocessing function, \\(h\\) , in slice, \\(f = g \\circ h\\) .","title":"from_cut"},{"location":"api/slices/#trulens.nn.slices.Slice.to_cut","text":"Cut representing the output of the sub-model, \\(g\\) , in slice, \\(f = g \\circ h\\) .","title":"to_cut"},{"location":"api/slices/#trulens.nn.slices.Slice.__init__","text":"Parameters: Name Type Description Default from_cut Cut Cut representing the output of the preprocessing function, \\(h\\) , in slice, \\(f = g \\circ h\\) . required to_cut Cut Cut representing the output of the sub-model, \\(g\\) , in slice, \\(f = g \\circ h\\) . required Source code in trulens/nn/slices.py def __init__ ( self , from_cut : Cut , to_cut : Cut ): \"\"\" Parameters: from_cut: Cut representing the output of the preprocessing function, $h$, in slice, $f = g \\\\circ h$. to_cut: Cut representing the output of the sub-model, $g$, in slice, $f = g \\\\circ h$. \"\"\" self . _from_cut = from_cut self . _to_cut = to_cut","title":"__init__()"},{"location":"api/slices/#trulens.nn.slices.Slice.full_network","text":"","title":"full_network()"},{"location":"api/slices/#trulens.nn.slices.Slice.full_network--returns","text":"Slice A slice representing the entire model, i.e., :math: f = g \\circ h , where :math: h is the identity function and :math: g = f . Source code in trulens/nn/slices.py @staticmethod def full_network (): \"\"\" Returns ------- Slice A slice representing the entire model, i.e., :math:`f = g \\\\circ h`, where :math:`h` is the identity function and :math:`g = f`. \"\"\" return Slice ( InputCut (), OutputCut ())","title":"Returns"},{"location":"api/visualizations/","text":"Visualization Methods \u00b6 One clear use case for measuring attributions is for human consumption. In order to be fully leveraged by humans, explanations need to be interpretable \u2014 a large vector of numbers doesn\u2019t in general make us more confident we understand what a network is doing. We therefore view an explanation as comprised of both an attribution measurement and an interpretation of what the attribution values represent. One obvious way to interpret attributions, particularly in the image domain, is via visualization. This module provides several visualization methods for interpreting attributions as images. ChannelMaskVisualizer \u00b6 Uses internal influence to visualize the pixels that are most salient towards a particular internal channel or neuron. Source code in trulens/visualizations.py class ChannelMaskVisualizer ( object ): \"\"\" Uses internal influence to visualize the pixels that are most salient towards a particular internal channel or neuron. \"\"\" def __init__ ( self , model , layer , channel , channel_axis = None , agg_fn = None , doi = None , blur = None , threshold = 0.5 , masked_opacity = 0.2 , combine_channels : bool = True , use_attr_as_opacity = None , positive_only = None ): \"\"\" Configures the default parameters for the `__call__` method (these can be overridden by passing in values to `__call__`). Parameters: model: The wrapped model whose channel we're visualizing. layer: The identifier (either index or name) of the layer in which the channel we're visualizing resides. channel: Index of the channel (for convolutional layers) or internal neuron (for fully-connected layers) that we'd like to visualize. channel_axis: If different from the channel axis specified by the backend, the supplied `channel_axis` will be used if operating on a convolutional layer with 4-D image format. agg_fn: Function with which to aggregate the remaining dimensions (except the batch dimension) in order to get a single scalar value for each channel; If `None`, a sum over each neuron in the channel will be taken. This argument is not used when the channels are scalars, e.g., for dense layers. doi: The distribution of interest to use when computing the input attributions towards the specified channel. If `None`, `PointDoI` will be used. blur: Gives the radius of a Gaussian blur to be applied to the attributions before visualizing. This can be used to help focus on salient regions rather than specific salient pixels. threshold: Value in the range [0, 1]. Attribution values at or below the percentile given by `threshold` (after normalization, blurring, etc.) will be masked. masked_opacity: Value in the range [0, 1] specifying the opacity for the parts of the image that are masked. combine_channels: If `True`, the attributions will be averaged across the channel dimension, resulting in a 1-channel attribution map. use_attr_as_opacity: If `True`, instead of using `threshold` and `masked_opacity`, the opacity of each pixel is given by the 0-1-normalized attribution value. positive_only: If `True`, only pixels with positive attribution will be unmasked (or given nonzero opacity when `use_attr_as_opacity` is true). \"\"\" B = get_backend () if ( B is not None and ( channel_axis is None or channel_axis < 0 )): channel_axis = B . channel_axis elif ( channel_axis is None or channel_axis < 0 ): channel_axis = 1 self . mask_visualizer = MaskVisualizer ( blur , threshold , masked_opacity , combine_channels , use_attr_as_opacity , positive_only ) self . infl_input = InternalInfluence ( model , ( InputCut (), Cut ( layer )), InternalChannelQoI ( channel , channel_axis , agg_fn ), PointDoi () if doi is None else doi ) def __call__ ( self , x , x_preprocessed = None , output_file = None , blur = None , threshold = None , masked_opacity = None , combine_channels = None ): \"\"\" Visualizes the given attributions by overlaying an attribution heatmap over the given image. Parameters ---------- attributions : numpy.ndarray The attributions to visualize. Expected to be in 4-D image format. x : numpy.ndarray The original image(s) over which the attributions are calculated. Must be the same shape as expected by the model used with this visualizer. x_preprocessed : numpy.ndarray, optional If the model requires a preprocessed input (e.g., with the mean subtracted) that is different from how the image should be visualized, ``x_preprocessed`` should be specified. In this case ``x`` will be used for visualization, and ``x_preprocessed`` will be passed to the model when calculating attributions. Must be the same shape as ``x``. output_file : str, optional If specified, the resulting visualization will be saved to a file with the name given by ``output_file``. blur : float, optional If specified, gives the radius of a Gaussian blur to be applied to the attributions before visualizing. This can be used to help focus on salient regions rather than specific salient pixels. If None, defaults to the value supplied to the constructor. Default None. threshold : float Value in the range [0, 1]. Attribution values at or below the percentile given by ``threshold`` will be masked. If None, defaults to the value supplied to the constructor. Default None. masked_opacity: float Value in the range [0, 1] specifying the opacity for the parts of the image that are masked. Default 0.2. If None, defaults to the value supplied to the constructor. Default None. combine_channels : bool If True, the attributions will be averaged across the channel dimension, resulting in a 1-channel attribution map. If None, defaults to the value supplied to the constructor. Default None. \"\"\" attrs_input = self . infl_input . attributions ( x if x_preprocessed is None else x_preprocessed ) return self . mask_visualizer ( attrs_input , x , output_file , blur , threshold , masked_opacity , combine_channels ) __call__ ( self , x , x_preprocessed = None , output_file = None , blur = None , threshold = None , masked_opacity = None , combine_channels = None ) special \u00b6 Visualizes the given attributions by overlaying an attribution heatmap over the given image. Parameters \u00b6 attributions : numpy.ndarray The attributions to visualize. Expected to be in 4-D image format. x : numpy.ndarray The original image(s) over which the attributions are calculated. Must be the same shape as expected by the model used with this visualizer. x_preprocessed : numpy.ndarray, optional If the model requires a preprocessed input (e.g., with the mean subtracted) that is different from how the image should be visualized, x_preprocessed should be specified. In this case x will be used for visualization, and x_preprocessed will be passed to the model when calculating attributions. Must be the same shape as x . output_file : str, optional If specified, the resulting visualization will be saved to a file with the name given by output_file . blur : float, optional If specified, gives the radius of a Gaussian blur to be applied to the attributions before visualizing. This can be used to help focus on salient regions rather than specific salient pixels. If None, defaults to the value supplied to the constructor. Default None. threshold : float Value in the range [0, 1]. Attribution values at or below the percentile given by threshold will be masked. If None, defaults to the value supplied to the constructor. Default None. float Value in the range [0, 1] specifying the opacity for the parts of the image that are masked. Default 0.2. If None, defaults to the value supplied to the constructor. Default None. combine_channels : bool If True, the attributions will be averaged across the channel dimension, resulting in a 1-channel attribution map. If None, defaults to the value supplied to the constructor. Default None. Source code in trulens/visualizations.py def __call__ ( self , x , x_preprocessed = None , output_file = None , blur = None , threshold = None , masked_opacity = None , combine_channels = None ): \"\"\" Visualizes the given attributions by overlaying an attribution heatmap over the given image. Parameters ---------- attributions : numpy.ndarray The attributions to visualize. Expected to be in 4-D image format. x : numpy.ndarray The original image(s) over which the attributions are calculated. Must be the same shape as expected by the model used with this visualizer. x_preprocessed : numpy.ndarray, optional If the model requires a preprocessed input (e.g., with the mean subtracted) that is different from how the image should be visualized, ``x_preprocessed`` should be specified. In this case ``x`` will be used for visualization, and ``x_preprocessed`` will be passed to the model when calculating attributions. Must be the same shape as ``x``. output_file : str, optional If specified, the resulting visualization will be saved to a file with the name given by ``output_file``. blur : float, optional If specified, gives the radius of a Gaussian blur to be applied to the attributions before visualizing. This can be used to help focus on salient regions rather than specific salient pixels. If None, defaults to the value supplied to the constructor. Default None. threshold : float Value in the range [0, 1]. Attribution values at or below the percentile given by ``threshold`` will be masked. If None, defaults to the value supplied to the constructor. Default None. masked_opacity: float Value in the range [0, 1] specifying the opacity for the parts of the image that are masked. Default 0.2. If None, defaults to the value supplied to the constructor. Default None. combine_channels : bool If True, the attributions will be averaged across the channel dimension, resulting in a 1-channel attribution map. If None, defaults to the value supplied to the constructor. Default None. \"\"\" attrs_input = self . infl_input . attributions ( x if x_preprocessed is None else x_preprocessed ) return self . mask_visualizer ( attrs_input , x , output_file , blur , threshold , masked_opacity , combine_channels ) __init__ ( self , model , layer , channel , channel_axis = None , agg_fn = None , doi = None , blur = None , threshold = 0.5 , masked_opacity = 0.2 , combine_channels = True , use_attr_as_opacity = None , positive_only = None ) special \u00b6 Configures the default parameters for the __call__ method (these can be overridden by passing in values to __call__ ). Parameters: Name Type Description Default model The wrapped model whose channel we're visualizing. required layer The identifier (either index or name) of the layer in which the channel we're visualizing resides. required channel Index of the channel (for convolutional layers) or internal neuron (for fully-connected layers) that we'd like to visualize. required channel_axis If different from the channel axis specified by the backend, the supplied channel_axis will be used if operating on a convolutional layer with 4-D image format. None agg_fn Function with which to aggregate the remaining dimensions (except the batch dimension) in order to get a single scalar value for each channel; If None , a sum over each neuron in the channel will be taken. This argument is not used when the channels are scalars, e.g., for dense layers. None doi The distribution of interest to use when computing the input attributions towards the specified channel. If None , PointDoI will be used. None blur Gives the radius of a Gaussian blur to be applied to the attributions before visualizing. This can be used to help focus on salient regions rather than specific salient pixels. None threshold Value in the range [0, 1]. Attribution values at or below the percentile given by threshold (after normalization, blurring, etc.) will be masked. 0.5 masked_opacity Value in the range [0, 1] specifying the opacity for the parts of the image that are masked. 0.2 combine_channels bool If True , the attributions will be averaged across the channel dimension, resulting in a 1-channel attribution map. True use_attr_as_opacity If True , instead of using threshold and masked_opacity , the opacity of each pixel is given by the 0-1-normalized attribution value. None positive_only If True , only pixels with positive attribution will be unmasked (or given nonzero opacity when use_attr_as_opacity is true). None Source code in trulens/visualizations.py def __init__ ( self , model , layer , channel , channel_axis = None , agg_fn = None , doi = None , blur = None , threshold = 0.5 , masked_opacity = 0.2 , combine_channels : bool = True , use_attr_as_opacity = None , positive_only = None ): \"\"\" Configures the default parameters for the `__call__` method (these can be overridden by passing in values to `__call__`). Parameters: model: The wrapped model whose channel we're visualizing. layer: The identifier (either index or name) of the layer in which the channel we're visualizing resides. channel: Index of the channel (for convolutional layers) or internal neuron (for fully-connected layers) that we'd like to visualize. channel_axis: If different from the channel axis specified by the backend, the supplied `channel_axis` will be used if operating on a convolutional layer with 4-D image format. agg_fn: Function with which to aggregate the remaining dimensions (except the batch dimension) in order to get a single scalar value for each channel; If `None`, a sum over each neuron in the channel will be taken. This argument is not used when the channels are scalars, e.g., for dense layers. doi: The distribution of interest to use when computing the input attributions towards the specified channel. If `None`, `PointDoI` will be used. blur: Gives the radius of a Gaussian blur to be applied to the attributions before visualizing. This can be used to help focus on salient regions rather than specific salient pixels. threshold: Value in the range [0, 1]. Attribution values at or below the percentile given by `threshold` (after normalization, blurring, etc.) will be masked. masked_opacity: Value in the range [0, 1] specifying the opacity for the parts of the image that are masked. combine_channels: If `True`, the attributions will be averaged across the channel dimension, resulting in a 1-channel attribution map. use_attr_as_opacity: If `True`, instead of using `threshold` and `masked_opacity`, the opacity of each pixel is given by the 0-1-normalized attribution value. positive_only: If `True`, only pixels with positive attribution will be unmasked (or given nonzero opacity when `use_attr_as_opacity` is true). \"\"\" B = get_backend () if ( B is not None and ( channel_axis is None or channel_axis < 0 )): channel_axis = B . channel_axis elif ( channel_axis is None or channel_axis < 0 ): channel_axis = 1 self . mask_visualizer = MaskVisualizer ( blur , threshold , masked_opacity , combine_channels , use_attr_as_opacity , positive_only ) self . infl_input = InternalInfluence ( model , ( InputCut (), Cut ( layer )), InternalChannelQoI ( channel , channel_axis , agg_fn ), PointDoi () if doi is None else doi ) HTML ( Output ) \u00b6 HTML visualization output format. Source code in trulens/visualizations.py class HTML ( Output ): \"\"\"HTML visualization output format.\"\"\" def __init__ ( self ): try : self . m_html = importlib . import_module ( \"html\" ) except : raise ImportError ( \"HTML output requires html python module. Try 'pip install html'.\" ) def blank ( self ): return \"\" def space ( self ): return \"&nbsp;\" def escape ( self , s ): return self . m_html . escape ( s ) def linebreak ( self ): return \"<br/>\" def line ( self , s ): return f \"<span style='padding: 2px; margin: 2px; background: gray; border-radius: 4px;'> { s } </span>\" def magnitude_colored ( self , s , mag ): red = 0.0 green = 0.0 if mag > 0 : green = 1.0 # 0.5 + mag * 0.5 red = 1.0 - mag * 0.5 else : red = 1.0 green = 1.0 + mag * 0.5 #red = 0.5 - mag * 0.5 blue = min ( red , green ) # blue = 1.0 - max(red, green) return f \"<span title=' { mag : 0.3f } ' style='margin: 1px; padding: 1px; border-radius: 4px; background: black; color: rgb( { red * 255 } , { green * 255 } , { blue * 255 } );'> { s } </span>\" def append ( self , * pieces ): return '' . join ( pieces ) def render ( self , s ): return s HeatmapVisualizer ( Visualizer ) \u00b6 Visualizes attributions by overlaying an attribution heatmap over the original image, similar to how GradCAM visualizes attributions. Source code in trulens/visualizations.py class HeatmapVisualizer ( Visualizer ): \"\"\" Visualizes attributions by overlaying an attribution heatmap over the original image, similar to how GradCAM visualizes attributions. \"\"\" def __init__ ( self , overlay_opacity = 0.5 , normalization_type = None , blur = 10. , cmap = 'jet' ): \"\"\" Configures the default parameters for the `__call__` method (these can be overridden by passing in values to `__call__`). Parameters: overlay_opacity: float Value in the range [0, 1] specifying the opacity for the heatmap overlay. normalization_type: Specifies one of the following configurations for normalizing the attributions (each item is normalized separately): - `'unsigned_max'`: normalizes the attributions to the range [-1, 1] by dividing the attributions by the maximum absolute attribution value. - `'unsigned_max_positive_centered'`: same as above, but scales the values to the range [0, 1], with negative scores less than 0.5 and positive scores greater than 0.5. - `'magnitude_max'`: takes the absolute value of the attributions, then normalizes the attributions to the range [0, 1] by dividing by the maximum absolute attribution value. - `'magnitude_sum'`: takes the absolute value of the attributions, then scales them such that they sum to 1. If this option is used, each channel is normalized separately, such that each channel sums to 1. - `'signed_max'`: normalizes the attributions to the range [-1, 1] by dividing the positive values by the maximum positive attribution value and the negative values by the minimum negative attribution value. - `'signed_max_positive_centered'`: same as above, but scales the values to the range [0, 1], with negative scores less than 0.5 and positive scores greater than 0.5. - `'signed_sum'`: scales the positive attributions such that they sum to 1 and the negative attributions such that they scale to -1. If this option is used, each channel is normalized separately. - `'01'`: normalizes the attributions to the range [0, 1] by subtracting the minimum attribution value then dividing by the maximum attribution value. - `'unnormalized'`: leaves the attributions unaffected. If `None`, either `'unsigned_max'` (for single-channel data) or `'unsigned_max_positive_centered'` (for multi-channel data) is used. blur: Gives the radius of a Gaussian blur to be applied to the attributions before visualizing. This can be used to help focus on salient regions rather than specific salient pixels. cmap: matplotlib.colors.Colormap | str, optional Colormap or name of a Colormap to use for the visualization. If `None`, the colormap will be chosen based on the normalization type. This argument is only used for single-channel data (including when `combine_channels` is True). \"\"\" super () . __init__ ( combine_channels = True , normalization_type = normalization_type , blur = blur , cmap = cmap ) self . default_overlay_opacity = overlay_opacity def __call__ ( self , attributions , x , output_file = None , imshow = True , fig = None , return_tiled = False , overlay_opacity = None , normalization_type = None , blur = None , cmap = None ) -> np . ndarray : \"\"\" Visualizes the given attributions by overlaying an attribution heatmap over the given image. Parameters: attributions: A `np.ndarray` containing the attributions to be visualized. x: A `np.ndarray` of items in the same shape as `attributions` corresponding to the records explained by the given attributions. The visualization will be superimposed onto the corresponding set of records. output_file: File name to save the visualization image to. If `None`, no image will be saved, but the figure can still be displayed. imshow: If true, a the visualization will be displayed. Otherwise the figure will not be displayed, but the figure can still be saved. fig: The `pyplot` figure to display the visualization in. If `None`, a new figure will be created. return_tiled: If true, the returned array will be in the same shape as the visualization, with no batch dimension and the samples in the batch tiled along the width and height dimensions. If false, the returned array will be reshaped to match `attributions`. overlay_opacity: float Value in the range [0, 1] specifying the opacity for the heatmap overlay. If `None`, defaults to the value supplied to the constructor. normalization_type: Specifies one of the following configurations for normalizing the attributions (each item is normalized separately): - `'unsigned_max'`: normalizes the attributions to the range [-1, 1] by dividing the attributions by the maximum absolute attribution value. - `'unsigned_max_positive_centered'`: same as above, but scales the values to the range [0, 1], with negative scores less than 0.5 and positive scores greater than 0.5. - `'magnitude_max'`: takes the absolute value of the attributions, then normalizes the attributions to the range [0, 1] by dividing by the maximum absolute attribution value. - `'magnitude_sum'`: takes the absolute value of the attributions, then scales them such that they sum to 1. If this option is used, each channel is normalized separately, such that each channel sums to 1. - `'signed_max'`: normalizes the attributions to the range [-1, 1] by dividing the positive values by the maximum positive attribution value and the negative values by the minimum negative attribution value. - `'signed_max_positive_centered'`: same as above, but scales the values to the range [0, 1], with negative scores less than 0.5 and positive scores greater than 0.5. - `'signed_sum'`: scales the positive attributions such that they sum to 1 and the negative attributions such that they scale to -1. If this option is used, each channel is normalized separately. - `'01'`: normalizes the attributions to the range [0, 1] by subtracting the minimum attribution value then dividing by the maximum attribution value. - `'unnormalized'`: leaves the attributions unaffected. If `None`, defaults to the value supplied to the constructor. blur: Gives the radius of a Gaussian blur to be applied to the attributions before visualizing. This can be used to help focus on salient regions rather than specific salient pixels. If `None`, defaults to the value supplied to the constructor. cmap: matplotlib.colors.Colormap | str, optional Colormap or name of a Colormap to use for the visualization. If `None`, defaults to the value supplied to the constructor. Returns: A `np.ndarray` array of the numerical representation of the attributions as modified for the visualization. This includes normalization, blurring, etc. \"\"\" _ , normalization_type , blur , cmap = self . _check_args ( attributions , None , normalization_type , blur , cmap ) # Combine the channels. attributions = attributions . mean ( axis = get_backend () . channel_axis , keepdims = True ) # Blur the attributions so the explanation is smoother. if blur : attributions = self . _blur ( attributions , blur ) # Normalize the attributions. attributions = self . _normalize ( attributions , normalization_type ) tiled_attributions = self . tiler . tile ( attributions ) # Normalize the pixels to be in the range [0, 1]. x = self . _normalize ( x , '01' ) tiled_x = self . tiler . tile ( x ) if cmap is None : cmap = self . default_cmap if overlay_opacity is None : overlay_opacity = self . default_overlay_opacity # Display the figure: _fig = plt . figure () if fig is None else fig plt . axis ( 'off' ) plt . imshow ( tiled_x ) plt . imshow ( tiled_attributions , alpha = overlay_opacity , cmap = cmap ) if output_file : plt . savefig ( output_file , bbox_inches = 0 ) if imshow : plt . show () elif fig is None : plt . close ( _fig ) return tiled_attributions if return_tiled else attributions __call__ ( self , attributions , x , output_file = None , imshow = True , fig = None , return_tiled = False , overlay_opacity = None , normalization_type = None , blur = None , cmap = None ) special \u00b6 Visualizes the given attributions by overlaying an attribution heatmap over the given image. Parameters: Name Type Description Default attributions A np.ndarray containing the attributions to be visualized. required x A np.ndarray of items in the same shape as attributions corresponding to the records explained by the given attributions. The visualization will be superimposed onto the corresponding set of records. required output_file File name to save the visualization image to. If None , no image will be saved, but the figure can still be displayed. None imshow If true, a the visualization will be displayed. Otherwise the figure will not be displayed, but the figure can still be saved. True fig The pyplot figure to display the visualization in. If None , a new figure will be created. None return_tiled If true, the returned array will be in the same shape as the visualization, with no batch dimension and the samples in the batch tiled along the width and height dimensions. If false, the returned array will be reshaped to match attributions . False overlay_opacity float Value in the range [0, 1] specifying the opacity for the heatmap overlay. If None , defaults to the value supplied to the constructor. None normalization_type Specifies one of the following configurations for normalizing the attributions (each item is normalized separately): 'unsigned_max' : normalizes the attributions to the range [-1, 1] by dividing the attributions by the maximum absolute attribution value. 'unsigned_max_positive_centered' : same as above, but scales the values to the range [0, 1], with negative scores less than 0.5 and positive scores greater than 0.5. 'magnitude_max' : takes the absolute value of the attributions, then normalizes the attributions to the range [0, 1] by dividing by the maximum absolute attribution value. 'magnitude_sum' : takes the absolute value of the attributions, then scales them such that they sum to 1. If this option is used, each channel is normalized separately, such that each channel sums to 1. 'signed_max' : normalizes the attributions to the range [-1, 1] by dividing the positive values by the maximum positive attribution value and the negative values by the minimum negative attribution value. 'signed_max_positive_centered' : same as above, but scales the values to the range [0, 1], with negative scores less than 0.5 and positive scores greater than 0.5. 'signed_sum' : scales the positive attributions such that they sum to 1 and the negative attributions such that they scale to -1. If this option is used, each channel is normalized separately. '01' : normalizes the attributions to the range [0, 1] by subtracting the minimum attribution value then dividing by the maximum attribution value. 'unnormalized' : leaves the attributions unaffected. If None , defaults to the value supplied to the constructor. None blur Gives the radius of a Gaussian blur to be applied to the attributions before visualizing. This can be used to help focus on salient regions rather than specific salient pixels. If None , defaults to the value supplied to the constructor. None cmap matplotlib.colors.Colormap | str, optional Colormap or name of a Colormap to use for the visualization. If None , defaults to the value supplied to the constructor. None Returns: Type Description ndarray A np.ndarray array of the numerical representation of the attributions as modified for the visualization. This includes normalization, blurring, etc. Source code in trulens/visualizations.py def __call__ ( self , attributions , x , output_file = None , imshow = True , fig = None , return_tiled = False , overlay_opacity = None , normalization_type = None , blur = None , cmap = None ) -> np . ndarray : \"\"\" Visualizes the given attributions by overlaying an attribution heatmap over the given image. Parameters: attributions: A `np.ndarray` containing the attributions to be visualized. x: A `np.ndarray` of items in the same shape as `attributions` corresponding to the records explained by the given attributions. The visualization will be superimposed onto the corresponding set of records. output_file: File name to save the visualization image to. If `None`, no image will be saved, but the figure can still be displayed. imshow: If true, a the visualization will be displayed. Otherwise the figure will not be displayed, but the figure can still be saved. fig: The `pyplot` figure to display the visualization in. If `None`, a new figure will be created. return_tiled: If true, the returned array will be in the same shape as the visualization, with no batch dimension and the samples in the batch tiled along the width and height dimensions. If false, the returned array will be reshaped to match `attributions`. overlay_opacity: float Value in the range [0, 1] specifying the opacity for the heatmap overlay. If `None`, defaults to the value supplied to the constructor. normalization_type: Specifies one of the following configurations for normalizing the attributions (each item is normalized separately): - `'unsigned_max'`: normalizes the attributions to the range [-1, 1] by dividing the attributions by the maximum absolute attribution value. - `'unsigned_max_positive_centered'`: same as above, but scales the values to the range [0, 1], with negative scores less than 0.5 and positive scores greater than 0.5. - `'magnitude_max'`: takes the absolute value of the attributions, then normalizes the attributions to the range [0, 1] by dividing by the maximum absolute attribution value. - `'magnitude_sum'`: takes the absolute value of the attributions, then scales them such that they sum to 1. If this option is used, each channel is normalized separately, such that each channel sums to 1. - `'signed_max'`: normalizes the attributions to the range [-1, 1] by dividing the positive values by the maximum positive attribution value and the negative values by the minimum negative attribution value. - `'signed_max_positive_centered'`: same as above, but scales the values to the range [0, 1], with negative scores less than 0.5 and positive scores greater than 0.5. - `'signed_sum'`: scales the positive attributions such that they sum to 1 and the negative attributions such that they scale to -1. If this option is used, each channel is normalized separately. - `'01'`: normalizes the attributions to the range [0, 1] by subtracting the minimum attribution value then dividing by the maximum attribution value. - `'unnormalized'`: leaves the attributions unaffected. If `None`, defaults to the value supplied to the constructor. blur: Gives the radius of a Gaussian blur to be applied to the attributions before visualizing. This can be used to help focus on salient regions rather than specific salient pixels. If `None`, defaults to the value supplied to the constructor. cmap: matplotlib.colors.Colormap | str, optional Colormap or name of a Colormap to use for the visualization. If `None`, defaults to the value supplied to the constructor. Returns: A `np.ndarray` array of the numerical representation of the attributions as modified for the visualization. This includes normalization, blurring, etc. \"\"\" _ , normalization_type , blur , cmap = self . _check_args ( attributions , None , normalization_type , blur , cmap ) # Combine the channels. attributions = attributions . mean ( axis = get_backend () . channel_axis , keepdims = True ) # Blur the attributions so the explanation is smoother. if blur : attributions = self . _blur ( attributions , blur ) # Normalize the attributions. attributions = self . _normalize ( attributions , normalization_type ) tiled_attributions = self . tiler . tile ( attributions ) # Normalize the pixels to be in the range [0, 1]. x = self . _normalize ( x , '01' ) tiled_x = self . tiler . tile ( x ) if cmap is None : cmap = self . default_cmap if overlay_opacity is None : overlay_opacity = self . default_overlay_opacity # Display the figure: _fig = plt . figure () if fig is None else fig plt . axis ( 'off' ) plt . imshow ( tiled_x ) plt . imshow ( tiled_attributions , alpha = overlay_opacity , cmap = cmap ) if output_file : plt . savefig ( output_file , bbox_inches = 0 ) if imshow : plt . show () elif fig is None : plt . close ( _fig ) return tiled_attributions if return_tiled else attributions __init__ ( self , overlay_opacity = 0.5 , normalization_type = None , blur = 10.0 , cmap = 'jet' ) special \u00b6 Configures the default parameters for the __call__ method (these can be overridden by passing in values to __call__ ). Parameters: Name Type Description Default overlay_opacity float Value in the range [0, 1] specifying the opacity for the heatmap overlay. 0.5 normalization_type Specifies one of the following configurations for normalizing the attributions (each item is normalized separately): 'unsigned_max' : normalizes the attributions to the range [-1, 1] by dividing the attributions by the maximum absolute attribution value. 'unsigned_max_positive_centered' : same as above, but scales the values to the range [0, 1], with negative scores less than 0.5 and positive scores greater than 0.5. 'magnitude_max' : takes the absolute value of the attributions, then normalizes the attributions to the range [0, 1] by dividing by the maximum absolute attribution value. 'magnitude_sum' : takes the absolute value of the attributions, then scales them such that they sum to 1. If this option is used, each channel is normalized separately, such that each channel sums to 1. 'signed_max' : normalizes the attributions to the range [-1, 1] by dividing the positive values by the maximum positive attribution value and the negative values by the minimum negative attribution value. 'signed_max_positive_centered' : same as above, but scales the values to the range [0, 1], with negative scores less than 0.5 and positive scores greater than 0.5. 'signed_sum' : scales the positive attributions such that they sum to 1 and the negative attributions such that they scale to -1. If this option is used, each channel is normalized separately. '01' : normalizes the attributions to the range [0, 1] by subtracting the minimum attribution value then dividing by the maximum attribution value. 'unnormalized' : leaves the attributions unaffected. If None , either 'unsigned_max' (for single-channel data) or 'unsigned_max_positive_centered' (for multi-channel data) is used. None blur Gives the radius of a Gaussian blur to be applied to the attributions before visualizing. This can be used to help focus on salient regions rather than specific salient pixels. 10.0 cmap matplotlib.colors.Colormap | str, optional Colormap or name of a Colormap to use for the visualization. If None , the colormap will be chosen based on the normalization type. This argument is only used for single-channel data (including when combine_channels is True). 'jet' Source code in trulens/visualizations.py def __init__ ( self , overlay_opacity = 0.5 , normalization_type = None , blur = 10. , cmap = 'jet' ): \"\"\" Configures the default parameters for the `__call__` method (these can be overridden by passing in values to `__call__`). Parameters: overlay_opacity: float Value in the range [0, 1] specifying the opacity for the heatmap overlay. normalization_type: Specifies one of the following configurations for normalizing the attributions (each item is normalized separately): - `'unsigned_max'`: normalizes the attributions to the range [-1, 1] by dividing the attributions by the maximum absolute attribution value. - `'unsigned_max_positive_centered'`: same as above, but scales the values to the range [0, 1], with negative scores less than 0.5 and positive scores greater than 0.5. - `'magnitude_max'`: takes the absolute value of the attributions, then normalizes the attributions to the range [0, 1] by dividing by the maximum absolute attribution value. - `'magnitude_sum'`: takes the absolute value of the attributions, then scales them such that they sum to 1. If this option is used, each channel is normalized separately, such that each channel sums to 1. - `'signed_max'`: normalizes the attributions to the range [-1, 1] by dividing the positive values by the maximum positive attribution value and the negative values by the minimum negative attribution value. - `'signed_max_positive_centered'`: same as above, but scales the values to the range [0, 1], with negative scores less than 0.5 and positive scores greater than 0.5. - `'signed_sum'`: scales the positive attributions such that they sum to 1 and the negative attributions such that they scale to -1. If this option is used, each channel is normalized separately. - `'01'`: normalizes the attributions to the range [0, 1] by subtracting the minimum attribution value then dividing by the maximum attribution value. - `'unnormalized'`: leaves the attributions unaffected. If `None`, either `'unsigned_max'` (for single-channel data) or `'unsigned_max_positive_centered'` (for multi-channel data) is used. blur: Gives the radius of a Gaussian blur to be applied to the attributions before visualizing. This can be used to help focus on salient regions rather than specific salient pixels. cmap: matplotlib.colors.Colormap | str, optional Colormap or name of a Colormap to use for the visualization. If `None`, the colormap will be chosen based on the normalization type. This argument is only used for single-channel data (including when `combine_channels` is True). \"\"\" super () . __init__ ( combine_channels = True , normalization_type = normalization_type , blur = blur , cmap = cmap ) self . default_overlay_opacity = overlay_opacity IPython ( HTML ) \u00b6 Interactive python visualization output format. Source code in trulens/visualizations.py class IPython ( HTML ): \"\"\"Interactive python visualization output format.\"\"\" def __init__ ( self ): super ( IPython , self ) . __init__ () try : self . m_ipy = importlib . import_module ( \"IPython\" ) except : raise ImportError ( \"Jupyter output requires IPython python module. Try 'pip install ipykernel'.\" ) def render ( self , s : str ): html = HTML . render ( self , s ) return self . m_ipy . display . HTML ( html ) MaskVisualizer \u00b6 Visualizes attributions by masking the original image to highlight the regions with influence above a given threshold percentile. Intended particularly for use with input-attributions. Source code in trulens/visualizations.py class MaskVisualizer ( object ): \"\"\" Visualizes attributions by masking the original image to highlight the regions with influence above a given threshold percentile. Intended particularly for use with input-attributions. \"\"\" def __init__ ( self , blur = 5. , threshold = 0.5 , masked_opacity = 0.2 , combine_channels = True , use_attr_as_opacity = False , positive_only = True ): \"\"\" Configures the default parameters for the `__call__` method (these can be overridden by passing in values to `__call__`). Parameters: blur: Gives the radius of a Gaussian blur to be applied to the attributions before visualizing. This can be used to help focus on salient regions rather than specific salient pixels. threshold: Value in the range [0, 1]. Attribution values at or below the percentile given by `threshold` (after normalization, blurring, etc.) will be masked. masked_opacity: Value in the range [0, 1] specifying the opacity for the parts of the image that are masked. combine_channels: If `True`, the attributions will be averaged across the channel dimension, resulting in a 1-channel attribution map. use_attr_as_opacity: If `True`, instead of using `threshold` and `masked_opacity`, the opacity of each pixel is given by the 0-1-normalized attribution value. positive_only: If `True`, only pixels with positive attribution will be unmasked (or given nonzero opacity when `use_attr_as_opacity` is true). \"\"\" self . default_blur = blur self . default_thresh = threshold self . default_masked_opacity = masked_opacity self . default_combine_channels = combine_channels # TODO(klas): in the future we can allow configuring of tiling settings # by allowing the user to specify the tiler. self . tiler = Tiler () def __call__ ( self , attributions , x , output_file = None , imshow = True , fig = None , return_tiled = True , blur = None , threshold = None , masked_opacity = None , combine_channels = None , use_attr_as_opacity = None , positive_only = None ): channel_axis = get_backend () . channel_axis if attributions . shape != x . shape : raise ValueError ( 'Shape of `attributions` {} must match shape of `x` {} ' . format ( attributions . shape , x . shape ) ) if blur is None : blur = self . default_blur if threshold is None : threshold = self . default_thresh if masked_opacity is None : masked_opacity = self . default_masked_opacity if combine_channels is None : combine_channels = self . default_combine_channels if len ( attributions . shape ) != 4 : raise ValueError ( '`MaskVisualizer` is inteded for 4-D image-format data. Given ' 'input with dimension {} ' . format ( len ( attributions . shape )) ) if combine_channels is None : combine_channels = self . default_combine_channels if combine_channels : attributions = attributions . mean ( axis = channel_axis , keepdims = True ) if x . shape [ channel_axis ] not in ( 1 , 3 , 4 ): raise ValueError ( 'To visualize, attributions must have either 1, 3, or 4 color ' 'channels, but Visualizer got {} channels. \\n ' 'If you are visualizing an internal layer, consider setting ' '`combine_channels` to True' . format ( attributions . shape [ channel_axis ] ) ) # Blur the attributions so the explanation is smoother. if blur is not None : attributions = [ gaussian_filter ( a , blur ) for a in attributions ] # If `positive_only` clip attributions. if positive_only : attributions = np . maximum ( attributions , 0 ) # Normalize the attributions to be in the range [0, 1]. attributions = [ a - a . min () for a in attributions ] attributions = [ 0. * a if a . max () == 0. else a / a . max () for a in attributions ] # Normalize the pixels to be in the range [0, 1] x = [ xc - xc . min () for xc in x ] x = np . array ([ 0. * xc if xc . max () == 0. else xc / xc . max () for xc in x ]) # Threshold the attributions to create a mask. if threshold is not None : percentiles = [ np . percentile ( a , 100 * threshold ) for a in attributions ] masks = np . array ( [ np . maximum ( a > p , masked_opacity ) for a , p in zip ( attributions , percentiles ) ] ) else : masks = np . array ( attributions ) # Use the mask on the original image to visualize the explanation. attributions = masks * x tiled_attributions = self . tiler . tile ( attributions ) if imshow : plt . axis ( 'off' ) plt . imshow ( tiled_attributions ) if output_file : plt . savefig ( output_file , bbox_inches = 0 ) return tiled_attributions if return_tiled else attributions __init__ ( self , blur = 5.0 , threshold = 0.5 , masked_opacity = 0.2 , combine_channels = True , use_attr_as_opacity = False , positive_only = True ) special \u00b6 Configures the default parameters for the __call__ method (these can be overridden by passing in values to __call__ ). Parameters: Name Type Description Default blur Gives the radius of a Gaussian blur to be applied to the attributions before visualizing. This can be used to help focus on salient regions rather than specific salient pixels. 5.0 threshold Value in the range [0, 1]. Attribution values at or below the percentile given by threshold (after normalization, blurring, etc.) will be masked. 0.5 masked_opacity Value in the range [0, 1] specifying the opacity for the parts of the image that are masked. 0.2 combine_channels If True , the attributions will be averaged across the channel dimension, resulting in a 1-channel attribution map. True use_attr_as_opacity If True , instead of using threshold and masked_opacity , the opacity of each pixel is given by the 0-1-normalized attribution value. False positive_only If True , only pixels with positive attribution will be unmasked (or given nonzero opacity when use_attr_as_opacity is true). True Source code in trulens/visualizations.py def __init__ ( self , blur = 5. , threshold = 0.5 , masked_opacity = 0.2 , combine_channels = True , use_attr_as_opacity = False , positive_only = True ): \"\"\" Configures the default parameters for the `__call__` method (these can be overridden by passing in values to `__call__`). Parameters: blur: Gives the radius of a Gaussian blur to be applied to the attributions before visualizing. This can be used to help focus on salient regions rather than specific salient pixels. threshold: Value in the range [0, 1]. Attribution values at or below the percentile given by `threshold` (after normalization, blurring, etc.) will be masked. masked_opacity: Value in the range [0, 1] specifying the opacity for the parts of the image that are masked. combine_channels: If `True`, the attributions will be averaged across the channel dimension, resulting in a 1-channel attribution map. use_attr_as_opacity: If `True`, instead of using `threshold` and `masked_opacity`, the opacity of each pixel is given by the 0-1-normalized attribution value. positive_only: If `True`, only pixels with positive attribution will be unmasked (or given nonzero opacity when `use_attr_as_opacity` is true). \"\"\" self . default_blur = blur self . default_thresh = threshold self . default_masked_opacity = masked_opacity self . default_combine_channels = combine_channels # TODO(klas): in the future we can allow configuring of tiling settings # by allowing the user to specify the tiler. self . tiler = Tiler () NLP \u00b6 NLP Visualization tools. Source code in trulens/visualizations.py class NLP ( object ): \"\"\"NLP Visualization tools.\"\"\" # Batches of text inputs not yet tokenized. TextBatch = TypeVar ( \"TextBatch\" ) # Inputs that are directly accepted by wrapped models, tokenized. # TODO(piotrm): Reuse other typevars/aliases from elsewhere. ModelInput = TypeVar ( \"ModelInput\" ) # Outputs produced by wrapped models. # TODO(piotrm): Reuse other typevars/aliases from elsewhere. ModelOutput = TypeVar ( \"ModelOutput\" ) def __init__ ( self , wrapper : ModelWrapper , output : Optional [ Output ] = None , labels : Optional [ Iterable [ str ]] = None , tokenize : Optional [ Callable [[ TextBatch ], ModelInputs ]] = None , decode : Optional [ Callable [[ Tensor ], str ]] = None , input_accessor : Optional [ Callable [[ ModelInputs ], Iterable [ Tensor ]]] = None , output_accessor : Optional [ Callable [[ ModelOutput ], Iterable [ Tensor ]]] = None , attr_aggregate : Optional [ Callable [[ Tensor ], Tensor ]] = None , hidden_tokens : Optional [ Set [ int ]] = set () ): \"\"\"Initializate NLP visualization tools for a given environment. Parameters: wrapper: ModelWrapper The wrapped model whose channel we're visualizing. output: Output, optional Visualization output format. Defaults to PlainText unless ipython is detected and in which case defaults to IPython format. labels: Iterable[str], optional Names of prediction classes for classification models. tokenize: Callable[[TextBatch], ModelInput], optional Method to tokenize an instance. decode: Callable[[Tensor], str], optional Method to invert/decode the tokenization. input_accessor: Callable[[ModelInputs], Iterable[Tensor]], optional Method to extract input/token ids from model inputs (tokenize output) if needed. output_accessor: Callable[[ModelOutput], Iterable[Tensor]], optional Method to extract outout logits from output structures if needed. attr_aggregate: Callable[[Tensor], Tensor], optional Method to aggregate attribution for embedding into a single value. Defaults to sum. hidden_tokens: Set[int], optional For token-based visualizations, which tokens to hide. \"\"\" if output is None : try : # check if running in interactive python (jupyer, colab, etc) to # use appropriate output format get_ipython () output = IPython () except NameError : output = PlainText () tru_logger ( \"WARNING: could not guess preferred visualization output format, using PlainText\" ) # TODO: automatic inference of various parameters for common repositories like huggingface, tfhub. self . output = output self . labels = labels self . tokenize = tokenize self . decode = decode self . wrapper = wrapper self . input_accessor = input_accessor # could be inferred self . output_accessor = output_accessor # could be inferred B = get_backend () if attr_aggregate is None : attr_aggregate = B . sum self . attr_aggregate = attr_aggregate self . hidden_tokens = hidden_tokens def token_attribution ( self , texts : Iterable [ str ], attr : AttributionMethod ): \"\"\"Visualize a token-based input attribution on given `texts` inputs via the attribution method `attr`. Parameters: texts: Iterable[str] The input texts to visualize. attr: AttributionMethod The attribution method to generate the token importances with. Returns: Any The visualization in the format specified by this class's `output` parameter. \"\"\" B = get_backend () if self . tokenize is None : return ValueError ( \"tokenize not provided to NLP visualizer.\" ) inputs = self . tokenize ( texts ) outputs = inputs . call_on ( self . wrapper . _model ) attrs = inputs . call_on ( attr . attributions ) content = self . output . blank () input_ids = inputs if self . input_accessor is not None : input_ids = self . input_accessor ( inputs ) if ( not isinstance ( input_ids , Iterable )) or isinstance ( input_ids , dict ): raise ValueError ( f \"Inputs ( { input_ids . __class__ . __name__ } ) need to be iterable over instances. You might need to set input_accessor.\" ) output_logits = outputs if self . output_accessor is not None : output_logits = self . output_accessor ( outputs ) if ( not isinstance ( output_logits , Iterable )) or isinstance ( output_logits , dict ): raise ValueError ( f \"Outputs ( { output_logits . __class__ . __name__ } ) need to be iterable over instances. You might need to set output_accessor.\" ) for i , ( sentence_word_id , attr , logits ) in enumerate ( zip ( input_ids , attrs , output_logits )): logits = logits . to ( 'cpu' ) . detach () . numpy () pred = logits . argmax () if self . labels is not None : pred_name = self . labels [ pred ] else : pred_name = str ( pred ) sent = self . output . append ( self . output . escape ( pred_name ), \":\" , self . output . space () ) for word_id , attr in zip ( sentence_word_id , attr ): word_id = int ( B . as_array ( word_id )) if word_id in self . hidden_tokens : continue if self . decode is not None : word = self . decode ( word_id ) else : word = str ( word_id ) mag = self . attr_aggregate ( attr ) if word [ 0 ] == ' ' : word = word [ 1 :] sent = self . output . append ( sent , self . output . space ()) sent = self . output . append ( sent , self . output . magnitude_colored ( self . output . escape ( word ), mag ) ) content = self . output . append ( content , self . output . line ( sent ), self . output . linebreak (), self . output . linebreak () ) return self . output . render ( content ) __init__ ( self , wrapper , output = None , labels = None , tokenize = None , decode = None , input_accessor = None , output_accessor = None , attr_aggregate = None , hidden_tokens = set ()) special \u00b6 Initializate NLP visualization tools for a given environment. Parameters: Name Type Description Default wrapper ModelWrapper ModelWrapper The wrapped model whose channel we're visualizing. required output Optional[trulens.visualizations.Output] Output, optional Visualization output format. Defaults to PlainText unless ipython is detected and in which case defaults to IPython format. None labels Optional[Iterable[str]] Iterable[str], optional Names of prediction classes for classification models. None tokenize Optional[Callable[[~TextBatch], trulens.utils.ModelInputs]] Callable[[TextBatch], ModelInput], optional Method to tokenize an instance. None decode Optional[Callable[[~Tensor], str]] Callable[[Tensor], str], optional Method to invert/decode the tokenization. None input_accessor Optional[Callable[[trulens.utils.ModelInputs], Iterable[~Tensor]]] Callable[[ModelInputs], Iterable[Tensor]], optional Method to extract input/token ids from model inputs (tokenize output) if needed. None output_accessor Optional[Callable[[~ModelOutput], Iterable[~Tensor]]] Callable[[ModelOutput], Iterable[Tensor]], optional Method to extract outout logits from output structures if needed. None attr_aggregate Optional[Callable[[~Tensor], ~Tensor]] Callable[[Tensor], Tensor], optional Method to aggregate attribution for embedding into a single value. Defaults to sum. None hidden_tokens Optional[Set[int]] Set[int], optional For token-based visualizations, which tokens to hide. set() Source code in trulens/visualizations.py def __init__ ( self , wrapper : ModelWrapper , output : Optional [ Output ] = None , labels : Optional [ Iterable [ str ]] = None , tokenize : Optional [ Callable [[ TextBatch ], ModelInputs ]] = None , decode : Optional [ Callable [[ Tensor ], str ]] = None , input_accessor : Optional [ Callable [[ ModelInputs ], Iterable [ Tensor ]]] = None , output_accessor : Optional [ Callable [[ ModelOutput ], Iterable [ Tensor ]]] = None , attr_aggregate : Optional [ Callable [[ Tensor ], Tensor ]] = None , hidden_tokens : Optional [ Set [ int ]] = set () ): \"\"\"Initializate NLP visualization tools for a given environment. Parameters: wrapper: ModelWrapper The wrapped model whose channel we're visualizing. output: Output, optional Visualization output format. Defaults to PlainText unless ipython is detected and in which case defaults to IPython format. labels: Iterable[str], optional Names of prediction classes for classification models. tokenize: Callable[[TextBatch], ModelInput], optional Method to tokenize an instance. decode: Callable[[Tensor], str], optional Method to invert/decode the tokenization. input_accessor: Callable[[ModelInputs], Iterable[Tensor]], optional Method to extract input/token ids from model inputs (tokenize output) if needed. output_accessor: Callable[[ModelOutput], Iterable[Tensor]], optional Method to extract outout logits from output structures if needed. attr_aggregate: Callable[[Tensor], Tensor], optional Method to aggregate attribution for embedding into a single value. Defaults to sum. hidden_tokens: Set[int], optional For token-based visualizations, which tokens to hide. \"\"\" if output is None : try : # check if running in interactive python (jupyer, colab, etc) to # use appropriate output format get_ipython () output = IPython () except NameError : output = PlainText () tru_logger ( \"WARNING: could not guess preferred visualization output format, using PlainText\" ) # TODO: automatic inference of various parameters for common repositories like huggingface, tfhub. self . output = output self . labels = labels self . tokenize = tokenize self . decode = decode self . wrapper = wrapper self . input_accessor = input_accessor # could be inferred self . output_accessor = output_accessor # could be inferred B = get_backend () if attr_aggregate is None : attr_aggregate = B . sum self . attr_aggregate = attr_aggregate self . hidden_tokens = hidden_tokens token_attribution ( self , texts , attr ) \u00b6 Visualize a token-based input attribution on given texts inputs via the attribution method attr . Parameters: Name Type Description Default texts Iterable[str] Iterable[str] The input texts to visualize. required attr AttributionMethod AttributionMethod The attribution method to generate the token importances with. required Any The visualization in the format specified by this class's output parameter. Source code in trulens/visualizations.py def token_attribution ( self , texts : Iterable [ str ], attr : AttributionMethod ): \"\"\"Visualize a token-based input attribution on given `texts` inputs via the attribution method `attr`. Parameters: texts: Iterable[str] The input texts to visualize. attr: AttributionMethod The attribution method to generate the token importances with. Returns: Any The visualization in the format specified by this class's `output` parameter. \"\"\" B = get_backend () if self . tokenize is None : return ValueError ( \"tokenize not provided to NLP visualizer.\" ) inputs = self . tokenize ( texts ) outputs = inputs . call_on ( self . wrapper . _model ) attrs = inputs . call_on ( attr . attributions ) content = self . output . blank () input_ids = inputs if self . input_accessor is not None : input_ids = self . input_accessor ( inputs ) if ( not isinstance ( input_ids , Iterable )) or isinstance ( input_ids , dict ): raise ValueError ( f \"Inputs ( { input_ids . __class__ . __name__ } ) need to be iterable over instances. You might need to set input_accessor.\" ) output_logits = outputs if self . output_accessor is not None : output_logits = self . output_accessor ( outputs ) if ( not isinstance ( output_logits , Iterable )) or isinstance ( output_logits , dict ): raise ValueError ( f \"Outputs ( { output_logits . __class__ . __name__ } ) need to be iterable over instances. You might need to set output_accessor.\" ) for i , ( sentence_word_id , attr , logits ) in enumerate ( zip ( input_ids , attrs , output_logits )): logits = logits . to ( 'cpu' ) . detach () . numpy () pred = logits . argmax () if self . labels is not None : pred_name = self . labels [ pred ] else : pred_name = str ( pred ) sent = self . output . append ( self . output . escape ( pred_name ), \":\" , self . output . space () ) for word_id , attr in zip ( sentence_word_id , attr ): word_id = int ( B . as_array ( word_id )) if word_id in self . hidden_tokens : continue if self . decode is not None : word = self . decode ( word_id ) else : word = str ( word_id ) mag = self . attr_aggregate ( attr ) if word [ 0 ] == ' ' : word = word [ 1 :] sent = self . output . append ( sent , self . output . space ()) sent = self . output . append ( sent , self . output . magnitude_colored ( self . output . escape ( word ), mag ) ) content = self . output . append ( content , self . output . line ( sent ), self . output . linebreak (), self . output . linebreak () ) return self . output . render ( content ) Output ( ABC ) \u00b6 Base class for visualization output formats. Source code in trulens/visualizations.py class Output ( ABC ): \"\"\"Base class for visualization output formats.\"\"\" @abstractmethod def blank ( self ) -> str : ... @abstractmethod def space ( self ) -> str : ... @abstractmethod def escape ( self , s : str ) -> str : ... @abstractmethod def line ( self , s : str ) -> str : ... @abstractmethod def magnitude_colored ( self , s : str , mag : float ) -> str : ... @abstractmethod def append ( self , * parts : Iterable [ str ]) -> str : ... @abstractmethod def render ( self , s : str ) -> str : ... PlainText ( Output ) \u00b6 Plain text visualization output format. Source code in trulens/visualizations.py class PlainText ( Output ): \"\"\"Plain text visualization output format.\"\"\" def blank ( self ): return \"\" def space ( self ): return \" \" def escape ( self , s ): return s def line ( self , s ): return s def magnitude_colored ( self , s , mag ): return f \" { s } ( { mag : 0.3f } )\" def append ( self , * parts ): return '' . join ( parts ) def render ( self , s ): return s Tiler \u00b6 Used to tile batched images or attributions. Source code in trulens/visualizations.py class Tiler ( object ): \"\"\" Used to tile batched images or attributions. \"\"\" def tile ( self , a : np . ndarray ) -> np . ndarray : \"\"\" Tiles the given array into a grid that is as square as possible. Parameters: a: An array of 4D batched image data. Returns: A tiled array of the images from `a`. The resulting array has rank 3 for color images, and 2 for grayscale images (the batch dimension is removed, as well as the channel dimension for grayscale images). The resulting array has its color channel dimension ordered last to fit the requirements of the `matplotlib` library. \"\"\" # `pyplot` expects the channels to come last. if get_backend () . dim_order == 'channels_first' : a = a . transpose (( 0 , 2 , 3 , 1 )) n , h , w , c = a . shape rows = int ( np . sqrt ( n )) cols = int ( np . ceil ( float ( n ) / rows )) new_a = np . zeros (( h * rows , w * cols , c )) for i , x in enumerate ( a ): row = i // cols col = i % cols new_a [ row * h :( row + 1 ) * h , col * w :( col + 1 ) * w ] = x return np . squeeze ( new_a ) tile ( self , a ) \u00b6 Tiles the given array into a grid that is as square as possible. Parameters: Name Type Description Default a ndarray An array of 4D batched image data. required Returns: Type Description ndarray A tiled array of the images from a . The resulting array has rank 3 for color images, and 2 for grayscale images (the batch dimension is removed, as well as the channel dimension for grayscale images). The resulting array has its color channel dimension ordered last to fit the requirements of the matplotlib library. Source code in trulens/visualizations.py def tile ( self , a : np . ndarray ) -> np . ndarray : \"\"\" Tiles the given array into a grid that is as square as possible. Parameters: a: An array of 4D batched image data. Returns: A tiled array of the images from `a`. The resulting array has rank 3 for color images, and 2 for grayscale images (the batch dimension is removed, as well as the channel dimension for grayscale images). The resulting array has its color channel dimension ordered last to fit the requirements of the `matplotlib` library. \"\"\" # `pyplot` expects the channels to come last. if get_backend () . dim_order == 'channels_first' : a = a . transpose (( 0 , 2 , 3 , 1 )) n , h , w , c = a . shape rows = int ( np . sqrt ( n )) cols = int ( np . ceil ( float ( n ) / rows )) new_a = np . zeros (( h * rows , w * cols , c )) for i , x in enumerate ( a ): row = i // cols col = i % cols new_a [ row * h :( row + 1 ) * h , col * w :( col + 1 ) * w ] = x return np . squeeze ( new_a ) Visualizer \u00b6 Visualizes attributions directly as a color image. Intended particularly for use with input-attributions. This can also be used for viewing images (rather than attributions). Source code in trulens/visualizations.py class Visualizer ( object ): \"\"\" Visualizes attributions directly as a color image. Intended particularly for use with input-attributions. This can also be used for viewing images (rather than attributions). \"\"\" def __init__ ( self , combine_channels : bool = False , normalization_type : str = None , blur : float = 0. , cmap : Colormap = None ): \"\"\" Configures the default parameters for the `__call__` method (these can be overridden by passing in values to `__call__`). Parameters: combine_channels: If `True`, the attributions will be averaged across the channel dimension, resulting in a 1-channel attribution map. normalization_type: Specifies one of the following configurations for normalizing the attributions (each item is normalized separately): - `'unsigned_max'`: normalizes the attributions to the range [-1, 1] by dividing the attributions by the maximum absolute attribution value. - `'unsigned_max_positive_centered'`: same as above, but scales the values to the range [0, 1], with negative scores less than 0.5 and positive scores greater than 0.5. - `'magnitude_max'`: takes the absolute value of the attributions, then normalizes the attributions to the range [0, 1] by dividing by the maximum absolute attribution value. - `'magnitude_sum'`: takes the absolute value of the attributions, then scales them such that they sum to 1. If this option is used, each channel is normalized separately, such that each channel sums to 1. - `'signed_max'`: normalizes the attributions to the range [-1, 1] by dividing the positive values by the maximum positive attribution value and the negative values by the minimum negative attribution value. - `'signed_max_positive_centered'`: same as above, but scales the values to the range [0, 1], with negative scores less than 0.5 and positive scores greater than 0.5. - `'signed_sum'`: scales the positive attributions such that they sum to 1 and the negative attributions such that they scale to -1. If this option is used, each channel is normalized separately. - `'01'`: normalizes the attributions to the range [0, 1] by subtracting the minimum attribution value then dividing by the maximum attribution value. - `'unnormalized'`: leaves the attributions unaffected. If `None`, either `'unsigned_max'` (for single-channel data) or `'unsigned_max_positive_centered'` (for multi-channel data) is used. blur: Gives the radius of a Gaussian blur to be applied to the attributions before visualizing. This can be used to help focus on salient regions rather than specific salient pixels. cmap: matplotlib.colors.Colormap | str, optional Colormap or name of a Colormap to use for the visualization. If `None`, the colormap will be chosen based on the normalization type. This argument is only used for single-channel data (including when `combine_channels` is True). \"\"\" self . default_combine_channels = combine_channels self . default_normalization_type = normalization_type self . default_blur = blur self . default_cmap = cmap if cmap is not None else self . _get_hotcold () # TODO(klas): in the future we can allow configuring of tiling settings # by allowing the user to specify the tiler. self . tiler = Tiler () def __call__ ( self , attributions , output_file = None , imshow = True , fig = None , return_tiled = False , combine_channels = None , normalization_type = None , blur = None , cmap = None ) -> np . ndarray : \"\"\" Visualizes the given attributions. Parameters: attributions: A `np.ndarray` containing the attributions to be visualized. output_file: File name to save the visualization image to. If `None`, no image will be saved, but the figure can still be displayed. imshow: If true, a the visualization will be displayed. Otherwise the figure will not be displayed, but the figure can still be saved. fig: The `pyplot` figure to display the visualization in. If `None`, a new figure will be created. return_tiled: If true, the returned array will be in the same shape as the visualization, with no batch dimension and the samples in the batch tiled along the width and height dimensions. If false, the returned array will be reshaped to match `attributions`. combine_channels: If `True`, the attributions will be averaged across the channel dimension, resulting in a 1-channel attribution map. If `None`, defaults to the value supplied to the constructor. normalization_type: Specifies one of the following configurations for normalizing the attributions (each item is normalized separately): - `'unsigned_max'`: normalizes the attributions to the range [-1, 1] by dividing the attributions by the maximum absolute attribution value. - `'unsigned_max_positive_centered'`: same as above, but scales the values to the range [0, 1], with negative scores less than 0.5 and positive scores greater than 0.5. - `'magnitude_max'`: takes the absolute value of the attributions, then normalizes the attributions to the range [0, 1] by dividing by the maximum absolute attribution value. - `'magnitude_sum'`: takes the absolute value of the attributions, then scales them such that they sum to 1. If this option is used, each channel is normalized separately, such that each channel sums to 1. - `'signed_max'`: normalizes the attributions to the range [-1, 1] by dividing the positive values by the maximum positive attribution value and the negative values by the minimum negative attribution value. - `'signed_max_positive_centered'`: same as above, but scales the values to the range [0, 1], with negative scores less than 0.5 and positive scores greater than 0.5. - `'signed_sum'`: scales the positive attributions such that they sum to 1 and the negative attributions such that they scale to -1. If this option is used, each channel is normalized separately. - `'01'`: normalizes the attributions to the range [0, 1] by subtracting the minimum attribution value then dividing by the maximum attribution value. - `'unnormalized'`: leaves the attributions unaffected. If `None`, defaults to the value supplied to the constructor. blur: Gives the radius of a Gaussian blur to be applied to the attributions before visualizing. This can be used to help focus on salient regions rather than specific salient pixels. If `None`, defaults to the value supplied to the constructor. cmap: matplotlib.colors.Colormap | str, optional Colormap or name of a Colormap to use for the visualization. If `None`, defaults to the value supplied to the constructor. Returns: A `np.ndarray` array of the numerical representation of the attributions as modified for the visualization. This includes normalization, blurring, etc. \"\"\" combine_channels , normalization_type , blur , cmap = self . _check_args ( attributions , combine_channels , normalization_type , blur , cmap ) # Combine the channels if specified. if combine_channels : attributions = attributions . mean ( axis = get_backend () . channel_axis , keepdims = True ) # Blur the attributions so the explanation is smoother. if blur : attributions = self . _blur ( attributions , blur ) # Normalize the attributions. attributions = self . _normalize ( attributions , normalization_type ) tiled_attributions = self . tiler . tile ( attributions ) # Display the figure: _fig = plt . figure () if fig is None else fig plt . axis ( 'off' ) plt . imshow ( tiled_attributions , cmap = cmap ) if output_file : plt . savefig ( output_file , bbox_inches = 0 ) if imshow : plt . show () elif fig is None : plt . close ( _fig ) return tiled_attributions if return_tiled else attributions def _check_args ( self , attributions , combine_channels , normalization_type , blur , cmap ): \"\"\" Validates the arguments, and sets them to their default values if they are not specified. \"\"\" if attributions . ndim != 4 : raise ValueError ( '`Visualizer` is inteded for 4-D image-format data. Given ' 'input with dimension {} ' . format ( attributions . ndim ) ) if combine_channels is None : combine_channels = self . default_combine_channels channel_axis = get_backend () . channel_axis if not ( attributions . shape [ channel_axis ] in ( 1 , 3 , 4 ) or combine_channels ): raise ValueError ( 'To visualize, attributions must have either 1, 3, or 4 color ' 'channels, but `Visualizer` got {} channels. \\n ' 'If you are visualizing an internal layer, consider setting ' '`combine_channels` to True' . format ( attributions . shape [ channel_axis ] ) ) if normalization_type is None : normalization_type = self . default_normalization_type if normalization_type is None : if combine_channels or attributions . shape [ channel_axis ] == 1 : normalization_type = 'unsigned_max' else : normalization_type = 'unsigned_max_positive_centered' valid_normalization_types = [ 'unsigned_max' , 'unsigned_max_positive_centered' , 'magnitude_max' , 'magnitude_sum' , 'signed_max' , 'signed_max_positive_centered' , 'signed_sum' , '01' , 'unnormalized' , ] if normalization_type not in valid_normalization_types : raise ValueError ( '`norm` must be None or one of the following options:' + ',' . join ( [ ' \\' {} \\' ' . form ( norm_type ) for norm_type in valid_normalization_types ] ) ) if blur is None : blur = self . default_blur if cmap is None : cmap = self . default_cmap return combine_channels , normalization_type , blur , cmap def _normalize ( self , attributions , normalization_type , eps = 1e-20 ): channel_axis = get_backend () . channel_axis if normalization_type == 'unnormalized' : return attributions split_by_channel = normalization_type . endswith ( 'sum' ) channel_split = [ attributions ] if split_by_channel else np . split ( attributions , attributions . shape [ channel_axis ], axis = channel_axis ) normalized_attributions = [] for c_map in channel_split : if normalization_type == 'magnitude_max' : c_map = np . abs ( c_map ) / ( np . abs ( c_map ) . max ( axis = ( 1 , 2 , 3 ), keepdims = True ) + eps ) elif normalization_type == 'magnitude_sum' : c_map = np . abs ( c_map ) / ( np . abs ( c_map ) . sum ( axis = ( 1 , 2 , 3 ), keepdims = True ) + eps ) elif normalization_type . startswith ( 'signed_max' ): postive_max = c_map . max ( axis = ( 1 , 2 , 3 ), keepdims = True ) negative_max = ( - c_map ) . max ( axis = ( 1 , 2 , 3 ), keepdims = True ) # Normalize the postive socres to [0, 1] and negative socresn to # [-1, 0]. normalization_factor = np . where ( c_map >= 0 , postive_max , negative_max ) c_map = c_map / ( normalization_factor + eps ) # If positive-centered, normalize so that all scores are in the # range [0, 1], with negative scores less than 0.5 and positive # scores greater than 0.5. if normalization_type . endswith ( 'positive_centered' ): c_map = c_map / 2. + 0.5 elif normalization_type == 'signed_sum' : postive_max = np . maximum ( c_map , 0 ) . sum ( axis = ( 1 , 2 , 3 ), keepdims = True ) negative_max = np . maximum ( - c_map , 0 ) . sum ( axis = ( 1 , 2 , 3 ), keepdims = True ) # Normalize the postive socres to ensure they sum to 1 and the # negative scores to ensure they sum to -1. normalization_factor = np . where ( c_map >= 0 , postive_max , negative_max ) c_map = c_map / ( normalization_factor + eps ) elif normalization_type . startswith ( 'unsigned_max' ): c_map = c_map / ( np . abs ( c_map ) . max ( axis = ( 1 , 2 , 3 ), keepdims = True ) + eps ) # If positive-centered, normalize so that all scores are in the # range [0, 1], with negative scores less than 0.5 and positive # scores greater than 0.5. if normalization_type . endswith ( 'positive_centered' ): c_map = c_map / 2. + 0.5 elif normalization_type == '01' : c_map = c_map - c_map . min ( axis = ( 1 , 2 , 3 ), keepdims = True ) c_map = c_map / ( c_map . max ( axis = ( 1 , 2 , 3 ), keepdims = True ) + eps ) normalized_attributions . append ( c_map ) return np . concatenate ( normalized_attributions , axis = channel_axis ) def _blur ( self , attributions , blur ): for i in range ( attributions . shape [ 0 ]): attributions [ i ] = gaussian_filter ( attributions [ i ], blur ) return attributions def _get_hotcold ( self ): hot = cm . get_cmap ( 'hot' , 128 ) cool = cm . get_cmap ( 'cool' , 128 ) binary = cm . get_cmap ( 'binary' , 128 ) hotcold = np . vstack ( ( binary ( np . linspace ( 0 , 1 , 128 )) * cool ( np . linspace ( 0 , 1 , 128 )), hot ( np . linspace ( 0 , 1 , 128 )) ) ) return ListedColormap ( hotcold , name = 'hotcold' ) __call__ ( self , attributions , output_file = None , imshow = True , fig = None , return_tiled = False , combine_channels = None , normalization_type = None , blur = None , cmap = None ) special \u00b6 Visualizes the given attributions. Parameters: Name Type Description Default attributions A np.ndarray containing the attributions to be visualized. required output_file File name to save the visualization image to. If None , no image will be saved, but the figure can still be displayed. None imshow If true, a the visualization will be displayed. Otherwise the figure will not be displayed, but the figure can still be saved. True fig The pyplot figure to display the visualization in. If None , a new figure will be created. None return_tiled If true, the returned array will be in the same shape as the visualization, with no batch dimension and the samples in the batch tiled along the width and height dimensions. If false, the returned array will be reshaped to match attributions . False combine_channels If True , the attributions will be averaged across the channel dimension, resulting in a 1-channel attribution map. If None , defaults to the value supplied to the constructor. None normalization_type Specifies one of the following configurations for normalizing the attributions (each item is normalized separately): 'unsigned_max' : normalizes the attributions to the range [-1, 1] by dividing the attributions by the maximum absolute attribution value. 'unsigned_max_positive_centered' : same as above, but scales the values to the range [0, 1], with negative scores less than 0.5 and positive scores greater than 0.5. 'magnitude_max' : takes the absolute value of the attributions, then normalizes the attributions to the range [0, 1] by dividing by the maximum absolute attribution value. 'magnitude_sum' : takes the absolute value of the attributions, then scales them such that they sum to 1. If this option is used, each channel is normalized separately, such that each channel sums to 1. 'signed_max' : normalizes the attributions to the range [-1, 1] by dividing the positive values by the maximum positive attribution value and the negative values by the minimum negative attribution value. 'signed_max_positive_centered' : same as above, but scales the values to the range [0, 1], with negative scores less than 0.5 and positive scores greater than 0.5. 'signed_sum' : scales the positive attributions such that they sum to 1 and the negative attributions such that they scale to -1. If this option is used, each channel is normalized separately. '01' : normalizes the attributions to the range [0, 1] by subtracting the minimum attribution value then dividing by the maximum attribution value. 'unnormalized' : leaves the attributions unaffected. If None , defaults to the value supplied to the constructor. None blur Gives the radius of a Gaussian blur to be applied to the attributions before visualizing. This can be used to help focus on salient regions rather than specific salient pixels. If None , defaults to the value supplied to the constructor. None cmap matplotlib.colors.Colormap | str, optional Colormap or name of a Colormap to use for the visualization. If None , defaults to the value supplied to the constructor. None Returns: Type Description ndarray A np.ndarray array of the numerical representation of the attributions as modified for the visualization. This includes normalization, blurring, etc. Source code in trulens/visualizations.py def __call__ ( self , attributions , output_file = None , imshow = True , fig = None , return_tiled = False , combine_channels = None , normalization_type = None , blur = None , cmap = None ) -> np . ndarray : \"\"\" Visualizes the given attributions. Parameters: attributions: A `np.ndarray` containing the attributions to be visualized. output_file: File name to save the visualization image to. If `None`, no image will be saved, but the figure can still be displayed. imshow: If true, a the visualization will be displayed. Otherwise the figure will not be displayed, but the figure can still be saved. fig: The `pyplot` figure to display the visualization in. If `None`, a new figure will be created. return_tiled: If true, the returned array will be in the same shape as the visualization, with no batch dimension and the samples in the batch tiled along the width and height dimensions. If false, the returned array will be reshaped to match `attributions`. combine_channels: If `True`, the attributions will be averaged across the channel dimension, resulting in a 1-channel attribution map. If `None`, defaults to the value supplied to the constructor. normalization_type: Specifies one of the following configurations for normalizing the attributions (each item is normalized separately): - `'unsigned_max'`: normalizes the attributions to the range [-1, 1] by dividing the attributions by the maximum absolute attribution value. - `'unsigned_max_positive_centered'`: same as above, but scales the values to the range [0, 1], with negative scores less than 0.5 and positive scores greater than 0.5. - `'magnitude_max'`: takes the absolute value of the attributions, then normalizes the attributions to the range [0, 1] by dividing by the maximum absolute attribution value. - `'magnitude_sum'`: takes the absolute value of the attributions, then scales them such that they sum to 1. If this option is used, each channel is normalized separately, such that each channel sums to 1. - `'signed_max'`: normalizes the attributions to the range [-1, 1] by dividing the positive values by the maximum positive attribution value and the negative values by the minimum negative attribution value. - `'signed_max_positive_centered'`: same as above, but scales the values to the range [0, 1], with negative scores less than 0.5 and positive scores greater than 0.5. - `'signed_sum'`: scales the positive attributions such that they sum to 1 and the negative attributions such that they scale to -1. If this option is used, each channel is normalized separately. - `'01'`: normalizes the attributions to the range [0, 1] by subtracting the minimum attribution value then dividing by the maximum attribution value. - `'unnormalized'`: leaves the attributions unaffected. If `None`, defaults to the value supplied to the constructor. blur: Gives the radius of a Gaussian blur to be applied to the attributions before visualizing. This can be used to help focus on salient regions rather than specific salient pixels. If `None`, defaults to the value supplied to the constructor. cmap: matplotlib.colors.Colormap | str, optional Colormap or name of a Colormap to use for the visualization. If `None`, defaults to the value supplied to the constructor. Returns: A `np.ndarray` array of the numerical representation of the attributions as modified for the visualization. This includes normalization, blurring, etc. \"\"\" combine_channels , normalization_type , blur , cmap = self . _check_args ( attributions , combine_channels , normalization_type , blur , cmap ) # Combine the channels if specified. if combine_channels : attributions = attributions . mean ( axis = get_backend () . channel_axis , keepdims = True ) # Blur the attributions so the explanation is smoother. if blur : attributions = self . _blur ( attributions , blur ) # Normalize the attributions. attributions = self . _normalize ( attributions , normalization_type ) tiled_attributions = self . tiler . tile ( attributions ) # Display the figure: _fig = plt . figure () if fig is None else fig plt . axis ( 'off' ) plt . imshow ( tiled_attributions , cmap = cmap ) if output_file : plt . savefig ( output_file , bbox_inches = 0 ) if imshow : plt . show () elif fig is None : plt . close ( _fig ) return tiled_attributions if return_tiled else attributions __init__ ( self , combine_channels = False , normalization_type = None , blur = 0.0 , cmap = None ) special \u00b6 Configures the default parameters for the __call__ method (these can be overridden by passing in values to __call__ ). Parameters: Name Type Description Default combine_channels bool If True , the attributions will be averaged across the channel dimension, resulting in a 1-channel attribution map. False normalization_type str Specifies one of the following configurations for normalizing the attributions (each item is normalized separately): 'unsigned_max' : normalizes the attributions to the range [-1, 1] by dividing the attributions by the maximum absolute attribution value. 'unsigned_max_positive_centered' : same as above, but scales the values to the range [0, 1], with negative scores less than 0.5 and positive scores greater than 0.5. 'magnitude_max' : takes the absolute value of the attributions, then normalizes the attributions to the range [0, 1] by dividing by the maximum absolute attribution value. 'magnitude_sum' : takes the absolute value of the attributions, then scales them such that they sum to 1. If this option is used, each channel is normalized separately, such that each channel sums to 1. 'signed_max' : normalizes the attributions to the range [-1, 1] by dividing the positive values by the maximum positive attribution value and the negative values by the minimum negative attribution value. 'signed_max_positive_centered' : same as above, but scales the values to the range [0, 1], with negative scores less than 0.5 and positive scores greater than 0.5. 'signed_sum' : scales the positive attributions such that they sum to 1 and the negative attributions such that they scale to -1. If this option is used, each channel is normalized separately. '01' : normalizes the attributions to the range [0, 1] by subtracting the minimum attribution value then dividing by the maximum attribution value. 'unnormalized' : leaves the attributions unaffected. If None , either 'unsigned_max' (for single-channel data) or 'unsigned_max_positive_centered' (for multi-channel data) is used. None blur float Gives the radius of a Gaussian blur to be applied to the attributions before visualizing. This can be used to help focus on salient regions rather than specific salient pixels. 0.0 cmap Colormap matplotlib.colors.Colormap | str, optional Colormap or name of a Colormap to use for the visualization. If None , the colormap will be chosen based on the normalization type. This argument is only used for single-channel data (including when combine_channels is True). None Source code in trulens/visualizations.py def __init__ ( self , combine_channels : bool = False , normalization_type : str = None , blur : float = 0. , cmap : Colormap = None ): \"\"\" Configures the default parameters for the `__call__` method (these can be overridden by passing in values to `__call__`). Parameters: combine_channels: If `True`, the attributions will be averaged across the channel dimension, resulting in a 1-channel attribution map. normalization_type: Specifies one of the following configurations for normalizing the attributions (each item is normalized separately): - `'unsigned_max'`: normalizes the attributions to the range [-1, 1] by dividing the attributions by the maximum absolute attribution value. - `'unsigned_max_positive_centered'`: same as above, but scales the values to the range [0, 1], with negative scores less than 0.5 and positive scores greater than 0.5. - `'magnitude_max'`: takes the absolute value of the attributions, then normalizes the attributions to the range [0, 1] by dividing by the maximum absolute attribution value. - `'magnitude_sum'`: takes the absolute value of the attributions, then scales them such that they sum to 1. If this option is used, each channel is normalized separately, such that each channel sums to 1. - `'signed_max'`: normalizes the attributions to the range [-1, 1] by dividing the positive values by the maximum positive attribution value and the negative values by the minimum negative attribution value. - `'signed_max_positive_centered'`: same as above, but scales the values to the range [0, 1], with negative scores less than 0.5 and positive scores greater than 0.5. - `'signed_sum'`: scales the positive attributions such that they sum to 1 and the negative attributions such that they scale to -1. If this option is used, each channel is normalized separately. - `'01'`: normalizes the attributions to the range [0, 1] by subtracting the minimum attribution value then dividing by the maximum attribution value. - `'unnormalized'`: leaves the attributions unaffected. If `None`, either `'unsigned_max'` (for single-channel data) or `'unsigned_max_positive_centered'` (for multi-channel data) is used. blur: Gives the radius of a Gaussian blur to be applied to the attributions before visualizing. This can be used to help focus on salient regions rather than specific salient pixels. cmap: matplotlib.colors.Colormap | str, optional Colormap or name of a Colormap to use for the visualization. If `None`, the colormap will be chosen based on the normalization type. This argument is only used for single-channel data (including when `combine_channels` is True). \"\"\" self . default_combine_channels = combine_channels self . default_normalization_type = normalization_type self . default_blur = blur self . default_cmap = cmap if cmap is not None else self . _get_hotcold () # TODO(klas): in the future we can allow configuring of tiling settings # by allowing the user to specify the tiler. self . tiler = Tiler ()","title":"Visualizations"},{"location":"api/visualizations/#visualization-methods","text":"One clear use case for measuring attributions is for human consumption. In order to be fully leveraged by humans, explanations need to be interpretable \u2014 a large vector of numbers doesn\u2019t in general make us more confident we understand what a network is doing. We therefore view an explanation as comprised of both an attribution measurement and an interpretation of what the attribution values represent. One obvious way to interpret attributions, particularly in the image domain, is via visualization. This module provides several visualization methods for interpreting attributions as images.","title":"Visualization Methods"},{"location":"api/visualizations/#trulens.visualizations.ChannelMaskVisualizer","text":"Uses internal influence to visualize the pixels that are most salient towards a particular internal channel or neuron. Source code in trulens/visualizations.py class ChannelMaskVisualizer ( object ): \"\"\" Uses internal influence to visualize the pixels that are most salient towards a particular internal channel or neuron. \"\"\" def __init__ ( self , model , layer , channel , channel_axis = None , agg_fn = None , doi = None , blur = None , threshold = 0.5 , masked_opacity = 0.2 , combine_channels : bool = True , use_attr_as_opacity = None , positive_only = None ): \"\"\" Configures the default parameters for the `__call__` method (these can be overridden by passing in values to `__call__`). Parameters: model: The wrapped model whose channel we're visualizing. layer: The identifier (either index or name) of the layer in which the channel we're visualizing resides. channel: Index of the channel (for convolutional layers) or internal neuron (for fully-connected layers) that we'd like to visualize. channel_axis: If different from the channel axis specified by the backend, the supplied `channel_axis` will be used if operating on a convolutional layer with 4-D image format. agg_fn: Function with which to aggregate the remaining dimensions (except the batch dimension) in order to get a single scalar value for each channel; If `None`, a sum over each neuron in the channel will be taken. This argument is not used when the channels are scalars, e.g., for dense layers. doi: The distribution of interest to use when computing the input attributions towards the specified channel. If `None`, `PointDoI` will be used. blur: Gives the radius of a Gaussian blur to be applied to the attributions before visualizing. This can be used to help focus on salient regions rather than specific salient pixels. threshold: Value in the range [0, 1]. Attribution values at or below the percentile given by `threshold` (after normalization, blurring, etc.) will be masked. masked_opacity: Value in the range [0, 1] specifying the opacity for the parts of the image that are masked. combine_channels: If `True`, the attributions will be averaged across the channel dimension, resulting in a 1-channel attribution map. use_attr_as_opacity: If `True`, instead of using `threshold` and `masked_opacity`, the opacity of each pixel is given by the 0-1-normalized attribution value. positive_only: If `True`, only pixels with positive attribution will be unmasked (or given nonzero opacity when `use_attr_as_opacity` is true). \"\"\" B = get_backend () if ( B is not None and ( channel_axis is None or channel_axis < 0 )): channel_axis = B . channel_axis elif ( channel_axis is None or channel_axis < 0 ): channel_axis = 1 self . mask_visualizer = MaskVisualizer ( blur , threshold , masked_opacity , combine_channels , use_attr_as_opacity , positive_only ) self . infl_input = InternalInfluence ( model , ( InputCut (), Cut ( layer )), InternalChannelQoI ( channel , channel_axis , agg_fn ), PointDoi () if doi is None else doi ) def __call__ ( self , x , x_preprocessed = None , output_file = None , blur = None , threshold = None , masked_opacity = None , combine_channels = None ): \"\"\" Visualizes the given attributions by overlaying an attribution heatmap over the given image. Parameters ---------- attributions : numpy.ndarray The attributions to visualize. Expected to be in 4-D image format. x : numpy.ndarray The original image(s) over which the attributions are calculated. Must be the same shape as expected by the model used with this visualizer. x_preprocessed : numpy.ndarray, optional If the model requires a preprocessed input (e.g., with the mean subtracted) that is different from how the image should be visualized, ``x_preprocessed`` should be specified. In this case ``x`` will be used for visualization, and ``x_preprocessed`` will be passed to the model when calculating attributions. Must be the same shape as ``x``. output_file : str, optional If specified, the resulting visualization will be saved to a file with the name given by ``output_file``. blur : float, optional If specified, gives the radius of a Gaussian blur to be applied to the attributions before visualizing. This can be used to help focus on salient regions rather than specific salient pixels. If None, defaults to the value supplied to the constructor. Default None. threshold : float Value in the range [0, 1]. Attribution values at or below the percentile given by ``threshold`` will be masked. If None, defaults to the value supplied to the constructor. Default None. masked_opacity: float Value in the range [0, 1] specifying the opacity for the parts of the image that are masked. Default 0.2. If None, defaults to the value supplied to the constructor. Default None. combine_channels : bool If True, the attributions will be averaged across the channel dimension, resulting in a 1-channel attribution map. If None, defaults to the value supplied to the constructor. Default None. \"\"\" attrs_input = self . infl_input . attributions ( x if x_preprocessed is None else x_preprocessed ) return self . mask_visualizer ( attrs_input , x , output_file , blur , threshold , masked_opacity , combine_channels )","title":"ChannelMaskVisualizer"},{"location":"api/visualizations/#trulens.visualizations.ChannelMaskVisualizer.__call__","text":"Visualizes the given attributions by overlaying an attribution heatmap over the given image.","title":"__call__()"},{"location":"api/visualizations/#trulens.visualizations.ChannelMaskVisualizer.__call__--parameters","text":"attributions : numpy.ndarray The attributions to visualize. Expected to be in 4-D image format. x : numpy.ndarray The original image(s) over which the attributions are calculated. Must be the same shape as expected by the model used with this visualizer. x_preprocessed : numpy.ndarray, optional If the model requires a preprocessed input (e.g., with the mean subtracted) that is different from how the image should be visualized, x_preprocessed should be specified. In this case x will be used for visualization, and x_preprocessed will be passed to the model when calculating attributions. Must be the same shape as x . output_file : str, optional If specified, the resulting visualization will be saved to a file with the name given by output_file . blur : float, optional If specified, gives the radius of a Gaussian blur to be applied to the attributions before visualizing. This can be used to help focus on salient regions rather than specific salient pixels. If None, defaults to the value supplied to the constructor. Default None. threshold : float Value in the range [0, 1]. Attribution values at or below the percentile given by threshold will be masked. If None, defaults to the value supplied to the constructor. Default None. float Value in the range [0, 1] specifying the opacity for the parts of the image that are masked. Default 0.2. If None, defaults to the value supplied to the constructor. Default None. combine_channels : bool If True, the attributions will be averaged across the channel dimension, resulting in a 1-channel attribution map. If None, defaults to the value supplied to the constructor. Default None. Source code in trulens/visualizations.py def __call__ ( self , x , x_preprocessed = None , output_file = None , blur = None , threshold = None , masked_opacity = None , combine_channels = None ): \"\"\" Visualizes the given attributions by overlaying an attribution heatmap over the given image. Parameters ---------- attributions : numpy.ndarray The attributions to visualize. Expected to be in 4-D image format. x : numpy.ndarray The original image(s) over which the attributions are calculated. Must be the same shape as expected by the model used with this visualizer. x_preprocessed : numpy.ndarray, optional If the model requires a preprocessed input (e.g., with the mean subtracted) that is different from how the image should be visualized, ``x_preprocessed`` should be specified. In this case ``x`` will be used for visualization, and ``x_preprocessed`` will be passed to the model when calculating attributions. Must be the same shape as ``x``. output_file : str, optional If specified, the resulting visualization will be saved to a file with the name given by ``output_file``. blur : float, optional If specified, gives the radius of a Gaussian blur to be applied to the attributions before visualizing. This can be used to help focus on salient regions rather than specific salient pixels. If None, defaults to the value supplied to the constructor. Default None. threshold : float Value in the range [0, 1]. Attribution values at or below the percentile given by ``threshold`` will be masked. If None, defaults to the value supplied to the constructor. Default None. masked_opacity: float Value in the range [0, 1] specifying the opacity for the parts of the image that are masked. Default 0.2. If None, defaults to the value supplied to the constructor. Default None. combine_channels : bool If True, the attributions will be averaged across the channel dimension, resulting in a 1-channel attribution map. If None, defaults to the value supplied to the constructor. Default None. \"\"\" attrs_input = self . infl_input . attributions ( x if x_preprocessed is None else x_preprocessed ) return self . mask_visualizer ( attrs_input , x , output_file , blur , threshold , masked_opacity , combine_channels )","title":"Parameters"},{"location":"api/visualizations/#trulens.visualizations.ChannelMaskVisualizer.__init__","text":"Configures the default parameters for the __call__ method (these can be overridden by passing in values to __call__ ). Parameters: Name Type Description Default model The wrapped model whose channel we're visualizing. required layer The identifier (either index or name) of the layer in which the channel we're visualizing resides. required channel Index of the channel (for convolutional layers) or internal neuron (for fully-connected layers) that we'd like to visualize. required channel_axis If different from the channel axis specified by the backend, the supplied channel_axis will be used if operating on a convolutional layer with 4-D image format. None agg_fn Function with which to aggregate the remaining dimensions (except the batch dimension) in order to get a single scalar value for each channel; If None , a sum over each neuron in the channel will be taken. This argument is not used when the channels are scalars, e.g., for dense layers. None doi The distribution of interest to use when computing the input attributions towards the specified channel. If None , PointDoI will be used. None blur Gives the radius of a Gaussian blur to be applied to the attributions before visualizing. This can be used to help focus on salient regions rather than specific salient pixels. None threshold Value in the range [0, 1]. Attribution values at or below the percentile given by threshold (after normalization, blurring, etc.) will be masked. 0.5 masked_opacity Value in the range [0, 1] specifying the opacity for the parts of the image that are masked. 0.2 combine_channels bool If True , the attributions will be averaged across the channel dimension, resulting in a 1-channel attribution map. True use_attr_as_opacity If True , instead of using threshold and masked_opacity , the opacity of each pixel is given by the 0-1-normalized attribution value. None positive_only If True , only pixels with positive attribution will be unmasked (or given nonzero opacity when use_attr_as_opacity is true). None Source code in trulens/visualizations.py def __init__ ( self , model , layer , channel , channel_axis = None , agg_fn = None , doi = None , blur = None , threshold = 0.5 , masked_opacity = 0.2 , combine_channels : bool = True , use_attr_as_opacity = None , positive_only = None ): \"\"\" Configures the default parameters for the `__call__` method (these can be overridden by passing in values to `__call__`). Parameters: model: The wrapped model whose channel we're visualizing. layer: The identifier (either index or name) of the layer in which the channel we're visualizing resides. channel: Index of the channel (for convolutional layers) or internal neuron (for fully-connected layers) that we'd like to visualize. channel_axis: If different from the channel axis specified by the backend, the supplied `channel_axis` will be used if operating on a convolutional layer with 4-D image format. agg_fn: Function with which to aggregate the remaining dimensions (except the batch dimension) in order to get a single scalar value for each channel; If `None`, a sum over each neuron in the channel will be taken. This argument is not used when the channels are scalars, e.g., for dense layers. doi: The distribution of interest to use when computing the input attributions towards the specified channel. If `None`, `PointDoI` will be used. blur: Gives the radius of a Gaussian blur to be applied to the attributions before visualizing. This can be used to help focus on salient regions rather than specific salient pixels. threshold: Value in the range [0, 1]. Attribution values at or below the percentile given by `threshold` (after normalization, blurring, etc.) will be masked. masked_opacity: Value in the range [0, 1] specifying the opacity for the parts of the image that are masked. combine_channels: If `True`, the attributions will be averaged across the channel dimension, resulting in a 1-channel attribution map. use_attr_as_opacity: If `True`, instead of using `threshold` and `masked_opacity`, the opacity of each pixel is given by the 0-1-normalized attribution value. positive_only: If `True`, only pixels with positive attribution will be unmasked (or given nonzero opacity when `use_attr_as_opacity` is true). \"\"\" B = get_backend () if ( B is not None and ( channel_axis is None or channel_axis < 0 )): channel_axis = B . channel_axis elif ( channel_axis is None or channel_axis < 0 ): channel_axis = 1 self . mask_visualizer = MaskVisualizer ( blur , threshold , masked_opacity , combine_channels , use_attr_as_opacity , positive_only ) self . infl_input = InternalInfluence ( model , ( InputCut (), Cut ( layer )), InternalChannelQoI ( channel , channel_axis , agg_fn ), PointDoi () if doi is None else doi )","title":"__init__()"},{"location":"api/visualizations/#trulens.visualizations.HTML","text":"HTML visualization output format. Source code in trulens/visualizations.py class HTML ( Output ): \"\"\"HTML visualization output format.\"\"\" def __init__ ( self ): try : self . m_html = importlib . import_module ( \"html\" ) except : raise ImportError ( \"HTML output requires html python module. Try 'pip install html'.\" ) def blank ( self ): return \"\" def space ( self ): return \"&nbsp;\" def escape ( self , s ): return self . m_html . escape ( s ) def linebreak ( self ): return \"<br/>\" def line ( self , s ): return f \"<span style='padding: 2px; margin: 2px; background: gray; border-radius: 4px;'> { s } </span>\" def magnitude_colored ( self , s , mag ): red = 0.0 green = 0.0 if mag > 0 : green = 1.0 # 0.5 + mag * 0.5 red = 1.0 - mag * 0.5 else : red = 1.0 green = 1.0 + mag * 0.5 #red = 0.5 - mag * 0.5 blue = min ( red , green ) # blue = 1.0 - max(red, green) return f \"<span title=' { mag : 0.3f } ' style='margin: 1px; padding: 1px; border-radius: 4px; background: black; color: rgb( { red * 255 } , { green * 255 } , { blue * 255 } );'> { s } </span>\" def append ( self , * pieces ): return '' . join ( pieces ) def render ( self , s ): return s","title":"HTML"},{"location":"api/visualizations/#trulens.visualizations.HeatmapVisualizer","text":"Visualizes attributions by overlaying an attribution heatmap over the original image, similar to how GradCAM visualizes attributions. Source code in trulens/visualizations.py class HeatmapVisualizer ( Visualizer ): \"\"\" Visualizes attributions by overlaying an attribution heatmap over the original image, similar to how GradCAM visualizes attributions. \"\"\" def __init__ ( self , overlay_opacity = 0.5 , normalization_type = None , blur = 10. , cmap = 'jet' ): \"\"\" Configures the default parameters for the `__call__` method (these can be overridden by passing in values to `__call__`). Parameters: overlay_opacity: float Value in the range [0, 1] specifying the opacity for the heatmap overlay. normalization_type: Specifies one of the following configurations for normalizing the attributions (each item is normalized separately): - `'unsigned_max'`: normalizes the attributions to the range [-1, 1] by dividing the attributions by the maximum absolute attribution value. - `'unsigned_max_positive_centered'`: same as above, but scales the values to the range [0, 1], with negative scores less than 0.5 and positive scores greater than 0.5. - `'magnitude_max'`: takes the absolute value of the attributions, then normalizes the attributions to the range [0, 1] by dividing by the maximum absolute attribution value. - `'magnitude_sum'`: takes the absolute value of the attributions, then scales them such that they sum to 1. If this option is used, each channel is normalized separately, such that each channel sums to 1. - `'signed_max'`: normalizes the attributions to the range [-1, 1] by dividing the positive values by the maximum positive attribution value and the negative values by the minimum negative attribution value. - `'signed_max_positive_centered'`: same as above, but scales the values to the range [0, 1], with negative scores less than 0.5 and positive scores greater than 0.5. - `'signed_sum'`: scales the positive attributions such that they sum to 1 and the negative attributions such that they scale to -1. If this option is used, each channel is normalized separately. - `'01'`: normalizes the attributions to the range [0, 1] by subtracting the minimum attribution value then dividing by the maximum attribution value. - `'unnormalized'`: leaves the attributions unaffected. If `None`, either `'unsigned_max'` (for single-channel data) or `'unsigned_max_positive_centered'` (for multi-channel data) is used. blur: Gives the radius of a Gaussian blur to be applied to the attributions before visualizing. This can be used to help focus on salient regions rather than specific salient pixels. cmap: matplotlib.colors.Colormap | str, optional Colormap or name of a Colormap to use for the visualization. If `None`, the colormap will be chosen based on the normalization type. This argument is only used for single-channel data (including when `combine_channels` is True). \"\"\" super () . __init__ ( combine_channels = True , normalization_type = normalization_type , blur = blur , cmap = cmap ) self . default_overlay_opacity = overlay_opacity def __call__ ( self , attributions , x , output_file = None , imshow = True , fig = None , return_tiled = False , overlay_opacity = None , normalization_type = None , blur = None , cmap = None ) -> np . ndarray : \"\"\" Visualizes the given attributions by overlaying an attribution heatmap over the given image. Parameters: attributions: A `np.ndarray` containing the attributions to be visualized. x: A `np.ndarray` of items in the same shape as `attributions` corresponding to the records explained by the given attributions. The visualization will be superimposed onto the corresponding set of records. output_file: File name to save the visualization image to. If `None`, no image will be saved, but the figure can still be displayed. imshow: If true, a the visualization will be displayed. Otherwise the figure will not be displayed, but the figure can still be saved. fig: The `pyplot` figure to display the visualization in. If `None`, a new figure will be created. return_tiled: If true, the returned array will be in the same shape as the visualization, with no batch dimension and the samples in the batch tiled along the width and height dimensions. If false, the returned array will be reshaped to match `attributions`. overlay_opacity: float Value in the range [0, 1] specifying the opacity for the heatmap overlay. If `None`, defaults to the value supplied to the constructor. normalization_type: Specifies one of the following configurations for normalizing the attributions (each item is normalized separately): - `'unsigned_max'`: normalizes the attributions to the range [-1, 1] by dividing the attributions by the maximum absolute attribution value. - `'unsigned_max_positive_centered'`: same as above, but scales the values to the range [0, 1], with negative scores less than 0.5 and positive scores greater than 0.5. - `'magnitude_max'`: takes the absolute value of the attributions, then normalizes the attributions to the range [0, 1] by dividing by the maximum absolute attribution value. - `'magnitude_sum'`: takes the absolute value of the attributions, then scales them such that they sum to 1. If this option is used, each channel is normalized separately, such that each channel sums to 1. - `'signed_max'`: normalizes the attributions to the range [-1, 1] by dividing the positive values by the maximum positive attribution value and the negative values by the minimum negative attribution value. - `'signed_max_positive_centered'`: same as above, but scales the values to the range [0, 1], with negative scores less than 0.5 and positive scores greater than 0.5. - `'signed_sum'`: scales the positive attributions such that they sum to 1 and the negative attributions such that they scale to -1. If this option is used, each channel is normalized separately. - `'01'`: normalizes the attributions to the range [0, 1] by subtracting the minimum attribution value then dividing by the maximum attribution value. - `'unnormalized'`: leaves the attributions unaffected. If `None`, defaults to the value supplied to the constructor. blur: Gives the radius of a Gaussian blur to be applied to the attributions before visualizing. This can be used to help focus on salient regions rather than specific salient pixels. If `None`, defaults to the value supplied to the constructor. cmap: matplotlib.colors.Colormap | str, optional Colormap or name of a Colormap to use for the visualization. If `None`, defaults to the value supplied to the constructor. Returns: A `np.ndarray` array of the numerical representation of the attributions as modified for the visualization. This includes normalization, blurring, etc. \"\"\" _ , normalization_type , blur , cmap = self . _check_args ( attributions , None , normalization_type , blur , cmap ) # Combine the channels. attributions = attributions . mean ( axis = get_backend () . channel_axis , keepdims = True ) # Blur the attributions so the explanation is smoother. if blur : attributions = self . _blur ( attributions , blur ) # Normalize the attributions. attributions = self . _normalize ( attributions , normalization_type ) tiled_attributions = self . tiler . tile ( attributions ) # Normalize the pixels to be in the range [0, 1]. x = self . _normalize ( x , '01' ) tiled_x = self . tiler . tile ( x ) if cmap is None : cmap = self . default_cmap if overlay_opacity is None : overlay_opacity = self . default_overlay_opacity # Display the figure: _fig = plt . figure () if fig is None else fig plt . axis ( 'off' ) plt . imshow ( tiled_x ) plt . imshow ( tiled_attributions , alpha = overlay_opacity , cmap = cmap ) if output_file : plt . savefig ( output_file , bbox_inches = 0 ) if imshow : plt . show () elif fig is None : plt . close ( _fig ) return tiled_attributions if return_tiled else attributions","title":"HeatmapVisualizer"},{"location":"api/visualizations/#trulens.visualizations.HeatmapVisualizer.__call__","text":"Visualizes the given attributions by overlaying an attribution heatmap over the given image. Parameters: Name Type Description Default attributions A np.ndarray containing the attributions to be visualized. required x A np.ndarray of items in the same shape as attributions corresponding to the records explained by the given attributions. The visualization will be superimposed onto the corresponding set of records. required output_file File name to save the visualization image to. If None , no image will be saved, but the figure can still be displayed. None imshow If true, a the visualization will be displayed. Otherwise the figure will not be displayed, but the figure can still be saved. True fig The pyplot figure to display the visualization in. If None , a new figure will be created. None return_tiled If true, the returned array will be in the same shape as the visualization, with no batch dimension and the samples in the batch tiled along the width and height dimensions. If false, the returned array will be reshaped to match attributions . False overlay_opacity float Value in the range [0, 1] specifying the opacity for the heatmap overlay. If None , defaults to the value supplied to the constructor. None normalization_type Specifies one of the following configurations for normalizing the attributions (each item is normalized separately): 'unsigned_max' : normalizes the attributions to the range [-1, 1] by dividing the attributions by the maximum absolute attribution value. 'unsigned_max_positive_centered' : same as above, but scales the values to the range [0, 1], with negative scores less than 0.5 and positive scores greater than 0.5. 'magnitude_max' : takes the absolute value of the attributions, then normalizes the attributions to the range [0, 1] by dividing by the maximum absolute attribution value. 'magnitude_sum' : takes the absolute value of the attributions, then scales them such that they sum to 1. If this option is used, each channel is normalized separately, such that each channel sums to 1. 'signed_max' : normalizes the attributions to the range [-1, 1] by dividing the positive values by the maximum positive attribution value and the negative values by the minimum negative attribution value. 'signed_max_positive_centered' : same as above, but scales the values to the range [0, 1], with negative scores less than 0.5 and positive scores greater than 0.5. 'signed_sum' : scales the positive attributions such that they sum to 1 and the negative attributions such that they scale to -1. If this option is used, each channel is normalized separately. '01' : normalizes the attributions to the range [0, 1] by subtracting the minimum attribution value then dividing by the maximum attribution value. 'unnormalized' : leaves the attributions unaffected. If None , defaults to the value supplied to the constructor. None blur Gives the radius of a Gaussian blur to be applied to the attributions before visualizing. This can be used to help focus on salient regions rather than specific salient pixels. If None , defaults to the value supplied to the constructor. None cmap matplotlib.colors.Colormap | str, optional Colormap or name of a Colormap to use for the visualization. If None , defaults to the value supplied to the constructor. None Returns: Type Description ndarray A np.ndarray array of the numerical representation of the attributions as modified for the visualization. This includes normalization, blurring, etc. Source code in trulens/visualizations.py def __call__ ( self , attributions , x , output_file = None , imshow = True , fig = None , return_tiled = False , overlay_opacity = None , normalization_type = None , blur = None , cmap = None ) -> np . ndarray : \"\"\" Visualizes the given attributions by overlaying an attribution heatmap over the given image. Parameters: attributions: A `np.ndarray` containing the attributions to be visualized. x: A `np.ndarray` of items in the same shape as `attributions` corresponding to the records explained by the given attributions. The visualization will be superimposed onto the corresponding set of records. output_file: File name to save the visualization image to. If `None`, no image will be saved, but the figure can still be displayed. imshow: If true, a the visualization will be displayed. Otherwise the figure will not be displayed, but the figure can still be saved. fig: The `pyplot` figure to display the visualization in. If `None`, a new figure will be created. return_tiled: If true, the returned array will be in the same shape as the visualization, with no batch dimension and the samples in the batch tiled along the width and height dimensions. If false, the returned array will be reshaped to match `attributions`. overlay_opacity: float Value in the range [0, 1] specifying the opacity for the heatmap overlay. If `None`, defaults to the value supplied to the constructor. normalization_type: Specifies one of the following configurations for normalizing the attributions (each item is normalized separately): - `'unsigned_max'`: normalizes the attributions to the range [-1, 1] by dividing the attributions by the maximum absolute attribution value. - `'unsigned_max_positive_centered'`: same as above, but scales the values to the range [0, 1], with negative scores less than 0.5 and positive scores greater than 0.5. - `'magnitude_max'`: takes the absolute value of the attributions, then normalizes the attributions to the range [0, 1] by dividing by the maximum absolute attribution value. - `'magnitude_sum'`: takes the absolute value of the attributions, then scales them such that they sum to 1. If this option is used, each channel is normalized separately, such that each channel sums to 1. - `'signed_max'`: normalizes the attributions to the range [-1, 1] by dividing the positive values by the maximum positive attribution value and the negative values by the minimum negative attribution value. - `'signed_max_positive_centered'`: same as above, but scales the values to the range [0, 1], with negative scores less than 0.5 and positive scores greater than 0.5. - `'signed_sum'`: scales the positive attributions such that they sum to 1 and the negative attributions such that they scale to -1. If this option is used, each channel is normalized separately. - `'01'`: normalizes the attributions to the range [0, 1] by subtracting the minimum attribution value then dividing by the maximum attribution value. - `'unnormalized'`: leaves the attributions unaffected. If `None`, defaults to the value supplied to the constructor. blur: Gives the radius of a Gaussian blur to be applied to the attributions before visualizing. This can be used to help focus on salient regions rather than specific salient pixels. If `None`, defaults to the value supplied to the constructor. cmap: matplotlib.colors.Colormap | str, optional Colormap or name of a Colormap to use for the visualization. If `None`, defaults to the value supplied to the constructor. Returns: A `np.ndarray` array of the numerical representation of the attributions as modified for the visualization. This includes normalization, blurring, etc. \"\"\" _ , normalization_type , blur , cmap = self . _check_args ( attributions , None , normalization_type , blur , cmap ) # Combine the channels. attributions = attributions . mean ( axis = get_backend () . channel_axis , keepdims = True ) # Blur the attributions so the explanation is smoother. if blur : attributions = self . _blur ( attributions , blur ) # Normalize the attributions. attributions = self . _normalize ( attributions , normalization_type ) tiled_attributions = self . tiler . tile ( attributions ) # Normalize the pixels to be in the range [0, 1]. x = self . _normalize ( x , '01' ) tiled_x = self . tiler . tile ( x ) if cmap is None : cmap = self . default_cmap if overlay_opacity is None : overlay_opacity = self . default_overlay_opacity # Display the figure: _fig = plt . figure () if fig is None else fig plt . axis ( 'off' ) plt . imshow ( tiled_x ) plt . imshow ( tiled_attributions , alpha = overlay_opacity , cmap = cmap ) if output_file : plt . savefig ( output_file , bbox_inches = 0 ) if imshow : plt . show () elif fig is None : plt . close ( _fig ) return tiled_attributions if return_tiled else attributions","title":"__call__()"},{"location":"api/visualizations/#trulens.visualizations.HeatmapVisualizer.__init__","text":"Configures the default parameters for the __call__ method (these can be overridden by passing in values to __call__ ). Parameters: Name Type Description Default overlay_opacity float Value in the range [0, 1] specifying the opacity for the heatmap overlay. 0.5 normalization_type Specifies one of the following configurations for normalizing the attributions (each item is normalized separately): 'unsigned_max' : normalizes the attributions to the range [-1, 1] by dividing the attributions by the maximum absolute attribution value. 'unsigned_max_positive_centered' : same as above, but scales the values to the range [0, 1], with negative scores less than 0.5 and positive scores greater than 0.5. 'magnitude_max' : takes the absolute value of the attributions, then normalizes the attributions to the range [0, 1] by dividing by the maximum absolute attribution value. 'magnitude_sum' : takes the absolute value of the attributions, then scales them such that they sum to 1. If this option is used, each channel is normalized separately, such that each channel sums to 1. 'signed_max' : normalizes the attributions to the range [-1, 1] by dividing the positive values by the maximum positive attribution value and the negative values by the minimum negative attribution value. 'signed_max_positive_centered' : same as above, but scales the values to the range [0, 1], with negative scores less than 0.5 and positive scores greater than 0.5. 'signed_sum' : scales the positive attributions such that they sum to 1 and the negative attributions such that they scale to -1. If this option is used, each channel is normalized separately. '01' : normalizes the attributions to the range [0, 1] by subtracting the minimum attribution value then dividing by the maximum attribution value. 'unnormalized' : leaves the attributions unaffected. If None , either 'unsigned_max' (for single-channel data) or 'unsigned_max_positive_centered' (for multi-channel data) is used. None blur Gives the radius of a Gaussian blur to be applied to the attributions before visualizing. This can be used to help focus on salient regions rather than specific salient pixels. 10.0 cmap matplotlib.colors.Colormap | str, optional Colormap or name of a Colormap to use for the visualization. If None , the colormap will be chosen based on the normalization type. This argument is only used for single-channel data (including when combine_channels is True). 'jet' Source code in trulens/visualizations.py def __init__ ( self , overlay_opacity = 0.5 , normalization_type = None , blur = 10. , cmap = 'jet' ): \"\"\" Configures the default parameters for the `__call__` method (these can be overridden by passing in values to `__call__`). Parameters: overlay_opacity: float Value in the range [0, 1] specifying the opacity for the heatmap overlay. normalization_type: Specifies one of the following configurations for normalizing the attributions (each item is normalized separately): - `'unsigned_max'`: normalizes the attributions to the range [-1, 1] by dividing the attributions by the maximum absolute attribution value. - `'unsigned_max_positive_centered'`: same as above, but scales the values to the range [0, 1], with negative scores less than 0.5 and positive scores greater than 0.5. - `'magnitude_max'`: takes the absolute value of the attributions, then normalizes the attributions to the range [0, 1] by dividing by the maximum absolute attribution value. - `'magnitude_sum'`: takes the absolute value of the attributions, then scales them such that they sum to 1. If this option is used, each channel is normalized separately, such that each channel sums to 1. - `'signed_max'`: normalizes the attributions to the range [-1, 1] by dividing the positive values by the maximum positive attribution value and the negative values by the minimum negative attribution value. - `'signed_max_positive_centered'`: same as above, but scales the values to the range [0, 1], with negative scores less than 0.5 and positive scores greater than 0.5. - `'signed_sum'`: scales the positive attributions such that they sum to 1 and the negative attributions such that they scale to -1. If this option is used, each channel is normalized separately. - `'01'`: normalizes the attributions to the range [0, 1] by subtracting the minimum attribution value then dividing by the maximum attribution value. - `'unnormalized'`: leaves the attributions unaffected. If `None`, either `'unsigned_max'` (for single-channel data) or `'unsigned_max_positive_centered'` (for multi-channel data) is used. blur: Gives the radius of a Gaussian blur to be applied to the attributions before visualizing. This can be used to help focus on salient regions rather than specific salient pixels. cmap: matplotlib.colors.Colormap | str, optional Colormap or name of a Colormap to use for the visualization. If `None`, the colormap will be chosen based on the normalization type. This argument is only used for single-channel data (including when `combine_channels` is True). \"\"\" super () . __init__ ( combine_channels = True , normalization_type = normalization_type , blur = blur , cmap = cmap ) self . default_overlay_opacity = overlay_opacity","title":"__init__()"},{"location":"api/visualizations/#trulens.visualizations.IPython","text":"Interactive python visualization output format. Source code in trulens/visualizations.py class IPython ( HTML ): \"\"\"Interactive python visualization output format.\"\"\" def __init__ ( self ): super ( IPython , self ) . __init__ () try : self . m_ipy = importlib . import_module ( \"IPython\" ) except : raise ImportError ( \"Jupyter output requires IPython python module. Try 'pip install ipykernel'.\" ) def render ( self , s : str ): html = HTML . render ( self , s ) return self . m_ipy . display . HTML ( html )","title":"IPython"},{"location":"api/visualizations/#trulens.visualizations.MaskVisualizer","text":"Visualizes attributions by masking the original image to highlight the regions with influence above a given threshold percentile. Intended particularly for use with input-attributions. Source code in trulens/visualizations.py class MaskVisualizer ( object ): \"\"\" Visualizes attributions by masking the original image to highlight the regions with influence above a given threshold percentile. Intended particularly for use with input-attributions. \"\"\" def __init__ ( self , blur = 5. , threshold = 0.5 , masked_opacity = 0.2 , combine_channels = True , use_attr_as_opacity = False , positive_only = True ): \"\"\" Configures the default parameters for the `__call__` method (these can be overridden by passing in values to `__call__`). Parameters: blur: Gives the radius of a Gaussian blur to be applied to the attributions before visualizing. This can be used to help focus on salient regions rather than specific salient pixels. threshold: Value in the range [0, 1]. Attribution values at or below the percentile given by `threshold` (after normalization, blurring, etc.) will be masked. masked_opacity: Value in the range [0, 1] specifying the opacity for the parts of the image that are masked. combine_channels: If `True`, the attributions will be averaged across the channel dimension, resulting in a 1-channel attribution map. use_attr_as_opacity: If `True`, instead of using `threshold` and `masked_opacity`, the opacity of each pixel is given by the 0-1-normalized attribution value. positive_only: If `True`, only pixels with positive attribution will be unmasked (or given nonzero opacity when `use_attr_as_opacity` is true). \"\"\" self . default_blur = blur self . default_thresh = threshold self . default_masked_opacity = masked_opacity self . default_combine_channels = combine_channels # TODO(klas): in the future we can allow configuring of tiling settings # by allowing the user to specify the tiler. self . tiler = Tiler () def __call__ ( self , attributions , x , output_file = None , imshow = True , fig = None , return_tiled = True , blur = None , threshold = None , masked_opacity = None , combine_channels = None , use_attr_as_opacity = None , positive_only = None ): channel_axis = get_backend () . channel_axis if attributions . shape != x . shape : raise ValueError ( 'Shape of `attributions` {} must match shape of `x` {} ' . format ( attributions . shape , x . shape ) ) if blur is None : blur = self . default_blur if threshold is None : threshold = self . default_thresh if masked_opacity is None : masked_opacity = self . default_masked_opacity if combine_channels is None : combine_channels = self . default_combine_channels if len ( attributions . shape ) != 4 : raise ValueError ( '`MaskVisualizer` is inteded for 4-D image-format data. Given ' 'input with dimension {} ' . format ( len ( attributions . shape )) ) if combine_channels is None : combine_channels = self . default_combine_channels if combine_channels : attributions = attributions . mean ( axis = channel_axis , keepdims = True ) if x . shape [ channel_axis ] not in ( 1 , 3 , 4 ): raise ValueError ( 'To visualize, attributions must have either 1, 3, or 4 color ' 'channels, but Visualizer got {} channels. \\n ' 'If you are visualizing an internal layer, consider setting ' '`combine_channels` to True' . format ( attributions . shape [ channel_axis ] ) ) # Blur the attributions so the explanation is smoother. if blur is not None : attributions = [ gaussian_filter ( a , blur ) for a in attributions ] # If `positive_only` clip attributions. if positive_only : attributions = np . maximum ( attributions , 0 ) # Normalize the attributions to be in the range [0, 1]. attributions = [ a - a . min () for a in attributions ] attributions = [ 0. * a if a . max () == 0. else a / a . max () for a in attributions ] # Normalize the pixels to be in the range [0, 1] x = [ xc - xc . min () for xc in x ] x = np . array ([ 0. * xc if xc . max () == 0. else xc / xc . max () for xc in x ]) # Threshold the attributions to create a mask. if threshold is not None : percentiles = [ np . percentile ( a , 100 * threshold ) for a in attributions ] masks = np . array ( [ np . maximum ( a > p , masked_opacity ) for a , p in zip ( attributions , percentiles ) ] ) else : masks = np . array ( attributions ) # Use the mask on the original image to visualize the explanation. attributions = masks * x tiled_attributions = self . tiler . tile ( attributions ) if imshow : plt . axis ( 'off' ) plt . imshow ( tiled_attributions ) if output_file : plt . savefig ( output_file , bbox_inches = 0 ) return tiled_attributions if return_tiled else attributions","title":"MaskVisualizer"},{"location":"api/visualizations/#trulens.visualizations.MaskVisualizer.__init__","text":"Configures the default parameters for the __call__ method (these can be overridden by passing in values to __call__ ). Parameters: Name Type Description Default blur Gives the radius of a Gaussian blur to be applied to the attributions before visualizing. This can be used to help focus on salient regions rather than specific salient pixels. 5.0 threshold Value in the range [0, 1]. Attribution values at or below the percentile given by threshold (after normalization, blurring, etc.) will be masked. 0.5 masked_opacity Value in the range [0, 1] specifying the opacity for the parts of the image that are masked. 0.2 combine_channels If True , the attributions will be averaged across the channel dimension, resulting in a 1-channel attribution map. True use_attr_as_opacity If True , instead of using threshold and masked_opacity , the opacity of each pixel is given by the 0-1-normalized attribution value. False positive_only If True , only pixels with positive attribution will be unmasked (or given nonzero opacity when use_attr_as_opacity is true). True Source code in trulens/visualizations.py def __init__ ( self , blur = 5. , threshold = 0.5 , masked_opacity = 0.2 , combine_channels = True , use_attr_as_opacity = False , positive_only = True ): \"\"\" Configures the default parameters for the `__call__` method (these can be overridden by passing in values to `__call__`). Parameters: blur: Gives the radius of a Gaussian blur to be applied to the attributions before visualizing. This can be used to help focus on salient regions rather than specific salient pixels. threshold: Value in the range [0, 1]. Attribution values at or below the percentile given by `threshold` (after normalization, blurring, etc.) will be masked. masked_opacity: Value in the range [0, 1] specifying the opacity for the parts of the image that are masked. combine_channels: If `True`, the attributions will be averaged across the channel dimension, resulting in a 1-channel attribution map. use_attr_as_opacity: If `True`, instead of using `threshold` and `masked_opacity`, the opacity of each pixel is given by the 0-1-normalized attribution value. positive_only: If `True`, only pixels with positive attribution will be unmasked (or given nonzero opacity when `use_attr_as_opacity` is true). \"\"\" self . default_blur = blur self . default_thresh = threshold self . default_masked_opacity = masked_opacity self . default_combine_channels = combine_channels # TODO(klas): in the future we can allow configuring of tiling settings # by allowing the user to specify the tiler. self . tiler = Tiler ()","title":"__init__()"},{"location":"api/visualizations/#trulens.visualizations.NLP","text":"NLP Visualization tools. Source code in trulens/visualizations.py class NLP ( object ): \"\"\"NLP Visualization tools.\"\"\" # Batches of text inputs not yet tokenized. TextBatch = TypeVar ( \"TextBatch\" ) # Inputs that are directly accepted by wrapped models, tokenized. # TODO(piotrm): Reuse other typevars/aliases from elsewhere. ModelInput = TypeVar ( \"ModelInput\" ) # Outputs produced by wrapped models. # TODO(piotrm): Reuse other typevars/aliases from elsewhere. ModelOutput = TypeVar ( \"ModelOutput\" ) def __init__ ( self , wrapper : ModelWrapper , output : Optional [ Output ] = None , labels : Optional [ Iterable [ str ]] = None , tokenize : Optional [ Callable [[ TextBatch ], ModelInputs ]] = None , decode : Optional [ Callable [[ Tensor ], str ]] = None , input_accessor : Optional [ Callable [[ ModelInputs ], Iterable [ Tensor ]]] = None , output_accessor : Optional [ Callable [[ ModelOutput ], Iterable [ Tensor ]]] = None , attr_aggregate : Optional [ Callable [[ Tensor ], Tensor ]] = None , hidden_tokens : Optional [ Set [ int ]] = set () ): \"\"\"Initializate NLP visualization tools for a given environment. Parameters: wrapper: ModelWrapper The wrapped model whose channel we're visualizing. output: Output, optional Visualization output format. Defaults to PlainText unless ipython is detected and in which case defaults to IPython format. labels: Iterable[str], optional Names of prediction classes for classification models. tokenize: Callable[[TextBatch], ModelInput], optional Method to tokenize an instance. decode: Callable[[Tensor], str], optional Method to invert/decode the tokenization. input_accessor: Callable[[ModelInputs], Iterable[Tensor]], optional Method to extract input/token ids from model inputs (tokenize output) if needed. output_accessor: Callable[[ModelOutput], Iterable[Tensor]], optional Method to extract outout logits from output structures if needed. attr_aggregate: Callable[[Tensor], Tensor], optional Method to aggregate attribution for embedding into a single value. Defaults to sum. hidden_tokens: Set[int], optional For token-based visualizations, which tokens to hide. \"\"\" if output is None : try : # check if running in interactive python (jupyer, colab, etc) to # use appropriate output format get_ipython () output = IPython () except NameError : output = PlainText () tru_logger ( \"WARNING: could not guess preferred visualization output format, using PlainText\" ) # TODO: automatic inference of various parameters for common repositories like huggingface, tfhub. self . output = output self . labels = labels self . tokenize = tokenize self . decode = decode self . wrapper = wrapper self . input_accessor = input_accessor # could be inferred self . output_accessor = output_accessor # could be inferred B = get_backend () if attr_aggregate is None : attr_aggregate = B . sum self . attr_aggregate = attr_aggregate self . hidden_tokens = hidden_tokens def token_attribution ( self , texts : Iterable [ str ], attr : AttributionMethod ): \"\"\"Visualize a token-based input attribution on given `texts` inputs via the attribution method `attr`. Parameters: texts: Iterable[str] The input texts to visualize. attr: AttributionMethod The attribution method to generate the token importances with. Returns: Any The visualization in the format specified by this class's `output` parameter. \"\"\" B = get_backend () if self . tokenize is None : return ValueError ( \"tokenize not provided to NLP visualizer.\" ) inputs = self . tokenize ( texts ) outputs = inputs . call_on ( self . wrapper . _model ) attrs = inputs . call_on ( attr . attributions ) content = self . output . blank () input_ids = inputs if self . input_accessor is not None : input_ids = self . input_accessor ( inputs ) if ( not isinstance ( input_ids , Iterable )) or isinstance ( input_ids , dict ): raise ValueError ( f \"Inputs ( { input_ids . __class__ . __name__ } ) need to be iterable over instances. You might need to set input_accessor.\" ) output_logits = outputs if self . output_accessor is not None : output_logits = self . output_accessor ( outputs ) if ( not isinstance ( output_logits , Iterable )) or isinstance ( output_logits , dict ): raise ValueError ( f \"Outputs ( { output_logits . __class__ . __name__ } ) need to be iterable over instances. You might need to set output_accessor.\" ) for i , ( sentence_word_id , attr , logits ) in enumerate ( zip ( input_ids , attrs , output_logits )): logits = logits . to ( 'cpu' ) . detach () . numpy () pred = logits . argmax () if self . labels is not None : pred_name = self . labels [ pred ] else : pred_name = str ( pred ) sent = self . output . append ( self . output . escape ( pred_name ), \":\" , self . output . space () ) for word_id , attr in zip ( sentence_word_id , attr ): word_id = int ( B . as_array ( word_id )) if word_id in self . hidden_tokens : continue if self . decode is not None : word = self . decode ( word_id ) else : word = str ( word_id ) mag = self . attr_aggregate ( attr ) if word [ 0 ] == ' ' : word = word [ 1 :] sent = self . output . append ( sent , self . output . space ()) sent = self . output . append ( sent , self . output . magnitude_colored ( self . output . escape ( word ), mag ) ) content = self . output . append ( content , self . output . line ( sent ), self . output . linebreak (), self . output . linebreak () ) return self . output . render ( content )","title":"NLP"},{"location":"api/visualizations/#trulens.visualizations.NLP.__init__","text":"Initializate NLP visualization tools for a given environment. Parameters: Name Type Description Default wrapper ModelWrapper ModelWrapper The wrapped model whose channel we're visualizing. required output Optional[trulens.visualizations.Output] Output, optional Visualization output format. Defaults to PlainText unless ipython is detected and in which case defaults to IPython format. None labels Optional[Iterable[str]] Iterable[str], optional Names of prediction classes for classification models. None tokenize Optional[Callable[[~TextBatch], trulens.utils.ModelInputs]] Callable[[TextBatch], ModelInput], optional Method to tokenize an instance. None decode Optional[Callable[[~Tensor], str]] Callable[[Tensor], str], optional Method to invert/decode the tokenization. None input_accessor Optional[Callable[[trulens.utils.ModelInputs], Iterable[~Tensor]]] Callable[[ModelInputs], Iterable[Tensor]], optional Method to extract input/token ids from model inputs (tokenize output) if needed. None output_accessor Optional[Callable[[~ModelOutput], Iterable[~Tensor]]] Callable[[ModelOutput], Iterable[Tensor]], optional Method to extract outout logits from output structures if needed. None attr_aggregate Optional[Callable[[~Tensor], ~Tensor]] Callable[[Tensor], Tensor], optional Method to aggregate attribution for embedding into a single value. Defaults to sum. None hidden_tokens Optional[Set[int]] Set[int], optional For token-based visualizations, which tokens to hide. set() Source code in trulens/visualizations.py def __init__ ( self , wrapper : ModelWrapper , output : Optional [ Output ] = None , labels : Optional [ Iterable [ str ]] = None , tokenize : Optional [ Callable [[ TextBatch ], ModelInputs ]] = None , decode : Optional [ Callable [[ Tensor ], str ]] = None , input_accessor : Optional [ Callable [[ ModelInputs ], Iterable [ Tensor ]]] = None , output_accessor : Optional [ Callable [[ ModelOutput ], Iterable [ Tensor ]]] = None , attr_aggregate : Optional [ Callable [[ Tensor ], Tensor ]] = None , hidden_tokens : Optional [ Set [ int ]] = set () ): \"\"\"Initializate NLP visualization tools for a given environment. Parameters: wrapper: ModelWrapper The wrapped model whose channel we're visualizing. output: Output, optional Visualization output format. Defaults to PlainText unless ipython is detected and in which case defaults to IPython format. labels: Iterable[str], optional Names of prediction classes for classification models. tokenize: Callable[[TextBatch], ModelInput], optional Method to tokenize an instance. decode: Callable[[Tensor], str], optional Method to invert/decode the tokenization. input_accessor: Callable[[ModelInputs], Iterable[Tensor]], optional Method to extract input/token ids from model inputs (tokenize output) if needed. output_accessor: Callable[[ModelOutput], Iterable[Tensor]], optional Method to extract outout logits from output structures if needed. attr_aggregate: Callable[[Tensor], Tensor], optional Method to aggregate attribution for embedding into a single value. Defaults to sum. hidden_tokens: Set[int], optional For token-based visualizations, which tokens to hide. \"\"\" if output is None : try : # check if running in interactive python (jupyer, colab, etc) to # use appropriate output format get_ipython () output = IPython () except NameError : output = PlainText () tru_logger ( \"WARNING: could not guess preferred visualization output format, using PlainText\" ) # TODO: automatic inference of various parameters for common repositories like huggingface, tfhub. self . output = output self . labels = labels self . tokenize = tokenize self . decode = decode self . wrapper = wrapper self . input_accessor = input_accessor # could be inferred self . output_accessor = output_accessor # could be inferred B = get_backend () if attr_aggregate is None : attr_aggregate = B . sum self . attr_aggregate = attr_aggregate self . hidden_tokens = hidden_tokens","title":"__init__()"},{"location":"api/visualizations/#trulens.visualizations.NLP.token_attribution","text":"Visualize a token-based input attribution on given texts inputs via the attribution method attr . Parameters: Name Type Description Default texts Iterable[str] Iterable[str] The input texts to visualize. required attr AttributionMethod AttributionMethod The attribution method to generate the token importances with. required Any The visualization in the format specified by this class's output parameter. Source code in trulens/visualizations.py def token_attribution ( self , texts : Iterable [ str ], attr : AttributionMethod ): \"\"\"Visualize a token-based input attribution on given `texts` inputs via the attribution method `attr`. Parameters: texts: Iterable[str] The input texts to visualize. attr: AttributionMethod The attribution method to generate the token importances with. Returns: Any The visualization in the format specified by this class's `output` parameter. \"\"\" B = get_backend () if self . tokenize is None : return ValueError ( \"tokenize not provided to NLP visualizer.\" ) inputs = self . tokenize ( texts ) outputs = inputs . call_on ( self . wrapper . _model ) attrs = inputs . call_on ( attr . attributions ) content = self . output . blank () input_ids = inputs if self . input_accessor is not None : input_ids = self . input_accessor ( inputs ) if ( not isinstance ( input_ids , Iterable )) or isinstance ( input_ids , dict ): raise ValueError ( f \"Inputs ( { input_ids . __class__ . __name__ } ) need to be iterable over instances. You might need to set input_accessor.\" ) output_logits = outputs if self . output_accessor is not None : output_logits = self . output_accessor ( outputs ) if ( not isinstance ( output_logits , Iterable )) or isinstance ( output_logits , dict ): raise ValueError ( f \"Outputs ( { output_logits . __class__ . __name__ } ) need to be iterable over instances. You might need to set output_accessor.\" ) for i , ( sentence_word_id , attr , logits ) in enumerate ( zip ( input_ids , attrs , output_logits )): logits = logits . to ( 'cpu' ) . detach () . numpy () pred = logits . argmax () if self . labels is not None : pred_name = self . labels [ pred ] else : pred_name = str ( pred ) sent = self . output . append ( self . output . escape ( pred_name ), \":\" , self . output . space () ) for word_id , attr in zip ( sentence_word_id , attr ): word_id = int ( B . as_array ( word_id )) if word_id in self . hidden_tokens : continue if self . decode is not None : word = self . decode ( word_id ) else : word = str ( word_id ) mag = self . attr_aggregate ( attr ) if word [ 0 ] == ' ' : word = word [ 1 :] sent = self . output . append ( sent , self . output . space ()) sent = self . output . append ( sent , self . output . magnitude_colored ( self . output . escape ( word ), mag ) ) content = self . output . append ( content , self . output . line ( sent ), self . output . linebreak (), self . output . linebreak () ) return self . output . render ( content )","title":"token_attribution()"},{"location":"api/visualizations/#trulens.visualizations.Output","text":"Base class for visualization output formats. Source code in trulens/visualizations.py class Output ( ABC ): \"\"\"Base class for visualization output formats.\"\"\" @abstractmethod def blank ( self ) -> str : ... @abstractmethod def space ( self ) -> str : ... @abstractmethod def escape ( self , s : str ) -> str : ... @abstractmethod def line ( self , s : str ) -> str : ... @abstractmethod def magnitude_colored ( self , s : str , mag : float ) -> str : ... @abstractmethod def append ( self , * parts : Iterable [ str ]) -> str : ... @abstractmethod def render ( self , s : str ) -> str : ...","title":"Output"},{"location":"api/visualizations/#trulens.visualizations.PlainText","text":"Plain text visualization output format. Source code in trulens/visualizations.py class PlainText ( Output ): \"\"\"Plain text visualization output format.\"\"\" def blank ( self ): return \"\" def space ( self ): return \" \" def escape ( self , s ): return s def line ( self , s ): return s def magnitude_colored ( self , s , mag ): return f \" { s } ( { mag : 0.3f } )\" def append ( self , * parts ): return '' . join ( parts ) def render ( self , s ): return s","title":"PlainText"},{"location":"api/visualizations/#trulens.visualizations.Tiler","text":"Used to tile batched images or attributions. Source code in trulens/visualizations.py class Tiler ( object ): \"\"\" Used to tile batched images or attributions. \"\"\" def tile ( self , a : np . ndarray ) -> np . ndarray : \"\"\" Tiles the given array into a grid that is as square as possible. Parameters: a: An array of 4D batched image data. Returns: A tiled array of the images from `a`. The resulting array has rank 3 for color images, and 2 for grayscale images (the batch dimension is removed, as well as the channel dimension for grayscale images). The resulting array has its color channel dimension ordered last to fit the requirements of the `matplotlib` library. \"\"\" # `pyplot` expects the channels to come last. if get_backend () . dim_order == 'channels_first' : a = a . transpose (( 0 , 2 , 3 , 1 )) n , h , w , c = a . shape rows = int ( np . sqrt ( n )) cols = int ( np . ceil ( float ( n ) / rows )) new_a = np . zeros (( h * rows , w * cols , c )) for i , x in enumerate ( a ): row = i // cols col = i % cols new_a [ row * h :( row + 1 ) * h , col * w :( col + 1 ) * w ] = x return np . squeeze ( new_a )","title":"Tiler"},{"location":"api/visualizations/#trulens.visualizations.Tiler.tile","text":"Tiles the given array into a grid that is as square as possible. Parameters: Name Type Description Default a ndarray An array of 4D batched image data. required Returns: Type Description ndarray A tiled array of the images from a . The resulting array has rank 3 for color images, and 2 for grayscale images (the batch dimension is removed, as well as the channel dimension for grayscale images). The resulting array has its color channel dimension ordered last to fit the requirements of the matplotlib library. Source code in trulens/visualizations.py def tile ( self , a : np . ndarray ) -> np . ndarray : \"\"\" Tiles the given array into a grid that is as square as possible. Parameters: a: An array of 4D batched image data. Returns: A tiled array of the images from `a`. The resulting array has rank 3 for color images, and 2 for grayscale images (the batch dimension is removed, as well as the channel dimension for grayscale images). The resulting array has its color channel dimension ordered last to fit the requirements of the `matplotlib` library. \"\"\" # `pyplot` expects the channels to come last. if get_backend () . dim_order == 'channels_first' : a = a . transpose (( 0 , 2 , 3 , 1 )) n , h , w , c = a . shape rows = int ( np . sqrt ( n )) cols = int ( np . ceil ( float ( n ) / rows )) new_a = np . zeros (( h * rows , w * cols , c )) for i , x in enumerate ( a ): row = i // cols col = i % cols new_a [ row * h :( row + 1 ) * h , col * w :( col + 1 ) * w ] = x return np . squeeze ( new_a )","title":"tile()"},{"location":"api/visualizations/#trulens.visualizations.Visualizer","text":"Visualizes attributions directly as a color image. Intended particularly for use with input-attributions. This can also be used for viewing images (rather than attributions). Source code in trulens/visualizations.py class Visualizer ( object ): \"\"\" Visualizes attributions directly as a color image. Intended particularly for use with input-attributions. This can also be used for viewing images (rather than attributions). \"\"\" def __init__ ( self , combine_channels : bool = False , normalization_type : str = None , blur : float = 0. , cmap : Colormap = None ): \"\"\" Configures the default parameters for the `__call__` method (these can be overridden by passing in values to `__call__`). Parameters: combine_channels: If `True`, the attributions will be averaged across the channel dimension, resulting in a 1-channel attribution map. normalization_type: Specifies one of the following configurations for normalizing the attributions (each item is normalized separately): - `'unsigned_max'`: normalizes the attributions to the range [-1, 1] by dividing the attributions by the maximum absolute attribution value. - `'unsigned_max_positive_centered'`: same as above, but scales the values to the range [0, 1], with negative scores less than 0.5 and positive scores greater than 0.5. - `'magnitude_max'`: takes the absolute value of the attributions, then normalizes the attributions to the range [0, 1] by dividing by the maximum absolute attribution value. - `'magnitude_sum'`: takes the absolute value of the attributions, then scales them such that they sum to 1. If this option is used, each channel is normalized separately, such that each channel sums to 1. - `'signed_max'`: normalizes the attributions to the range [-1, 1] by dividing the positive values by the maximum positive attribution value and the negative values by the minimum negative attribution value. - `'signed_max_positive_centered'`: same as above, but scales the values to the range [0, 1], with negative scores less than 0.5 and positive scores greater than 0.5. - `'signed_sum'`: scales the positive attributions such that they sum to 1 and the negative attributions such that they scale to -1. If this option is used, each channel is normalized separately. - `'01'`: normalizes the attributions to the range [0, 1] by subtracting the minimum attribution value then dividing by the maximum attribution value. - `'unnormalized'`: leaves the attributions unaffected. If `None`, either `'unsigned_max'` (for single-channel data) or `'unsigned_max_positive_centered'` (for multi-channel data) is used. blur: Gives the radius of a Gaussian blur to be applied to the attributions before visualizing. This can be used to help focus on salient regions rather than specific salient pixels. cmap: matplotlib.colors.Colormap | str, optional Colormap or name of a Colormap to use for the visualization. If `None`, the colormap will be chosen based on the normalization type. This argument is only used for single-channel data (including when `combine_channels` is True). \"\"\" self . default_combine_channels = combine_channels self . default_normalization_type = normalization_type self . default_blur = blur self . default_cmap = cmap if cmap is not None else self . _get_hotcold () # TODO(klas): in the future we can allow configuring of tiling settings # by allowing the user to specify the tiler. self . tiler = Tiler () def __call__ ( self , attributions , output_file = None , imshow = True , fig = None , return_tiled = False , combine_channels = None , normalization_type = None , blur = None , cmap = None ) -> np . ndarray : \"\"\" Visualizes the given attributions. Parameters: attributions: A `np.ndarray` containing the attributions to be visualized. output_file: File name to save the visualization image to. If `None`, no image will be saved, but the figure can still be displayed. imshow: If true, a the visualization will be displayed. Otherwise the figure will not be displayed, but the figure can still be saved. fig: The `pyplot` figure to display the visualization in. If `None`, a new figure will be created. return_tiled: If true, the returned array will be in the same shape as the visualization, with no batch dimension and the samples in the batch tiled along the width and height dimensions. If false, the returned array will be reshaped to match `attributions`. combine_channels: If `True`, the attributions will be averaged across the channel dimension, resulting in a 1-channel attribution map. If `None`, defaults to the value supplied to the constructor. normalization_type: Specifies one of the following configurations for normalizing the attributions (each item is normalized separately): - `'unsigned_max'`: normalizes the attributions to the range [-1, 1] by dividing the attributions by the maximum absolute attribution value. - `'unsigned_max_positive_centered'`: same as above, but scales the values to the range [0, 1], with negative scores less than 0.5 and positive scores greater than 0.5. - `'magnitude_max'`: takes the absolute value of the attributions, then normalizes the attributions to the range [0, 1] by dividing by the maximum absolute attribution value. - `'magnitude_sum'`: takes the absolute value of the attributions, then scales them such that they sum to 1. If this option is used, each channel is normalized separately, such that each channel sums to 1. - `'signed_max'`: normalizes the attributions to the range [-1, 1] by dividing the positive values by the maximum positive attribution value and the negative values by the minimum negative attribution value. - `'signed_max_positive_centered'`: same as above, but scales the values to the range [0, 1], with negative scores less than 0.5 and positive scores greater than 0.5. - `'signed_sum'`: scales the positive attributions such that they sum to 1 and the negative attributions such that they scale to -1. If this option is used, each channel is normalized separately. - `'01'`: normalizes the attributions to the range [0, 1] by subtracting the minimum attribution value then dividing by the maximum attribution value. - `'unnormalized'`: leaves the attributions unaffected. If `None`, defaults to the value supplied to the constructor. blur: Gives the radius of a Gaussian blur to be applied to the attributions before visualizing. This can be used to help focus on salient regions rather than specific salient pixels. If `None`, defaults to the value supplied to the constructor. cmap: matplotlib.colors.Colormap | str, optional Colormap or name of a Colormap to use for the visualization. If `None`, defaults to the value supplied to the constructor. Returns: A `np.ndarray` array of the numerical representation of the attributions as modified for the visualization. This includes normalization, blurring, etc. \"\"\" combine_channels , normalization_type , blur , cmap = self . _check_args ( attributions , combine_channels , normalization_type , blur , cmap ) # Combine the channels if specified. if combine_channels : attributions = attributions . mean ( axis = get_backend () . channel_axis , keepdims = True ) # Blur the attributions so the explanation is smoother. if blur : attributions = self . _blur ( attributions , blur ) # Normalize the attributions. attributions = self . _normalize ( attributions , normalization_type ) tiled_attributions = self . tiler . tile ( attributions ) # Display the figure: _fig = plt . figure () if fig is None else fig plt . axis ( 'off' ) plt . imshow ( tiled_attributions , cmap = cmap ) if output_file : plt . savefig ( output_file , bbox_inches = 0 ) if imshow : plt . show () elif fig is None : plt . close ( _fig ) return tiled_attributions if return_tiled else attributions def _check_args ( self , attributions , combine_channels , normalization_type , blur , cmap ): \"\"\" Validates the arguments, and sets them to their default values if they are not specified. \"\"\" if attributions . ndim != 4 : raise ValueError ( '`Visualizer` is inteded for 4-D image-format data. Given ' 'input with dimension {} ' . format ( attributions . ndim ) ) if combine_channels is None : combine_channels = self . default_combine_channels channel_axis = get_backend () . channel_axis if not ( attributions . shape [ channel_axis ] in ( 1 , 3 , 4 ) or combine_channels ): raise ValueError ( 'To visualize, attributions must have either 1, 3, or 4 color ' 'channels, but `Visualizer` got {} channels. \\n ' 'If you are visualizing an internal layer, consider setting ' '`combine_channels` to True' . format ( attributions . shape [ channel_axis ] ) ) if normalization_type is None : normalization_type = self . default_normalization_type if normalization_type is None : if combine_channels or attributions . shape [ channel_axis ] == 1 : normalization_type = 'unsigned_max' else : normalization_type = 'unsigned_max_positive_centered' valid_normalization_types = [ 'unsigned_max' , 'unsigned_max_positive_centered' , 'magnitude_max' , 'magnitude_sum' , 'signed_max' , 'signed_max_positive_centered' , 'signed_sum' , '01' , 'unnormalized' , ] if normalization_type not in valid_normalization_types : raise ValueError ( '`norm` must be None or one of the following options:' + ',' . join ( [ ' \\' {} \\' ' . form ( norm_type ) for norm_type in valid_normalization_types ] ) ) if blur is None : blur = self . default_blur if cmap is None : cmap = self . default_cmap return combine_channels , normalization_type , blur , cmap def _normalize ( self , attributions , normalization_type , eps = 1e-20 ): channel_axis = get_backend () . channel_axis if normalization_type == 'unnormalized' : return attributions split_by_channel = normalization_type . endswith ( 'sum' ) channel_split = [ attributions ] if split_by_channel else np . split ( attributions , attributions . shape [ channel_axis ], axis = channel_axis ) normalized_attributions = [] for c_map in channel_split : if normalization_type == 'magnitude_max' : c_map = np . abs ( c_map ) / ( np . abs ( c_map ) . max ( axis = ( 1 , 2 , 3 ), keepdims = True ) + eps ) elif normalization_type == 'magnitude_sum' : c_map = np . abs ( c_map ) / ( np . abs ( c_map ) . sum ( axis = ( 1 , 2 , 3 ), keepdims = True ) + eps ) elif normalization_type . startswith ( 'signed_max' ): postive_max = c_map . max ( axis = ( 1 , 2 , 3 ), keepdims = True ) negative_max = ( - c_map ) . max ( axis = ( 1 , 2 , 3 ), keepdims = True ) # Normalize the postive socres to [0, 1] and negative socresn to # [-1, 0]. normalization_factor = np . where ( c_map >= 0 , postive_max , negative_max ) c_map = c_map / ( normalization_factor + eps ) # If positive-centered, normalize so that all scores are in the # range [0, 1], with negative scores less than 0.5 and positive # scores greater than 0.5. if normalization_type . endswith ( 'positive_centered' ): c_map = c_map / 2. + 0.5 elif normalization_type == 'signed_sum' : postive_max = np . maximum ( c_map , 0 ) . sum ( axis = ( 1 , 2 , 3 ), keepdims = True ) negative_max = np . maximum ( - c_map , 0 ) . sum ( axis = ( 1 , 2 , 3 ), keepdims = True ) # Normalize the postive socres to ensure they sum to 1 and the # negative scores to ensure they sum to -1. normalization_factor = np . where ( c_map >= 0 , postive_max , negative_max ) c_map = c_map / ( normalization_factor + eps ) elif normalization_type . startswith ( 'unsigned_max' ): c_map = c_map / ( np . abs ( c_map ) . max ( axis = ( 1 , 2 , 3 ), keepdims = True ) + eps ) # If positive-centered, normalize so that all scores are in the # range [0, 1], with negative scores less than 0.5 and positive # scores greater than 0.5. if normalization_type . endswith ( 'positive_centered' ): c_map = c_map / 2. + 0.5 elif normalization_type == '01' : c_map = c_map - c_map . min ( axis = ( 1 , 2 , 3 ), keepdims = True ) c_map = c_map / ( c_map . max ( axis = ( 1 , 2 , 3 ), keepdims = True ) + eps ) normalized_attributions . append ( c_map ) return np . concatenate ( normalized_attributions , axis = channel_axis ) def _blur ( self , attributions , blur ): for i in range ( attributions . shape [ 0 ]): attributions [ i ] = gaussian_filter ( attributions [ i ], blur ) return attributions def _get_hotcold ( self ): hot = cm . get_cmap ( 'hot' , 128 ) cool = cm . get_cmap ( 'cool' , 128 ) binary = cm . get_cmap ( 'binary' , 128 ) hotcold = np . vstack ( ( binary ( np . linspace ( 0 , 1 , 128 )) * cool ( np . linspace ( 0 , 1 , 128 )), hot ( np . linspace ( 0 , 1 , 128 )) ) ) return ListedColormap ( hotcold , name = 'hotcold' )","title":"Visualizer"},{"location":"api/visualizations/#trulens.visualizations.Visualizer.__call__","text":"Visualizes the given attributions. Parameters: Name Type Description Default attributions A np.ndarray containing the attributions to be visualized. required output_file File name to save the visualization image to. If None , no image will be saved, but the figure can still be displayed. None imshow If true, a the visualization will be displayed. Otherwise the figure will not be displayed, but the figure can still be saved. True fig The pyplot figure to display the visualization in. If None , a new figure will be created. None return_tiled If true, the returned array will be in the same shape as the visualization, with no batch dimension and the samples in the batch tiled along the width and height dimensions. If false, the returned array will be reshaped to match attributions . False combine_channels If True , the attributions will be averaged across the channel dimension, resulting in a 1-channel attribution map. If None , defaults to the value supplied to the constructor. None normalization_type Specifies one of the following configurations for normalizing the attributions (each item is normalized separately): 'unsigned_max' : normalizes the attributions to the range [-1, 1] by dividing the attributions by the maximum absolute attribution value. 'unsigned_max_positive_centered' : same as above, but scales the values to the range [0, 1], with negative scores less than 0.5 and positive scores greater than 0.5. 'magnitude_max' : takes the absolute value of the attributions, then normalizes the attributions to the range [0, 1] by dividing by the maximum absolute attribution value. 'magnitude_sum' : takes the absolute value of the attributions, then scales them such that they sum to 1. If this option is used, each channel is normalized separately, such that each channel sums to 1. 'signed_max' : normalizes the attributions to the range [-1, 1] by dividing the positive values by the maximum positive attribution value and the negative values by the minimum negative attribution value. 'signed_max_positive_centered' : same as above, but scales the values to the range [0, 1], with negative scores less than 0.5 and positive scores greater than 0.5. 'signed_sum' : scales the positive attributions such that they sum to 1 and the negative attributions such that they scale to -1. If this option is used, each channel is normalized separately. '01' : normalizes the attributions to the range [0, 1] by subtracting the minimum attribution value then dividing by the maximum attribution value. 'unnormalized' : leaves the attributions unaffected. If None , defaults to the value supplied to the constructor. None blur Gives the radius of a Gaussian blur to be applied to the attributions before visualizing. This can be used to help focus on salient regions rather than specific salient pixels. If None , defaults to the value supplied to the constructor. None cmap matplotlib.colors.Colormap | str, optional Colormap or name of a Colormap to use for the visualization. If None , defaults to the value supplied to the constructor. None Returns: Type Description ndarray A np.ndarray array of the numerical representation of the attributions as modified for the visualization. This includes normalization, blurring, etc. Source code in trulens/visualizations.py def __call__ ( self , attributions , output_file = None , imshow = True , fig = None , return_tiled = False , combine_channels = None , normalization_type = None , blur = None , cmap = None ) -> np . ndarray : \"\"\" Visualizes the given attributions. Parameters: attributions: A `np.ndarray` containing the attributions to be visualized. output_file: File name to save the visualization image to. If `None`, no image will be saved, but the figure can still be displayed. imshow: If true, a the visualization will be displayed. Otherwise the figure will not be displayed, but the figure can still be saved. fig: The `pyplot` figure to display the visualization in. If `None`, a new figure will be created. return_tiled: If true, the returned array will be in the same shape as the visualization, with no batch dimension and the samples in the batch tiled along the width and height dimensions. If false, the returned array will be reshaped to match `attributions`. combine_channels: If `True`, the attributions will be averaged across the channel dimension, resulting in a 1-channel attribution map. If `None`, defaults to the value supplied to the constructor. normalization_type: Specifies one of the following configurations for normalizing the attributions (each item is normalized separately): - `'unsigned_max'`: normalizes the attributions to the range [-1, 1] by dividing the attributions by the maximum absolute attribution value. - `'unsigned_max_positive_centered'`: same as above, but scales the values to the range [0, 1], with negative scores less than 0.5 and positive scores greater than 0.5. - `'magnitude_max'`: takes the absolute value of the attributions, then normalizes the attributions to the range [0, 1] by dividing by the maximum absolute attribution value. - `'magnitude_sum'`: takes the absolute value of the attributions, then scales them such that they sum to 1. If this option is used, each channel is normalized separately, such that each channel sums to 1. - `'signed_max'`: normalizes the attributions to the range [-1, 1] by dividing the positive values by the maximum positive attribution value and the negative values by the minimum negative attribution value. - `'signed_max_positive_centered'`: same as above, but scales the values to the range [0, 1], with negative scores less than 0.5 and positive scores greater than 0.5. - `'signed_sum'`: scales the positive attributions such that they sum to 1 and the negative attributions such that they scale to -1. If this option is used, each channel is normalized separately. - `'01'`: normalizes the attributions to the range [0, 1] by subtracting the minimum attribution value then dividing by the maximum attribution value. - `'unnormalized'`: leaves the attributions unaffected. If `None`, defaults to the value supplied to the constructor. blur: Gives the radius of a Gaussian blur to be applied to the attributions before visualizing. This can be used to help focus on salient regions rather than specific salient pixels. If `None`, defaults to the value supplied to the constructor. cmap: matplotlib.colors.Colormap | str, optional Colormap or name of a Colormap to use for the visualization. If `None`, defaults to the value supplied to the constructor. Returns: A `np.ndarray` array of the numerical representation of the attributions as modified for the visualization. This includes normalization, blurring, etc. \"\"\" combine_channels , normalization_type , blur , cmap = self . _check_args ( attributions , combine_channels , normalization_type , blur , cmap ) # Combine the channels if specified. if combine_channels : attributions = attributions . mean ( axis = get_backend () . channel_axis , keepdims = True ) # Blur the attributions so the explanation is smoother. if blur : attributions = self . _blur ( attributions , blur ) # Normalize the attributions. attributions = self . _normalize ( attributions , normalization_type ) tiled_attributions = self . tiler . tile ( attributions ) # Display the figure: _fig = plt . figure () if fig is None else fig plt . axis ( 'off' ) plt . imshow ( tiled_attributions , cmap = cmap ) if output_file : plt . savefig ( output_file , bbox_inches = 0 ) if imshow : plt . show () elif fig is None : plt . close ( _fig ) return tiled_attributions if return_tiled else attributions","title":"__call__()"},{"location":"api/visualizations/#trulens.visualizations.Visualizer.__init__","text":"Configures the default parameters for the __call__ method (these can be overridden by passing in values to __call__ ). Parameters: Name Type Description Default combine_channels bool If True , the attributions will be averaged across the channel dimension, resulting in a 1-channel attribution map. False normalization_type str Specifies one of the following configurations for normalizing the attributions (each item is normalized separately): 'unsigned_max' : normalizes the attributions to the range [-1, 1] by dividing the attributions by the maximum absolute attribution value. 'unsigned_max_positive_centered' : same as above, but scales the values to the range [0, 1], with negative scores less than 0.5 and positive scores greater than 0.5. 'magnitude_max' : takes the absolute value of the attributions, then normalizes the attributions to the range [0, 1] by dividing by the maximum absolute attribution value. 'magnitude_sum' : takes the absolute value of the attributions, then scales them such that they sum to 1. If this option is used, each channel is normalized separately, such that each channel sums to 1. 'signed_max' : normalizes the attributions to the range [-1, 1] by dividing the positive values by the maximum positive attribution value and the negative values by the minimum negative attribution value. 'signed_max_positive_centered' : same as above, but scales the values to the range [0, 1], with negative scores less than 0.5 and positive scores greater than 0.5. 'signed_sum' : scales the positive attributions such that they sum to 1 and the negative attributions such that they scale to -1. If this option is used, each channel is normalized separately. '01' : normalizes the attributions to the range [0, 1] by subtracting the minimum attribution value then dividing by the maximum attribution value. 'unnormalized' : leaves the attributions unaffected. If None , either 'unsigned_max' (for single-channel data) or 'unsigned_max_positive_centered' (for multi-channel data) is used. None blur float Gives the radius of a Gaussian blur to be applied to the attributions before visualizing. This can be used to help focus on salient regions rather than specific salient pixels. 0.0 cmap Colormap matplotlib.colors.Colormap | str, optional Colormap or name of a Colormap to use for the visualization. If None , the colormap will be chosen based on the normalization type. This argument is only used for single-channel data (including when combine_channels is True). None Source code in trulens/visualizations.py def __init__ ( self , combine_channels : bool = False , normalization_type : str = None , blur : float = 0. , cmap : Colormap = None ): \"\"\" Configures the default parameters for the `__call__` method (these can be overridden by passing in values to `__call__`). Parameters: combine_channels: If `True`, the attributions will be averaged across the channel dimension, resulting in a 1-channel attribution map. normalization_type: Specifies one of the following configurations for normalizing the attributions (each item is normalized separately): - `'unsigned_max'`: normalizes the attributions to the range [-1, 1] by dividing the attributions by the maximum absolute attribution value. - `'unsigned_max_positive_centered'`: same as above, but scales the values to the range [0, 1], with negative scores less than 0.5 and positive scores greater than 0.5. - `'magnitude_max'`: takes the absolute value of the attributions, then normalizes the attributions to the range [0, 1] by dividing by the maximum absolute attribution value. - `'magnitude_sum'`: takes the absolute value of the attributions, then scales them such that they sum to 1. If this option is used, each channel is normalized separately, such that each channel sums to 1. - `'signed_max'`: normalizes the attributions to the range [-1, 1] by dividing the positive values by the maximum positive attribution value and the negative values by the minimum negative attribution value. - `'signed_max_positive_centered'`: same as above, but scales the values to the range [0, 1], with negative scores less than 0.5 and positive scores greater than 0.5. - `'signed_sum'`: scales the positive attributions such that they sum to 1 and the negative attributions such that they scale to -1. If this option is used, each channel is normalized separately. - `'01'`: normalizes the attributions to the range [0, 1] by subtracting the minimum attribution value then dividing by the maximum attribution value. - `'unnormalized'`: leaves the attributions unaffected. If `None`, either `'unsigned_max'` (for single-channel data) or `'unsigned_max_positive_centered'` (for multi-channel data) is used. blur: Gives the radius of a Gaussian blur to be applied to the attributions before visualizing. This can be used to help focus on salient regions rather than specific salient pixels. cmap: matplotlib.colors.Colormap | str, optional Colormap or name of a Colormap to use for the visualization. If `None`, the colormap will be chosen based on the normalization type. This argument is only used for single-channel data (including when `combine_channels` is True). \"\"\" self . default_combine_channels = combine_channels self . default_normalization_type = normalization_type self . default_blur = blur self . default_cmap = cmap if cmap is not None else self . _get_hotcold () # TODO(klas): in the future we can allow configuring of tiling settings # by allowing the user to specify the tiler. self . tiler = Tiler ()","title":"__init__()"}]}