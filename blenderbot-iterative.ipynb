{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed021a16",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0,'/home/rick/transformers/src')\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "trulens_path = (Path(os.getcwd()).parent.parent / \"trulens\")\n",
    "\n",
    "sys.path.insert(0, str(trulens_path))\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b6c4603",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BlenderbotForConditionalGeneration, BlenderbotTokenizer\n",
    "import torch\n",
    "\n",
    "\n",
    "mname=\"facebook/blenderbot-3B\"\n",
    "model = BlenderbotForConditionalGeneration.from_pretrained(mname)\n",
    "tokenizer = BlenderbotTokenizer.from_pretrained(mname)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a121650d",
   "metadata": {},
   "outputs": [],
   "source": [
    "device=\"cuda:0\"\n",
    "model.to(device)\n",
    "tok=tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fa84db7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "# Taken as a subset of the methods that do model input prep from transformers.generation.utils\n",
    "def prepare_inputs(inputs):\n",
    "    generation_config = copy.deepcopy(model.generation_config)\n",
    "\n",
    "    model_kwargs = generation_config.update(**inputs)\n",
    "    inputs_tensor, model_input_name, model_kwargs = model._prepare_model_inputs(\n",
    "        None, generation_config.bos_token_id, model_kwargs\n",
    "    )\n",
    "    batch_size = inputs_tensor.shape[0]\n",
    "    model_kwargs[\"output_attentions\"] = generation_config.output_attentions\n",
    "    model_kwargs[\"output_hidden_states\"] = generation_config.output_hidden_states\n",
    "    model_kwargs[\"use_cache\"] = generation_config.use_cache\n",
    "    model_kwargs[\"attention_mask\"] = model._prepare_attention_mask_for_generation(\n",
    "        inputs_tensor, generation_config.pad_token_id, generation_config.eos_token_id\n",
    "    )\n",
    "\n",
    "    #model_kwargs = model._prepare_encoder_decoder_kwargs_for_generation(\n",
    "    #                inputs_tensor, model_kwargs, model_input_name\n",
    "    #            )\n",
    "\n",
    "    input_ids = model._prepare_decoder_input_ids_for_generation(\n",
    "        batch_size,\n",
    "        decoder_start_token_id=generation_config.decoder_start_token_id,\n",
    "        bos_token_id=generation_config.bos_token_id,\n",
    "        model_kwargs=model_kwargs,\n",
    "        device=inputs_tensor.device,\n",
    "    )\n",
    "\n",
    "\n",
    "    #input_ids, model_kwargs = model._expand_inputs_for_generation(\n",
    "    #    input_ids=input_ids,\n",
    "    #    expand_size=generation_config.num_beams,\n",
    "    #    is_encoder_decoder=model.config.is_encoder_decoder,\n",
    "    #    **model_kwargs,\n",
    "    #)\n",
    "\n",
    "    model_inputs = model.prepare_inputs_for_generation(input_ids, **model_kwargs)\n",
    "    model_inputs['input_ids']=inputs['input_ids'] # NEW\n",
    "    \n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d63890bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Iterable, Union, Optional\n",
    "from truera.nlp.general.aiq.nlp_coloring import attributions_to_rgb\n",
    "from truera.nlp.general.aiq.nlp_coloring import generate_rgb_str\n",
    "from truera.nlp.general.aiq.nlp_coloring import MAX_INTENSITY\n",
    "from truera.nlp.general.aiq.nlp_coloring import rgb_str\n",
    "from IPython.display import HTML\n",
    "# copied from truera\n",
    "def _influence_examples(\n",
    "        tokens_list: Iterable[Iterable[str]],\n",
    "        attributions_list: Iterable[Iterable[float]],\n",
    "        *,\n",
    "        qoi_class: Union[int, str] = 0,\n",
    "        underline_list: Optional[Union[Iterable[Iterable[int]],\n",
    "                                       Iterable[int]]] = None,\n",
    "        prepends: Union[Iterable[str], str] = ''\n",
    "    ) -> HTML:\n",
    "        \"\"\"\n",
    "        plot the tokens & their attributions for list of token and attributions one by one\n",
    "        underline_list specify the token index to underline (used for plotting influence of a sentence\n",
    "        containing a specific token)\n",
    "        \"\"\"\n",
    "        if len(attributions_list) == 0:\n",
    "            return\n",
    "        norm_factor = np.max(\n",
    "            [\n",
    "                np.max(np.abs(attributions))\n",
    "                for attributions in attributions_list\n",
    "            ]\n",
    "        )\n",
    "        # Display legend\n",
    "        neg_infl_color = rgb_str(256, 256 - MAX_INTENSITY, 256 - MAX_INTENSITY)\n",
    "        neutral_color = rgb_str(256, 256, 256)\n",
    "        pos_infl_color = rgb_str(256 - MAX_INTENSITY, 256, 256 - MAX_INTENSITY)\n",
    "        if isinstance(qoi_class, int):\n",
    "            qoi_class = f\"Class: {qoi_class}\"\n",
    "        qoi_class = qoi_class.replace('_', ' ')\n",
    "        qoi_class = qoi_class.title()\n",
    "        html_str = [\n",
    "            f'''\n",
    "            <div style=\"margin:auto; width:50%; height:20px; display:flex; align-items:center;justify-content: space-between; background-image:linear-gradient(to right, {neg_infl_color}, {neutral_color}, {pos_infl_color});\">\n",
    "                <strong style=margin-left:4px>Negative Influence</strong>\n",
    "                <strong style=text-align:center>{qoi_class}</strong>\n",
    "                <strong style=margin-right:4px>Postive Influence</strong>\n",
    "            </div>\n",
    "            '''\n",
    "        ]\n",
    "\n",
    "        # Plot examples\n",
    "        if isinstance(prepends, str):\n",
    "            prepends = [prepends for _ in range(len(tokens_list))]\n",
    "        for si, (attributions, tokens, prepend) in enumerate(\n",
    "            zip(attributions_list, tokens_list, prepends)\n",
    "        ):\n",
    "            underline_idxs = [\n",
    "                underline_list[si]\n",
    "            ] if underline_list is not None else None\n",
    "            if isinstance(underline_idxs, int):\n",
    "                underline_idxs = [underline_idxs]\n",
    "            line_html = generate_rgb_str(\n",
    "                tokens,\n",
    "                attributions,\n",
    "                underline_idxs,\n",
    "                norm_factor=norm_factor,\n",
    "                max_intensity=MAX_INTENSITY\n",
    "            )\n",
    "            html = f'<p style=padding-bottom:2px><h4 style=margin:0;>{prepend}</h4> {line_html}'\n",
    "            html_str.append(html)\n",
    "\n",
    "        return HTML(\"\\n\".join(html_str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c048f6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from trulens.nn.models import get_model_wrapper\n",
    "from trulens.nn.distributions import PointDoi\n",
    "from trulens.nn.quantities import LambdaQoI\n",
    "from trulens.nn.attribution import IntegratedGradients, Saliency\n",
    "from trulens.nn.attribution import Cut, OutputCut,InputCut\n",
    "from trulens.utils.typing import ModelInputs\n",
    "import numpy as np\n",
    "def explain(inputs, model_inputs, chosen_token):\n",
    "    wrapper = get_model_wrapper(model)\n",
    "    qoi = LambdaQoI(lambda out: out[-1][-1][chosen_token])\n",
    "    infl = Saliency(\n",
    "            model=wrapper,\n",
    "            cuts=(Cut('layers_0_self_attn_q_proj', anchor='in'), OutputCut(accessor=lambda o: o['logits'])),\n",
    "            #qoi_cut=OutputCut(accessor=lambda o: o['logits']),\n",
    "            qoi=qoi)\n",
    "    infl = infl.attributions(**model_inputs)\n",
    "    influence_sums = np.sum(infl[0][0].cpu().detach().numpy()[0], axis = 1)\n",
    "    input_tokens = tokenizer.batch_decode(inputs['input_ids'][0])\n",
    "    display(_influence_examples([input_tokens], [influence_sums], prepends=f\"next token:{tokenizer.decode(chosen_token)}\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db4233b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "def explain_utterance(utterance, reply_ids):\n",
    "    \"\"\" ORIGINAL \n",
    "    inputs = tokenizer([utterance], return_tensors=\"pt\").to(device)\n",
    "    \n",
    "    model_inputs = prepare_inputs(inputs)\n",
    "    explain(inputs, model_inputs, reply_ids)\n",
    "    \"\"\"\n",
    "    \n",
    "    inputs = tokenizer([utterance], return_tensors=\"pt\").to(device)\n",
    "    model_inputs = prepare_inputs(inputs)\n",
    "    \n",
    "    for next_token_in_reply in range(1,len(reply_ids[0])):\n",
    "        chosen_token = int(reply_ids[0][next_token_in_reply])\n",
    "        model_inputs['decoder_input_ids']  = reply_ids[:,:chosen_token]\n",
    "        explain(inputs, model_inputs, chosen_token)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9ed62a9",
   "metadata": {},
   "source": [
    "# Model Context\n",
    "Blenderbot is a chat model that can handle multiple prompt and answer sessions with context conserved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "929ef7f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77e72a4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "UTTERANCE = \"<s> What Tools are used in baseball?. \"\n",
    "inputs = tokenizer([UTTERANCE], return_tensors=\"pt\").to(device)\n",
    "reply_ids = model.generate(**inputs)\n",
    "print(tokenizer.batch_decode(reply_ids))\n",
    "print(reply_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4c0dd4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "NEXT_UTTERANCE = (\n",
    "...     \"<s> What Tools are used in baseball?.</s> \"\n",
    "...     \"<s> A bat, a glove, and a batting glove. </s> \"\n",
    "...     \"<s> what about hockey?\"\n",
    "... )\n",
    "inputs = tokenizer([NEXT_UTTERANCE], return_tensors=\"pt\").to(device)\n",
    "reply_ids = model.generate(**inputs)\n",
    "print(tokenizer.batch_decode(reply_ids))\n",
    "print(reply_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c75c45a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "UTTERANCE = \"<s> What Tools are used in baseball?. </s> <s> A bat, a glove, and a \"\n",
    "inputs = tokenizer([UTTERANCE], return_tensors=\"pt\").to(device)\n",
    "reply_ids = model.generate(**inputs)\n",
    "print(tokenizer.batch_decode(reply_ids))\n",
    "print(reply_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ebaf0fc",
   "metadata": {},
   "source": [
    "# Explanations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16ff1559",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "UTTERANCE = \"<s> What Tools are used in baseball?. \"\n",
    "inputs = tokenizer([UTTERANCE], return_tensors=\"pt\").to(device)\n",
    "reply_ids = model.generate(**inputs)\n",
    "print([f\"{tok_id}: {tokenizer.decode(tok_id)}\" for tok_id in reply_ids[0]])\n",
    "print(tokenizer.batch_decode(reply_ids))\n",
    "#print(reply_ids)\n",
    "explain_utterance(UTTERANCE, reply_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f1c5ecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "UTTERANCE = (\n",
    "...     \"<s> What Tools are used in baseball?.</s> \"\n",
    "...     \"<s> A bat, a glove, a helmet, and a ball.</s> \"\n",
    "...     \"<s> what about hockey?\"\n",
    "... )\n",
    "inputs = tokenizer([UTTERANCE], return_tensors=\"pt\").to(device)\n",
    "reply_ids = model.generate(**inputs)\n",
    "print([f\"{tok_id}: {tokenizer.decode(tok_id)}\" for tok_id in reply_ids[0]])\n",
    "print(tokenizer.batch_decode(reply_ids))\n",
    "#print(reply_ids)\n",
    "explain_utterance(UTTERANCE, reply_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a0418e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "raise Exception(\"below is experimental beam explanations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9a65e4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "UTTERANCE = \"<s> What Tools are used in baseball?. \"\n",
    "inputs = tokenizer([UTTERANCE], return_tensors=\"pt\").to(device)\n",
    "test = prepare_inputs(inputs)\n",
    "print(test)\n",
    "model(**test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aad009e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# in generate and beam_score; if encoder_outputs is there, then input_ids construct the response\n",
    "# the hope; if we are not doing encoder_outputs, then it uses decoder_ids."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db86c759",
   "metadata": {},
   "outputs": [],
   "source": [
    "test['decoder_input_ids']  = torch.cat([test['decoder_input_ids'][0], torch.tensor([349]).to(device)]).unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6502887",
   "metadata": {},
   "outputs": [],
   "source": [
    "model(**test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ded3db1",
   "metadata": {},
   "outputs": [],
   "source": [
    "UTTERANCE = \"<s> What Tools are used in baseball?. </s> <s> A bat, a glove, and a \"\n",
    "inputs = tokenizer([UTTERANCE], return_tensors=\"pt\").to(device)\n",
    "reply_ids = model.generate(**inputs)\n",
    "print(tokenizer.batch_decode(reply_ids))\n",
    "print(reply_ids)\n",
    "explain_utterance(UTTERANCE, reply_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eddbdba",
   "metadata": {},
   "outputs": [],
   "source": [
    "NEXT_UTTERANCE = (\n",
    "...     \"<s> What Tools are used in baseball?.</s> \"\n",
    "...     \"<s> A bat, a glove, a helmet, and a ball.</s> \"\n",
    "...     \"<s> what about hockey? \"\n",
    "... )\n",
    "inputs = tokenizer([NEXT_UTTERANCE], return_tensors=\"pt\").to(device)\n",
    "reply_ids = model.generate(**inputs)\n",
    "print([f\"{tok_id}: {tokenizer.decode(tok_id)}\" for tok_id in reply_ids[0]])\n",
    "print(tokenizer.batch_decode(reply_ids))\n",
    "explain_utterance(NEXT_UTTERANCE, reply_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4d076ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "NEXT_UTTERANCE = (\n",
    "...     \"<s> What Tools are used in baseball?.</s> \"\n",
    "...     \"<s> A bat, a glove, a helmet, and a ball.</s> \"\n",
    "...     \"<s> what about hockey? <s> Hockey is a team sport played on ice, usually with a \"\n",
    "... )\n",
    "inputs = tokenizer([NEXT_UTTERANCE], return_tensors=\"pt\").to(device)\n",
    "reply_ids = model.generate(**inputs)\n",
    "print(tokenizer.batch_decode(reply_ids))\n",
    "print(reply_ids)\n",
    "explain_utterance(NEXT_UTTERANCE, reply_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa41a154",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1982f1f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_local_nov2022",
   "language": "python",
   "name": "nlp_local_nov2022"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
