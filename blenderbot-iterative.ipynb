{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ed021a16",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "#sys.path.insert(0,'/home/rick/transformers/src')\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "trulens_path = (Path(os.getcwd()).parent.parent / \"trulens\")\n",
    "\n",
    "sys.path.insert(0, str(trulens_path))\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4b6c4603",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "from transformers import BlenderbotForConditionalGeneration, BlenderbotTokenizer\n",
    "import torch\n",
    "\n",
    "\n",
    "mname=\"facebook/blenderbot-3B\"\n",
    "model = BlenderbotForConditionalGeneration.from_pretrained(mname)\n",
    "tokenizer = BlenderbotTokenizer.from_pretrained(mname)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a121650d",
   "metadata": {},
   "outputs": [],
   "source": [
    "device=\"cuda:0\"\n",
    "model.to(device)\n",
    "tok=tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0fa84db7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "# Taken as a subset of the methods that do model input prep from transformers.generation.utils\n",
    "def prepare_inputs(inputs):\n",
    "    generation_config = copy.deepcopy(model.generation_config)\n",
    "\n",
    "    model_kwargs = generation_config.update(**inputs)\n",
    "    inputs_tensor, model_input_name, model_kwargs = model._prepare_model_inputs(\n",
    "        None, generation_config.bos_token_id, model_kwargs\n",
    "    )\n",
    "    batch_size = inputs_tensor.shape[0]\n",
    "    model_kwargs[\"output_attentions\"] = generation_config.output_attentions\n",
    "    model_kwargs[\"output_hidden_states\"] = generation_config.output_hidden_states\n",
    "    model_kwargs[\"use_cache\"] = generation_config.use_cache\n",
    "    model_kwargs[\"attention_mask\"] = model._prepare_attention_mask_for_generation(\n",
    "        inputs_tensor, generation_config.pad_token_id, generation_config.eos_token_id\n",
    "    )\n",
    "\n",
    "    #model_kwargs = model._prepare_encoder_decoder_kwargs_for_generation(\n",
    "    #                inputs_tensor, model_kwargs, model_input_name\n",
    "    #            )\n",
    "\n",
    "    input_ids = model._prepare_decoder_input_ids_for_generation(\n",
    "        batch_size,\n",
    "        decoder_start_token_id=generation_config.decoder_start_token_id,\n",
    "        bos_token_id=generation_config.bos_token_id,\n",
    "        model_kwargs=model_kwargs,\n",
    "        device=inputs_tensor.device,\n",
    "    )\n",
    "\n",
    "\n",
    "    #input_ids, model_kwargs = model._expand_inputs_for_generation(\n",
    "    #    input_ids=input_ids,\n",
    "    #    expand_size=generation_config.num_beams,\n",
    "    #    is_encoder_decoder=model.config.is_encoder_decoder,\n",
    "    #    **model_kwargs,\n",
    "    #)\n",
    "\n",
    "    model_inputs = model.prepare_inputs_for_generation(input_ids, **model_kwargs)\n",
    "    model_inputs['input_ids']=inputs['input_ids'] # NEW\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d63890bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Iterable, Union, Optional\n",
    "from truera.nlp.general.aiq.nlp_coloring import attributions_to_rgb\n",
    "from truera.nlp.general.aiq.nlp_coloring import generate_rgb_str\n",
    "from truera.nlp.general.aiq.nlp_coloring import MAX_INTENSITY\n",
    "from truera.nlp.general.aiq.nlp_coloring import rgb_str\n",
    "from IPython.display import HTML\n",
    "# copied from truera\n",
    "def _influence_examples(\n",
    "        tokens_list: Iterable[Iterable[str]],\n",
    "        attributions_list: Iterable[Iterable[float]],\n",
    "        *,\n",
    "        qoi_class: Union[int, str] = 0,\n",
    "        underline_list: Optional[Union[Iterable[Iterable[int]],\n",
    "                                       Iterable[int]]] = None,\n",
    "        prepends: Union[Iterable[str], str] = ''\n",
    "    ) -> HTML:\n",
    "        \"\"\"\n",
    "        plot the tokens & their attributions for list of token and attributions one by one\n",
    "        underline_list specify the token index to underline (used for plotting influence of a sentence\n",
    "        containing a specific token)\n",
    "        \"\"\"\n",
    "        if len(attributions_list) == 0:\n",
    "            return\n",
    "        norm_factor = np.max(\n",
    "            [\n",
    "                np.max(np.abs(attributions))\n",
    "                for attributions in attributions_list\n",
    "            ]\n",
    "        )\n",
    "        # Display legend\n",
    "        neg_infl_color = rgb_str(256, 256 - MAX_INTENSITY, 256 - MAX_INTENSITY)\n",
    "        neutral_color = rgb_str(256, 256, 256)\n",
    "        pos_infl_color = rgb_str(256 - MAX_INTENSITY, 256, 256 - MAX_INTENSITY)\n",
    "        if isinstance(qoi_class, int):\n",
    "            qoi_class = f\"Class: {qoi_class}\"\n",
    "        qoi_class = qoi_class.replace('_', ' ')\n",
    "        qoi_class = qoi_class.title()\n",
    "        html_str = [\n",
    "            f'''\n",
    "            <div style=\"margin:auto; width:50%; height:20px; display:flex; align-items:center;justify-content: space-between; background-image:linear-gradient(to right, {neg_infl_color}, {neutral_color}, {pos_infl_color});\">\n",
    "                <strong style=margin-left:4px>Negative Influence</strong>\n",
    "                <strong style=text-align:center>{qoi_class}</strong>\n",
    "                <strong style=margin-right:4px>Postive Influence</strong>\n",
    "            </div>\n",
    "            '''\n",
    "        ]\n",
    "\n",
    "        # Plot examples\n",
    "        if isinstance(prepends, str):\n",
    "            prepends = [prepends for _ in range(len(tokens_list))]\n",
    "        for si, (attributions, tokens, prepend) in enumerate(\n",
    "            zip(attributions_list, tokens_list, prepends)\n",
    "        ):\n",
    "            underline_idxs = [\n",
    "                underline_list[si]\n",
    "            ] if underline_list is not None else None\n",
    "            if isinstance(underline_idxs, int):\n",
    "                underline_idxs = [underline_idxs]\n",
    "            line_html = generate_rgb_str(\n",
    "                tokens,\n",
    "                attributions,\n",
    "                underline_idxs,\n",
    "                norm_factor=norm_factor,\n",
    "                max_intensity=MAX_INTENSITY\n",
    "            )\n",
    "            html = f'<p style=padding-bottom:2px><h4 style=margin:0;>{prepend}</h4> {line_html}'\n",
    "            html_str.append(html)\n",
    "\n",
    "        return HTML(\"\\n\".join(html_str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7c048f6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from trulens.nn.models import get_model_wrapper\n",
    "from trulens.nn.distributions import PointDoi\n",
    "from trulens.nn.quantities import LambdaQoI\n",
    "from trulens.nn.attribution import IntegratedGradients, Saliency\n",
    "from trulens.nn.attribution import Cut, OutputCut,InputCut\n",
    "from trulens.utils.typing import ModelInputs\n",
    "import numpy as np\n",
    "def explain(inputs, model_inputs, reply_ids):\n",
    "    chosen_token = int(reply_ids[0][1])\n",
    "    #chosen_token = 298\n",
    "    wrapper = get_model_wrapper(model)\n",
    "    qoi = LambdaQoI(lambda out: out[-1][-1][chosen_token])\n",
    "    infl = Saliency(\n",
    "            model=wrapper,\n",
    "            cuts=(Cut('layers_0_self_attn_q_proj', anchor='in'), OutputCut(accessor=lambda o: o['logits'])),\n",
    "            #qoi_cut=OutputCut(accessor=lambda o: o['logits']),\n",
    "            qoi=qoi)\n",
    "    infl = infl.attributions(**model_inputs)\n",
    "    influence_sums = np.sum(infl[0][0].cpu().detach().numpy()[0], axis = 1)\n",
    "    input_tokens = tokenizer.batch_decode(inputs['input_ids'][0])\n",
    "    display(_influence_examples([input_tokens], [influence_sums], prepends=f\"next token:{tokenizer.decode(chosen_token)}\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "db4233b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "def explain_utterance(utterance, reply_ids):\n",
    "    #for i in range(len(reply_ids)-1):\n",
    "    #    next_token = i+1\n",
    "        \n",
    "    inputs = tokenizer([utterance], return_tensors=\"pt\").to(device)\n",
    "    \n",
    "    model_inputs = prepare_inputs(inputs)\n",
    "    explain(inputs, model_inputs, reply_ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "de94b2a6",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'GenerationConfig' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [12]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mGenerationConfig\u001b[49m\u001b[38;5;241m.\u001b[39mfrom_model_config(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'GenerationConfig' is not defined"
     ]
    }
   ],
   "source": [
    "from transformers..configuration_utils import GenerationConfig\n",
    "GenerationConfig.from_model_config(self.config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ca10e394",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1: <s>', '349:  A', '1703:  bat', '19: ,', '265:  a', '3561:  glo', '309: ve', '19: ,', '298:  and', '265:  a', '1703:  bat', '581: ting', '3561:  glo', '309: ve', '21: .', '1: <s>', '228:  ', '1: <s>', '1: <s>', '1: <s>', '2: </s>']\n",
      "['<s> A bat, a glove, and a batting glove.<s> <s><s><s></s>']\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'BlenderbotForConditionalGeneration' object has no attribute 'generation_config'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[0;32mIn [10]\u001b[0m, in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(tokenizer\u001b[38;5;241m.\u001b[39mbatch_decode(reply_ids))\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m#print(reply_ids)\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m \u001b[43mexplain_utterance\u001b[49m\u001b[43m(\u001b[49m\u001b[43mUTTERANCE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreply_ids\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [8]\u001b[0m, in \u001b[0;36mexplain_utterance\u001b[0;34m(utterance, reply_ids)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mexplain_utterance\u001b[39m(utterance, reply_ids):\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;66;03m#for i in range(len(reply_ids)-1):\u001b[39;00m\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;66;03m#    next_token = i+1\u001b[39;00m\n\u001b[1;32m      6\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m tokenizer([utterance], return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m----> 8\u001b[0m     model_inputs \u001b[38;5;241m=\u001b[39m \u001b[43mprepare_inputs\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m     explain(inputs, model_inputs, reply_ids)\n",
      "Input \u001b[0;32mIn [5]\u001b[0m, in \u001b[0;36mprepare_inputs\u001b[0;34m(inputs)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprepare_inputs\u001b[39m(inputs):\n\u001b[0;32m----> 3\u001b[0m     generation_config \u001b[38;5;241m=\u001b[39m copy\u001b[38;5;241m.\u001b[39mdeepcopy(\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgeneration_config\u001b[49m)\n\u001b[1;32m      5\u001b[0m     model_kwargs \u001b[38;5;241m=\u001b[39m generation_config\u001b[38;5;241m.\u001b[39mupdate(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minputs)\n\u001b[1;32m      6\u001b[0m     inputs_tensor, model_input_name, model_kwargs \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39m_prepare_model_inputs(\n\u001b[1;32m      7\u001b[0m         \u001b[38;5;28;01mNone\u001b[39;00m, generation_config\u001b[38;5;241m.\u001b[39mbos_token_id, model_kwargs\n\u001b[1;32m      8\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp_local_nov2022/lib/python3.9/site-packages/torch/nn/modules/module.py:1265\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1263\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[1;32m   1264\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[0;32m-> 1265\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   1266\u001b[0m     \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, name))\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'BlenderbotForConditionalGeneration' object has no attribute 'generation_config'"
     ]
    }
   ],
   "source": [
    "UTTERANCE = \"<s> What Tools are used in baseball?. \"\n",
    "inputs = tokenizer([UTTERANCE], return_tensors=\"pt\").to(device)\n",
    "reply_ids = model.generate(**inputs)\n",
    "print([f\"{tok_id}: {tokenizer.decode(tok_id)}\" for tok_id in reply_ids[0]])\n",
    "print(tokenizer.batch_decode(reply_ids))\n",
    "#print(reply_ids)\n",
    "explain_utterance(UTTERANCE, reply_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aff8d93",
   "metadata": {},
   "outputs": [],
   "source": [
    "raise Exception(\"below is experimental beam explanations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14015262",
   "metadata": {},
   "outputs": [],
   "source": [
    "UTTERANCE = \"<s> What Tools are used in baseball?. </s> <s> A bat, a glove, and a \"\n",
    "inputs = tokenizer([UTTERANCE], return_tensors=\"pt\").to(device)\n",
    "reply_ids = model.generate(**inputs)\n",
    "print(tokenizer.batch_decode(reply_ids))\n",
    "print(reply_ids)\n",
    "explain_utterance(UTTERANCE, reply_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "497aa874",
   "metadata": {},
   "outputs": [],
   "source": [
    "NEXT_UTTERANCE = (\n",
    "...     \"<s> What Tools are used in baseball?.</s> \"\n",
    "...     \"<s> A bat, a glove, a helmet, and a ball.</s> \"\n",
    "...     \"<s> what about hockey? \"\n",
    "... )\n",
    "inputs = tokenizer([NEXT_UTTERANCE], return_tensors=\"pt\").to(device)\n",
    "reply_ids = model.generate(**inputs)\n",
    "print([f\"{tok_id}: {tokenizer.decode(tok_id)}\" for tok_id in reply_ids[0]])\n",
    "print(tokenizer.batch_decode(reply_ids))\n",
    "explain_utterance(NEXT_UTTERANCE, reply_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4d076ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "NEXT_UTTERANCE = (\n",
    "...     \"<s> What Tools are used in baseball?.</s> \"\n",
    "...     \"<s> A bat, a glove, a helmet, and a ball.</s> \"\n",
    "...     \"<s> what about hockey? <s> Hockey is a team sport played on ice, usually with a \"\n",
    "... )\n",
    "inputs = tokenizer([NEXT_UTTERANCE], return_tensors=\"pt\").to(device)\n",
    "reply_ids = model.generate(**inputs)\n",
    "print(tokenizer.batch_decode(reply_ids))\n",
    "print(reply_ids)\n",
    "explain_utterance(NEXT_UTTERANCE, reply_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e652b90",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e20f5c85",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_local_nov2022",
   "language": "python",
   "name": "nlp_local_nov2022"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
